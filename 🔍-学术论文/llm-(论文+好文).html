<!doctype html><html><head><title>LLM (论文+好文)</title><base href="../"><meta id="root-path" root-path="../"><link rel="icon" sizes="96x96" href="https://publish-01.obsidian.md/access/f786db9fac45774fa4f0d8112e232d67/favicon-96x96.png"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=yes,minimum-scale=1,maximum-scale=5"><meta charset="UTF-8"><link rel="stylesheet" href="lib/styles/obsidian-styles.css"><link rel="stylesheet" href="lib/styles/theme.css"><link rel="stylesheet" href="lib/styles/plugin-styles.css"><link rel="stylesheet" href="lib/styles/snippets.css"><link rel="stylesheet" href="lib/styles/generated-styles.css"><style></style><script type="module" src="lib/scripts/graph_view.js"></script><script src="lib/scripts/graph_wasm.js"></script><script src="lib/scripts/tinycolor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.2.4/pixi.min.js" integrity="sha512-Ch/O6kL8BqUwAfCF7Ie5SX1Hin+BJgYH4pNjRqXdTEqMsis1TUYg+j6nnI9uduPjGaj7DN4UKCZgpvoExt6dkw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="lib/scripts/webpage.js"></script><script src="lib/scripts/generated.js"></script></head><body class="mod-macos native-scrollbars show-inline-title anp-td-none theme-light ctp-rosepine-light ctp-mocha ctp-accent-rosewater anuppuccin-accent-toggle anp-current-line anp-h1-red anp-h2-peach anp-h3-green anp-h4-teal anp-h5-lavender anp-h6-mauve anp-bold-red anp-italic-green anp-highlight-yellow anp-full-rainbow-color-toggle anp-default-tab loading"><div class="webpage-container"><div class="sidebar-left sidebar"><div class="sidebar-container"><div class="sidebar-sizer"><div class="sidebar-content-positioner"><div class="sidebar-content"><div><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="tree-container file-tree mod-nav-indicator" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">🌱 Digital-Garden</span><button class="clickable-icon collapse-tree-button is-collapsed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area"><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">🎨 文学艺术</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🎨-文学艺术/世界现代设计通识课-王受之.html"><span class="tree-item-title">世界现代设计通识课 - 王受之</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🎨-文学艺术/喜欢的音乐风格.html"><span class="tree-item-title">喜欢的音乐风格</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🎨-文学艺术/莎士比亚快速了解.html"><span class="tree-item-title">莎士比亚快速了解</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">🏛️ 哲学文化</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/人生哲学清单.html"><span class="tree-item-title">人生哲学清单</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/反击虚无主义的方法.html"><span class="tree-item-title">反击虚无主义的方法</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/哲学大问题.html"><span class="tree-item-title">哲学大问题</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/牛津通识读本-佛学概论.html"><span class="tree-item-title">牛津通识读本 - 佛学概论</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/白话黑格尔-总结.html"><span class="tree-item-title">白话黑格尔- 总结</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/般若波罗蜜多心经.html"><span class="tree-item-title">般若波罗蜜多心经</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/虚无主义是通往自由的必经之路.html"><span class="tree-item-title">虚无主义是通往自由的必经之路</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/马克思的商品价值和商品.html"><span class="tree-item-title">马克思的商品价值和商品</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">🏠 政治社会</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/&quot;女性主义&quot;-论述总章.html"><span class="tree-item-title">"女性主义" 论述总章</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/《论犯罪与刑罚》笔记.html"><span class="tree-item-title">《论犯罪与刑罚》笔记</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/中国主要城市机会.html"><span class="tree-item-title">中国主要城市机会</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/中国人口追踪.html"><span class="tree-item-title">中国人口追踪</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/优绩主义罪与罚.html"><span class="tree-item-title">优绩主义罪与罚</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/可能性的艺术：比较政治学30讲-刘瑜.html"><span class="tree-item-title">可能性的艺术：比较政治学30讲 - 刘瑜</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/增殖崇拜：一神教传统与消费主义.html"><span class="tree-item-title">增殖崇拜：一神教传统与消费主义</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/有关印度的疑问.html"><span class="tree-item-title">有关印度的疑问</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">💪 自我成长</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💪-自我成长/《人类决策中的陷阱》读书笔记.html"><span class="tree-item-title">《人类决策中的陷阱》读书笔记</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💪-自我成长/人际交流优化流程.html"><span class="tree-item-title">人际交流优化流程</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💪-自我成长/健身计划.html"><span class="tree-item-title">健身计划</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💪-自我成长/提供沟通能力的方法.html"><span class="tree-item-title">提供沟通能力的方法</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💪-自我成长/斯坦福大学人生规划课-stanford-open-office-hours_-dave-evans-and-bill-burnett.html"><span class="tree-item-title">斯坦福大学人生规划课 -Stanford Open Office Hours_ Dave Evans and Bill Burnett</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💪-自我成长/查理芒格(25条人类误判心理学-+-穷查理宝典).html"><span class="tree-item-title">查理芒格(25条人类误判心理学 + 穷查理宝典)</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💪-自我成长/爱欲分析.html"><span class="tree-item-title">爱欲分析</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">💼 经济商业</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💼-经济商业/产业分析.html"><span class="tree-item-title">产业分析</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💼-经济商业/和chatgpt聊经济.html"><span class="tree-item-title">和ChatGPT聊经济</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💼-经济商业/商业点子王.html"><span class="tree-item-title">商业点子王</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💼-经济商业/商业计划书(bp)写作指南-李自然说.html"><span class="tree-item-title">商业计划书(BP)写作指南 - 李自然说</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💼-经济商业/快速调研方法论.html"><span class="tree-item-title">快速调研方法论</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💼-经济商业/股市商战黑化.html"><span class="tree-item-title">股市商战黑化</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">💾 科技工程</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/ai初级工程师面试.html"><span class="tree-item-title">AI初级工程师面试</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/cs224n-nlp-with-deep-learning(2021冬).html"><span class="tree-item-title">CS224N NLP with Deep Learning(2021冬)</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/leetcode-热题100.html"><span class="tree-item-title">Leetcode 热题100</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/llm-全栈工程师(知乎).html"><span class="tree-item-title">LLM 全栈工程师(知乎)</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/智能风控平台(架构、设计与实现).html"><span class="tree-item-title">智能风控平台(架构、设计与实现)</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/机器学习常用算法.html"><span class="tree-item-title">机器学习常用算法</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/计算机组成原理.html"><span class="tree-item-title">计算机组成原理</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/这就是chatgpt.html"><span class="tree-item-title">这就是ChatGPT</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">🔍 学术论文</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🔍-学术论文/llm-(论文+好文).html"><span class="tree-item-title">LLM (论文+好文)</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🔍-学术论文/transformers-(论文+好文).html"><span class="tree-item-title">Transformers (论文+好文)</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🔍-学术论文/阅读论文的方法论.html"><span class="tree-item-title">阅读论文的方法论</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">🔢 数学物理</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🔢-数学物理/和chatgpt聊数学.html"><span class="tree-item-title">和ChatGPT聊数学</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🔢-数学物理/因果统计推理.html"><span class="tree-item-title">因果统计推理</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🔢-数学物理/如何使用信息论逼近wordle求解成绩的理论极限？.html"><span class="tree-item-title">如何使用信息论逼近Wordle求解成绩的理论极限？</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">🧑🏻‍💻 职业生涯</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧑🏻‍💻-职业生涯/pingpong-乒乓跨境支付服务.html"><span class="tree-item-title">PingPong 乒乓跨境支付服务</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧑🏻‍💻-职业生涯/职业规划.html"><span class="tree-item-title">职业规划</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧑🏻‍💻-职业生涯/职场小技巧.html"><span class="tree-item-title">职场小技巧</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">🧬 生物医疗</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/人格心理学大纲(首都师范大学).html"><span class="tree-item-title">人格心理学大纲(首都师范大学)</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/成瘾性药物总览.html"><span class="tree-item-title">成瘾性药物总览</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/改变心理学的40项研究.html"><span class="tree-item-title">改变心理学的40项研究</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/植物分类与系统发育.html"><span class="tree-item-title">植物分类与系统发育</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/牛津大学生物学动画.html"><span class="tree-item-title">牛津大学生物学动画</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/生物信号分析处理.html"><span class="tree-item-title">生物信号分析处理</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/脑科学总览.html"><span class="tree-item-title">脑科学总览</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/鬼谷的生物演化课.html"><span class="tree-item-title">鬼谷的生物演化课</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-file" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🌱-to-do-list.html"><span class="tree-item-title">🌱 To-do List</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💡-brain's-buzz.html"><span class="tree-item-title">💡 Brain's buzz</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="index.html"><span class="tree-item-title">index</span></a></div><div class="tree-item-children"></div></div></div></div></div></div></div></div><div class="sidebar-gutter"><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div></div><div class="document-container show"><div class="markdown-preview-view markdown-rendered node-insert-event is-readable-line-width allow-fold-headings show-indentation-guide allow-fold-lists show-properties" style="tab-size:4"><style id="MJX-CHTML-styles"></style><div class="markdown-preview-sizer markdown-preview-section" style="min-height:245px"><div class="markdown-preview-pusher" style="width:1px;height:.1px;margin-bottom:0"></div><div class="mod-header"><div class="inline-title" data-heading="LLM (论文+好文)" id="LLM_(论文+好文)" style="display:block">LLM (论文+好文)</div></div><div><p><a href="#人工智能" class="tag" target="_blank" rel="noopener">#人工智能</a> <a href="#chatgpt" class="tag" target="_blank" rel="noopener">#chatgpt</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/651023365" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/651023365" target="_blank">【LLM】大模型面试准备-1（题库整理篇） - 知乎</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/608820310?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1724538694068011010&amp;utm_source=io.raindrop.raindropio" rel="noopener" class="external-link" href="https://www.zhihu.com/question/608820310?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1724538694068011010&amp;utm_source=io.raindrop.raindropio" target="_blank">想学习大语言模型(LLM)，应该从哪个开源模型开始？ - 知乎</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/648127076?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1716622508470669312&amp;utm_source=io.raindrop.raindropio" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/648127076?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1716622508470669312&amp;utm_source=io.raindrop.raindropio" target="_blank">三万字最全解析！从零实现Transformer（小白必会版😃） - 知乎</a></p></div><div class="heading-wrapper"><h2 data-heading="LLM 做了什么? 能做什么?" class="heading" id="LLM_做了什么?_能做什么?"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>LLM 做了什么? 能做什么?<div class="heading-after">...</div></h2><div class="heading-children"><div><p><a data-href="这就是ChatGPT" href="💾-科技工程/这就是chatgpt.html" class="internal-link" target="_self" rel="noopener">这就是ChatGPT</a><br><a data-href="Transformers (论文+好文)" href="🔍-学术论文/transformers-(论文+好文).html" class="internal-link" target="_self" rel="noopener">Transformers (论文+好文)</a></p></div></div></div><div class="heading-wrapper"><h2 data-heading="大模型 局限&amp;机会" class="heading" id="大模型_局限&amp;机会"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>大模型 局限&amp;机会<div class="heading-after">...</div></h2><div class="heading-children"><div><ul><li data-line="0" class="lc-list-callout" style="--lc-callout-color:0,184,212"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 泛化能力缺陷(无法对训练数据进行全面有效的泛化)</span><ul><li data-line="1">反向推理差<ul><li data-line="2">虽然大模型在大量文本数据的训练上展现了惊人的泛化能力. 但在泛化抽象逻辑关系和推理新因果关系依然有局限, 因为训练数据不包含精准的逻辑和因果框架</li><li data-line="3">反向推理, “所有猫是动物”（A是B）是正确的，但是“所有动物是猫”（B是A）”是错误的. LLM在理解这样拥有非对称逻辑(简单颠倒因果命题不为真)关系时可能有很大的局限. 因为理解因果关系比单纯的关联关系要复杂得多.</li></ul></li><li data-line="4">负面知识处理有问题</li></ul></li><li data-line="5" class="lc-list-callout" style="--lc-callout-color:0,184,212"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 事实性缺陷(模型缺乏相对应的数据)</span><ul><li data-line="6">实时性知识差</li><li data-line="7">缺乏领域知识</li></ul></li><li data-line="8" class="lc-list-callout" style="--lc-callout-color:0,184,212"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 统计模型自身(由模型原理自身带来的缺陷)</span><ul><li data-line="9">“幻觉”(Hallucination, the generated content that is nonsensical or unfaithful to the provided source content)<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/635776684/answer/3336439291" rel="noopener" class="external-link" href="https://www.zhihu.com/question/635776684/answer/3336439291" target="_blank">为什么大模型会「说胡话」？如何解决大模型的「幻觉」问题？ - 知乎</a></li><li data-line="10">LLM缺乏可解释性, 无法预测它的输出(不能保证LLM不会产生偏见、误导、攻击、伤害用户的表述)</li></ul></li><li data-line="11" class="lc-list-callout" style="--lc-callout-color:158,158,158"><span class="lc-li-wrapper"><span class="lc-list-marker">%</span> 大模型训练难</span></li><li data-line="12" class="lc-list-callout" style="--lc-callout-color:255,23,68"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 大模型优化方向</span><ul><li data-line="13">减少训练的开销</li><li data-line="14">减少推理的开销<ul><li data-line="15">降低量化精度(FP32-&gt;INT8); 稀疏化; 剪枝法</li></ul></li></ul></li><li data-line="16" class="lc-list-callout" style="--lc-callout-color:255,23,68"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 大模型的机会</span><ul><li data-line="17">应用面: LLM + Scenario + Action</li><li data-line="18"></li></ul></li></ul></div></div></div><div class="heading-wrapper"><h2 data-heading="论文" class="heading" id="论文"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>论文<div class="heading-after">...</div></h2><div class="heading-children"><div><p>从GPT延展开来</p></div><div><p><strong>因果推理大类</strong><br><a data-tooltip-position="top" aria-label="https://www.zhihu.com/column/c_1561725946931122176" rel="noopener" class="external-link" href="https://www.zhihu.com/column/c_1561725946931122176" target="_blank">统计因果推理入门 - 知乎</a><br><a data-tooltip-position="top" aria-label="https://blog.csdn.net/VucNdnrzk8iwX/article/details/80479810" rel="noopener" class="external-link" href="https://blog.csdn.net/VucNdnrzk8iwX/article/details/80479810" target="_blank">什么是反事实推理？-CSDN博客</a><br><a data-tooltip-position="top" aria-label="https://blog.csdn.net/RM_Jin/article/details/134086497" rel="noopener" class="external-link" href="https://blog.csdn.net/RM_Jin/article/details/134086497" target="_blank">推理还是背诵？通过反事实任务探索语言模型的能力和局限性-CSDN博客</a></p></div><div><ul><li data-line="0">反事实推理(Counterfactual Reasoning) 是一种先否定现实后重新表达建立另一种可能性(“如果....(虚假前提)，那么.....(虚假结论)？), e.g."如果我再努力一点(否定现实)，这次考试就能及格了(新可能性)"是因果推理的重要工具</li><li data-line="1">反事实推理助于揭示大模型在处理复杂、非直观的因果关系时的局限性和潜力. 并评估模型的泛化能力(评估推理能力而不是背诵). <strong>即如果模型获得一种通用可转移的理解问题的能力那么模型在默认任务和反事实任务中应该有相同的预期, 模型在这个能力下应该获得了非语言结构的世界模型.</strong></li><li data-line="2">反事实的推理基于前提可能不完全, 并且得处的结论也可能在经验中不存在</li><li data-line="3">分类:<ul><li data-line="4">对事实添加内容(加法式), “如果我请了辅导老师(添加事实), 我就不会不及格了(事实否定)”</li><li data-line="5">对事实减法(减法式), “如果我在考试前不通宵玩游戏(减少事实), 我就不会不及格了(事实否定)”</li><li data-line="6">对事实否定, 修改条件以获得更好的结果(上行反事实推理), “如果我在考试前再努力一点(修改条件), 就可以考100分了(获得更好的结果)”</li><li data-line="7">对事实否定, 修改条件以获得更差的结果(下行反事实推理), “幸好我考试前看了下书(虚假前提), 要不我就考不到满分了(更差的结果)”</li></ul></li></ul></div><div><p><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1St4y1Z7Fg/?buvid=XYC937EC95A0F7A8F952377A4963129240071&amp;from_spmid=tm.recommend.0.0&amp;is_story_h5=false&amp;mid=bv6h%2BZaOBoH9Icq59%2BzpmQ%3D%3D&amp;p=1&amp;plat_id=114&amp;share_from=ugc&amp;share_medium=android&amp;share_plat=android&amp;share_session_id=55664b3c-41b8-4ce2-a5ac-52a39c6319bb&amp;share_source=GENERIC&amp;share_tag=s_i&amp;spmid=united.player-video-detail.0.0&amp;timestamp=1705168454&amp;unique_k=uYgElpF&amp;up_id=507524288&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1St4y1Z7Fg/?buvid=XYC937EC95A0F7A8F952377A4963129240071&amp;from_spmid=tm.recommend.0.0&amp;is_story_h5=false&amp;mid=bv6h%2BZaOBoH9Icq59%2BzpmQ%3D%3D&amp;p=1&amp;plat_id=114&amp;share_from=ugc&amp;share_medium=android&amp;share_plat=android&amp;share_session_id=55664b3c-41b8-4ce2-a5ac-52a39c6319bb&amp;share_source=GENERIC&amp;share_tag=s_i&amp;spmid=united.player-video-detail.0.0&amp;timestamp=1705168454&amp;unique_k=uYgElpF&amp;up_id=507524288&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">大语言模型的知识编辑：问题，方法与挑战_哔哩哔哩_bilibili</a><br><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2305.13172" rel="noopener" class="external-link" href="https://arxiv.org/abs/2305.13172" target="_blank">[2305.13172] Editing Large Language Models: Problems, Methods, and Opportunities</a></p></div><div><ul><li data-line="0">介绍了不同于Fine-tuning, RA 之外, 建立的的基准数据集, 评估了几种LLM编辑方法(特别是generalization和efficiency)并对他们未来的研究潜力进行了展望</li><li data-line="1">LLM Editing<ul><li data-line="2">定义: 使用一定方法对一定示例范围内的模型行为进行修改, 而不改变范围外模型的性能. 修改后的模型需要满足 Reliability, Generality, Locality 三个特性. 以满足LLM的准确性和及时性, 并对有偏回答进行修正.</li><li data-line="3">修改路径1: 冻结原模型层同时添加额外参数(Memory-based)<ul><li data-line="4">SERAC Mitchell 等人(2022b) 外挂一个范围分类器来评估存储编辑示例范围内的新输入的可能性, 如果输入与缓存中的任何编辑对齐，则获取具有最高概率的编辑，并根据输入和检索到的编辑返回反事实模型的预测</li><li data-line="5">T-Patcher Huang 等人(2023)和 CaliNET Dong 等人(2022)将可训练参数引入 PLM 的前馈模块, 冻结原模型后在修改后数据上的训练新引入的参数</li></ul></li><li data-line="6">修改路径2 : 修改模型的内部参数<ul><li data-line="7">Locate-then-edit: &nbsp;The Knowledge Neuron (KN) method&nbsp;Dai et&nbsp;al. (<a data-tooltip-position="top" aria-label="https://ar5iv.labs.arxiv.org/html/2305.13172?_immersive_translate_auto_translate=1#bib.bib8" rel="noopener" class="external-link" href="https://ar5iv.labs.arxiv.org/html/2305.13172?_immersive_translate_auto_translate=1#bib.bib8" target="_blank">2022a</a>)介绍了一种知识归属技术，以精确定位“知识神经元”(前馈网络(FFN)矩阵中的一个关键值对) ，这个关键值对体现了知识，然后继续更新这些神经元.ROME Meng 等(2022)应用因果中介分析来定位编辑区域。ROME 不修改 FFN 中的知识神经元，而是改变整个矩阵。但是，KN 和 ROME 一次只能编辑一个事实关联。，MEMIT Meng 等人(2023)被提出可以修改一系列层次，并促进数以千计的改变被同时执行。请注意，这些方法是基于事实知识的局部性假设，尚未得到<strong>广泛的验证</strong>，某些参数的变化可能会影响不相关的知识，并<strong>产生不可预见的结果</strong>。</li><li data-line="8">Meta-learning: 元学习方法使用一个超网络来学习编辑语言模型(LM)所必需的参数. 知识编辑(KE) De Chao 等人(2021)利用超网络(具体而言，是一个双向的 LSTM)来预测每个数据点的权重更新，从而使编辑目标知识的约束优化不会干扰其他人。但是，在编辑LLMs时，此方法不足。为了克服这个限制，使用梯度分解(MEND) Mitchell 等人(2022a)的模型编辑器网络能够使用单个输入输出对对语言模型进行有效的本地编辑。具体来说，MEND 学习通过使用梯度的低秩分解来转换经过微调的语言模型的梯度，这种方法可以以最小的资源开销应用于LLMs。与先定位后编辑的方法相比，元学习方法需要额外的训练阶段，这可能导致时间和记忆成本的增加。</li></ul></li></ul></li><li data-line="9">Evaluating Methods:<ul><li data-line="10">Dataset: ZsRE(QA数据集) 和 CounterFact</li><li data-line="11">Evaluated model: &nbsp;T5-XL (3B) (encode-decode) 和 GPT-J (6B) (decode-only)</li><li data-line="12">评价维度<ul><li data-line="13">Portability, 衡量知识转移到相关内容的有效性. （1）Subject replace，（2）Inverse relation（3）One-hop Reason</li><li data-line="14">Generality(success rate within scope)</li><li data-line="15">Locality(control editing within scope)</li><li data-line="16">Efficiency(time, gpu, memory)</li></ul></li></ul></li><li data-line="17">发现<ul><li data-line="18"></li></ul></li></ul></div><div><p><strong>Semantic Kernel</strong>：在机器学习中，核方法是一类算法，它们通过将数据映射到高维空间来寻找数据点间的关系。语义核（Semantic Kernel）是核方法的一种，特别是用于处理文本数据，它能够捕捉数据点（如文档、句子或单词）间的语义相似性。</p></div><div class="admonition-parent admonition-hint-parent"><div class="callout admonition admonition-hint admonition-plugin" style="--callout-color:0,191,165" data-callout="hint" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title"><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="fire" class="svg-inline--fa fa-fire fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M216 23.86c0-23.8-30.65-32.77-44.15-13.04C48 191.85 224 200 224 288c0 35.63-29.11 64.46-64.85 63.99-35.17-.45-63.15-29.77-63.15-64.94v-85.51c0-21.7-26.47-32.23-41.43-16.5C27.8 213.16 0 261.33 0 320c0 105.87 86.13 192 192 192s192-86.13 192-192c0-170.29-168-193-168-296.14z"></path></svg></div><div class="callout-title-inner admonition-title-content">Hint</div></div><div class="callout-content admonition-content"><p>Transformers 通过通过自注意力机制并行处理序列数据，捕捉元素间的全局依赖关系，以支持高效的序列到序列学习和生成，它们在自然语言处理（NLP）领域特别流行和有效。这种模型最初由 Vaswani 等人在 2017 年的论文《Attention Is All You Need》中介绍。📘</p><ol><li><p><strong>核心概念：</strong> Transformers 的核心是“自注意力”（self-attention）机制。这种机制能够让模型在处理一个序列（如一句话）时，同时关注序列中的每一个元素（如每个词）。🔍</p></li><li><p><strong>优势：</strong></p><ul><li><strong>并行处理：</strong> 与传统的循环神经网络（RNN）和长短期记忆网络（LSTM）不同，Transformers 可以并行处理整个文本序列，从而大大提高了训练效率。⚡️</li><li><strong>更好的上下文理解：</strong> 自注意力机制使得模型能够更有效地捕捉语句中长距离的依赖关系，即模型可以更好地理解文本的上下文。🌐</li></ul></li><li><p><strong>结构：</strong></p><ul><li><strong>编码器-解码器架构：</strong> 原始的 Transformer 包括编码器（encoder）和解码器（decoder）两部分。编码器用于处理输入数据，解码器用于生成输出数据。🔄</li><li><strong>多头注意力（Multi-Head Attention）：</strong> 这是一个提升自注意力效果的技术，它将注意力分成多个“头”，让模型能够同时从不同的角度理解数据。👥</li></ul></li><li><p><strong>应用：</strong></p><ul><li><strong>语言模型：</strong> 如 GPT（Generative Pre-trained Transformer）系列，用于文本生成、问答等。</li><li><strong>机器翻译：</strong> 如谷歌的神经机器翻译系统。</li><li><strong>文本分类、情感分析等多种 NLP 任务。</strong></li></ul></li><li><p><strong>影响：</strong> 自从 Transformer 的提出以来，它已经成为了许多最先进 NLP 模型的基础，极大地推动了自然语言处理技术的发展。🚀</p></li></ol><p>举个例子，就像你现在和我交谈，我作为一个基于 Transformer 的模型（GPT-4），能够理解和生成自然语言，这在很大程度上归功于 Transformer 架构的强大和灵活性。🤖💬</p></div></div></div><div class="admonition-parent admonition-hint-parent"><div class="callout admonition admonition-hint admonition-plugin" style="--callout-color:0,191,165" data-callout="hint" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title"><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="fire" class="svg-inline--fa fa-fire fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M216 23.86c0-23.8-30.65-32.77-44.15-13.04C48 191.85 224 200 224 288c0 35.63-29.11 64.46-64.85 63.99-35.17-.45-63.15-29.77-63.15-64.94v-85.51c0-21.7-26.47-32.23-41.43-16.5C27.8 213.16 0 261.33 0 320c0 105.87 86.13 192 192 192s192-86.13 192-192c0-170.29-168-193-168-296.14z"></path></svg></div><div class="callout-title-inner admonition-title-content">Hint</div></div><div class="callout-content admonition-content"><p><img alt="image.png" src="https://cdn.sa.net/2023/12/18/LxYgsicGaN9R5Cd.png" referrerpolicy="no-referrer"><br>左边Encoder, 右边Decoder</p><p><strong>Encoder</strong>:</p><ul><li><strong>输入（Inputs）：</strong> 这是起点。对于文本来说，单词通过词嵌入（Embedding）技术转换成向量。</li><li><strong>位置编码（Positional Encoding）：</strong> 由于模型本身不理解序列顺序，所以需要加入位置编码来给模型提供单词在句子中的位置信息。</li><li><strong>多头注意力（Multi-Head Attention）：</strong> 这部分使模型在预测每个单词时能够关注输入序列的不同部分。就像阅读句子时，思考一个词的同时，也考虑其他词的重要性。</li><li><strong>加法 &amp; 归一化（Add &amp; Norm）：</strong> 在注意力机制之后，向量与原始输入结合（残差连接），然后进行归一化。这有助于稳定学习过程。</li><li><strong>前馈网络（Feed Forward）：</strong> 一个神经网络分别且相同地处理每个位置。这一步提高了来自注意力机制的转换效果。</li><li><strong>Nx：</strong> 这个标记表示这些块（多头注意力，加法 &amp; 归一化，前馈网络）被堆叠了N次。这就像重复这个过程以提炼对文本的理解。</li></ul><p><strong>Decoder:</strong></p><ul><li><strong>屏蔽多头注意力（Masked Multi-Head Attention）：</strong> 这与左侧的注意力相似，但它被屏蔽，以防止位置关注到后续位置。这在训练期间是必要的，以保持自回归属性，即一个位置上的单词预测只依赖于之前位置的已知输出。</li><li><strong>加法 &amp; 归一化（Add &amp; Norm）：</strong> 就像编码器一样，它进行归一化并添加残差。</li><li><strong>前馈网络（Feed Forward）：</strong> 同编码器一样，用神经网络处理每个位置。</li><li><strong>线性变换 &amp; Softmax：</strong> 最后，解码器的输出被转换为输出序列中每个单词的预测概率。</li></ul><p>这个Transformer架构允许并行处理，并捕捉文本中的复杂依赖关系。它是如GPT（像我这样的模型）和BERT等模型的基础，彻底改变了机器理解和生成人类语言的方式。</p><ul><li><strong>并行化：</strong> 与以前的序列到序列模型不同，Transformer允许更多的并行化，意味着可以更快、更高效地进行训练。</li><li><strong>注意力机制：</strong> 它在处理文本的长距离依赖性方面更有效。</li><li><strong>灵活性：</strong> 这种架构可以适应多种NLP任务，如翻译、摘要和问答</li></ul></div></div></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin" style="--callout-color:0,200,83" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title"><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p><strong>多头自注意力机制（Multi-Head Self-Attention）</strong>是Transformer模型中的一个核心组成部分。它让模型在处理一个序列时能够同时关注序列中的多个位置，从而捕捉不同子空间中的信息。我们来逐步探讨它的工作原理：</p><ol><li><strong>自注意力（Self-Attention）</strong>：自注意力允许输入序列的每个元素查看序列中的所有其他元素，并计算它们之间的关系。具体来说，对于每个元素，它会计算一个注意力分数，表明其他元素对当前元素的重要性。</li><li><strong>为何需要“多头”</strong>：在自注意力中，如果我们只有一个“注意力头”，我们只能抓取序列中一种类型的关系。通过多头，我们可以让模型在不同的表示子空间中并行地学习到序列中不同的特征。例如，一个头可能会关注句子的语法结构，而另一个头可能会关注某些词的共现信息。</li><li><strong>头（Head）的工作方式</strong>：在多头注意力中，每个头会有不同的权重矩阵（通常是查询（Q）、键（K）和值（V）矩阵）。输入序列会与这些矩阵相乘，产生不同的Q、K、V表示。这些表示会用来计算注意力分数，并通过一个缩放点积注意力函数得到权重。</li><li><strong>缩放点积注意力</strong>：具体来说，注意力分数是通过取每个查询与所有键的点积，并缩放后通过Softmax函数得到的。Softmax确保所有的注意力分数加起来为1，这样可以当作概率来理解。</li><li><strong>输出和组合</strong>：每个头的输出是通过将注意力分数与值（V）相乘得到的。这确保了我们关注更重要的元素。最后，所有头的输出被拼接起来，并通过另一个线性层，以确保不同头的信息被有效地结合。</li><li><strong>增强模型能力</strong>：通过多头，模型可以在同一时间处理不同类型的信息，这样就可以在处理复杂任务时提高灵活性和效率。</li></ol><p>举个例子来说，当你读一个故事时，你可能会同时注意到不同的元素，如情感色彩、人物关系或者时间线索。多头自注意力就是在模拟这种能同时关注故事多个方面的能力。</p><p>综上所述，多头自注意力机制通过并行地关注输入序列的不同部分，使得Transformer模型能够更加全面和深入地理解文本数据。这种机制是现代NLP模型取得成功的关键因素之一。🔍🧠✨</p></div></div></div><div class="mod-footer"></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-gutter"><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-container"><div class="sidebar-sizer"><div class="sidebar-content-positioner"><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder"><div class="graph-view-container"><div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div><canvas id="graph-canvas" width="512px" height="512px"></canvas></div></div></div><div class="tree-container outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area"><div class="tree-item" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#LLM_(论文+好文)"><span class="tree-item-title"><p>LLM (论文+好文)</p></span></a></div><div class="tree-item-children"><div class="tree-item" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#LLM_做了什么?_能做什么?"><span class="tree-item-title"><p>LLM 做了什么? 能做什么?</p></span></a></div><div class="tree-item-children"></div></div><div class="tree-item" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#大模型_局限&amp;机会"><span class="tree-item-title"><p>大模型 局限&amp;机会</p></span></a></div><div class="tree-item-children"></div></div><div class="tree-item" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#论文"><span class="tree-item-title"><p>论文</p></span></a></div><div class="tree-item-children"></div></div></div></div></div></div></div></div></div></div></div></div></body></html>