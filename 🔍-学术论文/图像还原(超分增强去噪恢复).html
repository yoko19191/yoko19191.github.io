<!DOCTYPE html> <html><head>
		<title>图像还原(超分增强去噪恢复)</title>
		<base href="../">
		<meta id="root-path" root-path="../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="🌱 Digital-Garden - 图像还原(超分增强去噪恢复)">
		<meta property="og:title" content="图像还原(超分增强去噪恢复)">
		<meta property="og:description" content="🌱 Digital-Garden - 图像还原(超分增强去噪恢复)">
		<meta property="og:type" content="website">
		<meta property="og:url" content="🔍-学术论文/图像还原(超分增强去噪恢复).html">
		<meta property="og:image" content="https://cdn.sa.net/2024/04/14/AD1MrqvdYNwuTEX.png">
		<meta property="og:site_name" content="🌱 Digital-Garden">
		<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.4.0/pixi.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="https://cdn.jsdelivr.net/npm/minisearch@6.3.0/dist/umd/index.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="preload" href="lib/styles/other-plugins.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/other-plugins.css"></noscript><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript></head><body class="publish css-settings-manager native-scrollbars theme-light show-inline-title show-ribbon"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles">mjx-c.mjx-c2190::before{padding:.511em 1em .011em 0;content:"←"}mjx-c.mjx-c1D7CF.TEX-B::before{padding:.655em .575em 0 0;content:"1"}mjx-mspace{display:inline-block;text-align:left}mjx-c.mjx-c1D6F4.TEX-I::before{padding:.683em .806em 0 0;content:"Σ"}mjx-c.mjx-c211D.TEX-A::before{padding:.683em .722em 0 0;content:"R"}mjx-c.mjx-c3A6::before{padding:.683em .722em 0 0;content:"Φ"}mjx-c.mjx-c28.TEX-S4::before{padding:1.75em .792em 1.249em 0;content:"("}mjx-c.mjx-c29.TEX-S4::before{padding:1.75em .792em 1.249em 0;content:")"}mjx-c.mjx-c1D713.TEX-I::before{padding:.694em .651em .205em 0;content:"ψ"}mjx-c.mjx-cD7::before{padding:.491em .778em 0 0;content:"×"}mjx-c.mjx-c221A.TEX-S1::before{padding:.85em 1.02em .35em 0;content:"√"}mjx-c.mjx-c41::before{padding:.716em .75em 0 0;content:"A"}mjx-c.mjx-c4D::before{padding:.683em .917em 0 0;content:"M"}mjx-c.mjx-c48::before{padding:.683em .75em 0 0;content:"H"}mjx-c.mjx-c43::before{padding:.705em .722em .021em 0;content:"C"}mjx-c.mjx-c63::before{padding:.448em .444em .011em 0;content:"c"}mjx-c.mjx-c45::before{padding:.68em .681em 0 0;content:"E"}mjx-c.mjx-c28.TEX-S3::before{padding:1.45em .736em .949em 0;content:"("}mjx-c.mjx-c29.TEX-S3::before{padding:1.45em .736em .949em 0;content:")"}mjx-c.mjx-c4C::before{padding:.683em .625em 0 0;content:"L"}mjx-c.mjx-c79::before{padding:.431em .528em .204em 0;content:"y"}mjx-c.mjx-c4E::before{padding:.683em .75em 0 0;content:"N"}mjx-c.mjx-c53::before{padding:.705em .556em .022em 0;content:"S"}mjx-c.mjx-c46::before{padding:.68em .653em 0 0;content:"F"}mjx-c.mjx-c5B.TEX-S3::before{padding:1.45em .528em .949em 0;content:"["}mjx-c.mjx-c5D.TEX-S3::before{padding:1.45em .528em .949em 0;content:"]"}mjx-c.mjx-c22A4::before{padding:.668em .778em 0 0;content:"⊤"}mjx-c.mjx-c2299::before{padding:.583em .778em .083em 0;content:"⊙"}mjx-c.mjx-c2297::before{padding:.583em .778em .083em 0;content:"⊗"}mjx-c.mjx-c77::before{padding:.431em .722em .011em 0;content:"w"}mjx-c.mjx-c6B::before{padding:.694em .528em 0 0;content:"k"}mjx-c.mjx-c1D410.TEX-B::before{padding:.696em .864em .193em 0;content:"Q"}mjx-c.mjx-c1D40A.TEX-B::before{padding:.686em .901em 0 0;content:"K"}mjx-c.mjx-c1D413.TEX-B::before{padding:.675em .8em 0 0;content:"T"}mjx-c.mjx-c1D415.TEX-B::before{padding:.686em .869em .007em 0;content:"V"}mjx-c.mjx-c1D405.TEX-B::before{padding:.68em .724em 0 0;content:"F"}mjx-c.mjx-c1D53C.TEX-A::before{padding:.683em .667em 0 0;content:"E"}mjx-c.mjx-c44.TEX-C::before{padding:.683em .771em 0 0;content:"D"}mjx-c.mjx-c1D6FD.TEX-I::before{padding:.705em .566em .194em 0;content:"β"}mjx-c.mjx-c1D6FE.TEX-I::before{padding:.441em .543em .216em 0;content:"γ"}mjx-c.mjx-c5B.TEX-S2::before{padding:1.15em .472em .649em 0;content:"["}mjx-c.mjx-c5D.TEX-S2::before{padding:1.15em .472em .649em 0;content:"]"}mjx-c.mjx-c2248::before{padding:.483em .778em 0 0;content:"≈"}mjx-munderover{display:inline-block;text-align:left}mjx-munderover:not([limits=false]){padding-top:.1em}mjx-munderover:not([limits=false])>*{display:block}mjx-munder{display:inline-block;text-align:left}mjx-over{text-align:left}mjx-munder:not([limits=false]){display:inline-table}mjx-munder>mjx-row{text-align:left}mjx-under{padding-bottom:.1em}mjx-mtable{display:inline-block;text-align:center;vertical-align:.25em;position:relative;box-sizing:border-box;border-spacing:0px;border-collapse:collapse}mjx-mstyle[size="s"] mjx-mtable{vertical-align:.354em}mjx-labels{position:absolute;left:0;top:0}mjx-table{display:inline-block;vertical-align:-.5ex;box-sizing:border-box}mjx-table>mjx-itable{vertical-align:middle;text-align:left;box-sizing:border-box}mjx-labels>mjx-itable{position:absolute;top:0}mjx-mtable[justify=left]{text-align:left}mjx-mtable[justify=right]{text-align:right}mjx-mtable[justify=left][side=left]{padding-right:0!important}mjx-mtable[justify=left][side=right]{padding-left:0!important}mjx-mtable[justify=right][side=left]{padding-right:0!important}mjx-mtable[justify=right][side=right]{padding-left:0!important}mjx-mtable[align]{vertical-align:baseline}mjx-mtable[align=top]>mjx-table{vertical-align:top}mjx-mtable[align=bottom]>mjx-table{vertical-align:bottom}mjx-mtable[side=right] mjx-labels{min-width:100%}mjx-mtr{display:table-row;text-align:left}mjx-mtr[rowalign=top]>mjx-mtd{vertical-align:top}mjx-mtr[rowalign=center]>mjx-mtd{vertical-align:middle}mjx-mtr[rowalign=bottom]>mjx-mtd{vertical-align:bottom}mjx-mtr[rowalign=baseline]>mjx-mtd{vertical-align:baseline}mjx-mtr[rowalign=axis]>mjx-mtd{vertical-align:.25em}mjx-mtd{display:table-cell;text-align:center;padding:.215em .4em}mjx-mtd:first-child{padding-left:0}mjx-mtd:last-child{padding-right:0}mjx-mtable>*>mjx-itable>:first-child>mjx-mtd{padding-top:0}mjx-mtable>*>mjx-itable>:last-child>mjx-mtd{padding-bottom:0}mjx-tstrut{display:inline-block;height:1em;vertical-align:-.25em}mjx-labels[align=left]>mjx-mtr>mjx-mtd{text-align:left}mjx-labels[align=right]>mjx-mtr>mjx-mtd{text-align:right}mjx-mtd[extra]{padding:0}mjx-mtd[rowalign=top]{vertical-align:top}mjx-mtd[rowalign=center]{vertical-align:middle}mjx-mtd[rowalign=bottom]{vertical-align:bottom}mjx-mtd[rowalign=baseline]{vertical-align:baseline}mjx-mtd[rowalign=axis]{vertical-align:.25em}mjx-stretchy-v.mjx-c221A mjx-beg mjx-c::before{content:"";padding:.605em 1.056em .014em 0}mjx-stretchy-v.mjx-c221A mjx-ext mjx-c::before{content:"";width:1.056em}mjx-stretchy-v.mjx-c221A mjx-end mjx-c::before{content:"⎷";padding:.935em 1.056em .885em 0}mjx-stretchy-v.mjx-c221A>mjx-end{margin-top:-1.82em}mjx-stretchy-v.mjx-c221A>mjx-ext{border-top-width:.589em;border-bottom-width:1.79em}mjx-stretchy-v.mjx-c5B mjx-beg mjx-c::before{content:"⎡";padding:1.154em .667em .645em 0}mjx-stretchy-v.mjx-c5B mjx-ext mjx-c::before{content:"⎢";width:.667em}mjx-stretchy-v.mjx-c5B mjx-end mjx-c::before{content:"⎣";padding:1.155em .667em .644em 0}mjx-stretchy-v.mjx-c5B>mjx-end{margin-top:-1.799em}mjx-stretchy-v.mjx-c5B>mjx-ext{border-top-width:1.769em;border-bottom-width:1.769em}mjx-stretchy-v.mjx-c5D mjx-beg mjx-c::before{content:"⎤";padding:1.154em .667em .645em 0}mjx-stretchy-v.mjx-c5D mjx-ext mjx-c::before{content:"⎥";width:.667em}mjx-stretchy-v.mjx-c5D mjx-end mjx-c::before{content:"⎦";padding:1.155em .667em .644em 0}mjx-stretchy-v.mjx-c5D>mjx-end{margin-top:-1.799em}mjx-stretchy-v.mjx-c5D>mjx-ext{border-top-width:1.769em;border-bottom-width:1.769em}mjx-stretchy-v.mjx-c7B mjx-beg mjx-c::before{content:"⎧";padding:.899em .889em .01em 0}mjx-stretchy-v.mjx-c7B mjx-ext mjx-c::before{content:"⎪";width:.889em}mjx-stretchy-v.mjx-c7B mjx-end mjx-c::before{content:"⎩";padding:.01em .889em .899em 0}mjx-stretchy-v.mjx-c7B mjx-mid mjx-c::before{content:"⎨";padding:1.16em .889em .66em 0}mjx-stretchy-v.mjx-c7B>mjx-mid{margin-top:-.91em;margin-bottom:-.91em}mjx-stretchy-v.mjx-c7B>mjx-end{margin-top:-.909em}mjx-stretchy-v.mjx-c7B>mjx-ext{height:50%;border-top-width:.879em;border-bottom-width:.879em}mjx-stretchy-v.mjx-c7D mjx-beg mjx-c::before{content:"⎫";padding:.899em .889em .01em 0}mjx-stretchy-v.mjx-c7D mjx-ext mjx-c::before{content:"⎪";width:.889em}mjx-stretchy-v.mjx-c7D mjx-end mjx-c::before{content:"⎭";padding:.01em .889em .899em 0}mjx-stretchy-v.mjx-c7D mjx-mid mjx-c::before{content:"⎬";padding:1.16em .889em .66em 0}mjx-stretchy-v.mjx-c7D>mjx-mid{margin-top:-.91em;margin-bottom:-.91em}mjx-stretchy-v.mjx-c7D>mjx-end{margin-top:-.909em}mjx-stretchy-v.mjx-c7D>mjx-ext{height:50%;border-top-width:.879em;border-bottom-width:.879em}mjx-c.mjx-c2211.TEX-S2::before{padding:.95em 1.444em .45em 0;content:"∑"}mjx-c.mjx-c1D706.TEX-I::before{padding:.694em .583em .012em 0;content:"λ"}mjx-c.mjx-c1D464.TEX-I::before{padding:.443em .716em .011em 0;content:"w"}mjx-c.mjx-c50::before{padding:.683em .681em 0 0;content:"P"}mjx-c.mjx-c1D446.TEX-I::before{padding:.705em .645em .022em 0;content:"S"}mjx-c.mjx-c223C::before{padding:.367em .778em 0 0;content:"∼"}mjx-c.mjx-c1D437.TEX-I::before{padding:.683em .828em 0 0;content:"D"}mjx-c.mjx-c54::before{padding:.677em .722em 0 0;content:"T"}mjx-c.mjx-c65::before{padding:.448em .444em .011em 0;content:"e"}mjx-c.mjx-c2264::before{padding:.636em .778em .138em 0;content:"≤"}mjx-c.mjx-c221A.TEX-S4::before{padding:1.75em 1.02em 1.25em 0;content:"√"}mjx-c.mjx-c46.TEX-C::before{padding:.683em .829em .032em 0;content:"F"}mjx-c.mjx-c2F::before{padding:.75em .5em .25em 0;content:"/"}mjx-c.mjx-c1D6FF.TEX-I::before{padding:.717em .444em .01em 0;content:"δ"}mjx-c.mjx-cA0::before{padding:0 .25em 0 0;content:" "}mjx-c.mjx-c66::before{padding:.705em .372em 0 0;content:"f"}mjx-c.mjx-c20::before{padding:0 .25em 0 0;content:" "}mjx-c.mjx-c2208::before{padding:.54em .667em .04em 0;content:"∈"}mjx-c.mjx-c3E::before{padding:.54em .778em .04em 0;content:">"}mjx-c.mjx-c40::before{padding:.705em .778em .011em 0;content:"@"}mjx-c.mjx-c1D440.TEX-I::before{padding:.683em 1.051em 0 0;content:"M"}mjx-c.mjx-c1D44E.TEX-I::before{padding:.441em .529em .01em 0;content:"a"}mjx-c.mjx-c210E.TEX-I::before{padding:.694em .576em .011em 0;content:"h"}mjx-c.mjx-c1D436.TEX-I::before{padding:.705em .76em .022em 0;content:"C"}mjx-c.mjx-c1D44F.TEX-I::before{padding:.694em .429em .011em 0;content:"b"}mjx-c.mjx-c1D466.TEX-I::before{padding:.442em .49em .205em 0;content:"y"}mjx-c.mjx-c1D463.TEX-I::before{padding:.443em .485em .011em 0;content:"v"}mjx-c.mjx-c1D458.TEX-I::before{padding:.694em .521em .011em 0;content:"k"}mjx-c.mjx-c1D45C.TEX-I::before{padding:.441em .485em .011em 0;content:"o"}mjx-c.mjx-c5C::before{padding:.75em .5em .25em 0;content:"\\"}mjx-c.mjx-c1D45F.TEX-I::before{padding:.442em .451em .011em 0;content:"r"}mjx-c.mjx-c2192::before{padding:.511em 1em .011em 0;content:"→"}mjx-c.mjx-c1D462.TEX-I::before{padding:.442em .572em .011em 0;content:"u"}mjx-c.mjx-c1D459.TEX-I::before{padding:.694em .298em .011em 0;content:"l"}mjx-c.mjx-c1D45A.TEX-I::before{padding:.442em .878em .011em 0;content:"m"}mjx-c.mjx-c1D45D.TEX-I::before{padding:.442em .503em .194em 0;content:"p"}mjx-c.mjx-c1D43B.TEX-I::before{padding:.683em .888em 0 0;content:"H"}mjx-c.mjx-c24::before{padding:.75em .5em .056em 0;content:"$"}mjx-c.mjx-c36::before{padding:.666em .5em .022em 0;content:"6"}mjx-c.mjx-c37::before{padding:.676em .5em .022em 0;content:"7"}mjx-c.mjx-c39::before{padding:.666em .5em .022em 0;content:"9"}mjx-c.mjx-c35::before{padding:.666em .5em .022em 0;content:"5"}mjx-c.mjx-c394::before{padding:.716em .833em 0 0;content:"Δ"}mjx-c.mjx-c1D454.TEX-I::before{padding:.442em .477em .205em 0;content:"g"}mjx-c.mjx-c2260::before{padding:.716em .778em .215em 0;content:"≠"}mjx-c.mjx-c1D45E.TEX-I::before{padding:.442em .46em .194em 0;content:"q"}mjx-c.mjx-c1D429.TEX-B::before{padding:.45em .639em .194em 0;content:"p"}mjx-c.mjx-c1D42A.TEX-B::before{padding:.45em .607em .194em 0;content:"q"}mjx-c.mjx-c2211.TEX-S1::before{padding:.75em 1.056em .25em 0;content:"∑"}mjx-c.mjx-c1D43D.TEX-I::before{padding:.683em .633em .022em 0;content:"J"}mjx-c.mjx-c1D434.TEX-I::before{padding:.716em .75em 0 0;content:"A"}mjx-c.mjx-c1D435.TEX-I::before{padding:.683em .759em 0 0;content:"B"}mjx-c.mjx-c2229::before{padding:.598em .667em .022em 0;content:"∩"}mjx-c.mjx-c222A::before{padding:.598em .667em .022em 0;content:"∪"}mjx-c.mjx-c1D44C.TEX-I::before{padding:.683em .763em 0 0;content:"Y"}mjx-c.mjx-c1D447.TEX-I::before{padding:.677em .704em 0 0;content:"T"}mjx-c.mjx-c1D44A.TEX-I::before{padding:.683em 1.048em .022em 0;content:"W"}mjx-c.mjx-c1D457.TEX-I::before{padding:.661em .412em .204em 0;content:"j"}mjx-c.mjx-c2225::before{padding:.75em .5em .25em 0;content:"∥"}mjx-c.mjx-c1D442.TEX-I::before{padding:.704em .763em .022em 0;content:"O"}mjx-c.mjx-c1D43E.TEX-I::before{padding:.683em .889em 0 0;content:"K"}mjx-c.mjx-c2032::before{padding:.56em .275em 0 0;content:"′"}mjx-c.mjx-c2026::before{padding:.12em 1.172em 0 0;content:"…"}mjx-c.mjx-c22C5::before{padding:.31em .278em 0 0;content:"⋅"}mjx-c.mjx-c3A::before{padding:.43em .278em 0 0;content:":"}mjx-c.mjx-c1D448.TEX-I::before{padding:.683em .767em .022em 0;content:"U"}mjx-c.mjx-c68::before{padding:.694em .556em 0 0;content:"h"}mjx-c.mjx-c70::before{padding:.442em .556em .194em 0;content:"p"}mjx-c.mjx-c59::before{padding:.683em .75em 0 0;content:"Y"}mjx-c.mjx-c3C::before{padding:.54em .778em .04em 0;content:"<"}mjx-c.mjx-c1D43A.TEX-I::before{padding:.705em .786em .022em 0;content:"G"}mjx-c.mjx-c2217::before{padding:.465em .5em 0 0;content:"∗"}mjx-c.mjx-c1D443.TEX-I::before{padding:.683em .751em 0 0;content:"P"}mjx-c.mjx-c1D467.TEX-I::before{padding:.442em .465em .011em 0;content:"z"}mjx-c.mjx-c1D449.TEX-I::before{padding:.683em .769em .022em 0;content:"V"}mjx-c.mjx-c1D43C.TEX-I::before{padding:.683em .504em 0 0;content:"I"}mjx-mtext{display:inline-block;text-align:left}mjx-msqrt{display:inline-block;text-align:left}mjx-root{display:inline-block;white-space:nowrap}mjx-surd{display:inline-block;vertical-align:top}mjx-sqrt{display:inline-block;padding-top:.07em}mjx-sqrt>mjx-box{border-top:.07em solid}mjx-sqrt.mjx-tall>mjx-box{padding-left:.3em;margin-left:-.3em}mjx-mroot{display:inline-block;text-align:left}mjx-c.mjx-c6E::before{padding:.442em .556em 0 0;content:"n"}mjx-c.mjx-c6F::before{padding:.448em .5em .01em 0;content:"o"}mjx-c.mjx-c72::before{padding:.442em .392em 0 0;content:"r"}mjx-c.mjx-c6D::before{padding:.442em .833em 0 0;content:"m"}mjx-c.mjx-c69::before{padding:.669em .278em 0 0;content:"i"}mjx-c.mjx-c61::before{padding:.448em .5em .011em 0;content:"a"}mjx-c.mjx-c78::before{padding:.431em .528em 0 0;content:"x"}mjx-c.mjx-c6C::before{padding:.694em .278em 0 0;content:"l"}mjx-c.mjx-c67::before{padding:.453em .5em .206em 0;content:"g"}mjx-c.mjx-c2061::before{padding:0;content:""}mjx-c.mjx-c73::before{padding:.448em .394em .011em 0;content:"s"}mjx-c.mjx-c74::before{padding:.615em .389em .01em 0;content:"t"}mjx-c.mjx-c64::before{padding:.694em .556em .011em 0;content:"d"}mjx-c.mjx-c62::before{padding:.694em .556em .011em 0;content:"b"}mjx-c.mjx-c75::before{padding:.442em .556em .011em 0;content:"u"}mjx-c.mjx-c1D444.TEX-I::before{padding:.704em .791em .194em 0;content:"Q"}mjx-c.mjx-c33::before{padding:.665em .5em .022em 0;content:"3"}mjx-c.mjx-c1D465.TEX-I::before{padding:.442em .572em .011em 0;content:"x"}mjx-c.mjx-c2C::before{padding:.121em .278em .194em 0;content:","}mjx-c.mjx-c2E::before{padding:.12em .278em 0 0;content:"."}mjx-c.mjx-c1D45B.TEX-I::before{padding:.442em .6em .011em 0;content:"n"}mjx-c.mjx-c7C::before{padding:.75em .278em .249em 0;content:"|"}mjx-c.mjx-c221A.TEX-S2::before{padding:1.15em 1.02em .65em 0;content:"√"}mjx-c.mjx-c1D441.TEX-I::before{padding:.683em .888em 0 0;content:"N"}mjx-c.mjx-c221A::before{padding:.8em .853em .2em 0;content:"√"}mjx-container[jax=CHTML]{line-height:0}mjx-container [space="1"]{margin-left:.111em}mjx-container [space="2"]{margin-left:.167em}mjx-container [space="3"]{margin-left:.222em}mjx-container [space="4"]{margin-left:.278em}mjx-container [space="5"]{margin-left:.333em}mjx-container [rspace="1"]{margin-right:.111em}mjx-container [rspace="2"]{margin-right:.167em}mjx-container [rspace="3"]{margin-right:.222em}mjx-container [rspace="4"]{margin-right:.278em}mjx-container [rspace="5"]{margin-right:.333em}mjx-container [size="s"]{font-size:70.7%}mjx-container [size=ss]{font-size:50%}mjx-container [size=Tn]{font-size:60%}mjx-container [size=sm]{font-size:85%}mjx-container [size=lg]{font-size:120%}mjx-container [size=Lg]{font-size:144%}mjx-container [size=LG]{font-size:173%}mjx-container [size=hg]{font-size:207%}mjx-container [size=HG]{font-size:249%}mjx-container [width=full]{width:100%}mjx-box{display:inline-block}mjx-block{display:block}mjx-itable{display:inline-table}mjx-row{display:table-row}mjx-row>*{display:table-cell}mjx-mtext{display:inline-block}mjx-mstyle{display:inline-block}mjx-merror{display:inline-block;color:red;background-color:#ff0}mjx-mphantom{visibility:hidden}mjx-assistive-mml{top:0;left:0;clip:rect(1px,1px,1px,1px);user-select:none;position:absolute!important;padding:1px 0 0!important;border:0!important;display:block!important;width:auto!important;overflow:hidden!important}mjx-assistive-mml[display=block]{width:100%!important}mjx-math{display:inline-block;text-align:left;line-height:0;text-indent:0;font-style:normal;font-weight:400;font-size:100%;letter-spacing:normal;border-collapse:collapse;overflow-wrap:normal;word-spacing:normal;white-space:nowrap;direction:ltr;padding:1px 0}mjx-container[jax=CHTML][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=CHTML][display=true][width=full]{display:flex}mjx-container[jax=CHTML][display=true] mjx-math{padding:0}mjx-container[jax=CHTML][justify=left]{text-align:left}mjx-container[jax=CHTML][justify=right]{text-align:right}mjx-mi{display:inline-block;text-align:left}mjx-c{display:inline-block}mjx-utext{display:inline-block;padding:.75em 0 .2em}mjx-mo{display:inline-block;text-align:left}mjx-stretchy-h{display:inline-table;width:100%}mjx-stretchy-h>*{display:table-cell;width:0}mjx-stretchy-h>*>mjx-c{display:inline-block;transform:scaleX(1)}mjx-stretchy-h>*>mjx-c::before{display:inline-block;width:initial}mjx-stretchy-h>mjx-ext{overflow:clip visible;width:100%}mjx-stretchy-h>mjx-ext>mjx-c::before{transform:scaleX(500)}mjx-stretchy-h>mjx-ext>mjx-c{width:0}mjx-stretchy-h>mjx-beg>mjx-c{margin-right:-.1em}mjx-stretchy-h>mjx-end>mjx-c{margin-left:-.1em}mjx-stretchy-v{display:inline-block}mjx-stretchy-v>*{display:block}mjx-stretchy-v>mjx-beg{height:0}mjx-stretchy-v>mjx-end>mjx-c{display:block}mjx-stretchy-v>*>mjx-c{transform:scaleY(1);transform-origin:left center;overflow:hidden}mjx-stretchy-v>mjx-ext{display:block;height:100%;box-sizing:border-box;border:0 solid transparent;overflow:visible clip}mjx-stretchy-v>mjx-ext>mjx-c::before{width:initial;box-sizing:border-box}mjx-stretchy-v>mjx-ext>mjx-c{transform:scaleY(500) translateY(.075em);overflow:visible}mjx-mark{display:inline-block;height:0}mjx-msub{display:inline-block;text-align:left}mjx-texatom{display:inline-block;text-align:left}mjx-mn{display:inline-block;text-align:left}mjx-msubsup{display:inline-block;text-align:left}mjx-script{display:inline-block;padding-right:.05em;padding-left:.033em}mjx-script>mjx-spacer{display:block}mjx-msup{display:inline-block;text-align:left}mjx-mfrac{display:inline-block;text-align:left}mjx-frac{display:inline-block;vertical-align:.17em;padding:0 .22em}mjx-frac[type="d"]{vertical-align:.04em}mjx-frac[delims]{padding:0 .1em}mjx-frac[atop]{padding:0 .12em}mjx-frac[atop][delims]{padding:0}mjx-dtable{display:inline-table;width:100%}mjx-dtable>*{font-size:2000%}mjx-dbox{display:block;font-size:5%}mjx-num{display:block;text-align:center}mjx-den{display:block;text-align:center}mjx-mfrac[bevelled]>mjx-num{display:inline-block}mjx-mfrac[bevelled]>mjx-den{display:inline-block}mjx-den[align=right],mjx-num[align=right]{text-align:right}mjx-den[align=left],mjx-num[align=left]{text-align:left}mjx-nstrut{display:inline-block;height:.054em;width:0;vertical-align:-.054em}mjx-nstrut[type="d"]{height:.217em;vertical-align:-.217em}mjx-dstrut{display:inline-block;height:.505em;width:0}mjx-dstrut[type="d"]{height:.726em}mjx-line{display:block;box-sizing:border-box;min-height:1px;height:.06em;border-top:.06em solid;margin:.06em -.1em;overflow:hidden}mjx-line[type="d"]{margin:.18em -.1em}mjx-mrow{display:inline-block;text-align:left}mjx-c::before{display:block;width:0}.MJX-TEX{font-family:MJXZERO,MJXTEX}.TEX-B{font-family:MJXZERO,MJXTEX-B}.TEX-I{font-family:MJXZERO,MJXTEX-I}.TEX-MI{font-family:MJXZERO,MJXTEX-MI}.TEX-BI{font-family:MJXZERO,MJXTEX-BI}.TEX-S1{font-family:MJXZERO,MJXTEX-S1}.TEX-S2{font-family:MJXZERO,MJXTEX-S2}.TEX-S3{font-family:MJXZERO,MJXTEX-S3}.TEX-S4{font-family:MJXZERO,MJXTEX-S4}.TEX-A{font-family:MJXZERO,MJXTEX-A}.TEX-C{font-family:MJXZERO,MJXTEX-C}.TEX-CB{font-family:MJXZERO,MJXTEX-CB}.TEX-FR{font-family:MJXZERO,MJXTEX-FR}.TEX-FRB{font-family:MJXZERO,MJXTEX-FRB}.TEX-SS{font-family:MJXZERO,MJXTEX-SS}.TEX-SSB{font-family:MJXZERO,MJXTEX-SSB}.TEX-SSI{font-family:MJXZERO,MJXTEX-SSI}.TEX-SC{font-family:MJXZERO,MJXTEX-SC}.TEX-T{font-family:MJXZERO,MJXTEX-T}.TEX-V{font-family:MJXZERO,MJXTEX-V}.TEX-VB{font-family:MJXZERO,MJXTEX-VB}mjx-stretchy-h mjx-c,mjx-stretchy-v mjx-c{font-family:MJXZERO,MJXTEX-S1,MJXTEX-S4,MJXTEX,MJXTEX-A!important}@font-face{font-family:MJXZERO;src:url("lib/fonts/mathjax_zero.woff") format("woff")}@font-face{font-family:MJXTEX;src:url("lib/fonts/mathjax_main-regular.woff") format("woff")}@font-face{font-family:MJXTEX-B;src:url("lib/fonts/mathjax_main-bold.woff") format("woff")}@font-face{font-family:MJXTEX-I;src:url("lib/fonts/mathjax_math-italic.woff") format("woff")}@font-face{font-family:MJXTEX-MI;src:url("lib/fonts/mathjax_main-italic.woff") format("woff")}@font-face{font-family:MJXTEX-BI;src:url("lib/fonts/mathjax_math-bolditalic.woff") format("woff")}@font-face{font-family:MJXTEX-S1;src:url("lib/fonts/mathjax_size1-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S2;src:url("lib/fonts/mathjax_size2-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S3;src:url("lib/fonts/mathjax_size3-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S4;src:url("lib/fonts/mathjax_size4-regular.woff") format("woff")}@font-face{font-family:MJXTEX-A;src:url("lib/fonts/mathjax_ams-regular.woff") format("woff")}@font-face{font-family:MJXTEX-C;src:url("lib/fonts/mathjax_calligraphic-regular.woff") format("woff")}@font-face{font-family:MJXTEX-CB;src:url("lib/fonts/mathjax_calligraphic-bold.woff") format("woff")}@font-face{font-family:MJXTEX-FR;src:url("lib/fonts/mathjax_fraktur-regular.woff") format("woff")}@font-face{font-family:MJXTEX-FRB;src:url("lib/fonts/mathjax_fraktur-bold.woff") format("woff")}@font-face{font-family:MJXTEX-SS;src:url("lib/fonts/mathjax_sansserif-regular.woff") format("woff")}@font-face{font-family:MJXTEX-SSB;src:url("lib/fonts/mathjax_sansserif-bold.woff") format("woff")}@font-face{font-family:MJXTEX-SSI;src:url("lib/fonts/mathjax_sansserif-italic.woff") format("woff")}@font-face{font-family:MJXTEX-SC;src:url("lib/fonts/mathjax_script-regular.woff") format("woff")}@font-face{font-family:MJXTEX-T;src:url("lib/fonts/mathjax_typewriter-regular.woff") format("woff")}@font-face{font-family:MJXTEX-V;src:url("lib/fonts/mathjax_vector-regular.woff") format("woff")}@font-face{font-family:MJXTEX-VB;src:url("lib/fonts/mathjax_vector-bold.woff") format("woff")}mjx-c.mjx-c1D445.TEX-I::before{padding:.683em .759em .021em 0;content:"R"}mjx-c.mjx-c28::before{padding:.75em .389em .25em 0;content:"("}mjx-c.mjx-c1D70F.TEX-I::before{padding:.431em .517em .013em 0;content:"τ"}mjx-c.mjx-c29::before{padding:.75em .389em .25em 0;content:")"}mjx-c.mjx-c3D::before{padding:.583em .778em .082em 0;content:"="}mjx-c.mjx-c1D438.TEX-I::before{padding:.68em .764em 0 0;content:"E"}mjx-c.mjx-c5B::before{padding:.75em .278em .25em 0;content:"["}mjx-c.mjx-c1D44B.TEX-I::before{padding:.683em .852em 0 0;content:"X"}mjx-c.mjx-c1D461.TEX-I::before{padding:.626em .361em .011em 0;content:"t"}mjx-c.mjx-c2212::before{padding:.583em .778em .082em 0;content:"−"}mjx-c.mjx-c1D707.TEX-I::before{padding:.442em .603em .216em 0;content:"μ"}mjx-c.mjx-c2B::before{padding:.583em .778em .082em 0;content:"+"}mjx-c.mjx-c5D::before{padding:.75em .278em .25em 0;content:"]"}mjx-c.mjx-c1D450.TEX-I::before{padding:.442em .433em .011em 0;content:"c"}mjx-c.mjx-c1D719.TEX-I::before{padding:.694em .596em .205em 0;content:"ϕ"}mjx-c.mjx-c31::before{padding:.666em .5em 0 0;content:"1"}mjx-c.mjx-c32::before{padding:.666em .5em 0 0;content:"2"}mjx-c.mjx-c1D716.TEX-I::before{padding:.431em .406em .011em 0;content:"ϵ"}mjx-c.mjx-c1D703.TEX-I::before{padding:.705em .469em .01em 0;content:"θ"}mjx-c.mjx-c1D439.TEX-I::before{padding:.68em .749em 0 0;content:"F"}mjx-c.mjx-c1D714.TEX-I::before{padding:.443em .622em .011em 0;content:"ω"}mjx-c.mjx-c222B.TEX-S2::before{padding:1.36em .944em .862em 0;content:"∫"}mjx-c.mjx-c221E::before{padding:.442em 1em .011em 0;content:"∞"}mjx-c.mjx-c1D453.TEX-I::before{padding:.705em .55em .205em 0;content:"f"}mjx-c.mjx-c1D452.TEX-I::before{padding:.442em .466em .011em 0;content:"e"}mjx-c.mjx-c1D456.TEX-I::before{padding:.661em .345em .011em 0;content:"i"}mjx-c.mjx-c1D451.TEX-I::before{padding:.694em .52em .01em 0;content:"d"}mjx-c.mjx-c1D70B.TEX-I::before{padding:.431em .57em .011em 0;content:"π"}mjx-c.mjx-c1D43F.TEX-I::before{padding:.683em .681em 0 0;content:"L"}mjx-c.mjx-c7B::before{padding:.75em .5em .25em 0;content:"{"}mjx-c.mjx-c7D::before{padding:.75em .5em .25em 0;content:"}"}mjx-c.mjx-c1D460.TEX-I::before{padding:.442em .469em .01em 0;content:"s"}mjx-c.mjx-c30::before{padding:.666em .5em .022em 0;content:"0"}mjx-c.mjx-c1D70E.TEX-I::before{padding:.431em .571em .011em 0;content:"σ"}mjx-c.mjx-c1D6FC.TEX-I::before{padding:.442em .64em .011em 0;content:"α"}</style><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="图像还原(超分增强去噪恢复)"><p dir="auto">图像还原(超分增强去噪恢复)</p></h1><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/498364155/answer/2240224120" rel="noopener" class="external-link" href="https://www.zhihu.com/question/498364155/answer/2240224120" target="_blank"># 如何看待何恺明最新一作论文Masked Autoencoders？</a></p></div><div><ul>
<li data-line="0" dir="auto">图像的空间信息存在巨大冗余</li>
</ul></div><div class="admonition-parent admonition-hint-parent"><div class="callout admonition admonition-hint admonition-plugin " style="--callout-color: 0, 191, 165;" data-callout="hint" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="fire" class="svg-inline--fa fa-fire fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M216 23.86c0-23.8-30.65-32.77-44.15-13.04C48 191.85 224 200 224 288c0 35.63-29.11 64.46-64.85 63.99-35.17-.45-63.15-29.77-63.15-64.94v-85.51c0-21.7-26.47-32.23-41.43-16.5C27.8 213.16 0 261.33 0 320c0 105.87 86.13 192 192 192s192-86.13 192-192c0-170.29-168-193-168-296.14z"></path></svg></div><div class="callout-title-inner admonition-title-content">Hint</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Gt421G7oq/?spm_id_from=333.788.recommend_more_video.-1" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Gt421G7oq/?spm_id_from=333.788.recommend_more_video.-1" target="_blank">CVPR2023+2024底层视觉（超分辨率、增强、去噪、恢复等）论文汇总_哔哩哔哩_bilibili</a></p>
<p dir="auto"><strong>2024 - 图像去噪</strong></p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/pdf/2404.09389.pdf" rel="noopener" class="external-link" href="https://arxiv.org/pdf/2404.09389.pdf" target="_blank">Masked and Shuffled Blind Spot Denoising for Real-World Images</a></p>
<p dir="auto">这篇论文介绍了一种新的单图像去噪方法，称为MAsked and SHuffled Blind Spot Denoising (MASH)。该方法基于Blind Spot Denoising (BSD) 原理，专注于处理实际图像中常见的相关噪声。<strong>MASH通过仔细分析输入的遮蔽程度（masking）与未知噪声相关性之间的关系来实现，并引入了一种洗牌技术来削弱噪声的局部相关性，从而进一步提高去噪性能</strong>。通过在真实世界含噪声图像数据集上的广泛实验评估，MASH展示了与现有的自监督去噪方法相比具有最先进的结果。</p>
<p dir="auto"><strong>主要贡献：</strong></p>
<ol>
<li dir="auto">分析了BSD方法，展示了不同遮蔽比例对相关噪声的影响，并提出了一种估计噪声相关性水平的方法。</li>
<li dir="auto">提出了MASH，BSD的增强版本，能够动态选择最优的遮蔽比例，并引入了局部像素洗牌技术来从源头上解决噪声相关性问题。</li>
<li dir="auto">MASH在多个数据集上展示了其在真实世界去噪中的显著改进，与基线BSD相比取得了更好的结果。</li>
</ol>
<p dir="auto"><strong>相关工作：</strong></p>
<ul>
<li dir="auto">非基于学习的图像去噪器：传统方法通过手动设计图像先验和优化技术来提高重建精度和速度。</li>
<li dir="auto">基于无监督学习的图像去噪器：可以基于数据集或单图像进行训练。数据集基础的方法使用含噪声图像数据集来训练去噪模型，而单图像方法则一次学习一个图像的去噪器。</li>
</ul>
<p dir="auto"><strong>实验设置：</strong></p>
<ul>
<li dir="auto">使用了SIDD、FMDD和PolyU等真实世界噪声数据集进行评估。</li>
<li dir="auto">MASH的网络架构与Noise2Noise相同，使用Adam优化器和余弦退火从头开始训练去噪网络。</li>
</ul>
<p dir="auto"><strong>实验结果：</strong></p>
<ul>
<li dir="auto">MASH在SIDD和FMDD数据集上相比基线方法分别提高了约2dB和1.5dB的性能。</li>
<li dir="auto">通过自适应遮蔽和局部像素洗牌，MASH在FMDD数据集上超越了单图像和基于数据集的方法。</li>
</ul>
<p dir="auto"><strong>消融研究：</strong></p>
<ul>
<li dir="auto">自适应遮蔽方案：在没有应用自适应遮蔽时，使用默认遮蔽比例τ=0.5，自适应遮蔽在SIDD和FMDD数据集上都显著提高了性能。</li>
<li dir="auto">局部像素洗牌：在高空间相关噪声的情况下，应用局部像素洗牌可以提高去噪性能。</li>
<li dir="auto">邻域大小s的影响：随着s的增加，性能提高然后开始饱和。</li>
</ul>
<p dir="auto"><strong>结论：</strong> MASH是一种利用盲点去噪框架的单图像去噪方法，包括检测和缓解噪声相关性影响的分析。通过这种方法，MASH在多个公共基准测试中实现了与现有测试时间训练方法相比的最先进结果。</p></div></div></div><div><p dir="auto"><del>LAN: Learning to Adapt Noise for Image Denoising</del></p></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/pdf/2403.15132.pdf" rel="noopener" class="external-link" href="https://arxiv.org/pdf/2403.15132.pdf" target="_blank">Transfer CLIP for Generalizable Image Denoising</a></p>
<p dir="auto">这篇论文提出了一种新的图像去噪方法，称为CLIP Denoising，旨在提高对未见噪声（out-of-distribution, OOD）的泛化能力。当前基于深度学习的去噪方法在训练数据集中的噪声类型上表现出色，但对未见过的噪声类型泛化能力较差。<strong>CLIP Denoising利用了对比语言-图像预训练（CLIP）模型的特性，该模型在开放世界图像识别任务中展现出了卓越的泛化能力</strong>。</p>
<p dir="auto"><strong>主要贡献：</strong></p>
<ol>
<li dir="auto">发现CLIP的ResNet图像编码器中的某些密集特征对于噪声具有不变性和与内容相关的特性，这些特性对于泛化去噪非常有利。</li>
<li dir="auto">提出了一个不对称的编码器-解码器去噪网络，将CLIP的固定ResNet图像编码器的密集特征和噪声图像一起输入到可学习的图像解码器中，以实现泛化去噪。</li>
<li dir="auto">提出了一种渐进式特征增强策略，通过在训练过程中随机扰动这些特征，以提高解码器的鲁棒性并减少特征过拟合。</li>
<li dir="auto">在多种OOD噪声上进行了广泛的实验和比较，包括合成噪声、真实世界sRGB噪声和低剂量CT图像噪声，证明了该方法的优越泛化能力。</li>
</ol>
<p dir="auto"><strong>相关工作：</strong></p>
<ul>
<li dir="auto">基于深度学习的图像去噪方法通常依赖强大的深度架构和大规模配对数据集，但在未见噪声上泛化能力有限。</li>
<li dir="auto">为了提高泛化能力，研究者们尝试了多种方法，包括在不同噪声水平上训练去噪器，以及通过对抗性训练和元学习等提高模型对OOD噪声的鲁棒性。</li>
</ul>
<p dir="auto"><strong>方法：</strong></p>
<ul>
<li dir="auto">通过固定CLIP的ResNet图像编码器，将图像去噪任务转化为从固定特征中恢复干净图像的问题。</li>
<li dir="auto">使用一个4级可学习的图像解码器，逐步恢复高分辨率特征，并将多尺度特征信息融合到恢复中。</li>
<li dir="auto">引入了渐进式特征增强策略，通过在训练阶段对CLIP的密集特征进行随机扰动，以增加特征多样性并提高模型对OOD噪声的鲁棒性。</li>
</ul>
<p dir="auto"><strong>实验：</strong></p>
<ul>
<li dir="auto">在多种OOD噪声上进行了实验，包括合成噪声（高斯噪声、空间高斯噪声、泊松噪声、斑点噪声和盐与胡椒噪声）和真实世界噪声（sRGB噪声和低剂量CT噪声）。</li>
<li dir="auto">与多种现有的去噪方法进行了比较，包括MaskDenoising、DIL、HAT、DnCNN和Restormer等，CLIPDenoising在多种噪声类型上均展现出优越的泛化性能。</li>
</ul>
<p dir="auto"><strong>结论：</strong> CLIPDenoising通过利用CLIP模型的泛化能力，提出了一种新的图像去噪框架，有效地提高了模型在未见噪声上的泛化能力。该方法在多种噪声类型上均展现出了良好的性能，证明了其作为一种新的通用去噪方法的潜力。</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/pdf/2303.16783.pdf" rel="noopener" class="external-link" href="https://arxiv.org/pdf/2303.16783.pdf" target="_blank">Exploring Efficient Asymmetric Blind-Spots for Self-Supervised Denoising in Real-World Scenarios</a></p>
<p dir="auto">这篇论文提出了一种新的自监督去噪方法，称为Asymmetric Tunable Blind-Spot Network (AT-BSN)，旨在改善在真实世界场景中的去噪性能。在现实世界中，噪声通常是空间相关的，这导致了许多基于像素独立噪声假设的自监督算法在真实图像上表现不佳。现有的方法，如使用像素洗牌下采样（AP）来破坏噪声的空间相关性，但这种方法引入了混叠效应，而且消除这些效应的后处理可能会破坏图像的空间结构和高频细节，并且耗时。 主要贡献包括：</p>
<ol>
<li dir="auto">提出了一种新的范式，<strong>通过使用可调节的盲点大小来打破现实世界噪声的空间相关性，同时减少对全局结构的破坏</strong>。作者设计了一种在训练时使用<strong>较大盲点来抑制局部空间相关噪声，而在推理时使用较小盲点以最小化信息丢失的盲点网络</strong>。</li>
<li dir="auto">为了进一步提高性能并解决BSN中的计算冗余问题，提出了盲点自集成方法，并将不同大小盲点的知识蒸馏到一个轻量级且计算效率高的非盲点网络（NBSN）中，用于高效推理。</li>
<li dir="auto">实验结果表明，该方法在保持图像纹理、参数数量、计算成本和推理时间方面全面优于其他自监督方法，并在真实世界数据集上取得了最新的最佳结果。 具体技术细节包括：</li>
</ol>
<ul>
<li dir="auto">提出了一种可调节盲点大小的BSN，通过在训练和推理过程中使用不对称的盲点大小，平衡了抑制噪声空间相关性和保留局部信息的需求。</li>
<li dir="auto">为了整合不同盲点大小的优势，提出了盲点自集成方法，通过对不同盲点大小进行推理并平均结果来提高性能。</li>
<li dir="auto">为了解决由于旋转操作导致的计算冗余问题，提出了将不同大小盲点的知识蒸馏到非盲点网络的方法，从而在提高性能的同时减少了计算量。 实验部分，作者在两个知名的现实世界图像去噪数据集上进行了训练和评估，即智能手机图像去噪数据集（SIDD）和达姆施塔特噪声数据集（DND）。使用峰值信噪比（PSNR）和结构相似性（SSIM）指标来评估方法的性能。与其他方法相比，AT-BSN在参数数量较少的情况下，具有更少的推理时间，并显著提高了性能。此外，由于没有使用下采样过程，因此避免了混叠效应，更好地保留了图像纹理。 最后，作者对不同盲点大小在训练和推理中的组合进行了消融研究，以评估不对称盲点策略的有效性以及所提出方法的鲁棒性。结果表明，该方法对于不同的实验组显示出鲁棒性，即使在训练中使用较大的盲点大小，对最终性能的影响也很小。</li>
</ul></div></div></div><div><p dir="auto"><del>Patch2Self2: Self-supervised Denoising on Coresets via Matrix Sketching</del></p></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/pdf/2312.16519.pdf" rel="noopener" class="external-link" href="https://arxiv.org/pdf/2312.16519.pdf" target="_blank">Image Restoration by Denoising Diffusion Models With Iteratively Preconditioned Guidance</a></p>
<p dir="auto">这篇论文的标题是《Image Restoration by Denoising Diffusion Models with Iteratively Preconditioned Guidance》，作者是 Tomer Garber 和 Tom Tirer，来自以色列的 Open University 和 Bar-Ilan University。论文提出了一种新的图像恢复方法，该方法利用<strong>去噪扩散模型（denoising diffusion models, DDMs）并通过迭代预处理引导（iteratively preconditioned guidance）来改善图像去噪、去模糊和超分辨率等任务的性能</strong>。</p>
<p dir="auto">以下是对论文内容的总结：</p>
<ol>
<li dir="auto">
<p><strong>问题背景</strong>：图像恢复是计算机视觉和图像处理中的一个重要领域，目的是从退化的图像版本恢复出高质量的原始图像。退化可能是由于噪声、模糊、低分辨率等因素造成的。</p>
</li>
<li dir="auto">
<p><strong>现有方法</strong>：传统的图像恢复方法通常涉及训练深度神经网络（DNN）来学习从退化图像到高质量图像的映射。然而，这些“任务特定”的DNN在测试时的观测条件与训练时的假设不匹配时性能会大幅下降。</p>
</li>
<li dir="auto">
<p><strong>提出的解决方案</strong>：本文提出了一种替代方法，即使用预训练的深度去噪器来施加信号的先验知识，并通过迭代算法在测试时以“零样本”（zero-shot）的方式处理观测一致性。特别是，作者提出了一种新的引导技术，称为迭代预处理引导（IDPG），它可以根据迭代过程调整引导策略，从而在早期迭代中利用BP（backprojection）步骤的优势，在后期迭代中利用LS（least squares）步骤的鲁棒性。</p>
</li>
<li dir="auto">
<p><strong>方法优势</strong>：与仅基于BP或LS的引导方法相比，所提出的方法在计算复杂性较低的同时，对观测噪声有更好的鲁棒性。此外，作者还设计了一种基于DDM的采样方案，称为去噪扩散迭代预处理引导（DDPG），它在图像去噪和超分辨率任务中展示了比现有方法更好的性能。</p>
</li>
<li dir="auto">
<p><strong>实验结果</strong>：作者在CelebA-HQ和ImageNet数据集上对所提出的方法进行了测试，包括图像去模糊和超分辨率任务，并考虑了有无噪声的情况。实验结果表明，DDPG方法在感知质量和准确性方面都优于现有方法。</p>
</li>
<li dir="auto">
<p><strong>理论分析</strong>：论文还提供了对所提出的引导技术的数学动机和理论分析，包括偏差和方差的分解，以及优化算法的收敛率。</p>
</li>
<li dir="auto">
<p><strong>结论</strong>：作者展示了一种新的框架，该框架使用DNN去噪器/扩散器和新颖的预处理数据保真度引导方法来解决线性逆问题。新方法在各种去模糊和超分辨率设置中，无论是否存在观测噪声，都展示了性能优势。</p>
</li>
</ol>
<p dir="auto">这篇论文的主要贡献在于提出了一种新的图像恢复框架，该框架结合了去噪扩散模型的优势，并通过迭代预处理引导来提高图像恢复任务的性能，尤其是在存在噪声的情况下。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="Denoising" dir="auto" class="heading" id="Denoising"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Denoising</h2><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://github.com/z-bingo/awesome-image-denoising-state-of-the-art/blob/master/readme.md#image-noise-level-estimation" rel="noopener" class="external-link" href="https://github.com/z-bingo/awesome-image-denoising-state-of-the-art/blob/master/readme.md#image-noise-level-estimation" target="_blank">awesome-image-denoising-state-of-the-art/readme.md at master · z-bingo/awesome-image-denoising-state-of-the-art · GitHub</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> Single-image unsupervised denoising 
</span><ol>
<li data-line="1" dir="auto">APB-SN</li>
<li data-line="2" dir="auto">Noise2Noise</li>
<li data-line="3" dir="auto">Noise2Void</li>
<li data-line="4" dir="auto">Noise2Self</li>
<li data-line="5" dir="auto">Self2Self</li>
</ol>
</li>
</ul></div><div><ol>
<li data-line="0" dir="auto"></li>
</ol></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content heading-wrapper"><h3 data-heading="微调CLIP镜像医学图像异常检测" dir="auto" class="heading" id="微调CLIP镜像医学图像异常检测"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>微调CLIP镜像医学图像异常检测</h3>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/688152203" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/688152203" target="_blank">CVPR 2024 | 医学异常检测新工作！采用VLM进行医学图像中的通用异常检测 - 知乎</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2403.12570" rel="noopener" class="external-link" href="https://arxiv.org/abs/2403.12570" target="_blank">Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://github.com/MediaBrain-SJTU/MVFA-AD" rel="noopener" class="external-link" href="https://github.com/MediaBrain-SJTU/MVFA-AD" target="_blank">GitHub - MediaBrain-SJTU/MVFA-AD: [CVPR2024 Highlight] Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images</a></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/14/AD1MrqvdYNwuTEX.png" referrerpolicy="no-referrer"></p>
<p dir="auto">这篇论文的标题是《Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images》，作者是来自上海交通大学、新加坡国立大学和上海人工智能实验室的研究团队。论文的主要内容是提出了一种新颖的轻量级多层次适应和比较框架，用于将CLIP模型适配到医学图像的异常检测任务中。</p>
<p dir="auto"><strong>主要贡献和方法：</strong></p>
<ol>
<li dir="auto"><strong>多层次特征适配框架（MVFA）</strong>：作者设计了一个多层次视觉特征适配器（MVFA），这是首次尝试将预训练的视觉-语言模型适配到零样本/少样本的医学异常分类和分割任务中。该框架通过在预训练的视觉编码器中集成多个残差适配器，实现了不同层次上的视觉特征逐步增强。</li>
<li dir="auto"><strong>视觉-语言特征对齐损失函数</strong>：多层次适配过程由多级别、逐像素的视觉-语言特征对齐损失函数指导，这些损失函数重新校准模型的焦点，从自然图像中的对象语义转移到医学图像中的异常识别。</li>
<li dir="auto"><strong>实验验证</strong>：作者在多个医学异常检测基准数据集上进行了实验，包括脑MRI、肝脏CT、视网膜OCT、胸部X射线和数字组织病理学图像。实验结果表明，该方法在零样本和少样本设置下，相较于当前最先进的模型，分别在异常分类和异常分割任务上平均提高了6.24%和7.33%的AUC分数。</li>
</ol>
<p dir="auto"><strong>相关工作和挑战：</strong></p>
<ul>
<li dir="auto">论文回顾了传统的异常检测方法，包括无监督方法和少样本学习方法，并讨论了它们在医学领域的应用。</li>
<li dir="auto">论文还探讨了视觉-语言模型的最新进展，特别是CLIP模型在自然图像领域的成功应用，并提出了将其扩展到医学图像检测的挑战。</li>
</ul>
<p dir="auto"><strong>问题表述：</strong></p>
<ul>
<li dir="auto">论文明确了将最初在自然图像上训练的视觉-语言模型适配到医学图像异常检测的目标，并提出了零样本学习场景下的问题表述。</li>
</ul>
<p dir="auto"><strong>方法细节：</strong></p>
<ul>
<li dir="auto">论文详细介绍了多层次特征适配过程中的适配器结构、语言特征格式化和视觉-语言特征对齐的方法。</li>
<li dir="auto">论文还描述了测试阶段的多层次特征比较架构，包括零样本分支和少样本分支，以及如何结合这两个分支的结果进行最终的异常预测。</li>
</ul>
<p dir="auto"><strong>实验设置和评估协议：</strong></p>
<ul>
<li dir="auto">论文提供了详细的实验设置，包括使用的数据集、评估指标（AUC）、模型配置和训练细节。</li>
</ul>
<p dir="auto"><strong>结论：</strong></p>
<ul>
<li dir="auto">论文总结了提出的方法在跨不同医学数据模态和解剖区域的异常检测任务中的有效性，并展示了其在零样本和少样本学习场景下的优越性能。</li>
</ul><div class="heading-children"></div></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content heading-wrapper"><h4 data-heading="使用双流网络(Swin Transfomer) 进行图像恢复(有监督) - 2021" dir="auto" class="heading" id="使用双流网络(Swin_Transfomer)_进行图像恢复(有监督)_-_2021"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>使用双流网络(Swin Transfomer) 进行图像恢复(有监督) - 2021</h4>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://github.com/JingyunLiang/SwinIR" rel="noopener" class="external-link" href="https://github.com/JingyunLiang/SwinIR" target="_blank">GitHub - JingyunLiang/SwinIR: SwinIR: Image Restoration Using Swin Transformer (official repository)</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2108.10257" rel="noopener" class="external-link" href="https://arxiv.org/abs/2108.10257" target="_blank">[2108.10257] SwinIR: Image Restoration Using Swin Transformer</a></p>
<p dir="auto">这篇论文的标题是《SwinIR: Image Restoration Using Swin Transformer》，由Liang等人撰写，发表在2021年的国际计算机视觉会议（ICCV）的工作坊上。论文提出了一种新的图像修复方法，名为SwinIR，它利用了Swin Transformer这一强大的视觉Transformer模型来解决图像修复任务中的多个问题。</p>
<p dir="auto"><strong>主要贡献：</strong></p>
<ol>
<li dir="auto"><strong>Swin Transformer的应用：</strong> 论文首次将Swin Transformer应用于图像修复任务。Swin Transformer是一种基于视觉的Transformer模型，它通过使用分层的Transformer结构和相对位置编码，有效地处理了图像数据。</li>
<li dir="auto"><strong>新架构设计：</strong> 作者提出了一种新的网络架构，该架构能够处理不同尺度的图像损坏，并有效地恢复细节。这种架构利用了Swin Transformer的多尺度特性，可以在多个分辨率上进行图像修复。</li>
<li dir="auto"><strong>端到端训练：</strong> SwinIR采用端到端的训练策略，直接从损坏的图像到修复图像的映射进行学习，无需额外的预处理或后处理步骤。</li>
<li dir="auto"><strong>实验结果：</strong> 论文通过大量实验验证了SwinIR在多个图像修复任务上的有效性，包括去噪、去模糊、超分辨率和修复老照片等。实验结果表明，SwinIR在多个公开数据集上都取得了最先进的性能。</li>
</ol>
<p dir="auto"><strong>方法概述：</strong> SwinIR的核心是Swin Transformer，它通过将图像分割成一系列的patches，然后在这些patches上应用自注意力机制来捕捉图像的全局依赖关系。此外，SwinIR还引入了一种新颖的损失函数，该函数结合了对抗性损失和感知损失，以促进生成图像的真实性和视觉质量。</p>
<p dir="auto"><strong>结论：</strong> SwinIR通过结合Swin Transformer的强大能力和专门设计的网络架构，为图像修复任务提供了一种有效的解决方案。论文的实验结果证明了SwinIR在多个图像修复任务上的优势，展示了其在未来图像处理领域的潜力。</p>
<p dir="auto">这篇论文的提出，不仅推动了图像修复技术的发展，也为后续的研究者提供了新的视角和工具，以便更好地解决图像相关的其他问题。</p><div class="heading-children"></div></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="Dehaze/blurry" dir="auto" class="heading" id="Dehaze/blurry"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Dehaze/blurry</h2><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Gp4y1y73a/?spm_id_from=333.337.search-card.all.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Gp4y1y73a/?spm_id_from=333.337.search-card.all.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">经典图像去雾算法评测_哔哩哔哩_bilibili</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1CX4y1P7Sk/?spm_id_from=333.337.search-card.all.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1CX4y1P7Sk/?spm_id_from=333.337.search-card.all.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">图像去雾，论文讲解，简单易懂，一看就会_哔哩哔哩_bilibili</a></p></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=MDLHkkhBMgs" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=MDLHkkhBMgs" target="_blank">怎么对模糊图片进行语义分割？2023最新CVPR解读！ - YouTube</a></p>
<p dir="auto">在提出的CVPR2023的论文中，作者集中研究了如何提高语义分割模型对运动模糊的鲁棒性。他们<br>
的研究的主要贡献和创新点包括：</p>
<ol>
<li dir="auto"><strong>类中心的运动模糊增强(Class-Centric Motion-Blur Augmentation,CCMBA)策略</strong>：作者提出了一种新颖的数据增强方法，该方法随机选择图像中的一个子集的语义类，并利用分割图注释只对相应区域进行模糊。这样可以让网络同时学习干净图像的语义分割、自我运动模糊图像以及动态场景模糊图像的处理。</li>
<li dir="auto"><strong>提高泛化能力</strong>：尽管模型只在合成的模糊数据上训练，但CCMBA的类中心性质使得它能够在像GoProi和REDS这样的通用动态模糊数据集上表现良好，尤其是对于像人类这样的常见类别。</li>
<li dir="auto"><strong>改善性能</strong>：在PASCAL VOCi和Cityscapes数据集上，相对于基线方法，作者报告了DeepLabv3+模型在最高模糊等级上的性能提高了3.2%和3%，并在Cityscapes-C数据集上实现了最多9%的性能提高。</li>
<li dir="auto"><strong>不增加模型参数和推理时间</strong>：CCMBA策略可以提升任何监督的语义分割方法的鲁棒性，而不需要增加模型参数和推理时间。</li>
</ol>
<p dir="auto">这篇论文的重要性在于，它解决了现实应用中的一个关键问题：如何在运动模糊的情况下保持语义<br>
分割模型的性能。作者提供了一种在不牺牲清晰图像性能的前提下，通过数据增强提高对模糊图像<br>
处理能力的有效方法。</p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="Super Resolution" dir="auto" class="heading" id="Super_Resolution"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Super Resolution</h2><div class="heading-children"></div></div><div class="heading-wrapper"><h2 data-heading="Restoration" dir="auto" class="heading" id="Restoration"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Restoration</h2><div class="heading-children"><div><hr></div><div><p dir="auto">CVPR 2023 图像去噪</p></div><div><ul>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>格外注意
<ul>
<li data-line="1" dir="auto">非2D图像去噪</li>
<li data-line="2" dir="auto">有没有使用时序信息的去噪算法</li>
<li data-line="3" dir="auto">那些去噪算法是适合医学图像的</li>
<li data-line="4" dir="auto">那些去噪算法有强可解释性</li>
</ul>
</li>
</ul></div><div><p dir="auto">多Loss如何设计<br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/603893186/answer/3304348698?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1749026033922146304&amp;utm_source=io.raindrop.raindropio" rel="noopener" class="external-link" href="https://www.zhihu.com/question/603893186/answer/3304348698?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1749026033922146304&amp;utm_source=io.raindrop.raindropio" target="_blank">魔改一个loss 可以发啥水平的文章? - 知乎</a></p></div><div><p dir="auto">PSNR + SSIM </p></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto">在正电子发射断层扫描（Positron Emission Tomography, PET）图像降噪任务中，可以使用以下参数来度量降噪算法的性能：</p>
<ol>
<li dir="auto">
<p><strong>峰值信噪比（Peak Signal-to-Noise Ratio, PSNR）</strong>：这是一种常用的度量图像质量的参数，它比较了原始图像和降噪后的图像之间的差异。PSNR值越高，表示降噪效果越好。</p>
</li>
<li dir="auto">
<p><strong>结构相似性指数（Structural Similarity Index, SSIM）</strong>：SSIM是一种度量两个图像结构相似性的指数。在PET图像降噪中，我们希望降噪后的图像与原始图像在结构上尽可能相似。</p>
</li>
<li dir="auto">
<p><strong>体素重叠误差（Voxel Overlap Error）</strong>：如前面所述，体素重叠误差可以补充PSNR和SSIM的评估，提供更具体的局部信息。</p>
</li>
<li dir="auto">
<p><strong>均方误差（Mean Squared Error, MSE）</strong>：MSE是一种评估图像差异的常用方法，它计算的是原始图像和降噪后的图像之间每个像素差值的平方的平均值。MSE越小，表示降噪效果越好。</p>
</li>
<li dir="auto">
<p><strong>信号保留（Signal Preservation）</strong>：这是一个特定于PET图像的度量参数，它评估的是降噪算法能否保留原始图像中的信号。这可以通过比较降噪前后图像中的ROI（感兴趣区域）的信号强度来实现。</p>
</li>
<li dir="auto">
<p><strong>背景噪声（Background Noise）</strong>：这是另一个特定于PET图像的度量参数，它评估的是降噪算法能否有效地抑制背景噪声。这可以通过比较降噪前后图像中的背景区域的噪声水平来实现。</p>
</li>
</ol>
<p dir="auto">这些参数可以从不同的角度评估降噪算法的性能，包括图像质量、结构保留、信号保留和噪声抑制等。在实际应用中，我们通常会结合使用这些参数，以获得更全面的评估。</p></div></div></div><div><p dir="auto">NeRF 论文</p></div><div><ul>
<li data-line="0" dir="auto">如何从2D图像重建</li>
<li data-line="1" dir="auto">网络是如何搭建的?</li>
</ul></div><div><p dir="auto">迁移学习</p></div><div><p dir="auto">双流网络</p></div><div><p dir="auto">初始化</p></div><div><p dir="auto">Data augmentation:</p></div><div><ul>
<li data-line="0" dir="auto">MixUP</li>
</ul></div><div><p dir="auto">去噪的科学原理</p></div><div><p dir="auto">可解释性</p></div><div><p dir="auto">PointNet</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/577564172/answer/3463402785" rel="noopener" class="external-link" href="https://www.zhihu.com/question/577564172/answer/3463402785" target="_blank">小波与 BP 神经网络如何联合？ - 知乎</a></p></div><div><p dir="auto">Features Extractor</p></div><div><ul>
<li data-line="0" dir="auto">Wavelet </li>
<li data-line="1" dir="auto">Pre-train on Natural Image</li>
</ul></div><div><p dir="auto">Domain adaption</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://github.com/google-research/vision_transformer" rel="noopener" class="external-link" href="https://github.com/google-research/vision_transformer" target="_blank">GitHub - google-research/vision_transformer</a><br>
这里有 Patch + Pixel Shuffle 和 Global Shuffle 的代码实现. </p></div><div><p dir="auto">extra works :<br>
<a data-tooltip-position="top" aria-label="https://openaccess.thecvf.com/content_ICCV_2019/html/Hermosilla_Total_Denoising_Unsupervised_Learning_of_3D_Point_Cloud_Cleaning_ICCV_2019_paper.html" rel="noopener" class="external-link" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Hermosilla_Total_Denoising_Unsupervised_Learning_of_3D_Point_Cloud_Cleaning_ICCV_2019_paper.html" target="_blank">ICCV 2019 Open Access Repository</a></p></div><div><ul>
<li data-line="0" dir="auto">使用三维电云进行降噪, 引入先验项克服无监督降噪对噪声的i.i.d假设. </li>
</ul></div><div><p dir="auto">使用知识蒸馏让网络学习 SOTA 的 Transformer based 的去噪网络的知识</p></div><div><p dir="auto">MixUP效果好不好? 对降噪效果有没有帮助</p></div><div class="mod-footer"></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="🔍-学术论文/图像还原(超分增强去噪恢复).html#图像还原(超分增强去噪恢复)"><div class="tree-item-contents heading-link" heading-name="图像还原(超分增强去噪恢复)"><span class="tree-item-title">图像还原(超分增强去噪恢复)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="🔍-学术论文/图像还原(超分增强去噪恢复).html#Denoising"><div class="tree-item-contents heading-link" heading-name="Denoising"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">Denoising</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item mod-collapsible" data-depth="3"><a class="tree-link" href="🔍-学术论文/图像还原(超分增强去噪恢复).html#微调CLIP镜像医学图像异常检测"><div class="tree-item-contents heading-link" heading-name="微调CLIP镜像医学图像异常检测"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">微调CLIP镜像医学图像异常检测</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="4"><a class="tree-link" href="🔍-学术论文/图像还原(超分增强去噪恢复).html#使用双流网络(Swin_Transfomer)_进行图像恢复(有监督)_-_2021"><div class="tree-item-contents heading-link" heading-name="使用双流网络(Swin Transfomer) 进行图像恢复(有监督) - 2021"><span class="tree-item-title">使用双流网络(Swin Transfomer) 进行图像恢复(有监督) - 2021</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="🔍-学术论文/图像还原(超分增强去噪恢复).html#Dehaze/blurry"><div class="tree-item-contents heading-link" heading-name="Dehaze/blurry"><span class="tree-item-title">Dehaze/blurry</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="🔍-学术论文/图像还原(超分增强去噪恢复).html#Super_Resolution"><div class="tree-item-contents heading-link" heading-name="Super Resolution"><span class="tree-item-title">Super Resolution</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="🔍-学术论文/图像还原(超分增强去噪恢复).html#Restoration"><div class="tree-item-contents heading-link" heading-name="Restoration"><span class="tree-item-title">Restoration</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>