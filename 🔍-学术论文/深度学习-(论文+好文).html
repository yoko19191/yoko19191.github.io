<!doctype html><html><head><title>深度学习 (论文+好文)</title><base href="../"><meta id="root-path" root-path="../"><link rel="icon" sizes="96x96" href="https://publish-01.obsidian.md/access/f786db9fac45774fa4f0d8112e232d67/favicon-96x96.png"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=yes,minimum-scale=1,maximum-scale=5"><meta charset="UTF-8"><link rel="stylesheet" href="lib/styles/obsidian-styles.css"><link rel="stylesheet" href="lib/styles/theme.css"><link rel="stylesheet" href="lib/styles/plugin-styles.css"><link rel="stylesheet" href="lib/styles/snippets.css"><link rel="stylesheet" href="lib/styles/generated-styles.css"><style></style><script type="module" src="lib/scripts/graph_view.js"></script><script src="lib/scripts/graph_wasm.js"></script><script src="lib/scripts/tinycolor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.2.4/pixi.min.js" integrity="sha512-Ch/O6kL8BqUwAfCF7Ie5SX1Hin+BJgYH4pNjRqXdTEqMsis1TUYg+j6nnI9uduPjGaj7DN4UKCZgpvoExt6dkw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="lib/scripts/webpage.js"></script><script src="lib/scripts/generated.js"></script></head><body class="theme-light show-inline-title ctp-latte ctp-mocha ctp-accent-light-sky ctp-accent-rosewater anuppuccin-accent-toggle anp-current-line anp-td-none anp-h1-red anp-h2-peach anp-h3-green anp-h4-teal anp-h5-lavender anp-h6-mauve anp-bold-red anp-italic-green anp-highlight-yellow anp-full-rainbow-color-toggle anp-default-tab loading"><div class="webpage-container"><div class="sidebar-left sidebar"><div class="sidebar-container"><div class="sidebar-sizer"><div class="sidebar-content-positioner"><div class="sidebar-content"><div><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="tree-container file-tree mod-nav-indicator" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">🌱 Digital-Garden</span><button class="clickable-icon collapse-tree-button is-collapsed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area"><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">🎨 文学艺术</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🎨-文学艺术/莎士比亚.html"><span class="tree-item-title">莎士比亚</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🎨-文学艺术/世界现代设计通识课.html"><span class="tree-item-title">世界现代设计通识课</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🎨-文学艺术/喜欢的音乐风格.html"><span class="tree-item-title">喜欢的音乐风格</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">🏛️ 哲学文化</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/白话黑格尔-总结.html"><span class="tree-item-title">白话黑格尔- 总结</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/佛教自学小计.html"><span class="tree-item-title">佛教自学小计</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/精神分析自学小记.html"><span class="tree-item-title">精神分析自学小记</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/康德的哲学观念.html"><span class="tree-item-title">康德的哲学观念</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/拉康研讨班20-笔记.html"><span class="tree-item-title">拉康研讨班20 笔记</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/马克思的商品价值和商品.html"><span class="tree-item-title">马克思的商品价值和商品</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/马克思自学小计.html"><span class="tree-item-title">马克思自学小计</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/西方哲学史小记.html"><span class="tree-item-title">西方哲学史小记</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/虚无主义是通往自由的必经之路.html"><span class="tree-item-title">虚无主义是通往自由的必经之路</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/虚无主义小计.html"><span class="tree-item-title">虚无主义小计</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏛️-哲学文化/哲学大问题.html"><span class="tree-item-title">哲学大问题</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">🏠 政治社会</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/可能性的艺术：比较政治学30讲-刘瑜.html"><span class="tree-item-title">可能性的艺术：比较政治学30讲 - 刘瑜</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/论犯罪与刑罚-笔记.html"><span class="tree-item-title">论犯罪与刑罚-笔记</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/女性主义-小记.html"><span class="tree-item-title">女性主义 - 小记</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/有关印度的疑问.html"><span class="tree-item-title">有关印度的疑问</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/增殖崇拜：一神教传统与消费主义.html"><span class="tree-item-title">增殖崇拜：一神教传统与消费主义</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/中国主要城市机会.html"><span class="tree-item-title">中国主要城市机会</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🏠-政治社会/资本论在21世纪.html"><span class="tree-item-title">资本论在21世纪</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">💪 自我成长</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💪-自我成长/《人类决策中的陷阱》读书笔记.html"><span class="tree-item-title">《人类决策中的陷阱》读书笔记</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💪-自我成长/爱欲分析.html"><span class="tree-item-title">爱欲分析</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💪-自我成长/查理芒格(25条人类误判心理学-+-穷查理宝典).html"><span class="tree-item-title">查理芒格(25条人类误判心理学 + 穷查理宝典)</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💪-自我成长/健身计划.html"><span class="tree-item-title">健身计划</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💪-自我成长/人际交流优化流程.html"><span class="tree-item-title">人际交流优化流程</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💪-自我成长/斯坦福大学人生规划课-stanford-open-office-hours_-dave-evans-and-bill-burnett.html"><span class="tree-item-title">斯坦福大学人生规划课 -Stanford Open Office Hours_ Dave Evans and Bill Burnett</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💪-自我成长/提供沟通能力的方法.html"><span class="tree-item-title">提供沟通能力的方法</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">💼 经济商业</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💼-经济商业/产业分析.html"><span class="tree-item-title">产业分析</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💼-经济商业/股市商战黑化.html"><span class="tree-item-title">股市商战黑化</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💼-经济商业/和chatgpt聊经济.html"><span class="tree-item-title">和ChatGPT聊经济</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💼-经济商业/快速调研方法论.html"><span class="tree-item-title">快速调研方法论</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💼-经济商业/商业点子王.html"><span class="tree-item-title">商业点子王</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💼-经济商业/商业计划书(bp)写作指南-李自然说.html"><span class="tree-item-title">商业计划书(BP)写作指南 - 李自然说</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">💾 科技工程</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/大模型全栈工程师.html"><span class="tree-item-title">大模型全栈工程师</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/计算机组成原理.html"><span class="tree-item-title">计算机组成原理</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/这就是chatgpt.html"><span class="tree-item-title">这就是ChatGPT</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/智能风控平台(架构、设计与实现).html"><span class="tree-item-title">智能风控平台(架构、设计与实现)</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/ai初级工程师面试.html"><span class="tree-item-title">AI初级工程师面试</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/cs224n-nlp-基础-(2021冬).html"><span class="tree-item-title">CS224N NLP 基础 (2021冬)</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💾-科技工程/leetcode-热题100.html"><span class="tree-item-title">Leetcode 热题100</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">🔍 学术论文</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🔍-学术论文/机器学习常用算法.html"><span class="tree-item-title">机器学习常用算法</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🔍-学术论文/深度学习-(论文+好文).html"><span class="tree-item-title">深度学习 (论文+好文)</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🔍-学术论文/阅读论文的方法论.html"><span class="tree-item-title">阅读论文的方法论</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">🔢 数学物理</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🔢-数学物理/和chatgpt聊数学.html"><span class="tree-item-title">和ChatGPT聊数学</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🔢-数学物理/如何使用信息论逼近wordle求解成绩的理论极限？.html"><span class="tree-item-title">如何使用信息论逼近Wordle求解成绩的理论极限？</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🔢-数学物理/causal-inference-(基于mooc+judea-pearl's-books).html"><span class="tree-item-title">Causal Inference (基于MOOC+Judea Pearl's Books)</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">🧑🏻‍💻 职业生涯</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧑🏻‍💻-职业生涯/职场小技巧.html"><span class="tree-item-title">职场小技巧</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧑🏻‍💻-职业生涯/职业规划.html"><span class="tree-item-title">职业规划</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧑🏻‍💻-职业生涯/pingpong-乒乓跨境支付服务.html"><span class="tree-item-title">PingPong 乒乓跨境支付服务</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">🧬 生物医疗</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/成瘾性药物总览.html"><span class="tree-item-title">成瘾性药物总览</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/改变心理学的40项研究.html"><span class="tree-item-title">改变心理学的40项研究</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/鬼谷的生物演化课.html"><span class="tree-item-title">鬼谷的生物演化课</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/脑科学小记.html"><span class="tree-item-title">脑科学小记</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/牛津大学生物学动画.html"><span class="tree-item-title">牛津大学生物学动画</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/人格心理学大纲(首都师范大学)-+-其他.html"><span class="tree-item-title">人格心理学大纲(首都师范大学) + 其他</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/生物信号分析处理.html"><span class="tree-item-title">生物信号分析处理</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🧬-生物医疗/植物分类与系统发育.html"><span class="tree-item-title">植物分类与系统发育</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-file" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="🌱-to-do-list.html"><span class="tree-item-title">🌱 To-do List</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="💡-brain's-buzz.html"><span class="tree-item-title">💡 Brain's buzz</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="📖-读书笔记.html"><span class="tree-item-title">📖 读书笔记</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="index.html"><span class="tree-item-title">index</span></a></div><div class="tree-item-children"></div></div></div></div></div></div></div></div><div class="sidebar-gutter"><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div></div><div class="document-container show"><div class="markdown-preview-view markdown-rendered node-insert-event is-readable-line-width allow-fold-headings show-indentation-guide allow-fold-lists show-properties" style="tab-size:4"><style id="MJX-CHTML-styles">mjx-mn{display:inline-block;text-align:left}mjx-msup{display:inline-block;text-align:left}mjx-msub{display:inline-block;text-align:left}mjx-texatom{display:inline-block;text-align:left}mjx-munderover{display:inline-block;text-align:left}mjx-munderover:not([limits=false]){padding-top:.1em}mjx-munderover:not([limits=false])>*{display:block}mjx-msubsup{display:inline-block;text-align:left}mjx-script{display:inline-block;padding-right:.05em;padding-left:.033em}mjx-script>mjx-spacer{display:block}mjx-msqrt{display:inline-block;text-align:left}mjx-root{display:inline-block;white-space:nowrap}mjx-surd{display:inline-block;vertical-align:top}mjx-sqrt{display:inline-block;padding-top:.07em}mjx-sqrt>mjx-box{border-top:.07em solid}mjx-sqrt.mjx-tall>mjx-box{padding-left:.3em;margin-left:-.3em}mjx-c.mjx-c1D438.TEX-I::before{padding:.68em .764em 0 0;content:"E"}mjx-c.mjx-c1D45D.TEX-I::before{padding:.442em .503em .194em 0;content:"p"}mjx-c.mjx-c1D45C.TEX-I::before{padding:.441em .485em .011em 0;content:"o"}mjx-c.mjx-c1D460.TEX-I::before{padding:.442em .469em .01em 0;content:"s"}mjx-c.mjx-c2C::before{padding:.121em .278em .194em 0;content:","}mjx-c.mjx-c32::before{padding:.666em .5em 0 0;content:"2"}mjx-c.mjx-c1D456.TEX-I::before{padding:.661em .345em .011em 0;content:"i"}mjx-c.mjx-c1D45B.TEX-I::before{padding:.442em .6em .011em 0;content:"n"}mjx-c.mjx-c31::before{padding:.666em .5em 0 0;content:"1"}mjx-c.mjx-c30::before{padding:.666em .5em .022em 0;content:"0"}mjx-c.mjx-c1D451.TEX-I::before{padding:.694em .52em .01em 0;content:"d"}mjx-c.mjx-c1D45A.TEX-I::before{padding:.442em .878em .011em 0;content:"m"}mjx-c.mjx-c1D452.TEX-I::before{padding:.442em .466em .011em 0;content:"e"}mjx-c.mjx-c1D459.TEX-I::before{padding:.694em .298em .011em 0;content:"l"}mjx-c.mjx-c2B::before{padding:.583em .778em .082em 0;content:"+"}mjx-c.mjx-c1D450.TEX-I::before{padding:.442em .433em .011em 0;content:"c"}mjx-c.mjx-c1D461.TEX-I::before{padding:.626em .361em .011em 0;content:"t"}mjx-c.mjx-c1D444.TEX-I::before{padding:.704em .791em .194em 0;content:"Q"}mjx-c.mjx-c1D43E.TEX-I::before{padding:.683em .889em 0 0;content:"K"}mjx-c.mjx-c1D43F.TEX-I::before{padding:.683em .681em 0 0;content:"L"}mjx-c.mjx-c1D465.TEX-I::before{padding:.442em .572em .011em 0;content:"x"}mjx-c.mjx-c2211.TEX-S2::before{padding:.95em 1.444em .45em 0;content:"∑"}mjx-c.mjx-c1D446.TEX-I::before{padding:.705em .645em .022em 0;content:"S"}mjx-c.mjx-c1D44E.TEX-I::before{padding:.441em .529em .01em 0;content:"a"}mjx-c.mjx-c1D45F.TEX-I::before{padding:.442em .451em .011em 0;content:"r"}mjx-c.mjx-c1D466.TEX-I::before{padding:.442em .49em .205em 0;content:"y"}mjx-c.mjx-cD7::before{padding:.491em .778em 0 0;content:"×"}mjx-c.mjx-c1D449.TEX-I::before{padding:.683em .769em .022em 0;content:"V"}mjx-c.mjx-c1D44B.TEX-I::before{padding:.683em .852em 0 0;content:"X"}mjx-c.mjx-c2211.TEX-S1::before{padding:.75em 1.056em .25em 0;content:"∑"}mjx-c.mjx-c34::before{padding:.677em .5em 0 0;content:"4"}mjx-c.mjx-c1D453.TEX-I::before{padding:.705em .55em .205em 0;content:"f"}mjx-c.mjx-c22C5::before{padding:.31em .278em 0 0;content:"⋅"}mjx-c.mjx-c1D447.TEX-I::before{padding:.677em .704em 0 0;content:"T"}mjx-c.mjx-c221A::before{padding:.8em .853em .2em 0;content:"√"}mjx-c.mjx-c1D458.TEX-I::before{padding:.694em .521em .011em 0;content:"k"}mjx-container[jax=CHTML]{line-height:0}mjx-container [space="1"]{margin-left:.111em}mjx-container [space="2"]{margin-left:.167em}mjx-container [space="3"]{margin-left:.222em}mjx-container [space="4"]{margin-left:.278em}mjx-container [space="5"]{margin-left:.333em}mjx-container [rspace="1"]{margin-right:.111em}mjx-container [rspace="2"]{margin-right:.167em}mjx-container [rspace="3"]{margin-right:.222em}mjx-container [rspace="4"]{margin-right:.278em}mjx-container [rspace="5"]{margin-right:.333em}mjx-container [size="s"]{font-size:70.7%}mjx-container [size=ss]{font-size:50%}mjx-container [size=Tn]{font-size:60%}mjx-container [size=sm]{font-size:85%}mjx-container [size=lg]{font-size:120%}mjx-container [size=Lg]{font-size:144%}mjx-container [size=LG]{font-size:173%}mjx-container [size=hg]{font-size:207%}mjx-container [size=HG]{font-size:249%}mjx-container [width=full]{width:100%}mjx-box{display:inline-block}mjx-block{display:block}mjx-itable{display:inline-table}mjx-row{display:table-row}mjx-row>*{display:table-cell}mjx-mtext{display:inline-block}mjx-mstyle{display:inline-block}mjx-merror{display:inline-block;color:red;background-color:#ff0}mjx-mphantom{visibility:hidden}mjx-assistive-mml{top:0;left:0;clip:rect(1px,1px,1px,1px);user-select:none;position:absolute!important;padding:1px 0 0!important;border:0!important;display:block!important;width:auto!important;overflow:hidden!important}mjx-assistive-mml[display=block]{width:100%!important}mjx-math{display:inline-block;text-align:left;line-height:0;text-indent:0;font-style:normal;font-weight:400;font-size:100%;letter-spacing:normal;border-collapse:collapse;overflow-wrap:normal;word-spacing:normal;white-space:nowrap;direction:ltr;padding:1px 0}mjx-container[jax=CHTML][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=CHTML][display=true][width=full]{display:flex}mjx-container[jax=CHTML][display=true] mjx-math{padding:0}mjx-container[jax=CHTML][justify=left]{text-align:left}mjx-container[jax=CHTML][justify=right]{text-align:right}mjx-mi{display:inline-block;text-align:left}mjx-c{display:inline-block}mjx-utext{display:inline-block;padding:.75em 0 .2em}mjx-mo{display:inline-block;text-align:left}mjx-stretchy-h{display:inline-table;width:100%}mjx-stretchy-h>*{display:table-cell;width:0}mjx-stretchy-h>*>mjx-c{display:inline-block;transform:scaleX(1)}mjx-stretchy-h>*>mjx-c::before{display:inline-block;width:initial}mjx-stretchy-h>mjx-ext{overflow:clip visible;width:100%}mjx-stretchy-h>mjx-ext>mjx-c::before{transform:scaleX(500)}mjx-stretchy-h>mjx-ext>mjx-c{width:0}mjx-stretchy-h>mjx-beg>mjx-c{margin-right:-.1em}mjx-stretchy-h>mjx-end>mjx-c{margin-left:-.1em}mjx-stretchy-v{display:inline-block}mjx-stretchy-v>*{display:block}mjx-stretchy-v>mjx-beg{height:0}mjx-stretchy-v>mjx-end>mjx-c{display:block}mjx-stretchy-v>*>mjx-c{transform:scaleY(1);transform-origin:left center;overflow:hidden}mjx-stretchy-v>mjx-ext{display:block;height:100%;box-sizing:border-box;border:0 solid transparent;overflow:visible clip}mjx-stretchy-v>mjx-ext>mjx-c::before{width:initial;box-sizing:border-box}mjx-stretchy-v>mjx-ext>mjx-c{transform:scaleY(500) translateY(.075em);overflow:visible}mjx-mark{display:inline-block;height:0}mjx-mfrac{display:inline-block;text-align:left}mjx-frac{display:inline-block;vertical-align:.17em;padding:0 .22em}mjx-frac[type="d"]{vertical-align:.04em}mjx-frac[delims]{padding:0 .1em}mjx-frac[atop]{padding:0 .12em}mjx-frac[atop][delims]{padding:0}mjx-dtable{display:inline-table;width:100%}mjx-dtable>*{font-size:2000%}mjx-dbox{display:block;font-size:5%}mjx-num{display:block;text-align:center}mjx-den{display:block;text-align:center}mjx-mfrac[bevelled]>mjx-num{display:inline-block}mjx-mfrac[bevelled]>mjx-den{display:inline-block}mjx-den[align=right],mjx-num[align=right]{text-align:right}mjx-den[align=left],mjx-num[align=left]{text-align:left}mjx-nstrut{display:inline-block;height:.054em;width:0;vertical-align:-.054em}mjx-nstrut[type="d"]{height:.217em;vertical-align:-.217em}mjx-dstrut{display:inline-block;height:.505em;width:0}mjx-dstrut[type="d"]{height:.726em}mjx-line{display:block;box-sizing:border-box;min-height:1px;height:.06em;border-top:.06em solid;margin:.06em -.1em;overflow:hidden}mjx-line[type="d"]{margin:.18em -.1em}mjx-mrow{display:inline-block;text-align:left}mjx-c::before{display:block;width:0}.MJX-TEX{font-family:MJXZERO,MJXTEX}.TEX-B{font-family:MJXZERO,MJXTEX-B}.TEX-I{font-family:MJXZERO,MJXTEX-I}.TEX-MI{font-family:MJXZERO,MJXTEX-MI}.TEX-BI{font-family:MJXZERO,MJXTEX-BI}.TEX-S1{font-family:MJXZERO,MJXTEX-S1}.TEX-S2{font-family:MJXZERO,MJXTEX-S2}.TEX-S3{font-family:MJXZERO,MJXTEX-S3}.TEX-S4{font-family:MJXZERO,MJXTEX-S4}.TEX-A{font-family:MJXZERO,MJXTEX-A}.TEX-C{font-family:MJXZERO,MJXTEX-C}.TEX-CB{font-family:MJXZERO,MJXTEX-CB}.TEX-FR{font-family:MJXZERO,MJXTEX-FR}.TEX-FRB{font-family:MJXZERO,MJXTEX-FRB}.TEX-SS{font-family:MJXZERO,MJXTEX-SS}.TEX-SSB{font-family:MJXZERO,MJXTEX-SSB}.TEX-SSI{font-family:MJXZERO,MJXTEX-SSI}.TEX-SC{font-family:MJXZERO,MJXTEX-SC}.TEX-T{font-family:MJXZERO,MJXTEX-T}.TEX-V{font-family:MJXZERO,MJXTEX-V}.TEX-VB{font-family:MJXZERO,MJXTEX-VB}mjx-stretchy-h mjx-c,mjx-stretchy-v mjx-c{font-family:MJXZERO,MJXTEX-S1,MJXTEX-S4,MJXTEX,MJXTEX-A!important}@font-face{font-family:MJXZERO;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff")}@font-face{font-family:MJXTEX;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-B;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff")}@font-face{font-family:MJXTEX-I;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff")}@font-face{font-family:MJXTEX-MI;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff")}@font-face{font-family:MJXTEX-BI;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff")}@font-face{font-family:MJXTEX-S1;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-S2;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-S3;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-S4;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-A;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-C;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-CB;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff")}@font-face{font-family:MJXTEX-FR;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-FRB;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff")}@font-face{font-family:MJXTEX-SS;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-SSB;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff")}@font-face{font-family:MJXTEX-SSI;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff")}@font-face{font-family:MJXTEX-SC;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-T;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-V;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-VB;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff")}mjx-c.mjx-c1D443.TEX-I::before{padding:.683em .751em 0 0;content:"P"}mjx-c.mjx-c28::before{padding:.75em .389em .25em 0;content:"("}mjx-c.mjx-c1D434.TEX-I::before{padding:.716em .75em 0 0;content:"A"}mjx-c.mjx-c7C::before{padding:.75em .278em .249em 0;content:"|"}mjx-c.mjx-c1D435.TEX-I::before{padding:.683em .759em 0 0;content:"B"}mjx-c.mjx-c29::before{padding:.75em .389em .25em 0;content:")"}mjx-c.mjx-c3D::before{padding:.583em .778em .082em 0;content:"="}mjx-c.mjx-c2217::before{padding:.465em .5em 0 0;content:"∗"}mjx-c.mjx-c2223::before{padding:.75em .278em .249em 0;content:"∣"}</style><div class="markdown-preview-sizer markdown-preview-section" style="min-height:4976px"><div class="markdown-preview-pusher" style="width:1px;height:.1px;margin-bottom:0"></div><div class="mod-header"><div class="inline-title" data-heading="深度学习 (论文+好文)" id="深度学习_(论文+好文)" style="display:block">深度学习 (论文+好文)</div></div><div><p><a href="#计算机" class="tag" target="_blank" rel="noopener">#计算机</a> <a href="#人工智能" class="tag" target="_blank" rel="noopener">#人工智能</a> <a href="#实用" class="tag" target="_blank" rel="noopener">#实用</a> <a href="#小记" class="tag" target="_blank" rel="noopener">#小记</a></p></div><div class="heading-wrapper"><h2 data-heading="基本算法" class="heading" id="基本算法"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>基本算法<div class="heading-after">...</div></h2><div class="heading-children"><div class="heading-wrapper"><h3 data-heading="反向传播" class="heading" id="反向传播"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>反向传播<div class="heading-after">...</div></h3><div class="heading-children"></div></div><div class="heading-wrapper"><h3 data-heading="优化器" class="heading" id="优化器"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>优化器<div class="heading-after">...</div></h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/323747423/answer/2576604040" rel="noopener" class="external-link" href="https://www.zhihu.com/question/323747423/answer/2576604040" target="_blank">如何理解Adam算法(Adaptive Moment Estimation)？ - 知乎</a></p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="基本概念" class="heading" id="基本概念"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>基本概念<div class="heading-after">...</div></h2><div class="heading-children"><div><ul><li data-line="0">过拟合/欠拟合</li><li data-line="1">学习率</li><li data-line="2">超参</li><li data-line="3">损失函数</li><li data-line="4">参数共享</li></ul></div></div></div><div class="heading-wrapper"><h2 data-heading="优化器" class="heading" id="优化器"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>优化器<div class="heading-after">...</div></h2><div class="heading-children"><div class="heading-wrapper"><h3 data-heading="正则化" class="heading" id="正则化"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>正则化<div class="heading-after">...</div></h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6" rel="noopener" class="external-link" href="https://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6" target="_blank">Different Normalization Layers in Deep Learning | by Nilesh Vijayrania | Towards Data Science</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/680237923?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1735297419670257664&amp;utm_source=io.raindrop.raindropio" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/680237923?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1735297419670257664&amp;utm_source=io.raindrop.raindropio" target="_blank"># L2 正则化比大多数人想象的更加神奇</a></p></div><div><ul><li data-line="0"><code>batch normalizatoin</code><ul><li data-line="1">对不同序列中同一位置的数据进行归一化操作, CNN常用</li></ul></li><li data-line="2"><code>layer normalization</code><ul><li data-line="3">对相同序列中不同位置的数据进行归一化操作, Transforme常用(便于处理不定长)</li></ul></li></ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="编年史" class="heading" id="编年史"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>编年史<div class="heading-after">...</div></h2><div class="heading-children"><div><ul><li data-line="0"><strong>AlexNet" - "ImageNet Classification with Deep Convolutional Neural Networks" by Alex Krizhevsky, Ilya Sutskever, and Geoffrey H. Hinton (2012)</strong></li><li data-line="1"><strong>"VGGNet" - "Very Deep Convolutional Networks for Large-Scale Image Recognition" by Karen Simonyan and Andrew Zisserman (2014)</strong></li><li data-line="2"><strong>"GoogLeNet" (Inception) - "Going Deeper with Convolutions" by Szegedy et al. (2014)</strong></li><li data-line="3"><strong>"ResNet" - "Deep Residual Learning for Image Recognition" by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun (2015)</strong></li><li data-line="4"><strong>"Transformer" - "Attention Is All You Need" by Vaswani et al. (2017)</strong></li><li data-line="5"><strong>"BERT" - "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" by Jacob Devlin et al. (2018)</strong></li></ul></div></div></div><div class="heading-wrapper"><h2 data-heading="CNN" class="heading" id="CNN"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>CNN<div class="heading-after">...</div></h2><div class="heading-children"></div></div><div class="heading-wrapper"><h2 data-heading="RNN" class="heading" id="RNN"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>RNN<div class="heading-after">...</div></h2><div class="heading-children"><div><div data-callout-metadata="" data-callout-fold="" data-callout="faq" class="callout node-insert-event drop-shadow"><div class="callout-title"><div class="callout-icon"><svg height="16" style="width:16px;max-width:100%"></svg></div><div class="callout-title-inner">相比Transformer, RNN的优势在于</div></div><div class="callout-content"><ul><li data-line="1">串行结构天然更加适合于序列数据</li><li data-line="2">推理更高效 复杂度为 O(N), 而Transformer 为 O(N^2)</li></ul></div></div></div><div><p>RNN 接受两个输入：State和Token。它一次通过输入序列一个Token，每个Token更新状态。例如，我们可以使用 RNN 将文本处理成单个状态向量。然后，这可用于将文本分类为“正面”或“负面”。或者我们可以使用最终状态来预测下一个Token，这就是 RNN 用于生成文本的方式。</p></div><div class="heading-wrapper"><h3 data-heading="LSTM、GRU" class="heading" id="LSTM、GRU"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>LSTM、GRU<div class="heading-after">...</div></h3><div class="heading-children"></div></div><div class="heading-wrapper"><h3 data-heading="RWKV" class="heading" id="RWKV"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>RWKV<div class="heading-after">...</div></h3><div class="heading-children"><div><p><img src="https://pic1.zhimg.com/v2-34943728e7ffdb60829f6a82fc10c610_r.jpg" referrerpolicy="no-referrer"><br><img alt="GPT_versus_RWKV.svg" src="https://rwkv-wiki.github.io/img/GPT_versus_RWKV.svg" referrerpolicy="no-referrer"><br><a data-tooltip-position="top" aria-label="https://rwkv-wiki.github.io/" rel="noopener" class="external-link" href="https://rwkv-wiki.github.io/" target="_blank">RWKV Wiki - RWKV Wiki</a><br><a data-tooltip-position="top" aria-label="https://wiki.rwkv.com/advance/architecture.html#how-does-rwkv-differ-from-classic-rnn" rel="noopener" class="external-link" href="https://wiki.rwkv.com/advance/architecture.html#how-does-rwkv-differ-from-classic-rnn" target="_blank">RWKV Architecture</a><br><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/514840332" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/514840332" target="_blank">RWKV-v2-RNN 原理：超越 Transformer，实现 O(T) 的语言建模 - 知乎</a><br><a data-tooltip-position="top" aria-label="https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py" rel="noopener" class="external-link" href="https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py" target="_blank">ChatRWKV/RWKV_in_150_lines.py at main · BlinkDL/ChatRWKV · GitHub</a></p></div><div><ul><li data-line="0"><strong>RWKV</strong>&nbsp;是一种拥有RNN, CNN, Transformer三种优点的RNN网络. 在性能媲美Transformer的基础上，具有O(1)推理复杂度，更易收敛训练，模型参数和内存占用<ul><li data-line="1">inspired by AFT（Attention-Free Transformer）</li></ul></li><li data-line="2">背景: 所有Self-Attention 为基础的模型, 都不肯避免的需要Token之间相互计算, 造成了 O(n^2) 的复杂度; 且位置信息外挂</li><li data-line="3">RWKV针对这两个问题, 使用<ul><li data-line="4">WKV 计算过程直接向 Token 引入了具有平移不变性的位置编码，不需要引入额外的位置编码。</li><li data-line="5">使用两个RWKV层替换了Multi-head Attention 和 前馈网络</li><li data-line="6">Token 之间无需相互运算，WKV 计算过程只对各 Token 分别变换并累加结果, 成就了 O(n)的复杂度</li></ul></li><li data-line="7">RNN 在训练时并行<img alt="image.png" src="https://cdn.sa.net/2024/01/29/jgsuCJzViMv2PEK.png" referrerpolicy="no-referrer"><ul><li data-line="8"></li></ul></li><li data-line="9">RNN 在推理时串行</li></ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="Transformer" class="heading" id="Transformer"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>Transformer<div class="heading-after">...</div></h2><div class="heading-children"><div class="admonition-parent admonition-tip-parent"><div class="callout admonition admonition-tip admonition-plugin" style="--callout-color:0,191,165" data-callout="tip" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title"><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="fire" class="svg-inline--fa fa-fire fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M216 23.86c0-23.8-30.65-32.77-44.15-13.04C48 191.85 224 200 224 288c0 35.63-29.11 64.46-64.85 63.99-35.17-.45-63.15-29.77-63.15-64.94v-85.51c0-21.7-26.47-32.23-41.43-16.5C27.8 213.16 0 261.33 0 320c0 105.87 86.13 192 192 192s192-86.13 192-192c0-170.29-168-193-168-296.14z"></path></svg></div><div class="callout-title-inner admonition-title-content">Tip</div></div><div class="callout-content admonition-content"><p>历史</p><ul><li>注意力机制最早由Google团队在2014年<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1406.6247" rel="noopener" class="external-link" href="https://arxiv.org/abs/1406.6247" target="_blank"> Recurrent Models of Visual Attention</a>论文中提出, 使用RNN + 注意力机制对图片进行分类.</li><li><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1409.0473" rel="noopener" class="external-link" href="https://arxiv.org/abs/1409.0473" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a> 在同年, 将注意力机制引入NLP, 在机器翻译任务上将翻译和对其同时进行. 点燃了注意力机制在基于RNN/CNN的NLP任务的大放异彩.</li><li>2017年Google团队发表 Attention is all you need, 首次提出自注意力机制, 允许模型在序列中的不同位置之间建立动态关系</li><li>自2017年Transformer提出以后，关于Transformer模型结构的改进层出不穷，比如语音识别的Conformer、Branchformer、E-Branchformer、R-Transformer 等等</li><li>目前(2023), Transformer已经成为了自然语言处理领域绝对主流的网络架构</li><li></li></ul></div></div></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="faq" class="callout node-insert-event drop-shadow"><div class="callout-title"><div class="callout-icon"><svg height="16" style="width:16px;max-width:100%"></svg></div><div class="callout-title-inner">Transformer vs. RNN</div></div><div class="callout-content"><p>相比于RNN, Transformer 的优势主要来自于</p><ul><li data-line="2">由于注意力机制将序列中所有位置的信息都一视同仁地看待（除了表征位置信息的位置编码以外）, 而带来的 <span style="background:#fff88f">全局建模能力</span></li><li data-line="3">由于自注意力机制中的点积运算更加适合 GPU训练和Transformer 本身的并行性, 所导致的能够<span style="background:#fff88f">更好提高训练效率</span>和<span style="background:#fff88f">训练参数的scalability</span></li></ul></div></div></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="note" class="callout node-insert-event drop-shadow"><div class="callout-title"><div class="callout-icon"><svg height="16" style="width:16px;max-width:100%"></svg></div><div class="callout-title-inner"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/676892576" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/676892576" target="_blank">Transformer架构的局限已凸显，被取代还有多久？ - 知乎</a></div></div></div></div><div><p><a data-tooltip-position="top" aria-label="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener" class="external-link" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank">2017 - Attention Is All You Need.pdf</a><br><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Ja4y1B7zC/?spm_id_from=333.788.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Ja4y1B7zC/?spm_id_from=333.788.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">[重置版]从零实现transfomer模型 || 理解ChatGPT基石 || pytorch_哔哩哔哩_bilibili</a><br><a data-tooltip-position="top" aria-label="https://paperswithcode.com/paper/attention-is-all-you-need" rel="noopener" class="external-link" href="https://paperswithcode.com/paper/attention-is-all-you-need" target="_blank">Attention Is All You Need | Papers With Code</a><br><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/648127076?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1716622508470669312&amp;utm_source=io.raindrop.raindropio" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/648127076?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1716622508470669312&amp;utm_source=io.raindrop.raindropio" target="_blank">三万字最全解析！从零实现Transformer（小白必会版😃） - 知乎</a><br><a data-tooltip-position="top" aria-label="https://www.zhihu.com/zvideo/1722621858900451329?utm_psn=1733842036934078467" rel="noopener" class="external-link" href="https://www.zhihu.com/zvideo/1722621858900451329?utm_psn=1733842036934078467" target="_blank">哈工大PHD竟把Transformer讲的如此简单！ - 知乎</a><br><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/589738603/answer/3375664602" rel="noopener" class="external-link" href="https://www.zhihu.com/question/589738603/answer/3375664602" target="_blank">为什么只有基于Transformer的大模型，而没有其他的？ - 知乎</a></p></div><div><p><strong>"Attention is All You Need"</strong> 是一篇由Google Brain团队的研究员在2017年撰写的论文，这篇论文首次提出了 Transformer 模型, 使用自注意力机制(Self-attention将序列问题转换为全连接层问题, 使模型能够在不同位置之间建立动态的关系，避免了传统循环神经网络（RNN）和长短时记忆网络（LSTM）的局限(难以捕捉长序列之间关系, 容易梯度爆炸/消失, 序列结构难以并行, 模型参数随序列长度指数增长, 难以批处理). 强力推动了自然语言处理, 计算机视觉等多</p></div><div><p>这是 Transformer - model architecture, 其中有两个最关键关键组件. 多头注意力机制, 位置掩码.<br><img alt="image.png" src="https://cdn.sa.net/2024/01/22/hJ9e2S1ZvRHkOU8.png" referrerpolicy="no-referrer"></p></div><div><ul><li data-line="0"><p>Embedding 输入</p><ul><li data-line="1">经过Tokenizer之后的RAW数据变为Embedding</li><li data-line="2">为了让模型批量处理不同长度的音频，我们将同一个批次中的输入填充 (padding) 到同样长度</li></ul></li><li data-line="3"><p><strong>位置掩码(Positional Encoding)</strong></p><ul><li data-line="4">由于Transfomer没有像RNN那样隐含顺序信息, 所以位置掩码被添加到Embedding, 允许模型学到每个Embedding的位置和相对位置(无序数据)信息. 在这之前, 一般需要将相对位置和绝对位置直接注入到模型</li><li data-line="5">位置掩码和Embedding的维度相同, 两者直接相加. 具体的掩盖设计有如下两种<ul><li data-line="6">在模型输入位置添加一个可训练的层</li><li data-line="7">或者更加常用的是使用三角函数来编码位置信息<br><span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left:0;margin-right:0"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msup><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-script style="vertical-align:.53em"><mjx-mfrac size="s"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msub size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align:-.34em"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-script></mjx-msup></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span><span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left:0;margin-right:0"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msup><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-script style="vertical-align:.53em"><mjx-mfrac size="s"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msub size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align:-.34em"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-script></mjx-msup></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span>其中, pos是输入序列最长长度, i 是序列中位置, d_model是Embedding的维度</li></ul></li><li data-line="11">位置掩码扩容方法<ul><li data-line="12">简单外推(Extrapolation): 提前预留维度并设为0. 但是可能导致外推维度的训练不充分, 导致外推位置启用后模型性能严重下降.</li><li data-line="13">线性内推: 将数和输之间的区间变小(e.g. 1,2,3.... 1.5,2.2.5,3). 模型学习的特征不一样了, 需要微调让模型重新学习拥挤的映射关系.</li><li data-line="14">进制转换: 比如将10进制变为16进制, 表示数量范围变大. 数值范围的天花板变大, 模型一般有泛化能力. 或者将更低进制减少变化程度.</li></ul></li></ul></li><li data-line="15"><p><strong>自注意力机制(Self-attention)</strong> 是整个架构的基础 <span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left:0;margin-right:0"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munderover space="4"><mjx-over style="padding-bottom:.192em;padding-left:.279em"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-over><mjx-box><mjx-munder><mjx-row><mjx-base><mjx-mo class="mjx-lop"><mjx-c class="mjx-c2211 TEX-S2"></mjx-c></mjx-mo></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top:.167em;padding-left:.148em"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-under></mjx-row></mjx-munder></mjx-box></mjx-munderover><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span><img alt="image.png" src="https://cdn.sa.net/2024/01/23/MdOrH9oNbQRCKZz.png" referrerpolicy="no-referrer"></p><ul><li data-line="17" class="lc-list-callout" style="--lc-callout-color:0,184,212"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> Embedding 经过 Postion Encoding的修饰之后输入到自注意力机制当中</span></li><li data-line="18" class="lc-list-callout" style="--lc-callout-color:0,184,212"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 输入经过三个不同的全连接层 <code>nn.Linear</code> 获得和Embedding维度相同的 <code>query</code>, <code>key</code>, <code>value</code> 三个矩阵.</span><ul><li data-line="19">使用三个不同全连接层是因为独立层能够使得QKV保持独立, 使得更它们更有效地捕捉不同类型的信息和关系</li><li data-line="20" class="lc-list-callout" style="--lc-callout-color:255,214,0"><span class="lc-li-wrapper"><span class="lc-list-marker">&amp;</span> QK必须相同维度, 因为要点积. V不一定</span></li><li data-line="21">&nbsp;注意力机制本身并没有对&nbsp;QKV 的内容做出任何限制我们可以这样理解. 我们希望用Q 把 V 中的东西找出来, 而 K 是 V 的钥匙. 如果 Q和K的匹配度越高, 那么就可以在 K 这个位置对应的 V 中找出更多Q要在V中查询的信息.</li><li data-line="22">比如，我们现在希望计算音频和文本之间的注意力，或者说希望从音频特征中提取和文本相关的信息，那么这个时候应该将文本特征作为&nbsp;Q&nbsp;，音频特征作为&nbsp;K&nbsp;和&nbsp;V&nbsp;（<strong>交叉注意力机制</strong>）；又比如，我们希望计算文本和文本自身的注意力，那么就应该将文本特征同时作为&nbsp;，QKV.&nbsp;（<strong>自注意力机制</strong>）</li></ul></li><li data-line="23" class="lc-list-callout" style="--lc-callout-color:0,184,212"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 计算 <code>query</code> 和 <code>key</code> 之间的相似度(点积、拼接、感知机). 以下以点积为例. <img alt="image.png" src="https://cdn.sa.net/2024/01/22/z5lTBPSoeMfd12j.png" referrerpolicy="no-referrer"></span><ul><li data-line="24"><code>query</code> 先和 <code>key</code> 点积用于衡量每个词（查询）与句子中每个其他词（键）的相关性.</li><li data-line="25">然后，应用缩放因子对这些点积进行缩放。通常，这个缩放因子是键向量维度的平方根的倒数</li><li data-line="26">最后, 使用softmax函数将其归一化为概率. 这个softmax之后的概率表示该个Embedding在该位置的重要程度.</li><li data-line="27" class="lc-list-callout" style="--lc-callout-color:255,145,0"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> 矩阵点积为什么可以计算相似度? 因为两矩阵方向相同, 点积较大, 方向越垂直, 点积越小. 在几何上等同于计算两个向量的长度乘积和它们之间夹角余弦的乘积. 同时考虑了矩阵的长度和方向. 同时在GPU运算更加高效</span></li><li data-line="28" class="lc-list-callout" style="--lc-callout-color:255,23,68"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 自注意力机制本质上是求一个离散概率的数学期望. <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munderover space="4" limits="false"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2211 TEX-S1"></mjx-c></mjx-mo><mjx-script style="vertical-align:-.285em;margin-left:0"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom><mjx-spacer style="margin-top:.291em"></mjx-spacer><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-munderover><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align:-.15em;margin-left:-.024em"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align:-.15em"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span>; 离散分布P是softmax之后的值, X 是 V.</span></li></ul></li><li data-line="29" class="lc-list-callout" style="--lc-callout-color:0,184,212"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 最后, softmax后的值 和 <code>value</code> 相乘. 用于增强该位置Embedding重要性或者减少重要性</span></li><li data-line="30">🌰 : “I eat pizza today”<ul><li data-line="31">每个Token(I, eat, pizza, today)先转换为 Embedding</li><li data-line="32">对于每个Token我们使用三个不同的全连接层来生成QKV. 例如，“I”会生成一个Q向量、一个K向量和一个V向量。</li><li data-line="33">接下来，我们计算每个词的Q向量与所有词的K向量的点积。这相当于在评估句子中的每个词与其他每个词之间的关系。例如，计算“I”的Q向量与“I”，“eat”，“pizza”，“today”的K向量的点积</li><li data-line="34">之后对点积应用缩放因子，并使用softmax函数进行归一化，得到注意力权重。</li><li data-line="35">每个Token的Softmax值的V向量进行加权求和。这个加权和代表了考虑到了整个句子上下文的当前词的表示。例如，对于“I”，我们将它的注意力权重与所有词的V向量相乘，并加总起来，得到一个新的加权向量，这个向量就是考虑了整个句子上下文的“I”的表示。</li></ul></li><li data-line="36" class="lc-list-callout" style="--lc-callout-color:0,200,83"><span class="lc-li-wrapper"><span class="lc-list-marker">$</span> 根据具体任务的需要, 我们会添加掩码(Mask). 其作用是通过修改注意力权重, 以避免不不要或不需要的信息传递到模型中. 比如, 在在翻译任务中我们不希望Encoder访问到句子后面的内容, 我们就可以用Mask遮蔽避免泄漏. 或者在处理变长序列中补全长度(padding)</span></li></ul></li><li data-line="37"><p><strong>多头注意力机制(Multi-head Attention)</strong><img alt="image.png" src="https://cdn.sa.net/2024/01/22/9teJRY7VQnXATIE.png" referrerpolicy="no-referrer"></p><ul><li data-line="38">多头注意力机制允许模型在不同子空间中学习到相关信息, 以捕捉数据的不同方面. 提高并行度并简化模型的计算复杂度.</li><li data-line="39">将 QKV 分别用h个不同投影矩阵投影h次, 然后分别做点积注意力, 最后将每个头计算出的加权和简单拼接回一个完整的向量后通过一个全连接层产生最终的输出<ul><li data-line="40">相当于人从不同的视角观察一个物体</li></ul></li><li data-line="41" class="lc-list-callout" style="--lc-callout-color:255,23,68"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 《Are sixteen heads really better than one?》多头自注意力机制不一定比单头好</span></li><li data-line="42" class="lc-list-callout" style="--lc-callout-color:255,23,68"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 《Low-Rank Bottleneck in Multi-head Attention Models》指出Multi-Head Attention的表达能力瓶颈，并提出增大key_size来缓解。</span><ul><li data-line="43">我们将 QK维度称为 key_size, V的维度为 head_size</li><li data-line="44">h 是 注意力头的数量. 一般实际是将原始d维度的QKV投影到 d/h)维中, 单独计算后输出 d/h 的结果. 然后将 h*d/h 拼接起来获得最终注意力值</li><li data-line="45"><span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align:.363em;margin-left:.052em"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msqrt size="s"><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top:.082em"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align:-.15em"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span>可以看作一个二元联合分布. 假设序列长度为 n. 因为 QK维度一样所以有这个分布有 n*n = <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align:.363em"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container></span>的值</li><li data-line="46">但是将QK投影之后, 各自分布只有 n*(d/h) 的. 总的参数量为 2nd/h &lt;&lt; n^2. 参数量的减少导致表达能力的削弱. 这就是Low-rank bottleneck, 尤其是 h 较多时.这个叫低秩瓶颈.</li><li data-line="47">我们可以增加 d 的维度, 但是这会增加模型的复杂性. 或者减少 h, 但是多头本身就可以增加模型的表达能力</li><li data-line="48">我们可以通过只增加 key_size 以增加模型的表达能力, 而尽可能不增加模型的复杂性</li></ul></li><li data-line="49" class="lc-list-callout" style="--lc-callout-color:255,23,68"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 《Talking-head attention》, 将多头的低秩分布叠加(在softmax之前叠加, e.g.加权平均)增强模型的表达能力. 原理是多个高斯分布(GMM)叠加的数量够多, 就可以逼近任意概率分布.</span></li></ul></li><li data-line="50" class="lc-list-callout" style="--lc-callout-color:124,77,255"><span class="lc-li-wrapper"><p><span class="lc-list-marker">~</span> 爱因斯坦求和约定（Einstein Summation Convention）<code>torch.ensum</code> 是一种表达多重求和（比如矩阵乘法、点积、批量矩阵乘法等）和某些类型的数据重排操作的简介写法. 比如</p></span><ul><li data-line="51"><strong>矩阵乘法：</strong> 例如，矩阵乘法 <code>AB</code> 可以表示为 <code>torch.einsum('ij,jk-&gt;ik', [A, B])</code>，其中 <code>A</code> 是一个形状为 <code>(i, j)</code> 的矩阵，<code>B</code> 是一个形状为 <code>(j, k)</code> 的矩阵。</li><li data-line="52"><strong>批量矩阵乘法：</strong> 对于批量矩阵乘法，比如有批次的两个矩阵 <code>A</code> 和 <code>B</code>，其操作可以表示为 <code>torch.einsum('bij,bjk-&gt;bik', [A, B])</code>。</li><li data-line="53"><strong>求和操作：</strong> 如果你想对一个矩阵的行进行求和，可以使用 <code>torch.einsum('ij-&gt;i', A)</code>。</li></ul></li><li data-line="54" class="lc-list-callout" style="--lc-callout-color:0,200,83"><span class="lc-li-wrapper"><p><span class="lc-list-marker">$</span> 这里用序列重新梳理一下Attention的公式<img alt="gpt-transformer.png" src="https://cdn.sa.net/2024/01/24/J7fCRGVWFL5EabS.png" referrerpolicy="no-referrer"></p></span><ul><li data-line="55">令 <code>F[t]</code> 为 t时刻的系统状态(高维状态); 令 <code>x[t]</code> 为 t 时刻的外部输入信息状态;<ul><li data-line="56">预测 <code>F[t+1]</code> 时，需考虑 <code>F[0]</code>,<code>F[1]</code>, .. <code>F[t]</code>。因此，生成长度 T 的序列，需 <code>O(T^2)</code> 复杂度。</li></ul></li><li data-line="57">那么Attention可以简化为 <img alt="image.png" src="https://cdn.sa.net/2024/01/25/kvG1ogqlYeRINpH.png" referrerpolicy="no-referrer"><ul><li data-line="58">每个状态 &nbsp;i 对于后续的潜在贡献是&nbsp;<strong>V</strong>F[i]</li><li data-line="59">用&nbsp;<strong>Q</strong>x[t] 矢量，与此前的所有&nbsp;<strong>K</strong>F[i] 矢量分别做点乘，再 exp，得到 x[t] 与之前各个 F[i] 状态的匹配度。</li><li data-line="60">如果匹配度&nbsp;exp⁡(Q x[t]∗ K F[i])&nbsp;越大，<strong>V</strong>F[i] 的权重越大</li><li data-line="61">分母为归一化因子。</li><li data-line="62" class="lc-list-callout" style="--lc-callout-color:255,23,68"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 公式中没有显式出现 t 与 i 的距离信息。我们会采用其它方式（例如位置编码）将其注入系统。</span></li></ul></li></ul></li></ul></div><div><p>我们这里实现一个带Mask和多头的 <code>SelfAttention</code> 的 pytorch示例代码:</p></div><div><p>编码器-解码器相关论文起源于 <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1409.3215" rel="noopener" class="external-link" href="https://arxiv.org/abs/1409.3215" target="_blank">Sequence to Sequence Learning with Neural Networks</a><br>Transformer 完全基于注意力机制, 拥有编码器(Encoder)和解码器(Decoder)两个部分, 两者时常搭配使用, 但用途和模型结构相互独立, 也没有规定网络架构. 你可以编码器用Transformer解码器用LSTM...<br><img src="https://pic3.zhimg.com/v2-c7d7c326d453fa1d143c35abc543cb3a_r.jpg" referrerpolicy="no-referrer"></p></div><div><ul><li data-line="0"><strong>编码器(Encoder)</strong> 负责特征编码，即从原始的、比较底层的输入序列信号提取出抽象的、具有明显语义特征的特征信息. 由图可知是由 特征编码、位置编码、和若干个 TransformerBlock 组成(注意力+MLP+*归一化+*残差)</li><li data-line="1"><strong>解码器(Decoder)</strong> 负责从编码器得到的原始序列的特征信息中"破译"出目标序列的内容（比如从音频序列的特征中"破译"其对应的文本信息).</li><li data-line="2" class="lc-list-callout" style="--lc-callout-color:255,23,68"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 这种架构提出的原因是因为, 类似RNN和LSTM这样的模型输入-输出是对应的. 但在有的任务中我们不希望输入-输出的长度相同.</span></li><li data-line="3"><strong>信息流</strong>：输入序列首先进入编码器，经过一系列变换后，转换为一组高维表示。这些表示然后被传递到解码器，解码器利用这些信息以及自身的前一步输出来逐步生成最终的输出序列。</li></ul></div><div><p>我们在这里结合之前的<code>SelfAttention</code>代码, 构建一个 <code>TransformerBlock</code> , 后搭建Encoder和Decoder.</p></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="faq" class="callout node-insert-event drop-shadow"><div class="callout-title"><div class="callout-icon"><svg height="16" style="width:16px;max-width:100%"></svg></div><div class="callout-title-inner">为什么GPT要使用Decoder-only架构 而非 BERT的Encoder-only 架构?</div></div><div class="callout-content"><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/588325646/answer/2929224109" rel="noopener" class="external-link" href="https://www.zhihu.com/question/588325646/answer/2929224109" target="_blank">为什么现在的LLM都是Decoder only的架构？ - 知乎</a><br><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/608929992/answer/3119081297" rel="noopener" class="external-link" href="https://www.zhihu.com/question/608929992/answer/3119081297" target="_blank">decoder-only和encoder-decoder transformer在应用时最大的区别是？ - 知乎</a><br><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/642923989" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/642923989" target="_blank">LLM的3种架构：Encoder-only、Decoder-only、encode-decode - 知乎</a></p><ul><li data-line="4">Encoder-Decoder架构<ul><li data-line="5"><strong>适用任务</strong>: 这种架构非常适合于那些需要理解输入和生成输出的任务，如机器翻译、文本摘要等。在这些任务中，模型需要首先理解输入文本（通过Encoder部分），然后基于这个理解生成新的文本（通过Decoder部分）。</li><li data-line="6"><strong>代表性模型</strong>: 最著名的例子是原始的Transformer模型，它首次在论文《Attention is All You Need》中被提出。这个模型在机器翻译任务中取得了显著的成效。</li></ul></li><li data-line="7">Encoder-only架构<ul><li data-line="8"><strong>适用任务</strong>: 这种架构适用于只需理解输入文本的任务，比如文本分类、情感分析、命名实体识别等。在这些任务中，模型的目标是分析和理解输入，而不需要生成任何新的文本。</li><li data-line="9"><strong>代表性模型</strong>: BERT（Bidirectional Encoder Representations from Transformers）是这一类架构中最著名的例子。通过预训练一个大型的语料库，BERT学会了理解语言的上下文，可以被用于各种不同的自然语言处理任务</li></ul></li><li data-line="10">Decoder-only架构<ul><li data-line="11"><strong>适用任务</strong>: Decoder-only架构特别适合于文本生成任务，如语言模型训练、文本生成、代码生成等。这种架构通常会被训练来预测接下来的单词或者序列。</li><li data-line="12"><strong>代表性模型</strong>: GPT（Generative Pre-trained Transformer）系列是这一类架构中最知名的例子。GPT通过大量的预训练，能够生成连贯、相关的文本，可以用于各种生成型任务。</li></ul></li></ul></div></div></div><div><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=bQ5BoolX9Ag&amp;t=13s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=bQ5BoolX9Ag&amp;t=13s" target="_blank">StatQuest - Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly Explained!!!</a></p></div><div><p>GPT 论文<br><strong>"Improving Language Understanding by Generative Pre-Training" by Alec Radford et al. (2018)</strong></p></div><div><p><strong>"Language Models are Unsupervised Multitask Learners" by Alec Radford et al. (2019)</strong></p></div><div><p><strong>"Language Models are Few-Shot Learners" by Tom B. Brown et al. (2020)</strong></p></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="check" class="callout node-insert-event drop-shadow"><div class="callout-title"><div class="callout-icon"><svg height="16" style="width:16px;max-width:100%"></svg></div><div class="callout-title-inner">加速Transformer的有关尝试, 可以分为两个方向</div></div><div class="callout-content"><ul><li data-line="1">降低Attention运算的时间复杂度从 O(N^2) 到 O(N), 比如 Linear Transformer 和 Linformer, 一般是用近似技巧, 但是性能会有所下降</li><li data-line="2">在不改变attention理论时间复杂度的前提下，尽可能加速attention的运算, 比如 Flash Attention. 原理基本上是减少了GPU的SRAM (<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Static_random-access_memory" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Static_random-access_memory" target="_blank">Static Random Access Memory</a>) 和HBM (<a data-tooltip-position="top" aria-label="https://semiwiki.com/wikis/semiconductor-ip-wikis/high-bandwidth-memory-hbm-wiki/#:~:text=High%20Bandwidth%20Memory%20%28HBM%29%20is%20a%20high-performance%20RAM,HBM%20were%20the%20AMD%20Fiji%20GPUs%20in%202015" rel="noopener" class="external-link" href="https://semiwiki.com/wikis/semiconductor-ip-wikis/high-bandwidth-memory-hbm-wiki/#:~:text=High%20Bandwidth%20Memory%20%28HBM%29%20is%20a%20high-performance%20RAM,HBM%20were%20the%20AMD%20Fiji%20GPUs%20in%202015" target="_blank">High Bandwidth Memory</a>)之间的通信开销</li></ul></div></div></div><div><p>一个很好的大模型可视化项目<br><a data-tooltip-position="top" aria-label="https://bbycroft.net/llm" rel="noopener" class="external-link" href="https://bbycroft.net/llm" target="_blank">LLM Visualization</a></p></div><div class="heading-wrapper"><h4 data-heading="Variations of Transformer" class="heading" id="Variations_of_Transformer"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>Variations of Transformer<div class="heading-after">...</div></h4><div class="heading-children"><div><div data-callout-metadata="" data-callout-fold="" data-callout="check" class="callout node-insert-event drop-shadow"><div class="callout-title"><div class="callout-icon"><svg height="16" style="width:16px;max-width:100%"></svg></div><div class="callout-title-inner">Check</div></div><div class="callout-content"><p><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV13N4y1J7BD/?spm_id_from=333.1007.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV13N4y1J7BD/?spm_id_from=333.1007.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">论文研读之简化版Transformer：Simplifying Transformer Blocks_哔哩哔哩_bilibili</a><br><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/677511164" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/677511164" target="_blank">【论文详解】简化版Transformer：Simplifying Transformer Blocks - 知乎</a></p></div></div></div><div><p>R-Transformer</p></div><div><ul><li data-line="0">针对高维嵌入导致位置编码失效的解决方案</li></ul></div><div><p>Vision-Transformer<br><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/451568838" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/451568838" target="_blank"># Vison Transformer学习</a></p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="Mamba" class="heading" id="Mamba"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>Mamba<div class="heading-after">...</div></h2><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2312.00752" rel="noopener" class="external-link" href="https://arxiv.org/abs/2312.00752" target="_blank">[2023.12] Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a><br><a data-tooltip-position="top" aria-label="https://36kr.com/p/2547379574166664" rel="noopener" class="external-link" href="https://36kr.com/p/2547379574166664" target="_blank">颠覆Transformer霸权，CMU普林斯顿推Mamba新架构，解决致命bug推理速度暴增5倍-36氪</a></p></div><div><ul><li data-line="0">Mamba 为了解决Transformer核心注意力层上下文长度无法很好scale计算资源的局限(在大模型基础热门的背景). 为此提出了结构化空间状态模型(SSM, structured state space models). 实验显示Mamba具有参数线性扩展性(可最高百万Token级别), 和五倍的推理速度. 在多项任务中都达到SOTA. 比如Mamba-3B模型表現和两倍参数量的Transformer模型相当. 向我们展示了一个非常有希望的 Transformer 替代.</li><li data-line="1">研究背景<ul><li data-line="2">在深度学习领域，Transformer架构及其核心的注意力机制已经成为处理各种序列数据的强大工具。然而，Transformer在处理长序列数据时存在计算效率低下的问题，这限制了其在某些应用场景中的使用。为了解决这个问题，研究者们提出了多种子二次时间复杂度的架构，如线性注意力、门控卷积和循环模型等，但这些模型在处理语言等重要模态数据时表现并不如注意力机制。</li></ul></li><li data-line="3">过去的方案及问题<ul><li data-line="4">尽管这些子二次时间复杂度的模型在计算效率上有所提升，但它们在处理离散模态数据（如文本）时，往往无法有效地进行基于内容的推理。这些模型的一个关键弱点是它们无法根据当前输入有选择地传播或遗忘信息。此外，尽管这些模型试图通过高效的卷积操作来实现，但这种变化阻止了它们使用高效的并行计算，这在硬件层面上带来了挑战。</li></ul></li><li data-line="5">本文方案及具体步骤：<ul><li data-line="6">本文提出了一种新的选择性状态空间模型（Selective State Space Models，简称SSMs），它通过以下几个关键步骤来提高模型的性能：</li><li data-line="7"><strong>选择机制</strong>：通过将SSM参数设置为输入的函数，模型能够根据当前的输入选择性地传播或遗忘信息，从而在序列长度维度上动态地处理信息。在实现 Transformer 质量的性能，同时线性缩放序列长度</li><li data-line="8"><strong>硬件感知算法</strong>：为了解决选择性SSMs在计算上的挑战，研究者设计了一种硬件感知的并行算法，该算法以递归模式运行模型，并通过扫描操作而不是卷积来计算，同时避免了在GPU内存层次结构之间进行不必要的IO访问。</li><li data-line="9"><strong>简化的架构设计</strong>：将SSMs与Transformer的MLP块结合，形成了一个简化的、同质的架构（Mamba），这个架构在不使用注意力或MLP块的情况下，通过选择性状态空间实现了快速推理和线性序列长度扩展。</li></ul></li><li data-line="10">贡献<ul><li data-line="11">在多种模态（语言、音频和基因组学）上都取得SOTA性能，成为跨模态通用序列模型主干的有力候选者。</li></ul></li></ul></div></div></div><div class="heading-wrapper"><h2 data-heading="GAN" class="heading" id="GAN"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>GAN<div class="heading-after">...</div></h2><div class="heading-children"></div></div><div class="heading-wrapper"><h2 data-heading="AE" class="heading" id="AE"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>AE<div class="heading-after">...</div></h2><div class="heading-children"><div><p>Diffusion model</p></div></div></div><div class="heading-wrapper"><h2 data-heading="图神经网络" class="heading" id="图神经网络"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>图神经网络<div class="heading-after">...</div></h2><div class="heading-children"><div class="mod-footer"></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-gutter"><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-container"><div class="sidebar-sizer"><div class="sidebar-content-positioner"><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder"><div class="graph-view-container"><div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div><canvas id="graph-canvas" width="512px" height="512px"></canvas></div></div></div><div class="tree-container outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area"><div class="tree-item" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#深度学习_(论文+好文)"><span class="tree-item-title"><p>深度学习 (论文+好文)</p></span></a></div><div class="tree-item-children"><div class="tree-item mod-collapsible" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#基本算法"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title"><p>基本算法</p></span></a></div><div class="tree-item-children"><div class="tree-item" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#反向传播"><span class="tree-item-title"><p>反向传播</p></span></a></div><div class="tree-item-children"></div></div><div class="tree-item" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#优化器"><span class="tree-item-title"><p>优化器</p></span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#基本概念"><span class="tree-item-title"><p>基本概念</p></span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#优化器"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title"><p>优化器</p></span></a></div><div class="tree-item-children"><div class="tree-item" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#正则化"><span class="tree-item-title"><p>正则化</p></span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#编年史"><span class="tree-item-title"><p>编年史</p></span></a></div><div class="tree-item-children"></div></div><div class="tree-item" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#CNN"><span class="tree-item-title"><p>CNN</p></span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#RNN"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title"><p>RNN</p></span></a></div><div class="tree-item-children"><div class="tree-item" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#LSTM、GRU"><span class="tree-item-title"><p>LSTM、GRU</p></span></a></div><div class="tree-item-children"></div></div><div class="tree-item" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#RWKV"><span class="tree-item-title"><p>RWKV</p></span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#Transformer"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title"><p>Transformer</p></span></a></div><div class="tree-item-children"><div class="tree-item" data-depth="4"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#Variations_of_Transformer"><span class="tree-item-title"><p>Variations of Transformer</p></span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#Mamba"><span class="tree-item-title"><p>Mamba</p></span></a></div><div class="tree-item-children"></div></div><div class="tree-item" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#GAN"><span class="tree-item-title"><p>GAN</p></span></a></div><div class="tree-item-children"></div></div><div class="tree-item" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#AE"><span class="tree-item-title"><p>AE</p></span></a></div><div class="tree-item-children"></div></div><div class="tree-item" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#图神经网络"><span class="tree-item-title"><p>图神经网络</p></span></a></div><div class="tree-item-children"></div></div></div></div></div></div></div></div></div></div></div></div></body></html>