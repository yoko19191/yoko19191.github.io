<!DOCTYPE html> <html><head>
		<title>机器学习常用算法</title>
		<base href="../">
		<meta id="root-path" root-path="../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="🌱 Digital-Garden - 机器学习常用算法">
		<meta property="og:title" content="机器学习常用算法">
		<meta property="og:description" content="🌱 Digital-Garden - 机器学习常用算法">
		<meta property="og:type" content="website">
		<meta property="og:url" content="💾-科技工程/机器学习常用算法.html">
		<meta property="og:image" content="https://cdn.sa.net/2023/12/25/w7mTu54CofLkPnM.png">
		<meta property="og:site_name" content="🌱 Digital-Garden">
		<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.4.0/pixi.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="https://cdn.jsdelivr.net/npm/minisearch@6.3.0/dist/umd/index.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="preload" href="lib/styles/other-plugins.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/other-plugins.css"></noscript><link rel="stylesheet" href="lib/styles/theme.css"><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript></head><body class="publish css-settings-manager native-scrollbars theme-light show-inline-title ctp-latte ctp-mocha ctp-accent-light-sky ctp-accent-rosewater anuppuccin-accent-toggle anp-current-line anp-h1-red anp-h2-peach anp-h3-green anp-h4-teal anp-h5-lavender anp-h6-mauve anp-bold-red anp-italic-green anp-highlight-yellow anp-full-rainbow-color-toggle"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles">mjx-c.mjx-c1D6FC.TEX-I::before { padding: 0.442em 0.64em 0.011em 0px; content: "α"; }
mjx-c.mjx-c5B::before { padding: 0.75em 0.278em 0.25em 0px; content: "["; }
mjx-c.mjx-c5D::before { padding: 0.75em 0.278em 0.25em 0px; content: "]"; }
mjx-c.mjx-c1D436.TEX-I::before { padding: 0.705em 0.76em 0.022em 0px; content: "C"; }
mjx-c.mjx-c1D463.TEX-I::before { padding: 0.443em 0.485em 0.011em 0px; content: "v"; }
mjx-c.mjx-c1D44C.TEX-I::before { padding: 0.683em 0.763em 0px 0px; content: "Y"; }
mjx-c.mjx-c1D464.TEX-I::before { padding: 0.443em 0.716em 0.011em 0px; content: "w"; }
mjx-c.mjx-c1D6F4.TEX-I::before { padding: 0.683em 0.806em 0px 0px; content: "Σ"; }
mjx-c.mjx-c1D435.TEX-I::before { padding: 0.683em 0.759em 0px 0px; content: "B"; }
mjx-c.mjx-c1D44F.TEX-I::before { padding: 0.694em 0.429em 0.011em 0px; content: "b"; }
mjx-c.mjx-c33::before { padding: 0.665em 0.5em 0.022em 0px; content: "3"; }
mjx-c.mjx-c41::before { padding: 0.716em 0.75em 0px 0px; content: "A"; }
mjx-c.mjx-c65::before { padding: 0.448em 0.444em 0.011em 0px; content: "e"; }
mjx-c.mjx-c6E::before { padding: 0.442em 0.556em 0px 0px; content: "n"; }
mjx-c.mjx-c69::before { padding: 0.669em 0.278em 0px 0px; content: "i"; }
mjx-c.mjx-c5C::before { padding: 0.75em 0.5em 0.25em 0px; content: "\\"; }
mjx-c.mjx-c63::before { padding: 0.448em 0.444em 0.011em 0px; content: "c"; }
mjx-c.mjx-c64::before { padding: 0.694em 0.556em 0.011em 0px; content: "d"; }
mjx-mtext { display: inline-block; text-align: left; }
mjx-c.mjx-c73::before { padding: 0.448em 0.394em 0.011em 0px; content: "s"; }
mjx-c.mjx-c66::before { padding: 0.705em 0.372em 0px 0px; content: "f"; }
mjx-c.mjx-c74::before { padding: 0.615em 0.389em 0.01em 0px; content: "t"; }
mjx-c.mjx-c6D::before { padding: 0.442em 0.833em 0px 0px; content: "m"; }
mjx-c.mjx-c61::before { padding: 0.448em 0.5em 0.011em 0px; content: "a"; }
mjx-c.mjx-c78::before { padding: 0.431em 0.528em 0px 0px; content: "x"; }
mjx-c.mjx-c221A.TEX-S1::before { padding: 0.85em 1.02em 0.35em 0px; content: "√"; }
mjx-c.mjx-c2F::before { padding: 0.75em 0.5em 0.25em 0px; content: "/"; }
mjx-c.mjx-c1D45E.TEX-I::before { padding: 0.442em 0.46em 0.194em 0px; content: "q"; }
mjx-c.mjx-c7B::before { padding: 0.75em 0.5em 0.25em 0px; content: "{"; }
mjx-c.mjx-c7D::before { padding: 0.75em 0.5em 0.25em 0px; content: "}"; }
mjx-c.mjx-c1D70E.TEX-I::before { padding: 0.431em 0.571em 0.011em 0px; content: "σ"; }
mjx-c.mjx-c1D719.TEX-I::before { padding: 0.694em 0.596em 0.205em 0px; content: "ϕ"; }
mjx-c.mjx-c1D716.TEX-I::before { padding: 0.431em 0.406em 0.011em 0px; content: "ϵ"; }
mjx-c.mjx-c1D707.TEX-I::before { padding: 0.442em 0.603em 0.216em 0px; content: "μ"; }
mjx-c.mjx-c1D703.TEX-I::before { padding: 0.705em 0.469em 0.01em 0px; content: "θ"; }
mjx-c.mjx-c1D450.TEX-I::before { padding: 0.442em 0.433em 0.011em 0px; content: "c"; }
mjx-c.mjx-c1D434.TEX-I::before { padding: 0.716em 0.75em 0px 0px; content: "A"; }
mjx-c.mjx-c1D45F.TEX-I::before { padding: 0.442em 0.451em 0.011em 0px; content: "r"; }
mjx-c.mjx-c1D466.TEX-I::before { padding: 0.442em 0.49em 0.205em 0px; content: "y"; }
mjx-c.mjx-cD7::before { padding: 0.491em 0.778em 0px 0px; content: "×"; }
mjx-munderover { display: inline-block; text-align: left; }
mjx-munderover:not([limits="false"]) { padding-top: 0.1em; }
mjx-munderover:not([limits="false"]) > * { display: block; }
mjx-c.mjx-c2211.TEX-S1::before { padding: 0.75em 1.056em 0.25em 0px; content: "∑"; }
mjx-c.mjx-c34::before { padding: 0.677em 0.5em 0px 0px; content: "4"; }
mjx-c.mjx-c1D44E.TEX-I::before { padding: 0.441em 0.529em 0.01em 0px; content: "a"; }
mjx-c.mjx-c22C5::before { padding: 0.31em 0.278em 0px 0px; content: "⋅"; }
mjx-c.mjx-c1D458.TEX-I::before { padding: 0.694em 0.521em 0.011em 0px; content: "k"; }
mjx-c.mjx-c1D438.TEX-I::before { padding: 0.68em 0.764em 0px 0px; content: "E"; }
mjx-c.mjx-c1D45D.TEX-I::before { padding: 0.442em 0.503em 0.194em 0px; content: "p"; }
mjx-c.mjx-c1D45C.TEX-I::before { padding: 0.441em 0.485em 0.011em 0px; content: "o"; }
mjx-c.mjx-c1D460.TEX-I::before { padding: 0.442em 0.469em 0.01em 0px; content: "s"; }
mjx-c.mjx-c1D45B.TEX-I::before { padding: 0.442em 0.6em 0.011em 0px; content: "n"; }
mjx-c.mjx-c30::before { padding: 0.666em 0.5em 0.022em 0px; content: "0"; }
mjx-c.mjx-c1D45A.TEX-I::before { padding: 0.442em 0.878em 0.011em 0px; content: "m"; }
mjx-c.mjx-c1D459.TEX-I::before { padding: 0.694em 0.298em 0.011em 0px; content: "l"; }
mjx-msub { display: inline-block; text-align: left; }
mjx-munder { display: inline-block; text-align: left; }
mjx-over { text-align: left; }
mjx-munder:not([limits="false"]) { display: inline-table; }
mjx-munder > mjx-row { text-align: left; }
mjx-under { padding-bottom: 0.1em; }
mjx-msqrt { display: inline-block; text-align: left; }
mjx-root { display: inline-block; white-space: nowrap; }
mjx-surd { display: inline-block; vertical-align: top; }
mjx-sqrt { display: inline-block; padding-top: 0.07em; }
mjx-sqrt > mjx-box { border-top: 0.07em solid; }
mjx-sqrt.mjx-tall > mjx-box { padding-left: 0.3em; margin-left: -0.3em; }
mjx-c.mjx-c1D44B.TEX-I::before { padding: 0.683em 0.852em 0px 0px; content: "X"; }
mjx-c.mjx-c1D465.TEX-I::before { padding: 0.442em 0.572em 0.011em 0px; content: "x"; }
mjx-c.mjx-c1D43A.TEX-I::before { padding: 0.705em 0.786em 0.022em 0px; content: "G"; }
mjx-c.mjx-c1D443.TEX-I::before { padding: 0.683em 0.751em 0px 0px; content: "P"; }
mjx-c.mjx-c1D444.TEX-I::before { padding: 0.704em 0.791em 0.194em 0px; content: "Q"; }
mjx-c.mjx-c1D437.TEX-I::before { padding: 0.683em 0.828em 0px 0px; content: "D"; }
mjx-c.mjx-c1D43E.TEX-I::before { padding: 0.683em 0.889em 0px 0px; content: "K"; }
mjx-c.mjx-c1D43F.TEX-I::before { padding: 0.683em 0.681em 0px 0px; content: "L"; }
mjx-c.mjx-c7C::before { padding: 0.75em 0.278em 0.249em 0px; content: "|"; }
mjx-c.mjx-c2211.TEX-S2::before { padding: 0.95em 1.444em 0.45em 0px; content: "∑"; }
mjx-c.mjx-c6C::before { padding: 0.694em 0.278em 0px 0px; content: "l"; }
mjx-c.mjx-c6F::before { padding: 0.448em 0.5em 0.01em 0px; content: "o"; }
mjx-c.mjx-c67::before { padding: 0.453em 0.5em 0.206em 0px; content: "g"; }
mjx-c.mjx-c2061::before { padding: 0px; content: ""; }
mjx-c.mjx-c1D43D.TEX-I::before { padding: 0.683em 0.633em 0.022em 0px; content: "J"; }
mjx-c.mjx-c1D446.TEX-I::before { padding: 0.705em 0.645em 0.022em 0px; content: "S"; }
mjx-c.mjx-c1D440.TEX-I::before { padding: 0.683em 1.051em 0px 0px; content: "M"; }
mjx-c.mjx-c2B::before { padding: 0.583em 0.778em 0.082em 0px; content: "+"; }
mjx-c.mjx-c1D43B.TEX-I::before { padding: 0.683em 0.888em 0px 0px; content: "H"; }
mjx-c.mjx-c2C::before { padding: 0.121em 0.278em 0.194em 0px; content: ","; }
mjx-c.mjx-c221A::before { padding: 0.8em 0.853em 0.2em 0px; content: "√"; }
mjx-c.mjx-c221A.TEX-S4::before { padding: 1.75em 1.02em 1.25em 0px; content: "√"; }
mjx-c.mjx-c221A.TEX-S2::before { padding: 1.15em 1.02em 0.65em 0px; content: "√"; }
mjx-c.mjx-c1D447.TEX-I::before { padding: 0.677em 0.704em 0px 0px; content: "T"; }
mjx-c.mjx-c1D449.TEX-I::before { padding: 0.683em 0.769em 0.022em 0px; content: "V"; }
mjx-container[jax="CHTML"] { line-height: 0; }
mjx-container [space="1"] { margin-left: 0.111em; }
mjx-container [space="2"] { margin-left: 0.167em; }
mjx-container [space="3"] { margin-left: 0.222em; }
mjx-container [space="4"] { margin-left: 0.278em; }
mjx-container [space="5"] { margin-left: 0.333em; }
mjx-container [rspace="1"] { margin-right: 0.111em; }
mjx-container [rspace="2"] { margin-right: 0.167em; }
mjx-container [rspace="3"] { margin-right: 0.222em; }
mjx-container [rspace="4"] { margin-right: 0.278em; }
mjx-container [rspace="5"] { margin-right: 0.333em; }
mjx-container [size="s"] { font-size: 70.7%; }
mjx-container [size="ss"] { font-size: 50%; }
mjx-container [size="Tn"] { font-size: 60%; }
mjx-container [size="sm"] { font-size: 85%; }
mjx-container [size="lg"] { font-size: 120%; }
mjx-container [size="Lg"] { font-size: 144%; }
mjx-container [size="LG"] { font-size: 173%; }
mjx-container [size="hg"] { font-size: 207%; }
mjx-container [size="HG"] { font-size: 249%; }
mjx-container [width="full"] { width: 100%; }
mjx-box { display: inline-block; }
mjx-block { display: block; }
mjx-itable { display: inline-table; }
mjx-row { display: table-row; }
mjx-row > * { display: table-cell; }
mjx-mtext { display: inline-block; }
mjx-mstyle { display: inline-block; }
mjx-merror { display: inline-block; color: red; background-color: yellow; }
mjx-mphantom { visibility: hidden; }
mjx-assistive-mml { top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); user-select: none; position: absolute !important; padding: 1px 0px 0px !important; border: 0px !important; display: block !important; width: auto !important; overflow: hidden !important; }
mjx-assistive-mml[display="block"] { width: 100% !important; }
mjx-math { display: inline-block; text-align: left; line-height: 0; text-indent: 0px; font-style: normal; font-weight: normal; font-size: 100%; letter-spacing: normal; border-collapse: collapse; overflow-wrap: normal; word-spacing: normal; white-space: nowrap; direction: ltr; padding: 1px 0px; }
mjx-container[jax="CHTML"][display="true"] { display: block; text-align: center; margin: 1em 0px; }
mjx-container[jax="CHTML"][display="true"][width="full"] { display: flex; }
mjx-container[jax="CHTML"][display="true"] mjx-math { padding: 0px; }
mjx-container[jax="CHTML"][justify="left"] { text-align: left; }
mjx-container[jax="CHTML"][justify="right"] { text-align: right; }
mjx-mi { display: inline-block; text-align: left; }
mjx-c { display: inline-block; }
mjx-utext { display: inline-block; padding: 0.75em 0px 0.2em; }
mjx-mo { display: inline-block; text-align: left; }
mjx-stretchy-h { display: inline-table; width: 100%; }
mjx-stretchy-h > * { display: table-cell; width: 0px; }
mjx-stretchy-h > * > mjx-c { display: inline-block; transform: scaleX(1); }
mjx-stretchy-h > * > mjx-c::before { display: inline-block; width: initial; }
mjx-stretchy-h > mjx-ext { overflow: clip visible; width: 100%; }
mjx-stretchy-h > mjx-ext > mjx-c::before { transform: scaleX(500); }
mjx-stretchy-h > mjx-ext > mjx-c { width: 0px; }
mjx-stretchy-h > mjx-beg > mjx-c { margin-right: -0.1em; }
mjx-stretchy-h > mjx-end > mjx-c { margin-left: -0.1em; }
mjx-stretchy-v { display: inline-block; }
mjx-stretchy-v > * { display: block; }
mjx-stretchy-v > mjx-beg { height: 0px; }
mjx-stretchy-v > mjx-end > mjx-c { display: block; }
mjx-stretchy-v > * > mjx-c { transform: scaleY(1); transform-origin: left center; overflow: hidden; }
mjx-stretchy-v > mjx-ext { display: block; height: 100%; box-sizing: border-box; border: 0px solid transparent; overflow: visible clip; }
mjx-stretchy-v > mjx-ext > mjx-c::before { width: initial; box-sizing: border-box; }
mjx-stretchy-v > mjx-ext > mjx-c { transform: scaleY(500) translateY(0.075em); overflow: visible; }
mjx-mark { display: inline-block; height: 0px; }
mjx-msubsup { display: inline-block; text-align: left; }
mjx-script { display: inline-block; padding-right: 0.05em; padding-left: 0.033em; }
mjx-script > mjx-spacer { display: block; }
mjx-texatom { display: inline-block; text-align: left; }
mjx-msup { display: inline-block; text-align: left; }
mjx-mfrac { display: inline-block; text-align: left; }
mjx-frac { display: inline-block; vertical-align: 0.17em; padding: 0px 0.22em; }
mjx-frac[type="d"] { vertical-align: 0.04em; }
mjx-frac[delims] { padding: 0px 0.1em; }
mjx-frac[atop] { padding: 0px 0.12em; }
mjx-frac[atop][delims] { padding: 0px; }
mjx-dtable { display: inline-table; width: 100%; }
mjx-dtable > * { font-size: 2000%; }
mjx-dbox { display: block; font-size: 5%; }
mjx-num { display: block; text-align: center; }
mjx-den { display: block; text-align: center; }
mjx-mfrac[bevelled] > mjx-num { display: inline-block; }
mjx-mfrac[bevelled] > mjx-den { display: inline-block; }
mjx-den[align="right"], mjx-num[align="right"] { text-align: right; }
mjx-den[align="left"], mjx-num[align="left"] { text-align: left; }
mjx-nstrut { display: inline-block; height: 0.054em; width: 0px; vertical-align: -0.054em; }
mjx-nstrut[type="d"] { height: 0.217em; vertical-align: -0.217em; }
mjx-dstrut { display: inline-block; height: 0.505em; width: 0px; }
mjx-dstrut[type="d"] { height: 0.726em; }
mjx-line { display: block; box-sizing: border-box; min-height: 1px; height: 0.06em; border-top: 0.06em solid; margin: 0.06em -0.1em; overflow: hidden; }
mjx-line[type="d"] { margin: 0.18em -0.1em; }
mjx-mn { display: inline-block; text-align: left; }
mjx-mrow { display: inline-block; text-align: left; }
mjx-c::before { display: block; width: 0px; }
.MJX-TEX { font-family: MJXZERO, MJXTEX; }
.TEX-B { font-family: MJXZERO, MJXTEX-B; }
.TEX-I { font-family: MJXZERO, MJXTEX-I; }
.TEX-MI { font-family: MJXZERO, MJXTEX-MI; }
.TEX-BI { font-family: MJXZERO, MJXTEX-BI; }
.TEX-S1 { font-family: MJXZERO, MJXTEX-S1; }
.TEX-S2 { font-family: MJXZERO, MJXTEX-S2; }
.TEX-S3 { font-family: MJXZERO, MJXTEX-S3; }
.TEX-S4 { font-family: MJXZERO, MJXTEX-S4; }
.TEX-A { font-family: MJXZERO, MJXTEX-A; }
.TEX-C { font-family: MJXZERO, MJXTEX-C; }
.TEX-CB { font-family: MJXZERO, MJXTEX-CB; }
.TEX-FR { font-family: MJXZERO, MJXTEX-FR; }
.TEX-FRB { font-family: MJXZERO, MJXTEX-FRB; }
.TEX-SS { font-family: MJXZERO, MJXTEX-SS; }
.TEX-SSB { font-family: MJXZERO, MJXTEX-SSB; }
.TEX-SSI { font-family: MJXZERO, MJXTEX-SSI; }
.TEX-SC { font-family: MJXZERO, MJXTEX-SC; }
.TEX-T { font-family: MJXZERO, MJXTEX-T; }
.TEX-V { font-family: MJXZERO, MJXTEX-V; }
.TEX-VB { font-family: MJXZERO, MJXTEX-VB; }
mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c { font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A !important; }
@font-face { font-family: MJXZERO; src: url("lib/fonts/mathjax_zero.woff") format("woff"); }
@font-face { font-family: MJXTEX; src: url("lib/fonts/mathjax_main-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-B; src: url("lib/fonts/mathjax_main-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-I; src: url("lib/fonts/mathjax_math-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-MI; src: url("lib/fonts/mathjax_main-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-BI; src: url("lib/fonts/mathjax_math-bolditalic.woff") format("woff"); }
@font-face { font-family: MJXTEX-S1; src: url("lib/fonts/mathjax_size1-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S2; src: url("lib/fonts/mathjax_size2-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S3; src: url("lib/fonts/mathjax_size3-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S4; src: url("lib/fonts/mathjax_size4-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-A; src: url("lib/fonts/mathjax_ams-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-C; src: url("lib/fonts/mathjax_calligraphic-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-CB; src: url("lib/fonts/mathjax_calligraphic-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-FR; src: url("lib/fonts/mathjax_fraktur-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-FRB; src: url("lib/fonts/mathjax_fraktur-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-SS; src: url("lib/fonts/mathjax_sansserif-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-SSB; src: url("lib/fonts/mathjax_sansserif-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-SSI; src: url("lib/fonts/mathjax_sansserif-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-SC; src: url("lib/fonts/mathjax_script-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-T; src: url("lib/fonts/mathjax_typewriter-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-V; src: url("lib/fonts/mathjax_vector-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-VB; src: url("lib/fonts/mathjax_vector-bold.woff") format("woff"); }
mjx-c.mjx-c1D439.TEX-I::before { padding: 0.68em 0.749em 0px 0px; content: "F"; }
mjx-c.mjx-c28::before { padding: 0.75em 0.389em 0.25em 0px; content: "("; }
mjx-c.mjx-c1D714.TEX-I::before { padding: 0.443em 0.622em 0.011em 0px; content: "ω"; }
mjx-c.mjx-c29::before { padding: 0.75em 0.389em 0.25em 0px; content: ")"; }
mjx-c.mjx-c3D::before { padding: 0.583em 0.778em 0.082em 0px; content: "="; }
mjx-c.mjx-c222B.TEX-S2::before { padding: 1.36em 0.944em 0.862em 0px; content: "∫"; }
mjx-c.mjx-c221E::before { padding: 0.442em 1em 0.011em 0px; content: "∞"; }
mjx-c.mjx-c2212::before { padding: 0.583em 0.778em 0.082em 0px; content: "−"; }
mjx-c.mjx-c1D453.TEX-I::before { padding: 0.705em 0.55em 0.205em 0px; content: "f"; }
mjx-c.mjx-c1D461.TEX-I::before { padding: 0.626em 0.361em 0.011em 0px; content: "t"; }
mjx-c.mjx-c1D452.TEX-I::before { padding: 0.442em 0.466em 0.011em 0px; content: "e"; }
mjx-c.mjx-c1D456.TEX-I::before { padding: 0.661em 0.345em 0.011em 0px; content: "i"; }
mjx-c.mjx-c1D451.TEX-I::before { padding: 0.694em 0.52em 0.01em 0px; content: "d"; }
mjx-c.mjx-c31::before { padding: 0.666em 0.5em 0px 0px; content: "1"; }
mjx-c.mjx-c32::before { padding: 0.666em 0.5em 0px 0px; content: "2"; }
mjx-c.mjx-c1D70B.TEX-I::before { padding: 0.431em 0.57em 0.011em 0px; content: "π"; }
</style><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="机器学习常用算法"><p>机器学习常用算法</p></h1><div><p><img alt="image.png" src="https://cdn.sa.net/2023/12/25/w7mTu54CofLkPnM.png" referrerpolicy="no-referrer"></p></div><div><p><a data-tooltip-position="top" aria-label="https://ytzfhqs.github.io/AAAMLP-CN/" rel="noopener" class="external-link" href="https://ytzfhqs.github.io/AAAMLP-CN/" target="_blank">AAAMLP 中译版</a><br>
<a data-tooltip-position="top" aria-label="https://developers.google.com/machine-learning/crash-course/ml-intro?hl=zh-cn" rel="noopener" class="external-link" href="https://developers.google.com/machine-learning/crash-course/ml-intro?hl=zh-cn" target="_blank">机器学习简介 &nbsp;|&nbsp; Machine Learning &nbsp;|&nbsp; Google for Developers</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/Dod-o/Statistical-Learning-Method_Code" rel="noopener" class="external-link" href="https://github.com/Dod-o/Statistical-Learning-Method_Code" target="_blank">GitHub - Dod-o/Statistical-Learning-Method_Code: 手写实现李航《统计学习方法》书中全部算法</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://scikit-learn.org.cn/" rel="noopener" class="external-link" href="https://scikit-learn.org.cn/" target="_blank">scikit-learn中文社区</a><br>
<a data-tooltip-position="top" aria-label="https://scikitlearn.com.cn/" rel="noopener" class="external-link" href="https://scikitlearn.com.cn/" target="_blank">sklearn</a></p></div><div class="admonition-parent admonition-attention-parent"><div class="callout admonition admonition-attention admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="attention" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Attention</div></div><div class="callout-content admonition-content"><p>基本上, <code>XGboost</code>, <code>LightGBM</code>, <code>CatBoost</code>  是最强的模型, 没有特殊需求直接用.</p>
<p>autoguolun 很强大, 自己手动调参和自动调差别不是很大. </p></div></div></div><div class="heading-wrapper"><h2 data-heading="基础知识" class="heading" id="基础知识"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>基础知识</h2><div class="heading-children"><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>无监督和有监督学习</strong>
</span><ul>
<li data-line="1"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>有监督数据：总是有一个或多个与之相关的目标. 有监督问题比无监督问题更容易解决且更容易评估. 
<ul>
<li data-line="2">例如，如果问题是根据历史房价预测房价，那么医院、学校或超市的存在，与最近公共交通的距离等特征</li>
<li data-line="3"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>有监督问题可分为两个子类.
<ul>
<li data-line="4">分类：预测类别，如猫或狗</li>
<li data-line="5">回归：预测值，如房价</li>
</ul>
</li>
</ul>
</li>
<li data-line="6"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>无监督数据：没有任何目标变量。
<ul>
<li data-line="7"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>无监督问题有什么子类
<ul>
<li data-line="8">聚类（Clustering）：这是一种常见的无监督学习任务，目的是将数据集中的样本分组，使得同一组（即簇）内的样本相似度较高而不同组间的样本相似度较低。例如，客户细分、社交网络分析、市场研究、图像分割等。</li>
<li data-line="9">降维（Dimensionality Reduction）：当处理实际问题时，我们可能会遇到具有成千上万个特征的数据集。这种情况下，数据的维度极高，我们需要降低它的维度，同时尽可能保留原始数据的重要信息。例如，主成分分析（PCA）、线性判别分析（LDA）等。</li>
<li data-line="10">异常检测（Anomaly Detection）：这种任务的目标是识别出数据集中的异常样本，即那些与大多数其他样本的行为不同的样本。例如，信用卡欺诈检测、工业生产异常检测等。</li>
<li data-line="11">关联规则学习（Association Rule Learning）：这种任务的目标是在大型数据集中发现特征之间的有趣关联。例如，超市购物篮分析、网络流量分析等。</li>
<li data-line="12">自动编码器（Autoencoders）：这是一种特殊的神经网络，它试图使用较少的信息（编码）来表示输入数据，然后从这些编码中重构出原始数据。这可以用于降维或者特征学习。</li>
<li data-line="13">生成模型（Generative Models）：这类模型试图学习真实数据的分布，以便能够生成新的数据。例如，生成对抗网络（GANs）就是一种常见的生成模型。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li data-line="14" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>正则化</strong></span></li>
</ul></div><div><ol start="5">
<li data-line="0"><strong>组织机器学习</strong></li>
<li data-line="1"><strong>处理分类变量</strong></li>
<li data-line="2"><strong>特征工程</strong></li>
<li data-line="3"><strong>特征选择</strong></li>
<li data-line="4"><strong>超参数优化</strong></li>
<li data-line="5"><strong>图像分类和分割方法</strong></li>
<li data-line="6"><strong>文本分类或回归方法</strong></li>
</ol></div><div class="heading-wrapper"><h3 data-heading="数据预处理" class="heading" id="数据预处理"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>数据预处理</h3><div class="heading-children"><div><p>缺失值处理、异常值处理、数据归一化、特征选择和提取等</p></div></div></div><div class="heading-wrapper"><h3 data-heading="模型评估" class="heading" id="模型评估"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>模型评估</h3><div class="heading-children"><div><blockquote>
<p>目标 : <strong>Performance Estimation</strong> / <strong>Select between alternative options</strong><br>
避免: counterfeit utility (假钞, 人脑的奖赏系统. 吸毒, 游戏沉迷, 拖延, 考试, 死记硬背, 作弊, 大众柴油门)</p>
</blockquote></div><div><p><a data-tooltip-position="top" aria-label="https://ytzfhqs.github.io/AAAMLP-CN/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/" rel="noopener" class="external-link" href="https://ytzfhqs.github.io/AAAMLP-CN/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/" target="_blank">评估指标 - AAAMLP 中译版</a><br>
<a data-tooltip-position="top" aria-label="https://www.notion.so/smile-every-day/Evaluation-of-Machine-Learning-Methods-b4dfda3be3924f3dbfa66710441df5fc" rel="noopener" class="external-link" href="https://www.notion.so/smile-every-day/Evaluation-of-Machine-Learning-Methods-b4dfda3be3924f3dbfa66710441df5fc" target="_blank">Notion – Evaluation of Machine Learning Methods</a></p></div><div><p><img alt="geOcU9mWZFK5zt3.png" src="https://cdn.sa.net/2024/04/13/geOcU9mWZFK5zt3.png" referrerpolicy="no-referrer"></p></div><div><p>AccuracyScore=(TP+TN)/(TP+TN+FP+FN)</p></div><div><p>Precision=TP/(TP+FP)</p></div><div><p>Recall=TP/(TP+FN)</p></div><div><p>F1=2PR/(P+R)</p></div><div><p>F1=2TP/(2TP+FP+FN)</p></div><div><p>TPR=TP/(TP+FN)</p></div><div><p>FPR=FP/(TN+FP)</p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>评估指标</strong></p>
</span><ul>
<li data-line="1"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p>如果我们谈论分类问题，最常用的指标是</p>
<ul>
<li data-line="2">准确率（Accuracy）</li>
<li data-line="3">精确率（P）</li>
<li data-line="4">召回率（R）</li>
<li data-line="5">F1 分数（F1）</li>
<li data-line="6">AUC（AUC）</li>
<li data-line="7">对数损失（Log loss）</li>
<li data-line="8">k 精确率（P@k）</li>
<li data-line="9">k 平均精率（AP@k）</li>
<li data-line="10">k 均值平均精确率（MAP@k）</li>
</ul>
</li>
<li data-line="11"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p>说到回归，最常用的评价指标是</p>
<ul>
<li data-line="12">平均绝对误差 （MAE）</li>
<li data-line="13">均方误差 （MSE）</li>
<li data-line="14">均方根误差 （RMSE）</li>
<li data-line="15">均方根对数误差 （RMSLE）</li>
<li data-line="16">平均百分比误差 （MPE）</li>
<li data-line="17">平均绝对百分比误差 （MAPE）</li>
<li data-line="18">R2</li>
</ul>
</li>
<li data-line="20">
<p>准确率(Accuracy)：在分类问题中，准确率是最常用的评估指标，它是正确预测的数量占总预测的比例。</p>
</li>
<li data-line="21">
<p>精确率(Precision)：在信息检索中，精确率是检索出的相关文档数与检索出的所有文档数的比值。在机器学习中，精确率常用于二分类问题，尤其是在假阳性（误报）的代价很高的情况下，例如垃圾邮件检测。</p>
</li>
<li data-line="22">
<p>召回率(Recall)：在信息检索中，召回率是检索出的相关文档数与所有相关文档数的比值。在机器学习中，召回率常用于二分类问题，尤其是在假阴性（漏报）的代价很高的情况下，例如疾病诊断。</p>
</li>
<li data-line="23">
<p>F1分数(F1 Score)：F1分数是精确率和召回率的调和平均值，常用于二分类问题，尤其是在假阳性和假阴性的代价都很高的情况下。</p>
</li>
<li data-line="24">
<p>AUC-ROC：AUC-ROC是接收者操作特性曲线下的面积，常用于二分类问题，尤其是在分类器的阈值选择对结果影响很大的情况下。</p>
</li>
<li data-line="25">
<p>均方误差(MSE)和均方根误差(RMSE)：这两个指标常用于回归问题，用于衡量预测值和真实值的差距。</p>
</li>
<li data-line="26">
<p>平均绝对误差(MAE)：这个指标也常用于回归问题，用于衡量预测值和真实值的差距。</p>
</li>
<li data-line="27">
<p>R-Squared：这个指标常用于回归问题，用于衡量模型解释了多少真实值的变化。</p>
</li>
<li data-line="28">
<p>对数损失(Log Loss)：这个指标常用于概率预测，例如多分类问题。</p>
</li>
<li data-line="29">
<p>信息增益(Information Gain)：这个指标常用于决策树等模型的特征选择。</p>
</li>
<li data-line="30">
<p>Gini系数：这个指标常用于决策树等模型的特征选择，也可以用于衡量分类问题的不平衡性。真用Markdown List 的格式输出</p>
</li>
</ul>
</li>
<li data-line="33" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 我们有什么方法可以测试数据是随机的?</p>
</span><ul>
<li data-line="34">运行序列检验(Permutation Test)：这是一种统计方法，用于确定一个数字序列是否包含有意义的模式或结构。其基本假设是，如果数字序列是随机生成的，那么它不应该包含任何有意义的模式或结构。</li>
<li data-line="35">频率检验：频率检验是检查一组数据中的各个值出现的频率是否均匀分布。如果数据是随机的，我们期望各个值出现的频率大致相同。</li>
<li data-line="36">使用卡方检验：卡方检验是一种统计假设检验，用于确定观察结果与理论预期结果之间的偏离程度。如果数据是随机的，那么观察结果和理论预期结果之间的偏离程度应该较小。</li>
<li data-line="37">自相关检验：自相关检验是一种检验数据中是否存在时间序列相关性的方法。如果数据是随机的，那么数据的自相关系数应该接近零。</li>
<li data-line="38">熵检验：熵是一种用于度量数据集中信息的混乱程度的指标。如果数据是随机的，那么它的熵应该最大。</li>
<li data-line="39">使用Kolmogorov-Smirnov检验：这是一种非参数检验，用于检验两个分布是否相同。如果数据是随机的，那么它的累积分布函数应该接近理论上的均匀分布。</li>
<li data-line="40">Wald-Wolfowitz Runs检验：这种检验方法是基于数据序列中的上升和下降运行来检验数据的随机性。如果数据是随机的，那么运行的数量应该接近期望的数量。</li>
</ul>
</li>
<li data-line="44" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>过拟合与欠拟合</strong></p>
</span><ul>
<li data-line="45">过拟合（Overfitting）是指机器学习模型在训练数据上表现良好，但在测试数据或新数据上表现较差。换句话说，过拟合是模型过于复杂，以至于它开始“记住”训练数据，而不是“学习”潜在的模式。这种情况下，模型可能会在训练数据上产生很低的误差，但在新的、未见过的数据上可能产生很高的误差。</li>
<li data-line="46">欠拟合（Underfitting）是指模型在训练数据和测试数据上都表现不佳。这通常是因为模型过于简单，不能捕捉到数据的复杂性或模式，导致预测效果不佳。</li>
<li data-line="47"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>过拟合和欠拟合的判断常用的方法有以下几种
<ul>
<li data-line="48">训练误差和验证误差：如果模型在训练集上的误差很小，但在验证集（或测试集）上的误差较大，那么模型可能过拟合。相反，如果模型在训练集和验证集上的误差都较大，那么模型可能欠拟合。</li>
<li data-line="49">学习曲线：通过绘制模型的训练误差和验证误差随着训练次数的变化来观察模型的表现。如果训练误差和验证误差之间存在较大的差距，那么模型可能过拟合。如果两者都较高且相近，那么模型可能欠拟合。</li>
<li data-line="50">交叉验证：通过对不同的训练集和验证集进行多次训练和验证，如果模型在某些训练集上表现良好，但在对应的验证集上表现差，那么模型可能过拟合。如果在所有不同的训练集和验证集上表现都不佳，那么模型可能欠拟合。</li>
</ul>
</li>
</ul>
</li>
</ul></div><div><p>Overfitting curve/<br>
<img src="https://ytzfhqs.github.io/AAAMLP-CN/figures/AAAMLP_page20_image.png" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>模型选择准则</strong>（如AIC、BIC）
</span><ul>
<li data-line="1"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><code>Wilcoxon test</code> 无参检验, 用于检测两个样本是不是属于同一分布. 适用于数据不符合正态分布或者样本量较小的情况。相比于参数检验如t检验，Wilcoxon检验对数据的分布形态要求较低，因此在实际应用中更为广泛有 Wilcoxon符号秩检验（Wilcoxon signed-rank test）和Wilcoxon秩和检验（Wilcoxon rank-sum test）两种。
<ul>
<li data-line="2">Wilcoxon符号秩检验：主要用于比较两个相关样本或重复测量的数据，以确定它们的总体中位数是否相等。例如，一个人在接受治疗前后的体重变化，或者同一群人在不同条件下的测试分数。</li>
<li data-line="3">Wilcoxon秩和检验：主要用于比较两个独立样本，以确定它们的总体分布是否相等。例如，比较男性和女性的考试成绩，或者比较两种不同治疗方法的效果。</li>
</ul>
</li>
<li data-line="4"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>交叉验证</strong>是一种统计学上将数据样本切割成较小子集的实用方法。在这个过程中，我们首先对我们的模型进行训练，然后用这个模型去预测那些被留下来的或者没有被使用过的数据。然后我们根据一些预先设定的标准，比如准确率，来评估我们的模型。
<ul>
<li data-line="5"><strong>交叉验证的主要用处</strong></li>
<li data-line="6">避免过拟合：通过交叉验证，我们可以有效地防止模型过度学习训练数据，从而导致在新的未知数据上表现不佳的问题，即过拟合。</li>
<li data-line="7">模型选择：交叉验证也可以帮助我们在多个模型中选择一个最优的模型，这个模型在未知数据上的表现最好。</li>
<li data-line="8">参数调整：对于一些需要设定参数的模型，交叉验证可以帮助我们找到最优的参数设定。</li>
<li data-line="9"><strong>使用时需要注意的事项</strong>：</li>
<li data-line="10" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 数据分布：在交叉验证中，我们假设所有的数据都<strong>是独立同分布</strong>的。数据不能是非常不平衡或者长尾可能
</span><ul>
<li data-line="11">霍夫丁不等式（Hoeffding's inequality）—&gt; 用于检测i.i.d</li>
</ul>
</li>
<li data-line="12">训练/测试比例：在划分数据集的时候，我们需要确定训练集和测试集的比例。一般来说，如果数据足够多，我们可以采用70%/30%或者80%/20%的比例。但是，如果数据不够多，我们可能就需要采用更多的数据进行训练，比如90%/10%。</li>
<li data-line="13" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 数据泄露：在交叉验证中，我们需要确保在模型训练的过程中，不会有任何测试集的信息被泄露。否则，模型的表现可能会被高估。</span></li>
<li data-line="14">计算资源：交叉验证需要多次训练和测试模型，因此需要更多的计算资源。在实际应用中，我们需要根据可用的计算资源和数据的大小，来选择合适的交叉验证方法。例如，如果数据量很大，可能无法进行k-fold交叉验证，而需要采用更节省计算资源的方法，如留一法或者随机划分。</li>
</ul>
</li>
<li data-line="15">AIC (赤池信息准则): AIC是一种基于信息论的模型选择准则。它考虑了模型的复杂度和模型对数据的拟合程度。AIC的值越小，说明模型的性能越好。</li>
<li data-line="16">BIC (贝叶斯信息准则): BIC和AIC类似，都是基于信息论的模型选择准则。不同的是，BIC对模型的复杂度惩罚更严厉。因此，如果模型过于复杂，即使模型对数据的拟合程度很好，BIC的值也可能很大。</li>
<li data-line="17">调整的R方: 这是一种用于回归模型的模型选择准则。调整的R方不仅考虑了模型的拟合程度，还考虑了模型的复杂度。调整的R方的值越大，说明模型的性能越好。</li>
<li data-line="18">F检验: F检验是一种用于比较两个或多个模型的统计检验方法。通过计算F统计量和对应的p值，可以判断哪个模型的性能更好。</li>
<li data-line="19">Precision, Recall, F1 Score, ROC AUC等分类性能指标: 这些指标主要用于比较分类模型的性能。</li>
<li data-line="20">网格搜索和随机搜索: 这两种方法主要用于模型超参数的选择，但也可以间接用于模型选择。通过搜索不同的超参数组合，可以找到性能最好的模型和对应的超参数。</li>
<li data-line="21">学习曲线: 通过画出模型在训练集和验证集上的学习曲线，可以观察模型的学习情况，从而判断哪个模型的性能更好。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="特征选择" class="heading" id="特征选择"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>特征选择</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://ytzfhqs.github.io/AAAMLP-CN/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/" rel="noopener" class="external-link" href="https://ytzfhqs.github.io/AAAMLP-CN/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/" target="_blank">特征选择 - AAAMLP 中译版</a></p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>Features selection</strong> </p>
</span><ul>
<li data-line="1">好处: 提高准确性, 减少性能开销, 帮助我们理解</li>
<li data-line="2"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>方法
<ul>
<li data-line="3"><strong>Filters</strong> : use statistical test to find which features have highest correlation with the output</li>
<li data-line="4"><strong>Wrappers</strong> : Search algorithm trains and evaluates(like Kendall’s Tau)</li>
<li data-line="5"><strong>Embedded</strong> :Some methods(like Lasso) do automatic feature selection internally</li>
</ul>
</li>
<li data-line="6"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><em><strong>Correct way to do features selection</strong></em>
<ul>
<li data-line="7">re-do feature selection on each round of cross-validation on the training folds(nested cross validation)</li>
<li data-line="8"><img alt="kOiTYpeAhyv9W1f.png" src="https://cdn.sa.net/2024/04/13/kOiTYpeAhyv9W1f.png" referrerpolicy="no-referrer"></li>
</ul>
</li>
<li data-line="9"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>Wrong way to do features selection</strong> 
<ul>
<li data-line="10">do feature selection as pre-processing before cross-validation, because using test data for selecting features can lead to hugely overoptimistic result</li>
</ul>
</li>
</ul>
</li>
<li data-line="14" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> VarianceThreshold</p>
</span></li>
<li data-line="15" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> Persson' correlation</p>
</span></li>
<li data-line="16"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p>单变量</p>
<ul>
<li data-line="17"><strong>互信息</strong></li>
<li data-line="18"><strong>方差分析</strong></li>
<li data-line="19"><strong>F 检验</strong></li>
<li data-line="20"><strong>卡方检验</strong></li>
</ul>
</li>
<li data-line="22" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> 贪婪特征选择(简单暴力)</p>
</span></li>
<li data-line="23"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p>特征重要性</p>
<ul>
<li data-line="24">有些模型支持输出特征的重要性. e.g.随机森林</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="超参数优化" class="heading" id="超参数优化"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>超参数优化</h3><div class="heading-children"></div></div></div></div><div class="heading-wrapper"><h2 data-heading="监督学习(Supervised)" class="heading" id="监督学习(Supervised)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>监督学习(Supervised)</h2><div class="heading-children"><div class="heading-wrapper"><h3 data-heading="逻辑回归(LogR)" class="heading" id="逻辑回归(LogR)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>逻辑回归(LogR)</h3><div class="heading-children"><div><p>线性回归 (LR)</p></div><div><p>多项式回归 (PR)</p></div><div><p>Lasso 回归</p></div><div><p>Ridge 回归</p></div><div><p>弹性网络（Elastic Net)</p></div></div></div><div class="heading-wrapper"><h3 data-heading="决策树(DT)" class="heading" id="决策树(DT)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>决策树(DT)</h3><div class="heading-children"><div><p><span style="background:#fff88f">随机森林（Random Forest</span>）通过创建多个决策树，并对其输出进行平均或多数投票，以生成最终预测。<br>
每个决策树都是在从原始数据集中随机抽样得到的新数据集上进行训练的。这种随机抽样过程即为Bootstrap。每个决策树都是独立并行训练的，没有一个决策树会依赖于其他决策树的结果。</p></div><div><p><span style="background:#fff88f">Gradient Boosted Trees（梯度提升树）</span>是一种使用梯度提升方法训练决策树模型的机器学习技术。这种方法的基本思想是，通过迭代地添加新的决策树模型，每一步都根据前一步模型的预测误差来训练新的模型，从而逐步提升整个模型的预测性能。</p></div><div><p>梯度提升树的工作过程如下：</p></div><div><ol>
<li data-line="0"><strong>初始化</strong>：使用一个简单的模型（如常数）作为初始预测。</li>
<li data-line="1"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>循环</strong>：以下步骤会被重复多次：
<ul>
<li data-line="2">计算每个样本的残差（即真实值与当前模型的预测值之间的差）。</li>
<li data-line="3">使用这些残差作为目标，训练一个新的决策树模型。</li>
<li data-line="4">将新模型的预测加入到当前模型的预测中，通常会乘以一个小于1的步长（也叫做学习率）来降低每一步的影响，从而更稳定地提升性能。</li>
</ul>
</li>
<li data-line="5"><strong>输出</strong>：所有决策树模型的预测结果相加，得到最终的预测。</li>
</ol></div><div><p>梯度提升树模型的一大优点是，它可以自然地处理各种类型的特征（例如连续特征、离散特征等），并且通常可以提供较高的预测性能。然而，它也有一些缺点，例如训练过程可能需要较长的时间，以及需要仔细调整参数（例如树的数量、树的深度、学习率等）才能获得最佳性能。</p></div></div></div><div class="heading-wrapper"><h3 data-heading="支持向量机(SVM)" class="heading" id="支持向量机(SVM)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>支持向量机(SVM)</h3><div class="heading-children"><div><ul>
<li data-line="0">算法原理</li>
<li data-line="1"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>特点
<ul>
<li data-line="2">计算特别慢</li>
</ul>
</li>
<li data-line="3"></li>
</ul></div><div><p>线性判断分析(LDA)，原理和SVM类似，但是分割特征空间依赖EightValue。这个方法有点老了，效果也一般。</p></div></div></div><div class="heading-wrapper"><h3 data-heading="多层感知机(MLP)" class="heading" id="多层感知机(MLP)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>多层感知机(MLP)</h3><div class="heading-children"></div></div><div class="heading-wrapper"><h3 data-heading="最近邻(kNN)" class="heading" id="最近邻(kNN)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>最近邻(kNN)</h3><div class="heading-children"></div></div><div class="heading-wrapper"><h3 data-heading="朴素贝叶斯(NB)" class="heading" id="朴素贝叶斯(NB)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>朴素贝叶斯(NB)</h3><div class="heading-children"><div><p>贝叶斯优化器 (Bayesian Optimization)</p></div><div><p>贝叶斯网络 (Bayesian Network)</p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="集成学习(Ensemble)" class="heading" id="集成学习(Ensemble)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>集成学习(Ensemble)</h2><div class="heading-children"><div class="heading-wrapper"><h3 data-heading="堆叠（Stacking）" class="heading" id="堆叠（Stacking）"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>堆叠（Stacking）</h3><div class="heading-children"><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p>Stacking方法则是通过训练一个元模型（meta-model）来结合多个不同的预测模型的预测结果</p></div></div></div><div><p><img src="https://ytzfhqs.github.io/AAAMLP-CN/figures/AAAMLP_page280_image.png" referrerpolicy="no-referrer"></p></div></div></div><div class="heading-wrapper"><h3 data-heading="提升（Boosting）" class="heading" id="提升（Boosting）"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>提升（Boosting）</h3><div class="heading-children"><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p>Boosting方法（如AdaBoost或Gradient Boosting）是通过顺序地训练预测模型，每个模型都试图纠正其前一个模型的错误</p></div></div></div><div><p>梯度提升树 (GBT)</p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> XGBoost
</span><ul>
<li data-line="1">它是一种Gradient-Boosting Tree的实现方法，在大规模数据集上表现出色。计算效率高预测性能优良</li>
<li data-line="2"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>特点
<ul>
<li data-line="3"><strong>正则化</strong>：XGBoost在模型中引入了正则化项，以控制模型的复杂度。这有助于防止过拟合，提高模型的泛化能力。</li>
<li data-line="4"><strong>并行处理</strong>：虽然梯度提升本身是一个序列过程（每一步都需要前一步的输出），但XGBoost实现了特征的并行计算，从而提高了计算效率。</li>
<li data-line="5"><strong>灵活性</strong>：XGBoost支持自定义优化目标和评价准则，增加了模型的灵活性。</li>
<li data-line="6"><strong>内置交叉验证</strong>：在每一轮的提升迭代过程中，XGBoost可以利用内置的交叉验证，在确定最优次数的同时，防止过拟合。</li>
<li data-line="7"><strong>处理缺失值</strong>：XGBoost能够自动处理缺失值，这在实际的数据科学项目中非常有用，因为很多数据集都会包含一些缺失的数据。</li>
<li data-line="8"><strong>树剪枝</strong>：XGBoost使用了一种贪心算法来进行树的分裂和剪枝，这使得它生成的树比传统的梯度提升算法更加复杂，但也更有效。</li>
</ul>
</li>
</ul>
</li>
</ul></div><div><p>LightGBM</p></div><div><p>CatBoost</p></div><div><p>AdaBoost</p></div></div></div><div class="heading-wrapper"><h3 data-heading="装袋（Bootstrap aggregating, Bagging）" class="heading" id="装袋（Bootstrap_aggregating,_Bagging）"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>装袋（Bootstrap aggregating, Bagging）</h3><div class="heading-children"><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p>通过结合多个独立的模型来降低泛化误差。Bagging特别适用于处理高方差的学习算法，如决策树。</p>
<p>好处由提高模型稳定性，防止过拟合，处理不平衡数据，减缓极端样本干扰</p></div></div></div><div><p><span style="background:#fff88f">Extra Trees</span>是一种集成学习方法，全称是Extremely Randomized Trees（极度随机化树）。它是随机森林的一个变种，同样基于决策树算法，通过建立多个决策树并取其平均预测结果来降低模型的方差。</p></div><div><p>Extra Trees与随机森林的主要区别在于，它在构建决策树时引入了更多的随机性。在随机森林中，每个节点的分裂特征是从随机特征子集中选择的最优特征；而在Extra Trees中，每个节点的分裂特征是从随机特征子集中随机选择的，分裂阈值也是随机设定的。这样的随机性使得Extra Trees生成的决策树更加多样化，有可能降低模型的方差。</p></div><div><p>然而，这种增强的随机性也可能增加模型的偏差。因此，Extra Trees和随机森林之间的选择取决于具体的任务和数据。在某些情况下，Extra Trees可能会比随机森林表现得更好，反之亦然。</p></div><div><p><span style="background:#fff88f">XGBoost</span>，全称是Extreme Gradient Boosting，是一种优化的分布式梯度提升库，设计之初就致力于提高计算速度和模型性能。它是一种监督学习算法，可以用于分类、回归和排名等任务。</p></div><div><p>XGBoost的主要特性包括：</p></div><div><ol>
<li data-line="0"><strong>正则化</strong>：XGBoost在损失函数中引入了正则化项，用于控制模型的复杂度。这有助于防止过拟合，使得XGBoost在很多情况下的性能超过其他梯度提升算法。</li>
<li data-line="1"><strong>并行处理</strong>：虽然梯度提升本身是一个序列过程（每一步都需要知道前一步的结果），但XGBoost在特征层面上实现了并行化，这大大提高了计算速度。</li>
<li data-line="2"><strong>处理稀疏数据</strong>：XGBoost可以直接处理稀疏数据，无需先进行填充或者删除缺失值。</li>
<li data-line="3"><strong>内置交叉验证</strong>：XGBoost内置了交叉验证功能，在每一轮迭代过程中都可以使用交叉验证，从而找到最优的迭代停止点。</li>
<li data-line="4"><strong>灵活性</strong>：XGBoost允许用户自定义优化目标和评价准则，这使得它可以适用于各种各样的问题。</li>
<li data-line="5"><strong>可处理大规模数据</strong>：XGBoost支持分布式计算，可以处理大规模数据集。</li>
</ol></div><div><p>XGBoost已经在许多数据科学竞赛中取得了优异的成绩，并且被广泛应用于各种实际问题中。</p></div><div><p><span style="background:#fff88f">LightGBM</span>，全名为Light Gradient Boosting Machine，是一种基于梯度提升（Gradient Boosting）的高效机器学习算法。它由微软开发，旨在提供更快的训练速度和更高的效率。</p></div><div><p>以下是LightGBM的一些主要特点：</p></div><div><ol>
<li data-line="0"><strong>更高的效率和速度</strong>：通过使用直方图剪枝和梯度基础一阶和二阶统计量的直方图优化，LightGBM能够提供比传统的梯度提升方法更快的训练速度和更高的效率。</li>
<li data-line="1"><strong>更低的内存使用</strong>：LightGBM使用直方图减少技术，可以显著减少内存使用。</li>
<li data-line="2"><strong>支持类别特征</strong>：LightGBM可以直接处理类别特征（无需进行独热编码），这可以进一步提高模型的效率和效果。</li>
<li data-line="3"><strong>支持并行学习</strong>：LightGBM支持并行学习，可以利用多核CPU进行更快的训练。</li>
<li data-line="4"><strong>优秀的精度</strong>：尽管LightGBM在速度和效率上进行了优化，但它并未牺牲模型的精度。实际上，LightGBM在许多机器学习竞赛和任务中都表现出了优秀的预测性能。</li>
<li data-line="5"><strong>处理大规模数据</strong>：由于其高效的实现，LightGBM可以处理大规模的数据集，而不会导致内存溢出。</li>
<li data-line="6"><strong>支持GPU加速</strong>：LightGBM支持使用GPU进行训练，这可以进一步提高训练速度。</li>
</ol></div><div><p>LightGBM和XGBoost都是基于梯度提升的机器学习算法，都具有出色的预测性能和高度的灵活性。但是，它们在实现方式和一些特性上有所不同：</p></div><div><ol>
<li data-line="0"><strong>训练速度和内存使用</strong>：LightGBM通常比XGBoost更快，使用的内存也更少。这主要归功于LightGBM的两个主要技术：Gradient-based One-Side Sampling (GOSS) 和 Exclusive Feature Bundling (EFB)。GOSS保留了具有大梯度的数据样本，而忽略了小梯度的样本，因此可以减少计算的复杂性。EFB则是一种捆绑稀疏特征的方法，可以减少特征的数量，从而减少内存的使用。</li>
<li data-line="1"><strong>处理类别特征</strong>：LightGBM可以直接处理类别特征，而无需先进行独热编码。这可以减少数据的维度，并提高计算速度。</li>
<li data-line="2"><strong>树的生长策略</strong>：XGBoost使用的是预剪枝（pre-pruning），也就是在树达到一定深度后停止分裂，从而避免过拟合。而LightGBM使用的是带深度限制的后剪枝（leaf-wise with depth limit），它会优先选择最大梯度的叶子进行分裂，然后使用深度限制来防止过拟合。</li>
<li data-line="3"><strong>并行处理</strong>：虽然XGBoost和LightGBM都支持并行处理，但是LightGBM的并行效率通常更高。这是因为LightGBM使用了特征并行和数据并行两种并行策略，而XGBoost主要使用了特征并行。</li>
<li data-line="4"><strong>GPU支持</strong>：XGBoost和LightGBM都支持使用GPU进行训练，但LightGBM的GPU训练通常比XGBoost更快。</li>
</ol></div><div><p>因此，如果你在处理大规模数据集，或者对训练速度和内存使用有严格要求，那么LightGBM可能是更好的选择。然而，这并不意味着LightGBM在所有情况下都比XGBoost更优，哪种算法更适合取决于具体的应用场景和需求。</p></div><div><p><span style="background:#fff88f">CatBoost</span> ， 由Yandex发明， 是一种基于梯度提升树(Gradient Boosting Decision Trees,,GBDT)的机器学习算法，它<br>
专门针对类别特征(Categorical Features)进行了优化。CatBoost的名称中的 “cat"就是指这个特<br>
性。</p></div><div><p>以下是CatBoostl的基本原理：</p></div><div><ol>
<li data-line="0"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>梯度提升树(Gradient Boosting Decision Trees):
<ul>
<li data-line="1">CatBoost基于梯度提升树方法，这是一种集成学习技术，过将许多弱学习者（通常是决策树)组合成一个强学习者来进行预测。</li>
<li data-line="2">在梯度提升树中，模型是通过迭代地训练一系列的决策树而得到的。每个新的树都会尝试纠正前面所有树的预测误差。</li>
</ul>
</li>
<li data-line="3"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>特征处理：
<ul>
<li data-line="4">CatBoost专门优化了对类别特征的处理。传统的GBDT算法通常要求将类别特征转换为数值型特征，例如使用独热编码。而CatBoosti可以直接处理原始的类别特征，不需要额外的预处理步骤。</li>
</ul>
</li>
<li data-line="5"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>自动特征缩放：
<ul>
<li data-line="6">CatBoosti能够自动地处理特征的缩放问题，这意味着您无需手动进行特征缩放（如标准化或归一化)。</li>
</ul>
</li>
<li data-line="7">对称性生长<br>
CatBoost使用对称树性长策略，这意味着在决策树的海一层都会考虑到所有特征，而不是像其他算法那样依次考虑每个特征</li>
</ol></div><div><p><span style="background:#fff88f">AdaBoost</span>，全名是Adaptive Boosting，是一种集成学习算法，它的主要目标是将多个弱学习器结合起来，形成一个强学习器。弱学习器通常是指那些仅比随机猜测稍好的模型，例如深度为1的决策树（也被称为决策树桩）。</p></div><div><p>AdaBoost的工作原理如下</p></div><div><ol>
<li data-line="0"><strong>初始化权重</strong>：AdaBoost开始时，会给每个训练样本分配一个权重，这些权重初始时都是相等的。</li>
<li data-line="1"><strong>训练弱学习器</strong>：然后，AdaBoost会在权重分布下训练一个弱学习器。训练完成后，AdaBoost会计算这个弱学习器的错误率（即，被错误分类的样本的权重之和）。</li>
<li data-line="2"><strong>更新权重</strong>：接着，AdaBoost会增加那些被错误分类的样本的权重，并降低那些被正确分类的样本的权重。这样做的目的是让那些被错误分类的样本在下一轮的训练中得到更多的关注。</li>
<li data-line="3"><strong>重复训练</strong>：然后，AdaBoost会在更新过的权重分布下训练下一个弱学习器。这个过程会被重复多次，直到达到预定的学习器数量，或者错误率已经足够低。</li>
<li data-line="4"><strong>组合弱学习器</strong>：最后，AdaBoost会将所有的弱学习器组合起来，形成一个强学习器。每个弱学习器的投票权重取决于其自身的错误率——错误率越低的学习器，其投票权重越大。</li>
</ol></div><div><p>AdaBoost的这种自适应性使其在许多问题上都表现出了优秀的性能。然而，它也有一些缺点，例如对噪声和异常值敏感，以及可能会过拟合等。</p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="非监督学习(Unsupervised)" class="heading" id="非监督学习(Unsupervised)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>非监督学习(Unsupervised)</h2><div class="heading-children"><div class="heading-wrapper"><h3 data-heading="聚类" class="heading" id="聚类"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>聚类</h3><div class="heading-children"><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p>聚类稳健标准误(Cluster robust standard error) 是什么? 在什么场景中发挥作用?</p>
<p>聚类稳健标准误是一种用于处理数据中存在的聚类相关性的统计技术。在许多经济、社会和医学研究中，数据通常会存在某种形式的聚类结构，例如，数据可能是在不同的地理区域、时间点或组织中收集的。在这种情况下，数据中的观察值可能不是独立同分布的，而是在同一个聚类中的观察值可能比在不同聚类中的观察值更相关。这种聚类相关性可能会导致传统的标准误估计方法（例如，普通最小二乘法）产生偏误，因此需要使用聚类稳健标准误进行修正。</p>
<p>聚类稳健标准误（Cluster robust standard errors）的计算方法考虑了数据中的这种聚类相关性，能够提供更准确的标准误估计。</p>
<p>在以下场景中，聚类稳健标准误发挥重要作用：</p>
<ol>
<li>
<p>数据存在聚类结构：例如，数据是从不同的学校、医院、公司或地理区域收集的。</p>
</li>
<li>
<p>数据中存在聚类相关性：例如，同一聚类（如同一家公司的员工）中的观察值可能比不同聚类中的观察值更相关。</p>
</li>
<li>
<p>需要进行推断统计分析：例如，计算置信区间或进行假设检验。在这些情况下，准确的标准误估计是必要的。</p>
</li>
<li>
<p>在进行面板数据（Panel Data）分析时，例如固定效应模型，随机效应模型等，常常需要考虑聚类稳健标准误以获取更准确的推断。</p>
</li>
</ol></div></div></div><div class="heading-wrapper"><h4 data-heading="K-means 聚类" class="heading" id="K-means_聚类"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>K-means 聚类</h4><div class="heading-children"></div></div><div class="heading-wrapper"><h4 data-heading="层次聚类" class="heading" id="层次聚类"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>层次聚类</h4><div class="heading-children"></div></div><div class="heading-wrapper"><h4 data-heading="DBSCAN 聚类" class="heading" id="DBSCAN_聚类"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>DBSCAN 聚类</h4><div class="heading-children"></div></div><div class="heading-wrapper"><h4 data-heading="AP聚类" class="heading" id="AP聚类"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>AP聚类</h4><div class="heading-children"></div></div><div class="heading-wrapper"><h4 data-heading="谱聚类" class="heading" id="谱聚类"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>谱聚类</h4><div class="heading-children"></div></div></div></div><div class="heading-wrapper"><h3 data-heading="降维" class="heading" id="降维"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>降维</h3><div class="heading-children"><div><p>数据减维的作用是减低数据的维度同时尽可能保留原有数据的重要信息。其目的1是减少计算复杂度，2是避免维度灾难。</p></div><div class="heading-wrapper"><h4 data-heading="主成分分析 (PCA)" class="heading" id="主成分分析_(PCA)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>主成分分析 (PCA)</h4><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=shSiNlWeDh4" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=shSiNlWeDh4" target="_blank">PCA 主成分分析讲了什么？具体怎么使用？十分钟包教包会！</a></p></div><div><p>PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量。PCA的主要思想是将n维特征映射到k维上（k&lt;n），这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。PCA的目标是使得新的k维特征对应的<strong>方差尽可能大</strong>（即保留最多的原始信息）。</p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 调整空间基使其新维度特征对应的方差尽可能大</span></li>
</ul></div></div></div><div class="heading-wrapper"><h4 data-heading="独立成分分析 (ICA)" class="heading" id="独立成分分析_(ICA)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>独立成分分析 (ICA)</h4><div class="heading-children"><div><p>独立成分分析（Independent Component Analysis，ICA）是一种计算方法，用于从多元统计数据中找出隐藏的因素或成分。这些成分被假设为统计上的独立的非高斯随机变量。ICA在许多领域都有应用，包括数字图像处理、医学图像分析、生物信息学、脑机接口等。</p></div><div><p>ICA的主要思想是，观测到的多元数据（例如，多声道的声音记录）可以被看作是若干个统计独立的源（例如，不同人的声音）的线性混合。ICA的目标就是从观测数据中恢复出这些独立的源。</p></div><div><p>ICA的基本模型可以写成如下形式：</p></div><div><p><span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span></p></div><div><p>其中，x&nbsp;是我们观测到的数据，s&nbsp;是独立的源，A&nbsp;是混合矩阵。ICA的任务就是估计出混合矩阵&nbsp;A&nbsp;的逆矩阵&nbsp;<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math></mjx-container></span>，从而可以得到源&nbsp; s。</p></div><div><p>ICA与PCA（主成分分析）和因子分析等方法的主要区别在于，ICA假设源是非高斯的并且是统计独立的，而PCA和因子分析通常只假设源是不相关的（即协方差为零）。因此，ICA可以提供更强的源分离能力。</p></div></div></div><div class="heading-wrapper"><h4 data-heading="线性判别分析 (LDA)" class="heading" id="线性判别分析_(LDA)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>线性判别分析 (LDA)</h4><div class="heading-children"><div><p>LDA也是一种线性降维技术，但与PCA不同的是，LDA是一种监督学习的降维技术，它考虑了类别标签的信息，目标是使得同类样本的投影点尽可能接近，不同类样本的类均值之间的距离尽可能大。</p></div></div></div><div class="heading-wrapper"><h4 data-heading="t-分布邻近嵌入 (t-SNE)" class="heading" id="t-分布邻近嵌入_(t-SNE)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>t-分布邻近嵌入 (t-SNE)</h4><div class="heading-children"><div><ul>
<li data-line="0">由 Laurens van der Maaten 和 Geoffrey Hinton 在 2008 年提出的一种机器学习算法</li>
<li data-line="1">是否线性：否</li>
<li data-line="2">特点：</li>
</ul></div><div><p>t-SNE是一种非线性降维技术，它能够保持数据点之间的相对距离，特别适合于可视化高维数据。</p></div></div></div><div class="heading-wrapper"><h4 data-heading="UMAP" class="heading" id="UMAP"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>UMAP</h4><div class="heading-children"><div><ul>
<li data-line="0">UMAP是由Leland McInnes和John Healy在2018年提出的</li>
</ul></div><div><ul>
<li data-line="0">是否线性：否</li>
<li data-line="1">特点：比t-SNE计算效率更高可以对更大数据量进行有效scale；参数少对超参不敏感；更好的保留原始数据中高维数据的局部结构</li>
<li data-line="2"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>算法原理：
<ul>
<li data-line="3">UMAP的核心思想是基于拓扑结构设它通过在原始高维空间中寻找数据点之间的局部相似性，并将其映射到低维空间中的相应点，以保留数据的局部结构。</li>
<li data-line="4">UMAP的主要步骤包括构建局部邻域图、优化低维空间中的点之间的距离，并最终将高维数据映射到低维空间。</li>
</ul>
</li>
<li data-line="5">实际在MINST降维效果比t-SNE差，但是库的运行效率比sklearn的tSNE高</li>
</ul></div></div></div><div class="heading-wrapper"><h4 data-heading="自编码器(Autoencoder)" class="heading" id="自编码器(Autoencoder)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>自编码器(Autoencoder)</h4><div class="heading-children"><div><p>自编码器是一种神经网络，它可以学习数据的低维度表示。它由一个编码器和一个解码器组成，编码器将高维输入数据编码为低维表示，然后解码器将这个低维表示解码为高维数据。训练自编码器的目标是使得解码的输出尽可能接近原始输入。</p></div></div></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="强化学习(RF)" class="heading" id="强化学习(RF)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>强化学习(RF)</h2><div class="heading-children"><div><p>Q-learning</p></div><div><p>SARSA</p></div><div><p> PPO</p></div><div><p>DQN</p></div><div><p>DDPG</p></div><div><p>A3c</p></div><div><p>SAC</p></div><div class="admonition-parent admonition-info-parent"><div class="callout admonition admonition-info admonition-plugin " style="--callout-color: 0, 184, 212;" data-callout="info" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="info-circle" class="svg-inline--fa fa-info-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"></path></svg></div><div class="callout-title-inner admonition-title-content">Info</div></div><div class="callout-content admonition-content"><p>AlphaGo是由DeepMind（一个属于Google的公司）开发的人工智能程序，它是第一个战胜人类围棋世界冠军的机器。AlphaGo背后的算法原理主要包括深度学习和强化学习两部分。</p>
<ol>
<li><strong>深度学习</strong>：AlphaGo使用了深度神经网络来理解围棋的状态。它有两个主要的CNNs，一个是策略网络，用于预测下一步的最佳行动；另一个是价值网络，用于评估当前棋局的状态，即预测从当前状态开始的游戏结果。这两个网络都是通过监督学习（学习人类专家的棋局）和强化学习（自我对弈）进行训练的。</li>
<li><strong>蒙特卡洛树搜索（MCTS）</strong>：AlphaGo使用了一种被称为蒙特卡洛树搜索的算法来决定它的行动。在每一步，它使用其神经网络来评估可能的行动，并使用这些评估来指导搜索。通过这种方式，它可以专注于最有希望的行动，而不是浪费时间去考虑不太可能的行动。</li>
<li><strong>强化学习</strong>：AlphaGo通过强化学习进行自我对弈，不断改进其策略网络和价值网络。在每一次游戏中，它都会根据游戏的结果来调整其预测，使其更接近真实的结果。</li>
</ol>
<p>这三个部分相结合，使得AlphaGo能够理解和玩围棋的能力超越任何人类或者之前的计算机程序。</p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="模拟算法" class="heading" id="模拟算法"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>模拟算法</h2><div class="heading-children"><div><blockquote>
<p>算法都是通过模拟一种或者多种<strong>随机过程</strong>来进行学习或者优化，因此被称为基于模拟的算法。</p>
</blockquote></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 蒙特卡洛模拟(Monte Carlo Simulation)：这是一种统计模拟方法，通过从概率分布中随机抽样来计算数值结果。在机器学习中，蒙特卡洛模拟经常被用来估计复杂模型的参数，或者用来评估模型的预测性能。</span></li>
<li data-line="1" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 马尔科夫链蒙特卡洛法（MCMC）：这是一种基于蒙特卡洛模拟的统计方法，通过构建一个马尔科夫链来进行随机抽样。MCMC在机器学习中的应用广泛，例如在贝叶斯推断中用来估计后验分布。</span></li>
<li data-line="2" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 模拟退火算法：这种算法通过模拟物理退火过程来找到函数的全局最小值。在机器学习中，模拟退火算法常被用来解决优化问题，例如神经网络的权重优化。</span></li>
<li data-line="3" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 遗传算法：这种算法通过模拟自然选择过程来进行优化。在机器学习中，遗传算法常被用来解决特征选择或者参数调优等问题。</span></li>
<li data-line="4" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 粒子滤波：这是一种基于蒙特卡洛模拟的递归贝叶斯滤波方法。在机器学习中，粒子滤波常被用来解决非线性非高斯的状态估计问题。</span></li>
</ul></div></div></div><div class="heading-wrapper"><h2 data-heading="其他问题" class="heading" id="其他问题"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>其他问题</h2><div class="heading-children"><div><p>数据不平衡问题、模型可解释性问题、数据隐私问题</p></div></div></div><div class="heading-wrapper"><h2 data-heading="阅读笔记" class="heading" id="阅读笔记"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>阅读笔记</h2><div class="heading-children"><div class="admonition-parent admonition-note-parent"><div class="callout admonition admonition-note admonition-plugin " style="--callout-color: 68, 138, 255;" data-callout="note" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="pencil-alt" class="svg-inline--fa fa-pencil-alt fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Note</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://ytzfhqs.github.io/AAAMLP-CN/" rel="noopener" class="external-link" href="https://ytzfhqs.github.io/AAAMLP-CN/" target="_blank">AAAMLP 中译版</a></p>
<p><strong>Approaching (Almost) Any Machine Learning Problem</strong></p>
<p>章节总结:</p>
<ol>
<li><strong>准备环境</strong></li>
<li><strong>无监督和有监督学习</strong></li>
<li><strong>交叉检验</strong></li>
<li><strong>评估指标</strong>&nbsp;</li>
<li><strong>组织机器学习</strong></li>
<li><strong>处理分类变量</strong></li>
<li><strong>特征工程</strong></li>
<li><strong>特征选择</strong></li>
<li><strong>超参数优化</strong></li>
<li><strong>图像分类和分割方法</strong></li>
<li><strong>文本分类或回归方法</strong></li>
<li><strong>组合和堆叠方法</strong></li>
<li><strong>可重复代码和模型方法</strong> 使用Docker一键环境</li>
</ol></div></div></div><div><hr></div><div><p>高斯混合模型 (GMM)、聚类分析 (CA)、K 均值聚类 (K-means)、DBSCAN、HDBSCAN、层次聚类 (HC)</p></div><div><p>、深度信念网络 (DBN)、自动编码器 (AE)、强化学习 (RL)、Q-learning、SARSA、DDPG、A3C、SAC、时序差分学习 (TD)、Actor-Critic、对抗训练 (Adversarial Training)、梯度下降 (GD)、随机梯度下降 (SGD)、批量梯度下降 (BGD)、Adam、RMSprop、AdaGrad、AdaDelta、Nadam、交叉熵损失函数 (Cross-Entropy Loss)、均方误差损失函数 (Mean Squared Error Loss)、KL 散度损失函数 (KL Divergence Loss)、Hinge 损失函数、感知器 (Perceptron)、RBF 神经网络、Hopfield 网络、Boltzmann 机、深度强化学习 (DRL)、自监督学习 (Self-supervised Learning)、迁移学习 (Transfer Learning)、泛化对抗网络 (GAN)、对抗生成网络 (GAN)、训练生成网络 (TGAN)、CycleGAN、深度学习生成模型 (DLGM)、自动编码器生成对抗网络 (AEGAN)、分布式自编码器 (DAE)、网络激活优化器 (NAO)、自编码器 (Autoencoder)、VQ-VAE、LSTM-VAE、卷积自编码器 (CAE)、GAN 自编码器 (GANAE)、U-Net、深度 Q 网络 (DQN)、双重 DQN (DDQN)、优先回放 DQN (Prioritized Experience Replay DQN)、多智能体 DQN (Multi-agent DQN)、深度确定性策略梯度 (DDPG)、感知器 (Perceptron)、稀疏自编码器 (SAE)、稀疏表示分类 (SRC)、深度置信网络 (DBN)、</p></div><div><p>极限梯度提升树 (XGBoost)、AdaBoost、梯度提升机 (Gradient Boosting Machine)、</p></div><div><p>、EM 算法 (Expectation-Maximization Algorithm)、高斯过程 (Gaussian Process)、马尔科夫链蒙特卡洛 (MCMC)、</p></div><div><p>强化学习 (Reinforcement Learning)、无监督学习 (Unsupervised Learning)、半监督学习 (Semi-supervised Learning)、监督学习 (Supervised Learning)、迁移学习 (Transfer Learning)、维数约简 (Dimensionality Reduction)、特征选择 (Feature Selection)、特征提取 (Feature Extraction)、正则化 (Regularization)、标准化 (Normalization)、聚类 (Clustering)、分类 (Classification)、回归 (Regression)、降维 (Dimensionality Reduction)、特征映射 (Feature Mapping)、</p></div><div><p>神经元 (Neuron)、激活函数 (Activation Function)、损失函数 (Loss Function)、优化器 (Optimizer)、学习率 (Learning Rate)、批次大小 (Batch Size)、迭代次数 (Epoch)、超参数 (Hyperparameter)、模型评估 (Model Evaluation)、交叉验证 (Cross Validation)、混淆矩阵 (Confusion Matrix)、ROC 曲线 (ROC Curve)、AUC 值 (AUC Value)、精确度 (Precision)、召回率 (Recall)、F1 分数 (F1 Score)、模型解释 (Model Interpretability)、特征重要性 (Feature Importance)、局部解释 (Local Explanation)、全局解释 (Global Explanation)、机器学习管道 (Machine Learning Pipeline)、一键生成模型 (AutoML)、超参数优化 (Hyperparameter Tuning)、FFT、拉普拉斯变换、z变换、傅里叶变换、短时傅里叶变换 (STFT)、IIR、FIR、卡尔曼滤波、DIP算法、小波变换</p></div><div class="mod-footer"></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#机器学习常用算法"><div class="tree-item-contents heading-link" heading-name="机器学习常用算法"><span class="tree-item-title">机器学习常用算法</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#基础知识"><div class="tree-item-contents heading-link" heading-name="基础知识"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">基础知识</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#数据预处理"><div class="tree-item-contents heading-link" heading-name="数据预处理"><span class="tree-item-title">数据预处理</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#模型评估"><div class="tree-item-contents heading-link" heading-name="模型评估"><span class="tree-item-title">模型评估</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#特征选择"><div class="tree-item-contents heading-link" heading-name="特征选择"><span class="tree-item-title">特征选择</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#超参数优化"><div class="tree-item-contents heading-link" heading-name="超参数优化"><span class="tree-item-title">超参数优化</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#监督学习(Supervised)"><div class="tree-item-contents heading-link" heading-name="监督学习(Supervised)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">监督学习(Supervised)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#逻辑回归(LogR)"><div class="tree-item-contents heading-link" heading-name="逻辑回归(LogR)"><span class="tree-item-title">逻辑回归(LogR)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#决策树(DT)"><div class="tree-item-contents heading-link" heading-name="决策树(DT)"><span class="tree-item-title">决策树(DT)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#支持向量机(SVM)"><div class="tree-item-contents heading-link" heading-name="支持向量机(SVM)"><span class="tree-item-title">支持向量机(SVM)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#多层感知机(MLP)"><div class="tree-item-contents heading-link" heading-name="多层感知机(MLP)"><span class="tree-item-title">多层感知机(MLP)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#最近邻(kNN)"><div class="tree-item-contents heading-link" heading-name="最近邻(kNN)"><span class="tree-item-title">最近邻(kNN)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#朴素贝叶斯(NB)"><div class="tree-item-contents heading-link" heading-name="朴素贝叶斯(NB)"><span class="tree-item-title">朴素贝叶斯(NB)</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#集成学习(Ensemble)"><div class="tree-item-contents heading-link" heading-name="集成学习(Ensemble)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">集成学习(Ensemble)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#堆叠（Stacking）"><div class="tree-item-contents heading-link" heading-name="堆叠（Stacking）"><span class="tree-item-title">堆叠（Stacking）</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#提升（Boosting）"><div class="tree-item-contents heading-link" heading-name="提升（Boosting）"><span class="tree-item-title">提升（Boosting）</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#装袋（Bootstrap_aggregating,_Bagging）"><div class="tree-item-contents heading-link" heading-name="装袋（Bootstrap aggregating, Bagging）"><span class="tree-item-title">装袋（Bootstrap aggregating, Bagging）</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#非监督学习(Unsupervised)"><div class="tree-item-contents heading-link" heading-name="非监督学习(Unsupervised)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">非监督学习(Unsupervised)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item mod-collapsible" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#聚类"><div class="tree-item-contents heading-link" heading-name="聚类"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">聚类</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="4"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#K-means_聚类"><div class="tree-item-contents heading-link" heading-name="K-means 聚类"><span class="tree-item-title">K-means 聚类</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#层次聚类"><div class="tree-item-contents heading-link" heading-name="层次聚类"><span class="tree-item-title">层次聚类</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#DBSCAN_聚类"><div class="tree-item-contents heading-link" heading-name="DBSCAN 聚类"><span class="tree-item-title">DBSCAN 聚类</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#AP聚类"><div class="tree-item-contents heading-link" heading-name="AP聚类"><span class="tree-item-title">AP聚类</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#谱聚类"><div class="tree-item-contents heading-link" heading-name="谱聚类"><span class="tree-item-title">谱聚类</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="3"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#降维"><div class="tree-item-contents heading-link" heading-name="降维"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">降维</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="4"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#主成分分析_(PCA)"><div class="tree-item-contents heading-link" heading-name="主成分分析 (PCA)"><span class="tree-item-title">主成分分析 (PCA)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#独立成分分析_(ICA)"><div class="tree-item-contents heading-link" heading-name="独立成分分析 (ICA)"><span class="tree-item-title">独立成分分析 (ICA)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#线性判别分析_(LDA)"><div class="tree-item-contents heading-link" heading-name="线性判别分析 (LDA)"><span class="tree-item-title">线性判别分析 (LDA)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#t-分布邻近嵌入_(t-SNE)"><div class="tree-item-contents heading-link" heading-name="t-分布邻近嵌入 (t-SNE)"><span class="tree-item-title">t-分布邻近嵌入 (t-SNE)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#UMAP"><div class="tree-item-contents heading-link" heading-name="UMAP"><span class="tree-item-title">UMAP</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#自编码器(Autoencoder)"><div class="tree-item-contents heading-link" heading-name="自编码器(Autoencoder)"><span class="tree-item-title">自编码器(Autoencoder)</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#强化学习(RF)"><div class="tree-item-contents heading-link" heading-name="强化学习(RF)"><span class="tree-item-title">强化学习(RF)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#模拟算法"><div class="tree-item-contents heading-link" heading-name="模拟算法"><span class="tree-item-title">模拟算法</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#其他问题"><div class="tree-item-contents heading-link" heading-name="其他问题"><span class="tree-item-title">其他问题</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/机器学习常用算法.html#阅读笔记"><div class="tree-item-contents heading-link" heading-name="阅读笔记"><span class="tree-item-title">阅读笔记</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>