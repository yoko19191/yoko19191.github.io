<!DOCTYPE html> <html><head>
		<title>深度学习里程碑</title>
		<base href="../">
		<meta id="root-path" root-path="../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="🌱 Digital-Garden - 深度学习里程碑">
		<meta property="og:title" content="深度学习里程碑">
		<meta property="og:description" content="🌱 Digital-Garden - 深度学习里程碑">
		<meta property="og:type" content="website">
		<meta property="og:url" content="💾-科技工程/深度学习里程碑.html">
		<meta property="og:image" content="https://cdn.sa.net/2024/04/12/jUk6e3tn9Tq5uLA.png">
		<meta property="og:site_name" content="🌱 Digital-Garden">
		<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.4.0/pixi.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="https://cdn.jsdelivr.net/npm/minisearch@6.3.0/dist/umd/index.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="preload" href="lib/styles/other-plugins.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/other-plugins.css"></noscript><link rel="stylesheet" href="lib/styles/theme.css"><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript></head><body class="publish css-settings-manager native-scrollbars theme-light show-inline-title ctp-latte ctp-mocha ctp-accent-light-sky ctp-accent-rosewater anuppuccin-accent-toggle anp-current-line anp-h1-red anp-h2-peach anp-h3-green anp-h4-teal anp-h5-lavender anp-h6-mauve anp-bold-red anp-italic-green anp-highlight-yellow anp-full-rainbow-color-toggle"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles">mjx-c.mjx-c1D6FC.TEX-I::before { padding: 0.442em 0.64em 0.011em 0px; content: "α"; }
mjx-c.mjx-c5B::before { padding: 0.75em 0.278em 0.25em 0px; content: "["; }
mjx-c.mjx-c5D::before { padding: 0.75em 0.278em 0.25em 0px; content: "]"; }
mjx-c.mjx-c1D436.TEX-I::before { padding: 0.705em 0.76em 0.022em 0px; content: "C"; }
mjx-c.mjx-c1D463.TEX-I::before { padding: 0.443em 0.485em 0.011em 0px; content: "v"; }
mjx-c.mjx-c1D44C.TEX-I::before { padding: 0.683em 0.763em 0px 0px; content: "Y"; }
mjx-c.mjx-c1D464.TEX-I::before { padding: 0.443em 0.716em 0.011em 0px; content: "w"; }
mjx-c.mjx-c1D6F4.TEX-I::before { padding: 0.683em 0.806em 0px 0px; content: "Σ"; }
mjx-c.mjx-c1D435.TEX-I::before { padding: 0.683em 0.759em 0px 0px; content: "B"; }
mjx-c.mjx-c1D44F.TEX-I::before { padding: 0.694em 0.429em 0.011em 0px; content: "b"; }
mjx-c.mjx-c33::before { padding: 0.665em 0.5em 0.022em 0px; content: "3"; }
mjx-c.mjx-c41::before { padding: 0.716em 0.75em 0px 0px; content: "A"; }
mjx-c.mjx-c65::before { padding: 0.448em 0.444em 0.011em 0px; content: "e"; }
mjx-c.mjx-c6E::before { padding: 0.442em 0.556em 0px 0px; content: "n"; }
mjx-c.mjx-c69::before { padding: 0.669em 0.278em 0px 0px; content: "i"; }
mjx-c.mjx-c5C::before { padding: 0.75em 0.5em 0.25em 0px; content: "\\"; }
mjx-c.mjx-c63::before { padding: 0.448em 0.444em 0.011em 0px; content: "c"; }
mjx-c.mjx-c64::before { padding: 0.694em 0.556em 0.011em 0px; content: "d"; }
mjx-mtext { display: inline-block; text-align: left; }
mjx-c.mjx-c73::before { padding: 0.448em 0.394em 0.011em 0px; content: "s"; }
mjx-c.mjx-c66::before { padding: 0.705em 0.372em 0px 0px; content: "f"; }
mjx-c.mjx-c74::before { padding: 0.615em 0.389em 0.01em 0px; content: "t"; }
mjx-c.mjx-c6D::before { padding: 0.442em 0.833em 0px 0px; content: "m"; }
mjx-c.mjx-c61::before { padding: 0.448em 0.5em 0.011em 0px; content: "a"; }
mjx-c.mjx-c78::before { padding: 0.431em 0.528em 0px 0px; content: "x"; }
mjx-c.mjx-c221A.TEX-S1::before { padding: 0.85em 1.02em 0.35em 0px; content: "√"; }
mjx-c.mjx-c2F::before { padding: 0.75em 0.5em 0.25em 0px; content: "/"; }
mjx-c.mjx-c1D45E.TEX-I::before { padding: 0.442em 0.46em 0.194em 0px; content: "q"; }
mjx-c.mjx-c7B::before { padding: 0.75em 0.5em 0.25em 0px; content: "{"; }
mjx-c.mjx-c7D::before { padding: 0.75em 0.5em 0.25em 0px; content: "}"; }
mjx-c.mjx-c1D70E.TEX-I::before { padding: 0.431em 0.571em 0.011em 0px; content: "σ"; }
mjx-c.mjx-c1D719.TEX-I::before { padding: 0.694em 0.596em 0.205em 0px; content: "ϕ"; }
mjx-c.mjx-c1D716.TEX-I::before { padding: 0.431em 0.406em 0.011em 0px; content: "ϵ"; }
mjx-c.mjx-c1D707.TEX-I::before { padding: 0.442em 0.603em 0.216em 0px; content: "μ"; }
mjx-c.mjx-c1D703.TEX-I::before { padding: 0.705em 0.469em 0.01em 0px; content: "θ"; }
mjx-c.mjx-c1D450.TEX-I::before { padding: 0.442em 0.433em 0.011em 0px; content: "c"; }
mjx-c.mjx-c1D434.TEX-I::before { padding: 0.716em 0.75em 0px 0px; content: "A"; }
mjx-c.mjx-c1D45F.TEX-I::before { padding: 0.442em 0.451em 0.011em 0px; content: "r"; }
mjx-c.mjx-c1D466.TEX-I::before { padding: 0.442em 0.49em 0.205em 0px; content: "y"; }
mjx-c.mjx-cD7::before { padding: 0.491em 0.778em 0px 0px; content: "×"; }
mjx-munderover { display: inline-block; text-align: left; }
mjx-munderover:not([limits="false"]) { padding-top: 0.1em; }
mjx-munderover:not([limits="false"]) > * { display: block; }
mjx-c.mjx-c2211.TEX-S1::before { padding: 0.75em 1.056em 0.25em 0px; content: "∑"; }
mjx-c.mjx-c34::before { padding: 0.677em 0.5em 0px 0px; content: "4"; }
mjx-c.mjx-c1D44E.TEX-I::before { padding: 0.441em 0.529em 0.01em 0px; content: "a"; }
mjx-c.mjx-c22C5::before { padding: 0.31em 0.278em 0px 0px; content: "⋅"; }
mjx-c.mjx-c1D458.TEX-I::before { padding: 0.694em 0.521em 0.011em 0px; content: "k"; }
mjx-c.mjx-c1D438.TEX-I::before { padding: 0.68em 0.764em 0px 0px; content: "E"; }
mjx-c.mjx-c1D45D.TEX-I::before { padding: 0.442em 0.503em 0.194em 0px; content: "p"; }
mjx-c.mjx-c1D45C.TEX-I::before { padding: 0.441em 0.485em 0.011em 0px; content: "o"; }
mjx-c.mjx-c1D460.TEX-I::before { padding: 0.442em 0.469em 0.01em 0px; content: "s"; }
mjx-c.mjx-c1D45B.TEX-I::before { padding: 0.442em 0.6em 0.011em 0px; content: "n"; }
mjx-c.mjx-c30::before { padding: 0.666em 0.5em 0.022em 0px; content: "0"; }
mjx-c.mjx-c1D45A.TEX-I::before { padding: 0.442em 0.878em 0.011em 0px; content: "m"; }
mjx-c.mjx-c1D459.TEX-I::before { padding: 0.694em 0.298em 0.011em 0px; content: "l"; }
mjx-msub { display: inline-block; text-align: left; }
mjx-munder { display: inline-block; text-align: left; }
mjx-over { text-align: left; }
mjx-munder:not([limits="false"]) { display: inline-table; }
mjx-munder > mjx-row { text-align: left; }
mjx-under { padding-bottom: 0.1em; }
mjx-msqrt { display: inline-block; text-align: left; }
mjx-root { display: inline-block; white-space: nowrap; }
mjx-surd { display: inline-block; vertical-align: top; }
mjx-sqrt { display: inline-block; padding-top: 0.07em; }
mjx-sqrt > mjx-box { border-top: 0.07em solid; }
mjx-sqrt.mjx-tall > mjx-box { padding-left: 0.3em; margin-left: -0.3em; }
mjx-c.mjx-c1D44B.TEX-I::before { padding: 0.683em 0.852em 0px 0px; content: "X"; }
mjx-c.mjx-c1D465.TEX-I::before { padding: 0.442em 0.572em 0.011em 0px; content: "x"; }
mjx-c.mjx-c1D43A.TEX-I::before { padding: 0.705em 0.786em 0.022em 0px; content: "G"; }
mjx-c.mjx-c1D443.TEX-I::before { padding: 0.683em 0.751em 0px 0px; content: "P"; }
mjx-c.mjx-c1D444.TEX-I::before { padding: 0.704em 0.791em 0.194em 0px; content: "Q"; }
mjx-c.mjx-c1D437.TEX-I::before { padding: 0.683em 0.828em 0px 0px; content: "D"; }
mjx-c.mjx-c1D43E.TEX-I::before { padding: 0.683em 0.889em 0px 0px; content: "K"; }
mjx-c.mjx-c1D43F.TEX-I::before { padding: 0.683em 0.681em 0px 0px; content: "L"; }
mjx-c.mjx-c7C::before { padding: 0.75em 0.278em 0.249em 0px; content: "|"; }
mjx-c.mjx-c2211.TEX-S2::before { padding: 0.95em 1.444em 0.45em 0px; content: "∑"; }
mjx-c.mjx-c6C::before { padding: 0.694em 0.278em 0px 0px; content: "l"; }
mjx-c.mjx-c6F::before { padding: 0.448em 0.5em 0.01em 0px; content: "o"; }
mjx-c.mjx-c67::before { padding: 0.453em 0.5em 0.206em 0px; content: "g"; }
mjx-c.mjx-c2061::before { padding: 0px; content: ""; }
mjx-c.mjx-c1D43D.TEX-I::before { padding: 0.683em 0.633em 0.022em 0px; content: "J"; }
mjx-c.mjx-c1D446.TEX-I::before { padding: 0.705em 0.645em 0.022em 0px; content: "S"; }
mjx-c.mjx-c1D440.TEX-I::before { padding: 0.683em 1.051em 0px 0px; content: "M"; }
mjx-c.mjx-c2B::before { padding: 0.583em 0.778em 0.082em 0px; content: "+"; }
mjx-c.mjx-c1D43B.TEX-I::before { padding: 0.683em 0.888em 0px 0px; content: "H"; }
mjx-c.mjx-c2C::before { padding: 0.121em 0.278em 0.194em 0px; content: ","; }
mjx-c.mjx-c221A::before { padding: 0.8em 0.853em 0.2em 0px; content: "√"; }
mjx-c.mjx-c221A.TEX-S4::before { padding: 1.75em 1.02em 1.25em 0px; content: "√"; }
mjx-c.mjx-c221A.TEX-S2::before { padding: 1.15em 1.02em 0.65em 0px; content: "√"; }
mjx-c.mjx-c1D447.TEX-I::before { padding: 0.677em 0.704em 0px 0px; content: "T"; }
mjx-c.mjx-c1D449.TEX-I::before { padding: 0.683em 0.769em 0.022em 0px; content: "V"; }
mjx-container[jax="CHTML"] { line-height: 0; }
mjx-container [space="1"] { margin-left: 0.111em; }
mjx-container [space="2"] { margin-left: 0.167em; }
mjx-container [space="3"] { margin-left: 0.222em; }
mjx-container [space="4"] { margin-left: 0.278em; }
mjx-container [space="5"] { margin-left: 0.333em; }
mjx-container [rspace="1"] { margin-right: 0.111em; }
mjx-container [rspace="2"] { margin-right: 0.167em; }
mjx-container [rspace="3"] { margin-right: 0.222em; }
mjx-container [rspace="4"] { margin-right: 0.278em; }
mjx-container [rspace="5"] { margin-right: 0.333em; }
mjx-container [size="s"] { font-size: 70.7%; }
mjx-container [size="ss"] { font-size: 50%; }
mjx-container [size="Tn"] { font-size: 60%; }
mjx-container [size="sm"] { font-size: 85%; }
mjx-container [size="lg"] { font-size: 120%; }
mjx-container [size="Lg"] { font-size: 144%; }
mjx-container [size="LG"] { font-size: 173%; }
mjx-container [size="hg"] { font-size: 207%; }
mjx-container [size="HG"] { font-size: 249%; }
mjx-container [width="full"] { width: 100%; }
mjx-box { display: inline-block; }
mjx-block { display: block; }
mjx-itable { display: inline-table; }
mjx-row { display: table-row; }
mjx-row > * { display: table-cell; }
mjx-mtext { display: inline-block; }
mjx-mstyle { display: inline-block; }
mjx-merror { display: inline-block; color: red; background-color: yellow; }
mjx-mphantom { visibility: hidden; }
mjx-assistive-mml { top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); user-select: none; position: absolute !important; padding: 1px 0px 0px !important; border: 0px !important; display: block !important; width: auto !important; overflow: hidden !important; }
mjx-assistive-mml[display="block"] { width: 100% !important; }
mjx-math { display: inline-block; text-align: left; line-height: 0; text-indent: 0px; font-style: normal; font-weight: normal; font-size: 100%; letter-spacing: normal; border-collapse: collapse; overflow-wrap: normal; word-spacing: normal; white-space: nowrap; direction: ltr; padding: 1px 0px; }
mjx-container[jax="CHTML"][display="true"] { display: block; text-align: center; margin: 1em 0px; }
mjx-container[jax="CHTML"][display="true"][width="full"] { display: flex; }
mjx-container[jax="CHTML"][display="true"] mjx-math { padding: 0px; }
mjx-container[jax="CHTML"][justify="left"] { text-align: left; }
mjx-container[jax="CHTML"][justify="right"] { text-align: right; }
mjx-mi { display: inline-block; text-align: left; }
mjx-c { display: inline-block; }
mjx-utext { display: inline-block; padding: 0.75em 0px 0.2em; }
mjx-mo { display: inline-block; text-align: left; }
mjx-stretchy-h { display: inline-table; width: 100%; }
mjx-stretchy-h > * { display: table-cell; width: 0px; }
mjx-stretchy-h > * > mjx-c { display: inline-block; transform: scaleX(1); }
mjx-stretchy-h > * > mjx-c::before { display: inline-block; width: initial; }
mjx-stretchy-h > mjx-ext { overflow: clip visible; width: 100%; }
mjx-stretchy-h > mjx-ext > mjx-c::before { transform: scaleX(500); }
mjx-stretchy-h > mjx-ext > mjx-c { width: 0px; }
mjx-stretchy-h > mjx-beg > mjx-c { margin-right: -0.1em; }
mjx-stretchy-h > mjx-end > mjx-c { margin-left: -0.1em; }
mjx-stretchy-v { display: inline-block; }
mjx-stretchy-v > * { display: block; }
mjx-stretchy-v > mjx-beg { height: 0px; }
mjx-stretchy-v > mjx-end > mjx-c { display: block; }
mjx-stretchy-v > * > mjx-c { transform: scaleY(1); transform-origin: left center; overflow: hidden; }
mjx-stretchy-v > mjx-ext { display: block; height: 100%; box-sizing: border-box; border: 0px solid transparent; overflow: visible clip; }
mjx-stretchy-v > mjx-ext > mjx-c::before { width: initial; box-sizing: border-box; }
mjx-stretchy-v > mjx-ext > mjx-c { transform: scaleY(500) translateY(0.075em); overflow: visible; }
mjx-mark { display: inline-block; height: 0px; }
mjx-msubsup { display: inline-block; text-align: left; }
mjx-script { display: inline-block; padding-right: 0.05em; padding-left: 0.033em; }
mjx-script > mjx-spacer { display: block; }
mjx-texatom { display: inline-block; text-align: left; }
mjx-msup { display: inline-block; text-align: left; }
mjx-mfrac { display: inline-block; text-align: left; }
mjx-frac { display: inline-block; vertical-align: 0.17em; padding: 0px 0.22em; }
mjx-frac[type="d"] { vertical-align: 0.04em; }
mjx-frac[delims] { padding: 0px 0.1em; }
mjx-frac[atop] { padding: 0px 0.12em; }
mjx-frac[atop][delims] { padding: 0px; }
mjx-dtable { display: inline-table; width: 100%; }
mjx-dtable > * { font-size: 2000%; }
mjx-dbox { display: block; font-size: 5%; }
mjx-num { display: block; text-align: center; }
mjx-den { display: block; text-align: center; }
mjx-mfrac[bevelled] > mjx-num { display: inline-block; }
mjx-mfrac[bevelled] > mjx-den { display: inline-block; }
mjx-den[align="right"], mjx-num[align="right"] { text-align: right; }
mjx-den[align="left"], mjx-num[align="left"] { text-align: left; }
mjx-nstrut { display: inline-block; height: 0.054em; width: 0px; vertical-align: -0.054em; }
mjx-nstrut[type="d"] { height: 0.217em; vertical-align: -0.217em; }
mjx-dstrut { display: inline-block; height: 0.505em; width: 0px; }
mjx-dstrut[type="d"] { height: 0.726em; }
mjx-line { display: block; box-sizing: border-box; min-height: 1px; height: 0.06em; border-top: 0.06em solid; margin: 0.06em -0.1em; overflow: hidden; }
mjx-line[type="d"] { margin: 0.18em -0.1em; }
mjx-mn { display: inline-block; text-align: left; }
mjx-mrow { display: inline-block; text-align: left; }
mjx-c::before { display: block; width: 0px; }
.MJX-TEX { font-family: MJXZERO, MJXTEX; }
.TEX-B { font-family: MJXZERO, MJXTEX-B; }
.TEX-I { font-family: MJXZERO, MJXTEX-I; }
.TEX-MI { font-family: MJXZERO, MJXTEX-MI; }
.TEX-BI { font-family: MJXZERO, MJXTEX-BI; }
.TEX-S1 { font-family: MJXZERO, MJXTEX-S1; }
.TEX-S2 { font-family: MJXZERO, MJXTEX-S2; }
.TEX-S3 { font-family: MJXZERO, MJXTEX-S3; }
.TEX-S4 { font-family: MJXZERO, MJXTEX-S4; }
.TEX-A { font-family: MJXZERO, MJXTEX-A; }
.TEX-C { font-family: MJXZERO, MJXTEX-C; }
.TEX-CB { font-family: MJXZERO, MJXTEX-CB; }
.TEX-FR { font-family: MJXZERO, MJXTEX-FR; }
.TEX-FRB { font-family: MJXZERO, MJXTEX-FRB; }
.TEX-SS { font-family: MJXZERO, MJXTEX-SS; }
.TEX-SSB { font-family: MJXZERO, MJXTEX-SSB; }
.TEX-SSI { font-family: MJXZERO, MJXTEX-SSI; }
.TEX-SC { font-family: MJXZERO, MJXTEX-SC; }
.TEX-T { font-family: MJXZERO, MJXTEX-T; }
.TEX-V { font-family: MJXZERO, MJXTEX-V; }
.TEX-VB { font-family: MJXZERO, MJXTEX-VB; }
mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c { font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A !important; }
@font-face { font-family: MJXZERO; src: url("lib/fonts/mathjax_zero.woff") format("woff"); }
@font-face { font-family: MJXTEX; src: url("lib/fonts/mathjax_main-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-B; src: url("lib/fonts/mathjax_main-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-I; src: url("lib/fonts/mathjax_math-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-MI; src: url("lib/fonts/mathjax_main-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-BI; src: url("lib/fonts/mathjax_math-bolditalic.woff") format("woff"); }
@font-face { font-family: MJXTEX-S1; src: url("lib/fonts/mathjax_size1-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S2; src: url("lib/fonts/mathjax_size2-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S3; src: url("lib/fonts/mathjax_size3-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S4; src: url("lib/fonts/mathjax_size4-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-A; src: url("lib/fonts/mathjax_ams-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-C; src: url("lib/fonts/mathjax_calligraphic-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-CB; src: url("lib/fonts/mathjax_calligraphic-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-FR; src: url("lib/fonts/mathjax_fraktur-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-FRB; src: url("lib/fonts/mathjax_fraktur-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-SS; src: url("lib/fonts/mathjax_sansserif-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-SSB; src: url("lib/fonts/mathjax_sansserif-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-SSI; src: url("lib/fonts/mathjax_sansserif-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-SC; src: url("lib/fonts/mathjax_script-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-T; src: url("lib/fonts/mathjax_typewriter-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-V; src: url("lib/fonts/mathjax_vector-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-VB; src: url("lib/fonts/mathjax_vector-bold.woff") format("woff"); }
mjx-c.mjx-c1D439.TEX-I::before { padding: 0.68em 0.749em 0px 0px; content: "F"; }
mjx-c.mjx-c28::before { padding: 0.75em 0.389em 0.25em 0px; content: "("; }
mjx-c.mjx-c1D714.TEX-I::before { padding: 0.443em 0.622em 0.011em 0px; content: "ω"; }
mjx-c.mjx-c29::before { padding: 0.75em 0.389em 0.25em 0px; content: ")"; }
mjx-c.mjx-c3D::before { padding: 0.583em 0.778em 0.082em 0px; content: "="; }
mjx-c.mjx-c222B.TEX-S2::before { padding: 1.36em 0.944em 0.862em 0px; content: "∫"; }
mjx-c.mjx-c221E::before { padding: 0.442em 1em 0.011em 0px; content: "∞"; }
mjx-c.mjx-c2212::before { padding: 0.583em 0.778em 0.082em 0px; content: "−"; }
mjx-c.mjx-c1D453.TEX-I::before { padding: 0.705em 0.55em 0.205em 0px; content: "f"; }
mjx-c.mjx-c1D461.TEX-I::before { padding: 0.626em 0.361em 0.011em 0px; content: "t"; }
mjx-c.mjx-c1D452.TEX-I::before { padding: 0.442em 0.466em 0.011em 0px; content: "e"; }
mjx-c.mjx-c1D456.TEX-I::before { padding: 0.661em 0.345em 0.011em 0px; content: "i"; }
mjx-c.mjx-c1D451.TEX-I::before { padding: 0.694em 0.52em 0.01em 0px; content: "d"; }
mjx-c.mjx-c31::before { padding: 0.666em 0.5em 0px 0px; content: "1"; }
mjx-c.mjx-c32::before { padding: 0.666em 0.5em 0px 0px; content: "2"; }
mjx-c.mjx-c1D70B.TEX-I::before { padding: 0.431em 0.57em 0.011em 0px; content: "π"; }
</style><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="深度学习里程碑"><p>深度学习里程碑</p></h1><div><p><a href="?query=tag:%E8%AE%A1%E7%AE%97%E6%9C%BA" class="tag" target="_blank" rel="noopener">#计算机</a> <a href="?query=tag:%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD" class="tag" target="_blank" rel="noopener">#人工智能</a> <a href="?query=tag:%E5%AE%9E%E7%94%A8" class="tag" target="_blank" rel="noopener">#实用</a> <a href="?query=tag:%E5%B0%8F%E8%AE%B0" class="tag" target="_blank" rel="noopener">#小记</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/638660013/answer/3422970337" rel="noopener" class="external-link" href="https://www.zhihu.com/question/638660013/answer/3422970337" target="_blank">2024年，深度学习，你心目中的top10算法是什么？ - 知乎</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://zh.d2l.ai/index.html" rel="noopener" class="external-link" href="https://zh.d2l.ai/index.html" target="_blank">《动手学深度学习》 — 动手学深度学习 2.0.0 documentation</a></p></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=hvmTCesa30c" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=hvmTCesa30c" target="_blank"># 深度学习简史，从感知机到lenet到vgg到resnet到clip</a></p>
<p><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1SZ421a7v3/?spm_id_from=333.1365.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1SZ421a7v3/?spm_id_from=333.1365.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">何恺明教授在MIT的第一课：卷积神经网络_哔哩哔哩_bilibili</a></p>
<ul>
<li>1957 - 细胞感知机(Perceptron) 只能处理线性问题</li>
<li>1969 - 发现感知机不能处理XOR问题，陷入第一次寒冬</li>
<li>1974 - 第一次发明了反向传播算法，但没有引起注意</li>
<li>1986 - 重新发明了反向传播算法，引发深度学习的热潮</li>
<li>1990 - SVM的发明和深度学习的一些问题(e.g.梯度爆炸)，陷入第二次寒冬</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>1998 - LeNet成功运用在MINST手写数据集, 
<ul>
<li>introduced convolution, pooling, full-connected layer, train by backprop end to end</li>
</ul>
</li>
<li>2009 - ImageNet第一次举办</li>
<li>2012 - AlexNet在ImageNet上大放异彩</li>
<li>2014 - VGG证明网络越深越好</li>
<li>2014 - GoogLeNet 发明Inception 发明多个并行的卷积层和1x1卷积</li>
<li>2015 - ResNet </li>
<li>2016 - DenseNet 每两个Block一个Skip Connection</li>
<li>2017 - Xception改进了Inception将Max Pooling放在网络后部分</li>
<li>2017 - ResNext引入多头机制</li>
<li>2017 - MobileNets 引入了深度可变卷积</li>
<li>2018 - NasNet 引入了可变多头机制</li>
<li>2019 - EfficentNet 可以灵活改变网络的深度宽度和channel</li>
<li>2021 - CLIP引入对比学习鼓励模型将图像和描述该图像的文本表示靠近</li>
</ul></div></div></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p>深度学习是表征学习(representation learning). </p>
<ul>
<li>一种表征空间映射到另一个空间</li>
</ul>
<p>通过表征原始数据解决复杂的、高度抽象的问题. </p>
<ul>
<li>而原始数据可以有各种形式(pixel, word, DNA..)</li>
</ul>
<p>深度学习到的特征是可以迁移的(transferable)</p>
<ul>
<li>e.g. GPT</li>
</ul>
<p>深度学习解决的问题通常是非凸优化问题, 即有许多局部最小值和鞍点. </p></div></div></div><div class="heading-wrapper"><h2 data-heading="深度学习名人堂" class="heading" id="深度学习名人堂"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>深度学习名人堂</h2><div class="heading-children"><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p>谁是何恺明？他对深度学习做出了什么贡献？</p>
<p> <strong>何恺明（Kaiming He）</strong>是一位在计算机视觉和深度学习领域具有重要影响的科学家。他在深度学习，尤其是在计算机视觉方面的贡献，对推动该领域的发展起到了重要作用。</p>
<p>何恺明最为人熟知的工作之一是他对卷积神经网络（CNNs）结构改进的贡献。他参与开发了几种重要的网络结构和训练方法，包括：</p>
<ol>
<li>
<p><strong>残差网络（Residual Networks, ResNets）</strong>：在2015年的《深度残差网络》论文中，何恺明及其合作者提出了残差网络架构。这一架构通过引入“残差块”来解决深层网络训练难题，使得可以训练出比以往更深的神经网络而不会出现性能下降问题。ResNet极大地推动了图像识别、物体检测等领域的发展，并被广泛应用于商业和研究中。</p>
</li>
<li>
<p><strong>特征金字塔网络（Feature Pyramid Networks, FPNs）</strong>：在目标检测任务中，特征金字塔结构可以有效地处理不同尺寸的物体。FPN通过高效地结合不同层次的特征表示，显著提高了目标检测和分割任务的性能。</p>
</li>
<li>
<p><strong>Mask R-CNN</strong>：这是一种用于目标检测和实例分割任务的框架，基于Faster R-CNN而来，并加入了一个分支以输出物体掩码。Mask R-CNN简单、灵活且性能强大，对实例分割领域产生了深远影响。</p>
</li>
</ol>
<p>除此之外，何恺明还参与了其他多项重要研究工作，并且常常在顶级会议和期刊如CVPR、ICCV、NeurIPS、ECCV等发表论文。他所参与开发的技术已被广泛应用于多个领域，并促进了计算机视觉技术在实际应用中的落地。</p>
<p>何恺明目前是Facebook AI Research（FAIR）团队的一员，在业界享有很高声誉。通过持续地贡献高质量研究成果，何恺明为推动人工智能和机器学习技术向前发展做出了显著贡献。</p></div></div></div><div><p>深度学习领域有什么奠基性的学者? 他们各自都做了什么奠基性的工作?</p></div><div><p> 深度学习领域有很多奠基性的学者，以下是其中的一部分：</p></div><div><ol>
<li data-line="0">
<p>杨立昆（Yann LeCun）：现任Facebook首席人工智能科学家，他在卷积神经网络（Convolutional Neural Networks, CNN）的研发上做出了巨大贡献，这种网络是目前图像识别等领域广泛使用的工具。他的这项工作也为他赢得了2018年的图灵奖。</p>
</li>
<li data-line="2">
<p>吴恩达（Andrew Ng）：吴恩达是斯坦福大学的教授，他的在线课程“机器学习”引导了数百万人进入了这个领域。他还是Google大脑（Google Brain）项目的创始人，该项目对深度学习的商业应用做出了巨大贡献。此外，他还是Coursera的创始人之一。</p>
</li>
<li data-line="4">
<p>杰弗里·辛顿（Geoffrey Hinton）：被誉为“深度学习之父”，他在神经网络和深度学习领域的研究为当前的人工智能热潮打下了基础。他的研究包括反向传播、深度信念网络和长短期记忆网络（LSTM）等。</p>
</li>
<li data-line="6">
<p>Yoshua Bengio：他是加拿大蒙特利尔大学的教授，也是深度学习领域的先驱者之一。他的研究包括反向传播、深度学习和人工神经网络等。</p>
</li>
<li data-line="8">
<p>Ian Goodfellow：他是深度学习领域的重要人物，是生成对抗网络（Generative Adversarial Networks, GANs）的创造者，这是一种能够生成逼真图像的人工智能模型。</p>
</li>
<li data-line="10">
<p>Juergen Schmidhuber：他在循环神经网络（Recurrent Neural Networks, RNNs）和长短期记忆网络（Long Short-Term Memory, LSTM）的研发上做出了重要贡献。</p>
</li>
</ol></div><div><p>以上这些学者的工作都对深度学习领域产生了深远的影响，推动了人工智能的发展。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="基本算法" class="heading" id="基本算法"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>基本算法</h2><div class="heading-children"><div class="heading-wrapper"><h3 data-heading="反向传播 &amp; 梯度优化" class="heading" id="反向传播_&amp;_梯度优化"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>反向传播 &amp; 梯度优化</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=i7V1ci9dkxg" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=i7V1ci9dkxg" target="_blank">梯度下降是什么？为什么你不用知道具体原理？ - YouTube</a></p></div><div><ul>
<li data-line="0">(现在的)反向传播算法是由保罗·韦贝罗斯在1986年发明的。</li>
<li data-line="1" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 深度学习试图解决的是一个最优化问题，数学里叫<del>凸优化(Convex Optimization)</del>
</span><ul>
<li data-line="2">凸优化是一种特殊类型的优化问题，其中目标函数和约束都是凸的。在数学中，一个函数被称为凸函数，如果其定义域是一个凸集，并且对于定义域中的任何两点，函数在这两点之间的线段上的值总是小于或等于线段端点处的值。这种性质使得凸优化问题相对易于解决，因为它们有一个全局最优解，而且我们可以使用有效的算法找到这个解。</li>
</ul>
</li>
<li data-line="3" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 梯度下降方法是一种常见的用于解决优化问题的迭代方法，特别是在机器学习和深度学习中。在每一步，梯度下降方法都会沿着目标函数的负梯度方向（即函数下降最快的方向）更新参数，直到找到函数的最小值</span></li>
<li data-line="4">对于凸优化问题，梯度下降是一种非常有效的方法，因为我们知道只有一个全局最小值，而且梯度下降总是朝向这个最小值移动。然而，对于非凸函数，梯度下降可能会陷入局部最小值，并不能找到全局最小值。</li>
<li data-line="5" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 反向传播的基本思想是使用链式法则来计算损失函数关于网络参数的梯度。</span></li>
<li data-line="6">训练神经网络的目标通常是通过调整网络的参数（即神经元之间的权重和偏置）来最小化某个损失函数。损失函数衡量的是网络的输出与实际目标值之间的差距。为了找到使损失函数最小化的参数，我们需要知道每个参数的变化如何影响损失函数的值，这就需要计算损失函数关于每个参数的梯度。</li>
<li data-line="7"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>反向传播算法的步骤大致如下：
<ol>
<li data-line="8"><strong>前向传播</strong>：输入样本传递通过网络，生成预测输出，并计算出损失函数的值。</li>
<li data-line="9"><strong>反向传播</strong>：从输出层开始，计算损失函数关于每一层的梯度，并将这些梯度传递到前一层。这就是“反向传播”名字的来源。</li>
<li data-line="10"><strong>权重更新</strong>：使用这些梯度来更新网络的权重和偏置，通常使用一种优化算法，如梯度下降。<br>
这个过程在每个训练样本或者每个训练批次上重复进行，直到网络的性能达到满意的水平。</li>
</ol>
</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="模块" class="heading" id="模块"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>模块</h2><div class="heading-children"><div><ul>
<li data-line="0">过拟合/欠拟合</li>
<li data-line="1">学习率</li>
<li data-line="2">超参</li>
<li data-line="3">损失函数</li>
<li data-line="4">参数共享</li>
</ul></div><div class="heading-wrapper"><h3 data-heading="优化器" class="heading" id="优化器"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>优化器</h3><div class="heading-children"><div><blockquote>
<p>直接 Adma + Cos LR Schedule</p>
</blockquote></div><div><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/323747423/answer/2576604040" rel="noopener" class="external-link" href="https://www.zhihu.com/question/323747423/answer/2576604040" target="_blank">如何理解Adam算法(Adaptive Moment Estimation)？ - 知乎</a></p></div><div><p><img alt="lr.png" src="https://cdn.sa.net/2024/04/12/jUk6e3tn9Tq5uLA.png" referrerpolicy="no-referrer"></p></div><div><p><img alt="local_minima.png" src="https://cdn.sa.net/2024/04/12/gGlHIW1atNzVdb3.png" referrerpolicy="no-referrer"></p></div><div><p><img alt="gradient.png" src="https://cdn.sa.net/2024/04/12/47KZPnfkexdgSVJ.png" referrerpolicy="no-referrer"></p></div><div><p><img alt="batch.png" src="https://cdn.sa.net/2024/04/12/xVZaYLTvUhuPDXJ.png" referrerpolicy="no-referrer"></p></div></div></div><div class="heading-wrapper"><h3 data-heading="激活函数" class="heading" id="激活函数"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>激活函数</h3><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><img src="https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2424009416f21db643e21_Group%20807.jpg" referrerpolicy="no-referrer"></p>
<p>神经网络中为什么要有激活函数?</p>
<p> 神经网络中的激活函数有两个主要作用：</p>
<ol>
<li>
<p><strong>引入非线性因素</strong>：由于大多数真实世界的数据都是非线性的，所以我们需要激活函数来引入非线性因素，使得神经网络可以适应更复杂的数据，提高模型的预测能力。如果没有激活函数，无论神经网络有多少层，其输出都是输入的线性组合，这样的神经网络模型无法处理复杂的数据模式。</p>
</li>
<li>
<p><strong>模拟生物神经元的阈值触发机制</strong>：神经元在接收到的信号强度超过某个阈值时才会被激活，这种机制通过激活函数得以在神经网络中实现。</p>
</li>
</ol>
<p>总的来说，激活函数的存在使得神经网络更具有通用性和适应性，能够处理更复杂的问题。</p></div></div></div><div><p><img src="https://media.licdn.com/dms/image/D4D12AQH2F3GJ9wen_Q/article-cover_image-shrink_720_1280/0/1688885174323?e=2147483647&amp;v=beta&amp;t=gFWxErTLLWBc6iRWDxCBRxkdJ7ob24cmjWZAOuKN9o4" referrerpolicy="no-referrer"></p></div><div><blockquote>
<p>下面所有的激活函数都是为了解决非线性问题，因为如果网络中所有层都是线性的，那么无论网络有多少层，最后都可以被简化为一个线性函数。</p>
</blockquote></div><div><p><a data-tooltip-position="top" aria-label="https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html" rel="noopener" class="external-link" href="https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html" target="_blank">4.1. 多层感知机 — 动手学深度学习 2.0.0 documentation</a></p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> ReLU（Rectified Linear Unit) <span style="background:#fff88f">最常用</span></p>
</span><ul>
<li data-line="1"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>优势
<ul>
<li data-line="2">计算简单：ReLU函数只需要判断输入是否大于0，计算复杂度非常低。</li>
<li data-line="3">解决梯度消失问题：在反向传播过程中，ReLU函数的梯度要么为0，要么为1，不会出现梯度消失的问题，有助于深度神经网络的训练。</li>
<li data-line="4">稀疏激活性：ReLU会使部分神经元的输出为0，这样的结果使得神经网络的激活具有稀疏性，并且可以减少神经元之间的依赖。</li>
</ul>
</li>
<li data-line="5"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>劣势
<ul>
<li data-line="6">"死亡"ReLU问题：在训练过程中，有可能部分神经元始终不被激活，导致对应的参数无法更新(传递到激活函数的数值非常不符合正态分布)。一旦出现这种情况，这些神经元将对任何数据的预测都没有贡献，即出现了"死亡"ReLU问题。</li>
<li data-line="7">输出不以0为中心：ReLU函数的输出是非负的，这可能会影响模型的训练效果。</li>
</ul>
</li>
<li data-line="8"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>动机
<ul>
<li data-line="9">ReLU激活函数是为了解决神经网络训练过程中的一些问题而提出的。在ReLU之前，常用的激活函数如sigmoid或tanh，它们在输入值较大或较小时的梯度接近于0，导致梯度下降训练非常缓慢，这就是所谓的梯度消失问题。ReLU激活函数的提出，有效地解决了这个问题。</li>
</ul>
</li>
</ul>
</li>
<li data-line="10" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> Leaky ReLU</p>
</span><ul>
<li data-line="11">原理：Leaky ReLU是对ReLU的改进，解决了神经元“死亡”的问题，当x&lt;0时，有一个小的梯度，而不是完全的0。</li>
<li data-line="12">优点：解决了ReLU的神经元“死亡”问题；相比于ReLU函数，Leaky ReLU全程都有梯度，训练时的收敛速度会比较快。</li>
<li data-line="13">缺点：Leaky ReLU的参数通常都是经验设置，可能需要通过交叉验证来进行选择。</li>
</ul>
</li>
<li data-line="15" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> Parameterized ReLU (pReLU)</p>
</span><ul>
<li data-line="16"><a data-tooltip-position="top" aria-label="https://zh.d2l.ai/chapter_references/zreferences.html#id59" rel="noopener" class="external-link" title="He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). Delving deep into rectifiers: surpassing human-level performance on imagenet classification. Proceedings of the IEEE international conference on computer vision (pp. 1026–1034)." href="https://zh.d2l.ai/chapter_references/zreferences.html#id59" target="_blank">He&nbsp;<em>et al.</em>, 2015</a></li>
<li data-line="17">在负数部分添加了一个线性项</li>
</ul>
</li>
<li data-line="21" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> GELU（Gaussian Error Linear Units）</p>
</span><ul>
<li data-line="22"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>优势
<ul>
<li data-line="23">GELU激活函数在负数部分的处理上，相比ReLU等其他激活函数，提供了更为平滑的梯度，可以缓解梯度消失问题。</li>
<li data-line="24">GELU相比于其他激活函数，如sigmoid和tanh, 在正数部分提供近似线性的映射，这样可以减少模型的复杂度，也使得模型的训练更加稳定。</li>
<li data-line="25">在一些神经网络模型中，<span style="background:#fff88f">如Transformer等，GELU表现出了更好的性能</span>。</li>
</ul>
</li>
<li data-line="26"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>劣势
<ul>
<li data-line="27">GELU的计算复杂度相比ReLU等激活函数要高，因为它包含了tanh和指数运算。</li>
<li data-line="28">虽然GELU在一些模型上有更好的性能，但并非在所有情况下都是最优的。</li>
</ul>
</li>
<li data-line="29"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>动机
<ul>
<li data-line="30">GELU是为了解决ReLU等激活函数在处理负数输入时出现的梯度消失问题，同时保持在正数部分的近似线性映射，以此来提高模型的训练稳定性和性能。</li>
</ul>
</li>
</ul>
</li>
</ul></div><div><blockquote>
<p>Sigmoid和Tanh函数都是早期神经网络中最常用的激活函数，它们的主要目的是为了解决线性函数无法处理的复杂问题，如异或问题（XOR）。同时，由于它们都能将输入值压缩到一定范围内，因此在处理一些需要输出在特定范围内的问题时非常有用。</p>
</blockquote></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">%</span> Sigmoid函数：
</span><ul>
<li data-line="1">原理：Sigmoid函数可以将任何实数映射到（0，1）之间，其输出可以看作概率，用于二分类问题。</li>
<li data-line="2">优点：输出的结果在0和1之间，适合用于输出层，值域范围有限，优化稳定。</li>
<li data-line="3">缺点：存在梯度消失问题，对于深层网络训练困难；计算复杂，训练时间长；输出不以0为中心。</li>
</ul>
</li>
<li data-line="4" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">%</span> Tanh函数
</span><ul>
<li data-line="5">原理：Tanh函数是Sigmoid函数的变体，将实数映射到（-1，1）之间。</li>
<li data-line="6">优点：输出以0为中心，解决了Sigmoid函数的这个问题。</li>
<li data-line="7">缺点：同样存在梯度消失问题，对于深度神经网络存在困难。</li>
</ul>
</li>
<li data-line="8" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">%</span> Softmax函数
</span><ul>
<li data-line="9">原理：Softmax函数可以看做是多分类的Sigmoid，将任意实数映射成为一个概率分布。</li>
<li data-line="10">优点：适合多分类问题。</li>
<li data-line="11">缺点：由于Softmax的输出是一个概率分布，因此其输出节点之间存在依赖关系，这使得模型的解释性变差。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="正则化" class="heading" id="正则化"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>正则化</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6" rel="noopener" class="external-link" href="https://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6" target="_blank">Different Normalization Layers in Deep Learning | by Nilesh Vijayrania | Towards Data Science</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/680237923?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1735297419670257664&amp;utm_source=io.raindrop.raindropio" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/680237923?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1735297419670257664&amp;utm_source=io.raindrop.raindropio" target="_blank"># L2 正则化比大多数人想象的更加神奇</a></p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>数据增强(Data Augmentation)</strong></p>
</span><ul>
<li data-line="1"><strong>原理</strong>：通过对训练数据进行一系列的变换，如旋转、缩放、剪切等，生成新的训练样本，以增加训练数据的多样性，并防止过拟合。</li>
<li data-line="2"><strong>优劣势</strong>：数据增强可以有效地防止过拟合，提高模型的泛化能力，但是数据增强需要花费额外的计算资源。</li>
<li data-line="3"><strong>适用场景</strong>：主要用于图像、音频等领域，尤其是当训练数据较少时。</li>
<li data-line="4"><strong>提出动机</strong>：为了增加训练数据的多样性，防止过拟合。</li>
<li data-line="5"><strong>实际效果</strong>：在实际应用中，数据增强可以有效地防止过拟合，提高模型的泛化能力。</li>
</ul>
</li>
<li data-line="9" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span>  <strong>L1/L2正则化</strong></p>
</span><ul>
<li data-line="10"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>原理</strong>
<ul>
<li data-line="11">L1和L2正则化是一种用于防止模型过拟合的技术。它们通过在损失函数中添加一个惩罚项来实现。这个惩罚项会随着模型复杂度的增加而增加，从而鼓励模型选择更简单的解释数据的方式。</li>
<li data-line="12"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<ul>
<li data-line="12">L1正则化: L1正则化的惩罚项是模型系数的绝对值之和，即<code>||w||_1 = Σ|wi|</code>。L1正则化会产生稀疏的权重，即许多权重为0，这使得模型更加简单，易于解释，也有助于特征选择。</li>
</ul>
</li>
<li data-line="13">L2正则化: L2正则化的惩罚项是模型系数的平方和，即 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D6F4 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container></span>。L2正则化会产生较小的权重，但不会使它们为0，这使得模型更加稳定，对于噪声的抗性更强。</li>
</ul>
</li>
<li data-line="14"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>优劣势</strong>
<ul>
<li data-line="15">L1可以产生稀疏解，有特征选择作用，但是L1正则化的优化问题非光滑，难以求解。L2可以防止过拟合，但是不能产生稀疏解。</li>
<li data-line="16">L1正则化的优点是可以产生稀疏的模型，有助于特征选择。但是L1正则化的优化问题非光滑，难以求解, 导致可能会忽略一些有用的特征。</li>
<li data-line="17">L2正则化的优点是可以保留所有的特征，对于噪声的抗性更强。缺点是可能会让模型变得过于复杂，不易于解释。</li>
</ul>
</li>
<li data-line="18"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>适用场景</strong>：
<ul>
<li data-line="19">当特征数量很大，且有很多无关的特征时，可以使用L1正则化进行特征选择。<br>
-当特征都有一定的相关性，且希望模型对噪声有较强的抗性时，可以使用L2正则化。</li>
</ul>
</li>
<li data-line="21"><strong>提出动机</strong>：L1和L2正则化的提出是为了防止模型过拟合，通过添加一个惩罚项来限制模型的复杂度，使得模型更加稳健，对于噪声的抗性更强。</li>
<li data-line="22"><strong>实际效果</strong>：在实际应用中，L1/L2正则化可以有效地防止过拟合，提高模型的泛化能力。在某些任务中，L1正则化可以有效地进行特征选择，提高模型的解释性. 在其他任务中，L2正则化可以提高模型的稳定性和抗噪声能力</li>
<li data-line="23"></li>
</ul>
</li>
<li data-line="25" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>Dropout</strong></p>
</span><ul>
<li data-line="26"><strong>原理</strong>：在每次迭代训练过程中，随机地将部分神经元的输出设置为0。</li>
<li data-line="27"><strong>优劣势</strong>：Dropout可以有效地防止过拟合，提高模型的泛化能力，但是Dropout也会使得模型的收敛速度变慢。</li>
<li data-line="28"><strong>适用场景</strong>：用于防止神经网络的过拟合。</li>
<li data-line="29"><strong>提出动机</strong>：为了防止神经网络的过拟合，提高模型的泛化能力。</li>
<li data-line="30"><strong>实际效果</strong>：在实际应用中，Dropout可以有效地防止过拟合，提高模型的泛化能力。</li>
</ul>
</li>
<li data-line="33" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>池化层(Pooling layer)</strong></p>
</span><ul>
<li data-line="34">Max Pooling 最大池化, 可检测边缘</li>
<li data-line="35">Average Pooling, 平均池化可平滑图像</li>
<li data-line="36"><strong>原理</strong>：池化层是一种降采样操作，常见的有最大池化和平均池化。最大池化是取滑动窗口内的最大值作为输出，平均池化是取滑动窗口内的平均值作为输出。</li>
<li data-line="37"><strong>优劣势</strong>：池化层可以有效地降低数据的维度，防止过拟合，提高模型的泛化能力，但是也会丢失一部分信息。</li>
<li data-line="38"><strong>适用场景</strong>：用于卷积神经网络中，降低数据的维度，防止过拟合。</li>
<li data-line="39"><strong>提出动机</strong>：为了降低数据的维度，防止过拟合，提高模型的泛化能力。</li>
<li data-line="40"><strong>实际效果</strong>：在实际应用中，池化层可以有效地降低数据的维度，防止过拟合，提高模型的泛化能力。</li>
</ul>
</li>
<li data-line="42" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>Early stopping</strong></p>
</span><ul>
<li data-line="43"><strong>原理</strong>：在训练过程中，当验证集的损失函数值不再下降时，停止训练。</li>
<li data-line="44"><strong>优劣势</strong>：Early stopping可以有效地防止过拟合，提高模型的泛化能力，但是Early stopping的提前停止点的选择有一定的难度。</li>
<li data-line="45"><strong>适用场景</strong>：用于防止神经网络的过拟合。</li>
<li data-line="46"><strong>提出动机</strong>：为了防止神经网络的过拟合，提高模型的泛化能力。</li>
<li data-line="47"><strong>实际效果</strong>：在实际应用中，Early stopping可以有效地防止过拟合，提高模型的泛化能力。</li>
</ul>
</li>
<li data-line="49" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>批归一化(Batch normalization)</strong></p>
</span><ul>
<li data-line="50">Batch Normalization是由Sergey loffe和Christian Szegedy发明的。这项技术首次出现在2015年的一篇名为"Batch Normalization:Accelerating Deep Network Training by Reducing InternalCovariate Shift"的论文中。这篇论文彻底改变了深度学习训川练过程，特别是在训练非常深的神经网络时。</li>
<li data-line="51"><strong>原理</strong>：在每一层的输出上进行标准化操作，使得输出的均值为0，方差为1。减少所谓的内部协变量偏移(internal covariate shift)</li>
<li data-line="52"><strong>优劣势</strong>：Batch normalization可以加速神经网络的训练，同时也可以防止过拟合，但是Batch normalization对于小批量的样本效果不好。</li>
<li data-line="53"><strong>适用场景</strong>：用于加速神经网络的训练，防止过拟合。</li>
<li data-line="54"><strong>提出动机</strong>：为了加速神经网络的训练，防止过拟合。</li>
<li data-line="55"><strong>实际效果</strong>：在实际应用中，Batch normalization可以有效地加速神经网络的训练，防止过拟合。</li>
<li data-line="56" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 对不同序列中同一位置的数据进行归一化操作, 所有网络都常用</span></li>
</ul>
</li>
<li data-line="59" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>层归一化(Layer Normalization)</strong></p>
</span><ul>
<li data-line="60"><strong>原理</strong>：与批归一化类似，层归一化是对每一层的输出进行归一化，使得输出的均值为0，方差为1，但不同的是，层归一化是在每个样本内部进行，而不是在批次之间。</li>
<li data-line="61"><strong>优劣势</strong>：层归一化可以加速神经网络的训练，同时也可以防止过拟合，而且对于小批量的样本效果也好。</li>
<li data-line="62"><strong>适用场景</strong>：用于加速神经网络的训练，防止过拟合，尤其适用于批量较小的情况。</li>
<li data-line="63"><strong>提出动机</strong>：为了解决批归一化在小批量样本上的问题，提出了层归一化。</li>
<li data-line="64"><strong>实际效果</strong>：在实际应用中，层归一化可以有效地加速神经网络的训练，防止过拟合。</li>
<li data-line="65" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 对相同序列中不同位置的数据进行归一化操作, Transforme常用(便于处理不定长)</span></li>
</ul>
</li>
<li data-line="68" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>添加噪声(Noise Regularization)</strong></p>
</span><ul>
<li data-line="69">
<p><strong>原理</strong>：在训练过程中，将噪声添加到输入数据或网络权重中，以增强模型的鲁棒性，并防止过拟合。</p>
</li>
<li data-line="70">
<p><strong>优劣势</strong>：添加噪声可以有效地防止过拟合，提高模型的泛化能力，但是如果噪声添加过多，可能会影响模型的性能。</p>
</li>
<li data-line="71">
<p><strong>适用场景</strong>：用于防止神经网络的过拟合，尤其适用于在训练数据中存在噪声的情况。</p>
</li>
<li data-line="72">
<p><strong>提出动机</strong>：为了增强模型的鲁棒性，防止过拟合。</p>
</li>
<li data-line="73">
<p><strong>实际效果</strong>：在实际应用中，添加噪声可以有效地防止过拟合，提高模型的泛化能力。</p>
</li>
<li data-line="75" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>标签平滑（Label Smoothing）</strong> <span style="background:#fff88f">常用于分类任务和知识蒸馏</span></p>
</span><ul>
<li data-line="76"><strong>原理</strong>：标签平滑是一种正则化技术，旨在减少过拟合。在训练深度学习模型时，我们通常使用"硬"标签，例如，如果一个样本属于第i类，那么它的标签就是一个全为0的向量，只有第i位为1。标签平滑通过将这些"硬"标签转换为"软"标签，即在标签中引入一些噪声，使得模型不会过于自信，从而提高其泛化能力。</li>
<li data-line="77"><strong>优劣势</strong>：标签平滑的优点是可以防止模型过拟合，提高模型的泛化能力。此外，它还可以防止模型对于一些不确定或错误标签的过度反应。然而，它的缺点是可能会导致模型的训练速度变慢，因为它使得损失函数的优化变得更加复杂。</li>
<li data-line="78"><strong>适用场景</strong>：标签平滑主要用于深度学习领域，尤其是在处理分类问题时，如图像分类、文本分类等。</li>
<li data-line="79"><strong>提出动机</strong>：为了防止模型过拟合，提高模型的泛化能力。</li>
<li data-line="80"><strong>实际效果</strong>：在实际应用中，标签平滑已被证明可以提高模型的泛化能力，特别是在训练大规模深度学习模型时。</li>
</ul>
</li>
</ul>
</li>
<li data-line="89" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> <strong>集成方法（Ensemble Methods）</strong></p>
</span><ul>
<li data-line="90">训练多个模型，然后将它们的输出进行组合，以提高模型的泛化能力。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="数据增强" class="heading" id="数据增强"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>数据增强</h3><div class="heading-children"></div></div></div></div><div class="heading-wrapper"><h2 data-heading="CNN(卷积神经网络)" class="heading" id="CNN(卷积神经网络)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>CNN(卷积神经网络)</h2><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1SZ421a7v3/?spm_id_from=333.1365.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1SZ421a7v3/?spm_id_from=333.1365.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">何恺明教授在MIT的第一课：卷积神经网络_哔哩哔哩_bilibili</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/407763732/answer/3452197126" rel="noopener" class="external-link" href="https://www.zhihu.com/question/407763732/answer/3452197126" target="_blank">卷积神经网络中的特征到底是什么？它和传统图像处理中的比如轮廓、颜色等特征有什么异同？ - 知乎</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://ytzfhqs.github.io/AAAMLP-CN/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E5%88%86%E5%89%B2%E6%96%B9%E6%B3%95/" rel="noopener" class="external-link" href="https://ytzfhqs.github.io/AAAMLP-CN/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E5%88%86%E5%89%B2%E6%96%B9%E6%B3%95/" target="_blank">图像分类和分割方法 - AAAMLP 中译版</a><br>
里面有Padding 和 dilation 的说明.</p></div><div class="admonition-parent admonition-faq-parent"><div class="callout admonition admonition-faq admonition-plugin " style="--callout-color: 100, 221, 23;" data-callout="faq" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="question-circle" class="svg-inline--fa fa-question-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zM262.655 90c-54.497 0-89.255 22.957-116.549 63.758-3.536 5.286-2.353 12.415 2.715 16.258l34.699 26.31c5.205 3.947 12.621 3.008 16.665-2.122 17.864-22.658 30.113-35.797 57.303-35.797 20.429 0 45.698 13.148 45.698 32.958 0 14.976-12.363 22.667-32.534 33.976C247.128 238.528 216 254.941 216 296v4c0 6.627 5.373 12 12 12h56c6.627 0 12-5.373 12-12v-1.333c0-28.462 83.186-29.647 83.186-106.667 0-58.002-60.165-102-116.531-102zM256 338c-25.365 0-46 20.635-46 46 0 25.364 20.635 46 46 46s46-20.636 46-46c0-25.365-20.635-46-46-46z"></path></svg></div><div class="callout-title-inner admonition-title-content">Faq</div></div><div class="callout-content admonition-content"><p>CNN&nbsp;试图通过卷积来表达不同位置数值之间的关系，学习卷积值也就是学习矩阵里的数值之间的特征，所以适合用在图像里面。因为图像就是一个个的像素点形成的矩阵。</p>
<p>CNN 普遍有什么特性? </p>
<ol>
<li>
<p><strong>局部感受机制</strong> : CNN通过 convolution kernel 捕捉数据的局部特征. 每个kernel只专注于提取某个特定特征(e.g.边缘角点纹理), 有效降低参数量, 提高模型泛化能力.</p>
</li>
<li>
<p><strong>权重共享</strong>：卷积层的神经元使用相同的权重，这意味着在进行前向传播和反向传播时，这些权重会被同时更新。这种权重共享机制大大减少了模型的参数数量，从而减轻了过拟合的风险。</p>
</li>
<li>
<p><strong>池化</strong>：池化层可以对输入的特征图进行下采样，降低特征的维度，同时保留重要的特征信息。</p>
</li>
<li>
<p><strong>平移不变性</strong>：由于权重共享和池化操作，CNN具有平移不变性，即无论目标在图像中的位置如何移动，CNN都可以识别出来。</p>
</li>
<li>
<p><strong>多层结构</strong>：CNN通常由多个卷积层和池化层交替堆叠而成，可以提取出图像的高层次特征。</p>
</li>
<li>
<p><strong>端到端学习</strong>：CNN可以直接从原始像素数据中学习到有用的特征，无需人工设计特征，大大简化了机器学习的流程。</p>
</li>
</ol>
<ul>
<li class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> CNN有没有尺度不变性和旋转不变性</span></li>
<li>CNN（卷积神经网络）本身并不具有尺度不变性和旋转不变性。</li>
<li><strong>尺度不变性</strong>：CNN对于输入图片的尺寸是敏感的，如果输入图片的尺度（大小）发生变化，那么CNN可能无法正确地识别出图片中的对象。这是因为CNN在训练过程中，是对固定尺寸的图片进行学习的，如果输入图片的尺寸改变了，那么CNN的卷积核可能无法正确地匹配图片中的特征。</li>
<li><strong>旋转不变性</strong>：同样，CNN对于输入图片的旋转也是敏感的。如果输入图片被旋转了一定的角度，那么CNN可能无法正确地识别出图片中的对象。这是因为CNN在训练过程中，是假设所有的输入图片都是正向（未旋转）的，如果输入图片被旋转了，那么CNN的卷积核也可能无法正确地匹配图片中的特征。</li>
</ul>
<p>然而，虽然CNN本身不具有尺度和旋转不变性，但是我们可以通过一些方法来提高CNN对尺度和旋转的鲁棒性，例如数据增强（对训练图片进行缩放和旋转），或者在CNN中使用一些特殊的卷积层（例如空间变换网络层）来学习图片的尺度和旋转变换。</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=YkXSKZIQ5oo" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=YkXSKZIQ5oo" target="_blank">如何用深度学习提取图片特征？这样做有什么好处？ - YouTube</a></p>
<p><img src="https://cdn.sa.net/2024/04/15/n5zxyVfGKPI7FLC.png" referrerpolicy="no-referrer"></p>
<p><strong>介绍如何使用预训练深度学习进行特征提取</strong>:</p>
<ul>
<li>上图是VGG16. 预训练的话需要在7*515*512层截获, 进行特征输出</li>
<li></li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="LeNet(鼻祖) - 1998" class="heading" id="LeNet(鼻祖)_-_1998"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>LeNet(鼻祖) - 1998</h3><div class="heading-children"><div class="admonition-parent admonition-warning-parent"><div class="callout admonition admonition-warning admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="warning" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Warning</div></div><div class="callout-content admonition-content heading-wrapper"><h3 data-heading="LeNet 是什么? 对深度学习的发展有什么贡献?" class="heading" id="LeNet_是什么?_对深度学习的发展有什么贡献?"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>LeNet 是什么? 对深度学习的发展有什么贡献?</h3>
<p> LeNet(5层)是一种早期的卷积神经网络，由Yann LeCun于1998年提出，主要用于手写数字识别和机器阅读等任务。LeNet的出现使得神经网络能够处理更复杂的图像识别任务，是深度学习领域的一个重要里程碑。</p>
<p>LeNet对深度学习的发展贡献主要体现在以下几个方面：</p>
<ol>
<li>
<p>网络结构设计：LeNet提出了一种新的网络结构，引入了卷积层、池化层、全连接层的概念，使得神经网络可以更好地处理图像等高维数据。</p>
</li>
<li>
<p>参数共享：LeNet提出了参数共享的概念，大大减少了网络的参数数量，使得神经网络可以在有限的计算资源下处理更大规模的数据。</p>
</li>
<li>
<p>激活函数：LeNet提出了使用S型激活函数，使得神经网络可以学习和表示更复杂的函数关系。</p>
</li>
<li>
<p>反向传播：LeNet第一次使用了反向传播算法来训练神经网络，为后来的深度学习算法提供了重要的参考。</p>
</li>
<li>
<p>实践应用：LeNet在手写数字识别等任务上取得了显著的效果，证明了深度学习在实际应用中的有效性。</p>
</li>
</ol>
<p>总的来说，LeNet的出现对深度学习的发展起到了推动作用，为后来的深度学习算法提供了重要的理论基础和实践经验。</p><div class="heading-children"></div></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="AlexNet(重启深度学习) - 2012" class="heading" id="AlexNet(重启深度学习)_-_2012"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>AlexNet(重启深度学习) - 2012</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener" class="external-link" href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" target="_blank">ImageNet Classification with Deep Convolutional Neural Networks" by Alex Krizhevsky, Ilya Sutskever, and Geoffrey H. Hinton (2012)</a></p></div><div class="admonition-parent admonition-warning-parent"><div class="callout admonition admonition-warning admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="warning" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Warning</div></div><div class="callout-content admonition-content heading-wrapper"><p><img src="https://cdn.sa.net/2024/04/04/WUoK8NSL6svmPwj.png" referrerpolicy="no-referrer"></p>
<h3 data-heading="AlexNet 是什么? 对深度学习的发展有什么贡献?" class="heading" id="AlexNet_是什么?_对深度学习的发展有什么贡献?">AlexNet 是什么? 对深度学习的发展有什么贡献?</h3>
<p> AlexNet是一个在深度学习和计算机视觉领域非常知名的卷积神经网络模型，由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton在2012年提出，并在当年的ImageNet竞赛中取得了突破性的成果。</p>
<p>AlexNet的主要贡献在于：</p>
<ol>
<li>
<p>首次证明了深度学习在<strong>大规模数据集</strong>上的有效性：AlexNet在ImageNet数据集上获得了当时最好的结果，这个数据集包含了1000个类别，120万张训练图像，验证和测试各有5万张图像。</p>
</li>
<li>
<p>推动了深度学习在计算机视觉领域的应用：AlexNet的成功引起了工业界和学术界的广泛关注，从而推动了深度学习在计算机视觉领域的广泛应用。</p>
</li>
<li>
<p>网络结构创新：AlexNet引入了<strong>ReLU（Rectified Linear Unit）激活函数</strong>，相比于传统的sigmoid等激活函数，ReLU在深度网络中有更好的性能和更快的训练速度。此外，AlexNet还首次引入了<strong>Dropout</strong>技术，有效防止了模型过拟合。</p>
</li>
<li>
<p><strong>利用GPU加速训练</strong>：AlexNet是首个大规模利用GPU加速训练的深度网络模型，这对于后续深度学习模型的训练有重要影响。</p>
</li>
<li>
<p>证明了<strong>数据增强</strong>的有效性.</p>
</li>
<li>
<p>比LeNet的网络宽</p>
</li>
</ol></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=Z8tNqCrHRyM" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=Z8tNqCrHRyM" target="_blank">【如何读论文？】2012年让深度学习起死回生的开山之作：AlexNet - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1311.2901" rel="noopener" class="external-link" href="https://arxiv.org/abs/1311.2901" target="_blank">[1311.2901] Visualizing and Understanding Convolutional Networks - 2013</a></p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="ZfNet(第一次可视化隐藏层) - 2013" class="heading" id="ZfNet(第一次可视化隐藏层)_-_2013"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>ZfNet(第一次可视化隐藏层) - 2013</h3><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p>ZFNet（Zeiler and Fergus Network）是一种卷积神经网络（Convolutional Neural Network，简称CNN），由Matthew Zeiler和Rob Fergus在2013年提出。ZFNet在2013年的ImageNet竞赛中取得了第一名，证明了其在图像分类任务上的有效性。</p>
<p>ZFNet的主要贡献包括：</p>
<ol>
<li>
<p><strong>提出了反向可视化的方法</strong>：ZFNet的作者提出了一种名为反向可视化的方法，能够帮助我们理解并可视化CNN中间层的特征。这种方法通过将每一层的特征映射回像素空间，可以让我们看到网络在每一层中学习到的内容。这种方法对于理解深度学习模型的内部工作机制非常有帮助，特别是对于理解模型的错误和不足。</p>
</li>
<li>
<p><strong>对AlexNet进行了改进</strong>：ZFNet实际上是对2012年ImageNet竞赛冠军AlexNet的一个改进。ZFNet的作者通过他们的反向可视化方法发现，AlexNet的第一层使用的11x11的卷积核可能过大，导致了一些信息的丢失。因此，他们将第一层的卷积核大小改为7x7，并对其他一些参数进行了调整，从而改进了模型的性能。</p>
</li>
</ol>
<p>在深度学习发展史上，ZFNet有着重要的地位。首先，ZFNet的成功进一步证明了深度学习，特别是CNN在图像分类任务上的有效性。其次，ZFNet的<strong>反向可视化方法</strong>对于深度学习的理解和发展有着重要的影响。这种方法不仅帮助我们理解了深度学习模型的内部工作机制，也为后续的模型改进和设计提供了有价值的指导。</p>
<p>至于对模型可视化的贡献，ZFNet的反向可视化方法开创了深度学习模型可视化的新范式。这种方法能够将抽象的特征映射回像素空间，让我们可以直观地看到网络在每一层中学习到的内容。这对于理解深度学习模型的内部工作机制，特别是理解模型的错误和不足，有着极大的帮助。此外，这种方法也为后续的模型可视化研究提供了一个重要的基础。</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=ihs2JbTnhc4&amp;t=6s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=ihs2JbTnhc4&amp;t=6s" target="_blank">模型可视化开山之作ZFNet，一篇文章开创了多少新领域？！ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1311.2901" rel="noopener" class="external-link" href="https://arxiv.org/abs/1311.2901" target="_blank">Visualizing and Understanding Convolutional Networks</a></p>
<p><img src="https://cdn.sa.net/2024/04/14/QSarIYEXANu87J2.png" referrerpolicy="no-referrer"><br>
<img src="https://cdn.sa.net/2024/04/14/jNFTnZa9Oeo8SpM.png" referrerpolicy="no-referrer"></p>
<p>文章灵感来源AlexNet. </p>
<p>贡献: </p>
<ul>
<li>第一次发现 小 conv kernel 效果会好.</li>
<li>为了解释其效果, 就用反向可视化用CNN中间层. 为之后的可视化方法比如GradCAM提供启发. </li>
<li>展示了CNN的效果类似于高通滤波器的近似“边缘检测”</li>
<li>发现训练不足的话, 对深层的影响比浅层要大. </li>
<li>第一次使用对输入图像Mask, 进行消融实验.</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="R-CNN(分割网络开山之作) - 2013" class="heading" id="R-CNN(分割网络开山之作)_-_2013"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>R-CNN(分割网络开山之作) - 2013</h3><div class="heading-children"><div class="admonition-parent admonition-attention-parent"><div class="callout admonition admonition-attention admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="attention" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Attention</div></div><div class="callout-content admonition-content heading-wrapper"><h3 data-heading="历史上的分割网路" class="heading" id="历史上的分割网路"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>历史上的分割网路</h3>
<p><img alt="r2YpvkcWhUu94E5.png" src="https://cdn.sa.net/2024/04/11/r2YpvkcWhUu94E5.png" referrerpolicy="no-referrer"></p>
<h3 data-heading="R-CNN 是什么? 对深度学习的发展有什么贡献?" class="heading" id="R-CNN_是什么?_对深度学习的发展有什么贡献?">R-CNN 是什么? 对深度学习的发展有什么贡献?</h3>
<p>R-CNN，全称是 Region-based Convolutional Neural Networks，是一种区域卷积神经网络，用于目标检测任务。它是一种两阶段的目标检测器，首先对图像进行区域提议，然后对这些提议进行分类和位置修正。</p>
<p>R-CNN的主要贡献在于，它首次将深度学习用于目标检测任务，大幅度提高了目标检测的准确性。在此之前，目标检测主要依赖于手工设计的特征和简单的分类器，而R-CNN通过学习得到的特征和复杂的分类器，能够更好地处理各种各样的图像和目标。</p>
<p>此外，R-CNN也为后续的深度学习目标检测算法，如Fast R-CNN，Faster R-CNN等提供了基础和灵感。这些算法进一步优化了R-CNN的性能和速度，使得深度学习在目标检测任务上的应用更加广泛。</p>
<p>成为 目标检测(打框) 和 异常检测的主干网络.</p><div class="heading-children"></div></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=sHZVPHVQg9E" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=sHZVPHVQg9E" target="_blank">为RCNN奠基的工作，正式开启深度学习的大门！ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" rel="noopener" class="external-link" href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" target="_blank">huppelen.nl/publications/selectiveSearchDraft.pdf</a></p>
<p><img alt="aeZ5jmFU7goEnfq.png" src="https://cdn.sa.net/2024/04/14/aeZ5jmFU7goEnfq.png" referrerpolicy="no-referrer"></p>
<ul>
<li>监督学习. 训练样本首先(<del>随机</del>某种方式)生成了很多 anchor </li>
<li>使用 SVM 结合 HOG features 进行训练</li>
<li>然后重复迭代消除随机生成的 anchor 和 label 接近</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=ikq128JdQOI" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=ikq128JdQOI" target="_blank">开山之作RCNN做了什么？什么是图像分割、语义分割？</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1311.2524" rel="noopener" class="external-link" href="https://arxiv.org/abs/1311.2524" target="_blank">[1311.2524] Rich feature hierarchies for accurate object detection and semantic segmentation</a></p>
<p><img alt="lc5TbFJE6IYoyHk.png" src="https://cdn.sa.net/2024/04/14/lc5TbFJE6IYoyHk.png" referrerpolicy="no-referrer"><br>
<img src="https://cdn.sa.net/2024/04/14/bz4R8svkHMarj6G.png" referrerpolicy="no-referrer"><br>
<img alt="kpO5gQi7EGdTzUY.png" src="https://cdn.sa.net/2024/04/14/kpO5gQi7EGdTzUY.png" referrerpolicy="no-referrer"></p>
<p>R- CNN基本上和上面的流程一样. 不过使用CNN替换了SVM.<br>
由于CNN的特征提取能力更强, 所以效果也好很多. </p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="VGG(纯CNN最深网络) - 2014" class="heading" id="VGG(纯CNN最深网络)_-_2014"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>VGG(纯CNN最深网络) - 2014</h3><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=5zDFIe4k1Gw" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=5zDFIe4k1Gw" target="_blank">【博士Vlog】VGG是什么？为什么是卷积神经网络的巅峰之作？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1409.1556" rel="noopener" class="external-link" href="https://arxiv.org/abs/1409.1556" target="_blank">[1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition</a></p>
<p><img alt="dTwEcHBs56pGiMx.png" src="https://cdn.sa.net/2024/04/16/dTwEcHBs56pGiMx.png" referrerpolicy="no-referrer"><br>
VGG-16</p>
<ul>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>非常深
<ul>
<li>共19层</li>
<li>PS: 工程上使得网络变宽比变深更难</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>非常简单
<ul>
<li>只有  <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">卷</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">积</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">核</mjx-utext></mjx-mi><mjx-mi class="mjx-i"><mjx-utext variant="italic" style="font-family: MJXZERO, serif; font-style: italic;">，</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">步</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">长</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">为</mjx-utext></mjx-mi><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-utext variant="italic" style="font-family: MJXZERO, serif; font-style: italic;">，</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">填</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">充</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">为</mjx-utext></mjx-mi><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-utext variant="italic" style="font-family: MJXZERO, serif; font-style: italic;">，</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">所</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">有</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">的</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">池</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">化</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">层</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">都</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">是</mjx-utext></mjx-mi><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-math></mjx-container></span></li>
<li>然后简单模块重复</li>
<li>发现用小卷积核效果很好, 可能是防止 图 不要快速变得太小</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>效果好
<ul>
<li>网络越深越好</li>
</ul>
</li>
<li>但是参数数量较大，计算量大</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>用了stage-wise training
<ul>
<li>VGG-11, 13, <strong>16</strong>, <strong>19</strong> (现在只有16和19常用)</li>
</ul>
</li>
<li>适合迁移学习</li>
</ul></div></div></div><div class="admonition-parent admonition-hint-parent"><div class="callout admonition admonition-hint admonition-plugin " style="--callout-color: 0, 191, 165;" data-callout="hint" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="fire" class="svg-inline--fa fa-fire fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M216 23.86c0-23.8-30.65-32.77-44.15-13.04C48 191.85 224 200 224 288c0 35.63-29.11 64.46-64.85 63.99-35.17-.45-63.15-29.77-63.15-64.94v-85.51c0-21.7-26.47-32.23-41.43-16.5C27.8 213.16 0 261.33 0 320c0 105.87 86.13 192 192 192s192-86.13 192-192c0-170.29-168-193-168-296.14z"></path></svg></div><div class="callout-title-inner admonition-title-content">Hint</div></div><div class="callout-content admonition-content"><p>深度神经网络在训练过程中可能会遇到梯度爆炸的问题，这通常发生在梯度的值变得非常大，以至于更新的权重值导致网络不稳定。以下是一些常用的解决方法：</p>
<ol>
<li><strong>梯度裁剪（Gradient Clipping）</strong>：这是一种直接的方法，可以防止梯度值过大。基本思想是设置一个阈值，当梯度的值超过这个阈值时，就将其裁剪到这个阈值。这可以防止梯度爆炸，但不能解决梯度消失的问题。</li>
<li><strong>权重初始化（Weight Initialization）</strong>：合适的权重初始化可以在一定程度上防止梯度爆炸或梯度消失。例如，Xavier初始化和He初始化是两种针对不同类型的激活函数（如sigmoid和ReLU）设计的初始化方法。</li>
<li><strong>批量归一化（Batch Normalization）</strong>：批量归一化可以使网络中各层的输入保持相同的分布，这样可以稳定训练过程，防止梯度爆炸。</li>
<li><strong>使用适当的优化器（Optimizers）</strong>：某些优化器，如Adam、RMSprop等，可以自适应地调整学习率，从而避免梯度过大或过小的问题。</li>
<li><strong>残差连接（Residual Connections）</strong>：在深度神经网络中使用残差连接可以防止梯度消失和爆炸的问题，因为残差连接可以直接将梯度传递到较浅的层。</li>
<li><strong>正则化（Regularization）</strong>：L1和L2正则化可以防止权重变得过大，从而在一定程度上防止梯度爆炸。</li>
</ol>
<p>这些方法可以单独使用，也可以结合使用，具体取决于具体的任务和模型。</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="GoogLeNet / Inception(探索各种trick) - 2014" class="heading" id="GoogLeNet_/_Inception(探索各种trick)_-_2014"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>GoogLeNet / Inception(探索各种trick) - 2014</h3><div class="heading-children"><div><p>Going Deeper with Convolutions" by Szegedy et al. (2014)<br>
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20191202190625/modified.png" referrerpolicy="no-referrer"></p></div><div><p>GoogLeNet(非正式名称)也被称为Inception v1，是一种深度卷积神经网络, 名字来源于网络中的一种特殊结构——Inception模块，它在2014年的ImageNet大规模视觉识别挑战赛（ILSVRC）上取得了很好的成绩。GoogLeNet的主要创新点包括：</p></div><div><ol>
<li data-line="0">
<p><strong>Inception模块</strong>：GoogLeNet引入了Inception模块，这是一种并行连接多个不同类型的卷积核和池化操作的结构。Inception模块的设计理念是让<strong>网络自己决定使用哪种卷积核大小</strong>，或者<strong>是否使用全连接层。通过这种方式，网络可以在不同的尺度上学习特征，从而提高模型的表现力</strong>。</p>
<p>动机：在实际的图像中，有些重要的特征可能是局部的，有些可能是全局的，有些可能在中间尺度。通过使用不同大小的卷积核，我们可以让网络在不同的尺度上学习特征，从而更好地处理这种多尺度的问题。</p>
</li>
<li data-line="4">
<p><strong>深度和宽度的平衡</strong>：GoogLeNet通过增加网络的深度和宽度，提高了模型的表现力。但是，与此同时，GoogLeNet还使用了1x1的卷积核（也被称为点卷积）来减少参数的数量，从而降低了过拟合的风险，并提高了计算效率。</p>
<p>动机：深度神经网络有更强的表现力，但是也更容易过拟合，而且计算效率也较低。通过使用1x1的卷积核，我们可以在保持网络深度的同时，降低参数的数量，从而降低过拟合的风险，并提高计算效率。</p>
</li>
<li data-line="8">
<p><strong>辅助分类器</strong>：GoogLeNet在网络的中间层也添加了分类器，用于进行辅助的分类任务。这些辅助分类器的输出会在训练过程中与主分类器的输出一起被考虑，但是在测试过程中会被忽略。</p>
<p>动机：深度神经网络的一个常见问题是梯度消失，这会导致网络的前面几层难以训练。通过在网络的中间层添加辅助分类器，我们可以让这些层也接收到直接的反馈信号，从而缓解梯度消失的问题。</p>
</li>
</ol></div><div><p>这些创新点使GoogLeNet在当时的图像分类任务上表现优秀，并对后续的深度学习研究产生了深远影响。</p></div><div><p>随着研究的深入，Google后续还提出了Inception v2、Inception v3等更先进的版本。这些新版本在原有的Inception网络基础上进行了一些改进，例如引入了<strong>批量归一化（Batch Normalization）</strong>、<strong>分解卷积（Factorization into smaller convolutions）</strong>, Label Smoothing 等技术，以提高网络的性能和效率。<br>
Inception v4 添加了residual learning.</p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> <strong>Inception V1（GoogLeNet）</strong>
</span><ul>
<li data-line="1">这是Inception系列的第一个版本，它首次提出了Inception模块，这是一种并行结构，包含了不同尺寸的卷积和池化操作。Inception模块的设计目标是在保持网络深度和宽度的同时，提高网络的计算效率。此外，GoogLeNet还引入了辅助分类器（auxiliary classifiers）来解决梯度消失问题，以及全局平均池化（global average pooling）来替代全连接层，减少了模型的参数数量。</li>
</ul>
</li>
<li data-line="2" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> <strong>Inception V2和V3</strong>
</span><ul>
<li data-line="3">这两个版本的主要贡献是引入了两种新的技术：Batch Normalization（BN）和Factorized Convolutions。BN可以加速模型的训练，同时还可以起到一定的正则化效果。Factorized Convolutions则是一种将大的卷积核分解为多个小的卷积核的方法，这可以减少模型的计算复杂性，同时保持模型的性能。</li>
</ul>
</li>
<li data-line="4" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> <strong>Inception V4和Inception-ResNet</strong>：
</span><ul>
<li data-line="5">这两个版本的主要贡献是将ResNet的残差连接引入到Inception模块中。这种设计可以进一步提高模型的深度，同时避免梯度消失问题。此外，Inception V4还引入了一种新的Inception模块，这种模块包含了更复杂的并行结构，可以进一步提高模型的性能。</li>
</ul>
</li>
</ul></div><div><p>总的来说，Inception系列的主要贡献在于提出了一种新的网络架构（Inception模块），这种架构可以在保持网络深度和宽度的同时，提高网络的计算效率。此外，Inception系列还引入了许多新的技术，如BN、Factorized Convolutions和残差连接，这些技术都对深度学习的发展产生了深远影响。</p></div><div class="admonition-parent admonition-note-parent"><div class="callout admonition admonition-note admonition-plugin " style="--callout-color: 68, 138, 255;" data-callout="note" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="pencil-alt" class="svg-inline--fa fa-pencil-alt fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Note</div></div><div class="callout-content admonition-content"><p>正则化(normalization) 在深度学习中非常重要的技术.</p>
<p>早在LeNet就提到正则化可以帮助训练.</p>
<p>主要的作用是防止模型过拟合和帮助训练(初始化&amp;加速训练).</p>
<p>正则化技术有很多种，以下是一些常见的类型：</p>
<ol>
<li><strong>权重衰减（L2正则化）</strong>：这是最常见的正则化技术之一。它通过在损失函数中添加一个与模型参数的平方成正比的项，来鼓励模型使用较小的权重。这可以防止模型过于依赖某个特定的特征，从而提高模型的泛化能力。</li>
<li><strong>L1正则化</strong>：L1正则化类似于权重衰减，但是它添加的是与模型参数的绝对值成正比的项。这会导致模型的一些权重变为0，从而实现特征选择的效果。</li>
<li><strong>Dropout</strong>：这是一种在训练过程中随机关闭一部分神经元的技术。通过这种方式，模型需要学习如何在缺失一部分信息的情况下进行预测，这可以增强模型的鲁棒性，并防止过拟合。</li>
<li><strong>批量归一化（Batch Normalization）</strong>：虽然批量归一化主要是用来解决深度网络中的内部协变量偏移问题，但是它也具有一定的正则化效果。</li>
<li><strong>数据增强</strong>：通过对训练数据进行一些随机的变换（例如旋转、缩放、剪裁等），我们可以增加模型的训练数据，从而防止过拟合。</li>
</ol>
<p>这些正则化技术可以单独使用，也可以组合使用，以根据具体的任务和数据来调整模型的复杂性</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="ResNet(网络变深) - 2015" class="heading" id="ResNet(网络变深)_-_2015"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>ResNet(网络变深) - 2015</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1512.03385" rel="noopener" class="external-link" href="https://arxiv.org/abs/1512.03385" target="_blank">[1512.03385] Deep Residual Learning for Image Recognition</a></p></div><div><p><img src="https://cdn.sa.net/2024/04/14/73EKPz8eahS4NBm.png" referrerpolicy="no-referrer"></p></div><div><p><img src="https://miro.medium.com/v2/resize:fit:1200/1*6hF97Upuqg_LdsqWY6n_wg.png" referrerpolicy="no-referrer"></p></div><div><p><img src="https://upload.wikimedia.org/wikipedia/commons/b/ba/ResBlock.png" referrerpolicy="no-referrer"></p></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=4h5OkS2CzuU" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=4h5OkS2CzuU" target="_blank">【博士Vlog】2015年的深度学习冠军ResNet怎么工作的？有多厉害？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1512.03385" rel="noopener" class="external-link" href="https://arxiv.org/abs/1512.03385" target="_blank">[1512.03385] Deep Residual Learning for Image Recognition</a></p>
<ul>
<li>Kaiming He 的一作</li>
<li>动机: 网络变深, 训练很难传递到深层网络, 可能是梯度越深梯度误差越大. (深度网络中的梯度消失和网络退化)</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>提出了 residual learning , 每两个卷积层添加一个短路模块.
<ul>
<li>使用 1x1 的卷积层用于补足维度不同层级维度的不同.<br>
<img alt="jgmlAHL2WY3KF81.png" src="https://cdn.sa.net/2024/04/16/jgmlAHL2WY3KF81.png" referrerpolicy="no-referrer"></li>
<li>这种方法被称为 identity mapping</li>
</ul>
</li>
<li>这个工作的效果是有数学证明的</li>
<li>发现效果很好<br>
<img alt="MDdFVGnIvjBAxqP.png" src="https://cdn.sa.net/2024/04/16/MDdFVGnIvjBAxqP.png" referrerpolicy="no-referrer"></li>
</ul>
<p>ResNet的发明动机源于深度神经网络的训练困难问题。当网络的深度超过一定程度（如20层）时，即使使用了合适的权重初始化和正则化技术，网络的性能也会开始下降，这种现象被称为退化（degradation）。这与过拟合不同，过拟合是指模型在训练集上表现优秀，但在测试集上表现较差。而退化现象是指随着网络深度的增加，模型的训练误差和测试误差都会增大。这意味着，即使网络的容量增大，网络也不能很好地拟合训练数据，这是一个非常反直觉的现象。</p>
<p>为了解决这个问题，Kaiming He等人提出了残差学习的概念。他们假设优化残差映射（即输入和输出之间的差值）比优化原始的未经处理的映射更容易。因此，他们设计了一种新的网络结构，称为残差块（Residual Block）。每个残差块包含了几个卷积层和一个跳跃连接（Skip Connection）。跳跃连接的作用是将输入直接传递到输出，形成一个短路机制，这就是所谓的恒等映射（Identity Mapping）。这种设计使得网络可以随着深度的增加而不断地学习新的特征，而不是只依赖于前面的层来学习所有的特征。</p>
<p>利用这种残差学习的概念，ResNet成功地训练了一个超过100层的深度神经网络，这在当时是前所未有的。这使得ResNet在各种视觉任务上都取得了非常好的结果，包括图像分类、物体检测和语义分割等任务。这种深度学习的新范式不仅改变了我们对深度学习的理解，也为后续的深度学习研究提供了新的方向。</p></div></div></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin " style="--callout-color: 0, 200, 83;" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p>A checklist of training deep nets</p>
<p>All about signal propagation</p>
<ul>
<li>ReLU(or similar)</li>
<li>initialization</li>
<li>normalization</li>
<li>residual connection</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><img alt="ilQtqRkT9ngXmI4.png" src="https://cdn.sa.net/2024/04/14/ilQtqRkT9ngXmI4.png" referrerpolicy="no-referrer"></p>
<p>ResNeXt是一种深度学习模型，它的设计灵感来源于VGG和ResNet两种网络结构。ResNeXt的主要贡献是引入了“cardinality”（群体大小）这个新的维度，通过增加网络的宽度来提高模型的性能。</p>
<p>以下是ResNeXt的主要贡献：</p>
<ol>
<li>
<p><strong>引入了群体大小（Cardinality）</strong>：在深度学习中，我们通常通过增加网络的深度（更多的层）或宽度（更多的神经元）来提高模型的性能。然而，ResNeXt提出了一个新的维度，即群体大小（Cardinality），也就是在并行的路径中重复相同的变换。ResNeXt的研究发现，增加群体大小的效果在保持其他参数不变的情况下，优于增加深度或宽度。</p>
</li>
<li>
<p><strong>引入了分组卷积（Grouped Convolution）</strong>：ResNeXt中的每个模块都包含了多个并行的路径，每个路径都进行相同的操作，但是操作的输入和输出都是分开的。这种设计可以看作是一种分组卷积的特例，分组卷积是一种有效的方式，可以在保持模型复杂性不变的情况下，增加网络的容量。</p>
</li>
<li>
<p><strong>提供了一种易于扩展的模型设计</strong>：ResNeXt模型的设计十分灵活，可以通过简单地调整群体大小和每个群体的宽度来扩展模型。这种设计使得ResNeXt可以很容易地适应不同的计算资源和不同的任务需求。</p>
</li>
<li>
<p><strong>在多个视觉识别任务上获得了出色的性能</strong>：在ImageNet分类、COCO object detection、COCO segmentation等任务上，ResNeXt都显示出了优于其他模型的性能。</p>
</li>
<li>
<p>发明多头学习. </p>
</li>
</ol>
<p>总的来说，ResNeXt的贡献在于提出了一种新的、有效的网络设计策略，这种策略通过引入群体大小（Cardinality）这个新的维度，以及分组卷积（Grouped Convolution），在提高模型性能的同时，保持了模型的计算效率。</p></div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1502.01852" rel="noopener" class="external-link" href="https://arxiv.org/abs/1502.01852" target="_blank">[1502.01852] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></p>
<p>这篇2015年的论文发明了 Kaiming'He初始化, 用于初始化使用 ReLu 作为激活函数的网络的初始值. </p>
<p><code>torch::nn::init:kaiming_normal_</code></p>
<p>具体思路如下: </p>
<p>在深度学习中，权重初始化是一个重要的步骤，因为不同的初始化方法可能会影响模型的收敛速度和最终性能。如果权重初始化不当，可能会导致训练过程中的梯度消失或爆炸问题，从而使模型无法学习。</p>
<p>Kaiming初始化（或称He初始化）的思路和动机源于对ReLU（Rectified Linear Unit）激活函数特性的理解。ReLU函数在负数部分的输出为0，这意味着大约一半的神经元在初始化时会被“死亡”（即输出为0），这可能导致反向传播过程中的梯度消失。为了解决这个问题，我们需要一个特殊的初始化策略。</p>
<p>Kaiming初始化的基本思想是<strong>保持每一层输出的方差与输入的方差一致</strong>。</p>
<ul>
<li>一是训练过程中层数之间权重的方差过大或过小容易导致梯度爆炸&amp;消失</li>
<li>二是模型的权重方差过大，可能会导致模型过拟合，因为模型可能会过度依赖某些特定的特征</li>
</ul>
<p>具体来说，如果一个层的输入神经元的数量为 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>，那么权重的初始值应该从均值为0，标准差 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-sop"><mjx-c class="mjx-c221A TEX-S1"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.107em;"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math></mjx-container></span>​的正态分布中随机取值。这样，即使经过ReLU激活函数，输出的方差也能保持不变。</p>
<p>这种初始化方法可以有效地保持每一层的激活值在经过多层传播后仍然保持有用的信息，从而防止梯度消失或爆炸的问题。这是Kaiming初始化被广泛应用于深度学习中的主要原因</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="MobileNet(网络做小) - 2016" class="heading" id="MobileNet(网络做小)_-_2016"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>MobileNet(网络做小) - 2016</h3><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><strong>什么是MobileNet? MobileNet 对 深度学习领域优势有什么历史贡献?</strong></p>
<p> MobileNet是一种基于深度神经网络的轻量级模型，主要用于移动和嵌入式视觉应用。它由Google的研究人员在2017年提出，其目标是提供一种高效的深度学习模型，可以在资源受限的设备上运行，如智能手机或嵌入式设备。</p>
<p>MobileNet的主要特点是使用深度可分离的卷积（depthwise separable convolution）来替代传统的卷积。深度可分离的卷积将一个卷积操作分解为两个更轻量级的子操作，从而大大减少了计算量和模型大小，同时仍然保持了良好的性能。</p>
<p>MobileNet对深度学习领域的主要贡献如下：</p>
<ol>
<li>
<p>提出了一种新的<strong>深度可分离的卷积</strong>方法，这种方法大大减少了模型的计算量和大小，使得深度学习模型可以在资源受限的设备上运行。</p>
</li>
<li>
<p>提出了一种新的网络结构，这种结构可以<strong>根据特定的资源约束（如计算资源、能源需求等）进行调整</strong>，从而在各种不同的情况下都能提供良好的性能。</p>
</li>
<li>
<p>MobileNet在许多视觉任务上都表现出了<strong>优异的性能</strong>，如图像分类、物体检测、人脸识别等，这进一步证明了深度可分离卷积的有效性和灵活性。</p>
</li>
</ol>
<p>总的来说，MobileNet的出现为移动和嵌入式设备上的深度学习应用开启了新的可能性，它的设计思想和技术也对后续的研究工作产生了深远的影响。</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="Xception(常用) - 2016" class="heading" id="Xception(常用)_-_2016"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Xception(常用) - 2016</h3><div class="heading-children"><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=WyBP4r-l-kc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=WyBP4r-l-kc" target="_blank">大模型Xception为什么常用？主要用来做什么？ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1610.02357" rel="noopener" class="external-link" href="https://arxiv.org/abs/1610.02357" target="_blank">[1610.02357] Xception: Deep Learning with Depthwise Separable Convolutions</a></p>
<p>Inception的极限优化.</p>
<p>Xception是一种深度学习模型，由Google的研究员François Chollet开发。Xception是"Extreme Inception"的缩写，这个模型的设计灵感来源于Inception模块，但是它对Inception模块进行了一种更极端的解释。</p>
<p>Xception的主要贡献是提出了一种新的卷积操作，即深度可分离卷积（Depthwise Separable Convolution）。这种卷积操作包含两个步骤：深度卷积（Depthwise Convolution）和点卷积（Pointwise Convolution）。</p>
<p>深度卷积是对每一个输入通道进行单独的卷积操作，而点卷积则是用1x1的卷积核对所有的输入通道进行卷积。深度可分离卷积的设计目标是在减少模型的计算复杂性的同时，保持模型的性能。</p>
<p>除了深度可分离卷积，Xception还提出了一种新的网络架构，这种架构完全放弃了Inception模块中的并行结构，而是将所有的操作都串行化。这种设计基于一个假设，即跨通道的特征和空间特征是可以独立学习的。</p>
<p>在实验中，Xception在多个视觉识别任务上都表现出了优于Inception V3的性能，同时模型的计算复杂性也有所降低。</p>
<ul>
<li class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 发现有全连接层非常有必要</span></li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="U-Net(常用)" class="heading" id="U-Net(常用)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>U-Net(常用)</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/search?type=content&amp;q=U%20Net%20" rel="noopener" class="external-link" href="https://www.zhihu.com/search?type=content&amp;q=U%20Net%20" target="_blank">U Net - 搜索结果 - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/483261689/answer/2093332362" rel="noopener" class="external-link" href="https://www.zhihu.com/question/483261689/answer/2093332362" target="_blank">为什么U-Net在医学图像上表现优越？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/480371053/answer/3084572728" rel="noopener" class="external-link" href="https://www.zhihu.com/question/480371053/answer/3084572728" target="_blank">医学图像分割，除了魔改unet，还能有哪些创新点？ - 知乎</a></p></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin " style="--callout-color: 0, 200, 83;" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p><img src="https://cdn.sa.net/2024/04/14/xaL5f6JcCRpPHUY.png" referrerpolicy="no-referrer"></p>
<p>分割（Segmentation）是计算机视觉中相当流行的一项任务。在分割任务中，我们试图从背景中移除/提取前景。 前景和背景可以有不同的定义。我们也可以说，这是一项像素分类任务，你的工作是给给定图像中的每个像素分配一个类别。事实上，我们正在处理的气胸数据集就是一项分割任务。</p>
<p>分割任务的损失函数有:<br>
二元交叉熵、focal损失、dice损失等.</p>
<p>用于分割任务的最常用模型是 U-Net。</p>
<p>U-Net 包括两个部分：编码器和解码器。编码器与您目前所见过的任何 U-Net 都是一样的。解码器则有些不同。解码器由上卷积层组成。在上卷积（up-convolutions）（<strong>转置卷积</strong>transposed convolutions）中，我们使用滤波器，当应用到一个小图像时，会产生一个大图像。在 PyTorch 中，您可以使用 ConvTranspose2d 来完成这一操作。必须注意的是，上卷积与上采样并不相同。上采样是一个简单的过程，我们在图像上应用一个函数来调整它的大小。在上卷积中，我们要学习滤波器。我们将编码器的某些部分作为某些解码器的输入。这对 上卷积层非常重要。</p>
<p><strong>U-Net = 收缩层 + 瓶颈层 + 扩展层</strong></p>
<ul>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>收缩层</strong>
<ul>
<li>逐渐缩小输入图像的大小, 增加通道数量. 通过一系列卷积层和下采样, 提取图像的局部特征, 转化为更加高级别的抽象特征.</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>瓶颈层</strong>
<ul>
<li>由多个卷积层构成. 目标是捕获图像的高级特征, 减少特征图的维度, 保留重要的空间信息. 帮助UNet整合全局和局部星系, 以获取图像细节的上下文关系. </li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>扩展层</strong>
<ul>
<li>对特征图上采样, 通过一系列上采样和卷积层, 恢复到原始图像的尺寸. 使用跳跃连接, 讲收缩层和扩展层进行连接, 帮助保留细粒度的空间信息, 提高分割结果的准确性和稳定性. </li>
</ul>
</li>
</ul>
<p>U-Net 中的 skip connection 使得网络可以学习并纳入多个尺度的信息.</p>
<p><a data-tooltip-position="top" aria-label="https://ytzfhqs.github.io/AAAMLP-CN/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E5%88%86%E5%89%B2%E6%96%B9%E6%B3%95/" rel="noopener" class="external-link" href="https://ytzfhqs.github.io/AAAMLP-CN/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E5%88%86%E5%89%B2%E6%96%B9%E6%B3%95/" target="_blank">图像分类和分割方法 - AAAMLP 中译版</a></p>
<p>⬆️ 里面有一个 原始 U-Net 的 Pytorch 实现</p></div></div></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content heading-wrapper"><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1505.04597" rel="noopener" class="external-link" href="https://arxiv.org/abs/1505.04597" target="_blank">[1505.04597] U-Net: Convolutional Networks for Biomedical Image Segmentation</a></p>
<p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=Y-t5TmbY1Jc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=Y-t5TmbY1Jc" target="_blank">经典论文速读：U-Net讲了什么？为什么现在还在用？ - YouTube</a></p>
<h3 data-heading="摘要" class="heading" id="摘要">摘要</h3>
<ul>
<li>论文指出，深度网络的成功训练通常需要成千上万的标注样本。</li>
<li>作者提出了一种依赖于<strong>数据增强</strong>的网络和训练策略，以更高效地利用有限的标注样本。</li>
<li>U-Net架构包含一个收缩路径来捕获上下文信息和一个对称的扩展路径来进行精确定位。</li>
<li>论文展示了该网络可以从很少的图像中进行端到端训练，并在ISBI挑战赛中超过了之前最好的方法（滑动窗口卷积网络）。</li>
<li>U-Net在传输光显微镜图像（相差和DIC）上的训练也赢得了2015年ISBI细胞跟踪挑战赛的多个类别。</li>
<li>网络速度快，对512x512图像的分割少于一秒即可完成。</li>
</ul>
<h3 data-heading="引言" class="heading" id="引言">引言</h3>
<ul>
<li>过去两年中，深度卷积网络在许多视觉识别任务中超越了最先进的技术。</li>
<li>卷积网络虽然已经存在很长时间，但由于训练集的大小和考虑的网络规模限制了它们的成功。</li>
<li>论文基于“全卷积网络”架构进行了改进和扩展，使其能够在只有很少训练图像的情况下工作，并产生更精确的分割结果。</li>
</ul>
<h3 data-heading="网络架构" class="heading" id="网络架构">网络架构</h3>
<ul>
<li>U-Net的架构由收缩路径和扩展路径组成，如图1所示。</li>
<li>收缩路径遵循典型的卷积网络架构，通过重复应用3x3卷积、ReLU和2x2最大池化操作进行下采样。(<strong>用于获得上下文</strong>)</li>
<li>扩展路径的每一步包括特征图的上采样、“上卷积”和与收缩路径中相应裁剪的特征图的拼接。(<strong>用于精准定位</strong>)</li>
<li>U-Net中没有全连接层，<strong>通过互连卷积与反卷积过程中的特征，将上下文信息传递到更高层，实现了信息补充</strong>；</li>
<li>这种策略允许通过重叠瓦片策略无缝分割任意大的图像。</li>
<li>U-Net 在FCN 的基础上<strong>增加了上采样操作的次数和跳跃连接，使用跳跃连接将解码器的输出特征与编码器的语义特征融合，提高了分割精度，改善了 FCN 上采样不足的问题</strong>。</li>
</ul>
<h3 data-heading="训练" class="heading" id="训练">训练</h3>
<ul>
<li>使用Caffe实现的随机梯度下降来训练网络。</li>
<li>论文介绍了如何计算能量函数和使用权重图来进行训练。</li>
</ul>
<h3 data-heading="数据增强" class="heading" id="数据增强">数据增强</h3>
<ul>
<li>数据增强对于训练网络以获得期望的不变性和鲁棒性属性至关重要，尤其是在只有少量训练样本可用的情况下。</li>
<li>论文中提到，随机弹性变形是训练具有很少标注图像的分割网络的关键概念。</li>
</ul>
<p>实验中用到的方法:</p>
<ol>
<li><strong>弹性变形（Elastic Deformations）</strong>: 这是一种通过对训练样本应用随机位移向量来生成平滑变形的方法。位移是从标准差为10像素的高斯分布中采样的，然后使用双三次插值计算每个像素的位移。这种增强手段对于模拟组织中的变形特别有效，因为生物医学图像中的组织经常会出现此类变形。</li>
<li><strong>旋转和翻转</strong>: 通过对训练图像进行随机旋转和水平/垂直翻转，增加了模型对这些变换的不变性。</li>
<li><strong>强度变化</strong>: 通过对图像的亮度和对比度进行随机调整，模型能够学会在不同光照和成像条件下进行有效的分割。</li>
<li><strong>丢弃层（Drop-out Layers）</strong>: 在网络的收缩路径末端使用丢弃层进行进一步的隐式数据增强。</li>
</ol>
<h3 data-heading="实验" class="heading" id="实验">实验</h3>
<ul>
<li>论文展示了U-Net在三个不同的分割任务中的应用。</li>
<li>在电子显微镜记录的神经结构分割中，U-Net在ISBI挑战赛中取得了最佳成绩。</li>
<li>论文还展示了在ISBI细胞跟踪挑战赛2015中的细胞分割结果，U-Net在两个最具挑战性的2D传输光数据集上取得了显著的胜利。</li>
</ul>
<h3 data-heading="结论" class="heading" id="结论">结论</h3>
<ul>
<li>U-Net架构在多种生物医学分割应用中表现出色。</li>
<li>通过弹性变形的数据增强，U-Net只需要很少的标注图像，并且训练时间合理。</li>
<li>论文提供了完整的基于Caffe的实现和训练好的网络，并相信U-Net架构可以轻松应用于更多任务。</li>
</ul>
<h3 data-heading="后续改进" class="heading" id="后续改进">后续改进</h3>
<p>在U-Net的原始模型之后，有许多研究者对其进行了改进，以适应更复杂的任务和更大的数据集。这些改进主要集中在以下几个方面：</p>
<ol>
<li>
<p><strong>更深的网络</strong>：例如，V-Net和Res-U-Net。这些网络在U-Net的基础上添加了更多的层，以增加模型的容量和复杂性。</p>
</li>
<li>
<p><strong>更复杂的连接方式</strong>：例如，Dense U-Net。这种网络在每一层之间添加了密集连接，使得每一层的输出都成为下一层的输入。</p>
</li>
<li>
<p><strong>注意力机制</strong>：例如，Attention U-Net。这种网络在U-Net的基础上添加了注意力机制，使得网络在进行特征融合时，可以更加关注重要的区域。</p>
</li>
<li>
<p><strong>多尺度特征融合</strong>：例如，MultiRes U-Net。这种网络在U-Net的基础上添加了多尺度特征融合，使得网络可以同时捕捉到不同尺度的信息。</p>
</li>
<li>
<p><strong>3D U-Net</strong>：原始的U-Net是为2D图像设计的，但在许多医学图像任务中，我们需要处理3D图像。因此，有研究者提出了3D U-Net，以处理3D图像。</p>
</li>
<li>
<p><strong>更有效的训练策略</strong>：例如，使用更复杂的数据增强技术，或者使用更复杂的损失函数，如Dice损失或Focal损失。</p>
</li>
</ol>
<p>以上都是对U-Net的一些改进，但是需要注意的是，哪种改进最有效，很大程度上取决于具体的任务和数据。</p>
<p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/548613974/answer/3229770898" rel="noopener" class="external-link" href="https://www.zhihu.com/question/548613974/answer/3229770898" target="_blank">关于U-Net的魔改到了什么程度了？ - 知乎</a></p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2401.04722" rel="noopener" class="external-link" href="https://arxiv.org/abs/2401.04722" target="_blank">[2401.04722] U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation</a></p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="EfficientNet (终结网络scale经验法则) - 2019" class="heading" id="EfficientNet_(终结网络scale经验法则)_-_2019"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>EfficientNet (终结网络scale经验法则) - 2019</h3><div class="heading-children"><div><blockquote>
<p>有章法的缩放网络的宽度和深度, 避免可能的网络浪费</p>
</blockquote></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><img src="https://img-blog.csdnimg.cn/20210306162502756.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70#pic_center" referrerpolicy="no-referrer"></p>
<p><img src="https://cdn.sa.net/2024/04/16/5pRuksEVtmT69zi.png" referrerpolicy="no-referrer"></p>
<p><img src="https://cdn.sa.net/2024/04/16/4XnPsyhY6a3vRCd.png" referrerpolicy="no-referrer"></p>
<p>这篇文章是谷歌研究团队发表的《EfficientDet::Scalable and Efficient Object Detection》。论文系<br>
统地研究了针对目标检测任务神经网络架构设计的选择，并提出了一系列关键优化以提升效率。文<br>
章的主要贡献包括：</p>
<ol>
<li>提出了<strong>加权双向特征金字塔网络(BiFPN)</strong>,简化了多尺度特征融合的过程。</li>
<li>提出了一种<strong>复合缩放方法</strong>，均匀地缩放了所有背骨、特征网络以及分类/框预测网络的分辨率、深度和宽度。</li>
<li>基于这些优化以及更好的主干网络，开发了一系列称为EfficientDet的新型目标检测器，与先前的模型相比，各种资源限制下都能达到更好的效率。</li>
</ol>
<p>特别值得一提的是，使用单一模型和单一尺度的EfficientDet-D7在C0Co测试集上达到了55.1%的最<br>
新AP(平均精度)，并且只有77O0万参数和4100亿FLOPs,比以前的检测器小4-9倍，使用的FLOPs:少13-42倍。此外，EfficientDet在GPU/CPU上的运行速度比以前的检测器快4-11倍。论文还证明了EfficientDet在Pascal VOC2012语义分割上也表现出色，达到了8174%的mlOU精度，同时FLOPs比DeepLabV3+少9.8倍。</p>
<p><strong>什么是EfficientNet? 在深度学习领域有什么历史贡献?</strong></p>
<p> EfficientNet是一种在深度学习领域中使用的卷积神经网络(CNN)架构。它是由Google的研究员在2019年提出的，该网络使用了一种名为“复合缩放”的方法，通过均衡网络的深度、宽度和解析度，使得模型在保持相同计算资源的情况下，可以达到更高的精度。</p>
<p>EfficientNet的核心思想是：不同维度之间的缩放可以带来更好的性能。在之前的一些工作中，研究人员通常只关注单一维度的缩放，例如网络的深度（如ResNet）或宽度（如MobileNet）。然而，EfficientNet指出，这些单一维度的缩放往往会导致资源的浪费。因此，EfficientNet提出了一种复合缩放的方法，同时考虑了深度、宽度和解析度的缩放，从而在有限的资源下获得更好的性能。</p>
<p>EfficientNet的历史贡献主要体现在两个方面：</p>
<ol>
<li>
<p>EfficientNet提出了一种新的网络缩放方法，这种方法可以在有限的计算资源下，获得更好的性能。这对于那些资源有限，但希望获得高性能模型的场景（如移动设备、嵌入式设备等）具有重要的意义。</p>
</li>
<li>
<p>EfficientNet在一系列的图像识别任务上都取得了当时最好的性能，包括ImageNet，CIFAR-100等数据集，这进一步证明了其有效性和优越性。</p>
</li>
</ol>
<p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=jsHda3dhM-g" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=jsHda3dhM-g" target="_blank">【博士Vlog】EfficientNet 和 EfficientDet 讲了什么？有计算资源真的可以为所欲为！！ - YouTube</a></p>
<ul>
<li>介绍了EfficentNet(图像分类) 和 EfficientDet(物体检测)</li>
<li>网络速度快, 参数小, 效果又好</li>
<li>平衡了网络 <code>depth</code> <code>channel</code> <code>width</code> 的设计</li>
<li>在ImageNet训练, 上面的平衡法则基本上就是试出来的(Neural Architecture Search)</li>
<li></li>
</ul>
<p><a data-tooltip-position="top" aria-label="https://blog.csdn.net/qq_37541097/article/details/114434046" rel="noopener" class="external-link" href="https://blog.csdn.net/qq_37541097/article/details/114434046" target="_blank">EfficientNet网络详解-CSDN博客</a></p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><img src="https://cdn.sa.net/2024/04/16/VdE6cFM3r7aGgLx.png" referrerpolicy="no-referrer"></p>
<p>这张图展示了四种不同的特征网络设计用于多尺度特征融合，它们在目标检讽测模型中起着至关重要<br>
的作用。具体来说：</p>
<p>(a)FPN(Feature Pyramid Network):FPN引入了一个自顶向下的通道，以融合从第三层到第七<br>
层的多尺度特征。</p>
<p>(b)PANet(Path Aggregation Network):在FPN的基础上增加了一个自底向上的路径，以进一步<br>
增强特征的传递。</p>
<p>(c)NAS-FPN(Neural Architecture Search Feature Pyramid Network):使用神经架构搜索找到了<br>
一个不规则的特征网络拓扑结构，并且在网络中重复应用相同的块。</p>
<p>(d)BiFPN(Bidirectional Feature Pyramid Network):提出了BiFPN,具有更好的精度和效率权<br>
衡。BFPN加强了特征的上下文融合，通过增加额外的自底向上和自顶向下路径，并在多个尺度之间进行重复的特征融合。</p>
<p>整体上，这些设计展示了如何通过不同的策略来增强多尺度特征的融合能力，BFPN作为<br>
EfficientDet模型中的一个组件，它的设计旨在通过增加简单有效的双向路径来提升目标检测的性<br>
能。</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="BiT (最大有监督CNN)" class="heading" id="BiT_(最大有监督CNN)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>BiT (最大有监督CNN)</h3><div class="heading-children"><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=VzJ7us4eQcE" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=VzJ7us4eQcE" target="_blank">CNN之王什么样子？谷歌造出了世界上最大的CNN模型！效果惊人！ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1912.11370" rel="noopener" class="external-link" href="https://arxiv.org/abs/1912.11370" target="_blank">Big Transfer (BiT): General Visual Representation Learning - 2019</a></p>
<p> Google发表. 最大CNN直接表明更大规模CNN scale 能力已经受限了. 这里发现的CNN限制为ViT的发明做了铺垫工作. </p>
<p>文章内容:</p>
<ol>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>大力出奇迹把resnet做大到极致
<ol>
<li>ResNet *4 : channel 64 - 256</li>
</ol>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>不是一般学者能玩的起的
<ol>
<li>在ImageNet-21K 和 JFT-300M(最大公开数据集)</li>
<li>训练了3个月</li>
</ol>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>他们发现了一些困难，也就是大力出奇迹效果不好于是就有了ViT
<ol>
<li>发现 Batch Normazation 在并行训练效果不好. 于是就有了 Group Normalizaton.</li>
<li>发现 MixUP(数据标签线性组合) 效果不好</li>
</ol>
</li>
<li>把注意力机制做的灵活，效果会更好</li>
<li>大力出奇迹是末期表现，一般大力出奇迹之后就会有新架构出现. e.g.VGG, BigGAN, GPT-4?</li>
</ol>
<p>通过制造更高效的模型，更灵活的架构，而不是傻堆料</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="VanillaNet ?" class="heading" id="VanillaNet_?"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>VanillaNet ?</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2305.12972" rel="noopener" class="external-link" href="https://arxiv.org/abs/2305.12972" target="_blank">[2305.12972] VanillaNet: the Power of Minimalism in Deep Learning</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/search?type=content&amp;q=VanillaNet" rel="noopener" class="external-link" href="https://www.zhihu.com/search?type=content&amp;q=VanillaNet" target="_blank">VanillaNet - 搜索结果 - 知乎</a></p></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><strong>VanillaNet</strong>是一种注重极简主义和效率的神经网络架构。它的设计简单，层数较少，避免了像深度架构和自注意力这样的复杂操作。VanillaNet的关键特性包括<strong>深度训练策略</strong>，最初使用激活函数训练两个卷积层，随后这个激活函数逐渐简化为恒等映射，允许层合并。此外，VanillaNet还使用<strong>并行堆叠的激活函数</strong>来提高非线性，从而提升简单网络的性能。</p>
<p>里面的数学很复杂, 看不懂. </p></div></div></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="RNN(循环神经网络)" class="heading" id="RNN(循环神经网络)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>RNN(循环神经网络)</h2><div class="heading-children"><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p>RNN&nbsp;试图通过加入反馈机制来理解一串数值前后的关系，所以适用于语言模型，因为这些数值之间有前后关系，像我们的句子里有先后逻辑</p></div></div></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="faq" class="callout drop-shadow"><div class="callout-title"><div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-help-circle"><circle cx="12" cy="12" r="10"></circle><path d="M9.09 9a3 3 0 0 1 5.83 1c0 2-3 3-3 3"></path><path d="M12 17h.01"></path></svg></div><div class="callout-title-inner">相比Transformer, RNN的优势在于</div></div><div class="callout-content">
<ul>
<li data-line="1">串行结构天然更加适合于序列数据</li>
<li data-line="2">推理更高效 复杂度为 O(N), 而Transformer 为 O(N^2)</li>
</ul>
</div></div></div><div><p><img src="https://cdn.sa.net/2024/04/05/rtqmWbpCE9V7Qin.png" referrerpolicy="no-referrer"><br>
RNN 要从1D数据中学习到特征.<br>
CNN其实用一个1D卷积核配合池化层在数据上滑动其实也可以学习.</p></div><div><p><img src="https://cdn.sa.net/2024/04/05/EGtNkwJ3YXsaHQ9.png" referrerpolicy="no-referrer"><br>
循环神经网络（Recurrent Neural Network，RNN）的深度可以从两个方向进行扩展：时间步长和隐藏层的数量。</p></div><div><ol>
<li data-line="0"><strong>时间步长</strong>：在RNN中，每个时间步都可以看作是网络的一层。因此，处理更长的序列会使网络在时间维度上更深。例如，处理一个由100个词构成的句子的RNN可以被视为一个有100层的网络。然而，这种方法存在一个问题，那就是随着时间步长的增加，网络可能会遇到梯度消失或梯度爆炸的问题，这使得训练变得困难。为了解决这个问题，研究者提出了一些更复杂的RNN变体，如长短期记忆网络（Long Short-Term Memory，LSTM）和门控循环单元（Gated Recurrent Unit，GRU），它们通过引入门控机制来控制信息的流动，从而缓解了梯度消失的问题。</li>
<li data-line="1"><strong>隐藏层的数量</strong>：除了在时间维度上增加深度，我们也可以在空间维度上增加深度(如上图)，即增加隐藏层的数量。这可以通过堆叠多个RNN层来实现，每一层RNN的输出都作为下一层的输入。这种方法可以使网络学习更复杂的特征，但同样也会增加训练的难度。同样，可以使用LSTM或GRU等更复杂的RNN变体来缓解这个问题。</li>
</ol></div><div><p>总的来说，使RNN变得更深可以帮助网络学习更复杂的模式，但同时也会增加训练的难度。为了解决这个问题，研究者已经提出了许多方法，如使用更复杂的RNN变体、更好的优化算法、正则化技术等。</p></div><div><blockquote>
<p>RNN 接受两个输入：State和Token。它一次通过输入序列一个Token，每个Token更新状态。例如，我们可以使用 RNN 将文本处理成单个状态向量。然后，这可用于将文本分类为“正面”或“负面”。或者我们可以使用最终状态来预测下一个Token，这就是 RNN 用于生成文本的方式。</p>
</blockquote></div><div><p><img src="https://cdn.sa.net/2024/04/05/BcrxtwSMmb4ZED8.png" referrerpolicy="no-referrer"><br>
RNN 但是因为下一个参数的训练要等上一个参数算好才可以进行, 所以不适合多GPU训练.<br>
CNN 就没这个问题, 但它学习到的context windows 由取决于kernel size. </p></div><div class="heading-wrapper"><h3 data-heading="LSTM(长短期记忆网络) &amp; GRU(门控循环单元)" class="heading" id="LSTM(长短期记忆网络)_&amp;_GRU(门控循环单元)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>LSTM(长短期记忆网络) &amp; GRU(门控循环单元)</h3><div class="heading-children"><div><p><img alt="ZLubfyJEvpmVaYq.png" src="https://cdn.sa.net/2024/04/11/ZLubfyJEvpmVaYq.png" referrerpolicy="no-referrer"></p></div><div><p>Google's Neural Machine Translation System:Bridging the Gap between Human and Machine Translation",Wu,et al.,2016</p></div><div><p>Stack LSTM units<br>
going deep with residual connections</p></div><div><ul>
<li data-line="0">enable 16 layers</li>
<li data-line="1">degrade 4 layers if not using residual </li>
<li data-line="2">like observation in CNNs</li>
</ul></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin " style="--callout-color: 0, 200, 83;" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p>WaveNet是DeepMind在2016年提出的一种深度学习模型，它是一种生成模型，可以用于生成原始音频波形，特别是用于文本到语音（Text-to-Speech，TTS）的应用。</p>
<p>WaveNet的主要创新之处在于它使用了一种叫做“稀疏卷积”的结构，也被称为“扩张卷积”或“膨胀卷积”。这种结构使得每个输出样本可以看到的输入范围（即感受野）随着层级的增加而指数级增长，而不是像传统的卷积神经网络那样线性增长。这使得WaveNet可以处理长范围的依赖关系，这在音频生成等序列生成任务中是非常重要的。</p>
<p>WaveNet的另一个重要特性是它是一个完全生成的模型，也就是说，它一次生成一个样本，然后将这个样本馈送回模型，作为下一个样本的一部分输入。这种自回归的特性使得WaveNet可以生成非常自然和连贯的音频。</p>
<p>WaveNet的效果非常出色，它在多种语音生成任务上都取得了最好的效果，包括在非常挑战的文本到语音合成任务上。事实上，Google的云语音合成服务就是基于WaveNet的。</p>
<p>总的来说，WaveNet是一种强大的音频生成模型，它通过使用稀疏卷积和自回归结构，可以生成非常自然和连贯的音频。</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="RWKV" class="heading" id="RWKV"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>RWKV</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://fullstackdeeplearning.com/blog/posts/rwkv-explainer/" rel="noopener" class="external-link" href="https://fullstackdeeplearning.com/blog/posts/rwkv-explainer/" target="_blank">RWKV, Explained - The Full Stack</a><br>
国人开发, 基于RNN</p></div><div><p> <img src="https://pic1.zhimg.com/v2-34943728e7ffdb60829f6a82fc10c610_r.jpg" referrerpolicy="no-referrer"><br>
<img alt="GPT_versus_RWKV.svg" src="https://rwkv-wiki.github.io/img/GPT_versus_RWKV.svg" referrerpolicy="no-referrer"><br>
<a data-tooltip-position="top" aria-label="https://rwkv-wiki.github.io/" rel="noopener" class="external-link" href="https://rwkv-wiki.github.io/" target="_blank">RWKV Wiki - RWKV Wiki</a><br>
<a data-tooltip-position="top" aria-label="https://wiki.rwkv.com/advance/architecture.html#how-does-rwkv-differ-from-classic-rnn" rel="noopener" class="external-link" href="https://wiki.rwkv.com/advance/architecture.html#how-does-rwkv-differ-from-classic-rnn" target="_blank">RWKV Architecture</a><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/514840332" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/514840332" target="_blank">RWKV-v2-RNN 原理：超越 Transformer，实现 O(T) 的语言建模 - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py" rel="noopener" class="external-link" href="https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py" target="_blank">ChatRWKV/RWKV_in_150_lines.py at main · BlinkDL/ChatRWKV · GitHub</a></p></div><div><ul>
<li data-line="0"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>RWKV</strong>&nbsp;是一种拥有RNN, CNN, Transformer三种优点的RNN网络. 在性能媲美Transformer的基础上，具有O(1)推理复杂度，更易收敛训练，模型参数和内存占用
<ul>
<li data-line="1">inspired by AFT（Attention-Free Transformer）</li>
</ul>
</li>
<li data-line="2">背景: 所有Self-Attention 为基础的模型, 都不肯避免的需要Token之间相互计算, 造成了 O(n^2) 的复杂度; 且位置信息外挂</li>
<li data-line="3"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>RWKV针对这两个问题, 使用
<ul>
<li data-line="4">WKV 计算过程直接向 Token 引入了具有平移不变性的位置编码，不需要引入额外的位置编码。</li>
<li data-line="5">使用两个RWKV层替换了Multi-head Attention 和 前馈网络</li>
<li data-line="6">Token 之间无需相互运算，WKV 计算过程只对各 Token 分别变换并累加结果, 成就了 O(n)的复杂度</li>
</ul>
</li>
<li data-line="7"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>RNN 在训练时并行<img alt="image.png" src="https://cdn.sa.net/2024/01/29/jgsuCJzViMv2PEK.png" referrerpolicy="no-referrer">
<ul>
<li data-line="8"></li>
</ul>
</li>
<li data-line="9">RNN 在推理时串行</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="Transformer (2017)" class="heading" id="Transformer_(2017)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Transformer (2017)</h2><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener" class="external-link" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank">2017 - Attention Is All You Need.pdf</a><br>
<a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Ja4y1B7zC/?spm_id_from=333.788.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Ja4y1B7zC/?spm_id_from=333.788.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">[重置版]从零实现transfomer模型 || 理解ChatGPT基石 || pytorch_哔哩哔哩_bilibili</a><br>
<a data-tooltip-position="top" aria-label="https://paperswithcode.com/paper/attention-is-all-you-need" rel="noopener" class="external-link" href="https://paperswithcode.com/paper/attention-is-all-you-need" target="_blank">Attention Is All You Need | Papers With Code</a><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/648127076?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1716622508470669312&amp;utm_source=io.raindrop.raindropio" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/648127076?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1716622508470669312&amp;utm_source=io.raindrop.raindropio" target="_blank">三万字最全解析！从零实现Transformer（小白必会版😃） - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/zvideo/1722621858900451329?utm_psn=1733842036934078467" rel="noopener" class="external-link" href="https://www.zhihu.com/zvideo/1722621858900451329?utm_psn=1733842036934078467" target="_blank">哈工大PHD竟把Transformer讲的如此简单！ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/589738603/answer/3375664602" rel="noopener" class="external-link" href="https://www.zhihu.com/question/589738603/answer/3375664602" target="_blank">为什么只有基于Transformer的大模型，而没有其他的？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/596771388/answer/3454101159?utm_medium=social&amp;utm_psn=1760630215855116288&amp;utm_source=ZHShareTargetIDMore" rel="noopener" class="external-link" href="https://www.zhihu.com/question/596771388/answer/3454101159?utm_medium=social&amp;utm_psn=1760630215855116288&amp;utm_source=ZHShareTargetIDMore" target="_blank">为什么我还是无法理解transformer？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=eMlx5fFNoYc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=eMlx5fFNoYc" target="_blank">可视化注意力，变形金刚的心脏 | 第 6 章，深度学习 - YouTube</a> (强烈推荐)</p></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content heading-wrapper"><h3 data-heading="Transformer 普遍拥有什么特性?" class="heading" id="Transformer_普遍拥有什么特性?"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Transformer 普遍拥有什么特性?</h3>
<ol>
<li>
<p><strong>自注意力机制</strong>：Transformer 的核心是自注意力机制（Self-Attention），它能够处理输入和输出序列的每一个元素，并自动学习其内部结构。</p>
</li>
<li>
<p><strong>并行性</strong>：Transformer 的另一个显著特性是并行性。由于其自注意力机制的特性，它可以同时处理序列中的所有元素，而不需要像 RNN 那样逐个处理，大大提高了计算效率。</p>
</li>
<li>
<p><strong>位置编码</strong>：由于 Transformer 没有循环和卷积操作，无法捕获序列中的顺序关系，因此需要通过添加位置编码（Positional Encoding）来帮助模型理解序列中的位置信息。</p>
</li>
<li>
<p><strong>多头注意力机制</strong>：Transformer 使用了多头注意力机制，可以让模型同时关注输入序列的不同位置，捕获各种范围的信息。</p>
</li>
<li>
<p><strong>可扩展性</strong>：Transformer 结构具有很好的可扩展性，可以通过堆叠更多的层或增加更多的注意力头来增加模型的复杂性和表达能力。</p>
</li>
</ol>
<h3 data-heading="Transformer 的 感受域 和 CNN 比有什么异同?" class="heading" id="Transformer_的_感受域_和_CNN_比有什么异同?">Transformer 的 感受域 和 CNN 比有什么异同?</h3>
<p> 感受域（Receptive Field）是指神经网络中某一层的神经元可以感知到的输入数据的范围。在计算机视觉中，感受域的概念非常重要，因为图像的局部信息往往是非常重要的。</p>
<p>在卷积神经网络（CNN）中，感受域是逐层递增的，即一层的感受域是由其前一层的感受域决定的。例如，如果第一层的感受域是3x3，那么第二层的感受域就是5x5，第三层的感受域就是7x7，以此类推。这种设计使得CNN能够在保持计算效率的同时，捕捉到图像的局部信息。</p>
<p>而在Transformer模型中，每个位置的感受域都是全局的，也就是说，每个位置的输出都依赖于所有位置的输入。这种设计使得Transformer模型能够捕捉到序列中的长距离依赖关系，但同时也增加了计算的复杂性。</p>
<p>简单来说，CNN的感受域是局部的并且逐层递增，而Transformer的感受域是全局的。这两种模型各有优势，适用于不同的任务。</p><div class="heading-children"></div></div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p>block </p>
<ul>
<li>Self-Attention(QKV)</li>
<li>Encoder-decoder </li>
</ul>
<p>transformer 里的 QKV 研究问题和答案之间的关系。不去找前后，不去找相邻，就是单纯的问题（Query）和答案(Value)，最多加了一个(Key)来辅助. 因为这是谷歌搜索等搜索引擎最开始的结构。任何一个问题(Query)，会有很多的答案（Value），而之所以能找到这些答案，是因为这些答案里面包包含了有关于这个问题的关键信息（Key） <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span></p>
<p>这是一个万能形式，任何问题的答案都是通过“问题本身+相关的关键信息”找到的，比如你去谷歌搜索“今天天气怎么样”，这个问题本身就是 Q，而你的语言是“中文”，你的位置是“北京”，你的时间是“今天”，这些就都是 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>，那么找到的答案“下雨”就是 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>。</p>
<p>一般来说肯定是通过方法找到 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span> 中的一些系数，就可以找到正确的 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> 了。我们也可以把 V挪到公式右边，并且把他们存在的关系叫成 attention，那么就是：</p>
<div class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="4"><mjx-c class="mjx-c73"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c66"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c78"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.082em;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></div>
<p>这就是整个 transformer 的最基础结构，有了这个万能结构，只需要学习 F里的各个参数，就可以回答你想要的问题。</p>
<p>那么，又为什么叫 <strong>transformer</strong> 而不是简单的&nbsp;attention&nbsp;呢？</p>
<p>因为 transformer 它为了提高这个 F的 运算效率，做了一些规定，比如你的 attention 的输入输出维度需要一样，这样矩阵运算就可以加快。而且多个 attention 合在一起来算，也是为了加快运算速度和效率。</p>
<p>那整个&nbsp;Transformer里的 <strong>encoded</strong> <strong>decoder</strong> 又是干嘛呢？</p>
<p>这个 QKV 结构是最基础的单元，为了适应不同的目标，把多个 QKV 的 attention 合在一起以后，它分了两个部分，一个用来学怎么 encode，一个用来学怎么 decode。这个就跟 GAN，对抗神经网络很像，基本就是还是为了找到数据高维空间下的一个空间分割平面。而这里为了前后关系，它把上一个运算出来的结果又给进去去计算下一个部分（decoder&nbsp;部分），所以就可以放在语言任务上（因为语言有前后关系）。</p>
<p>对于像翻译这种任务，encoder 部分输入是第一种语言，而 decoder 部分是第二种语言，这样一来训练出来的模型就可以做翻译。</p>
<p>此外, 如果我们处理的是序列数据, 希望保留序列信息, 我们可以添加<strong>Position Encoding</strong>. Position Encoding 有很多方式, 最常用的是 cos 添加角度信息.</p>
<p>GPT&nbsp;的全名是 Generative Pretrained Transformers，它用的是&nbsp;<strong>self-attention</strong>&nbsp;结构，也就是 encoder 部分是前半句（一个字，一个句子），decoder 部分是后半句（或者下一个字，或者下个句子，都行），就是自己学自己，不是两种不同语言，所以就是 self-attention。</p>
<p>当然这只是语言上的应用，像&nbsp;ChatGPT，LLama 这种的。</p>
<p><img alt="bAflYsDOjLtaX7M.png" src="https://cdn.sa.net/2024/04/12/bAflYsDOjLtaX7M.png" referrerpolicy="no-referrer"></p>
<p>head的概念类似卷积中的通道，只不过每个通道的输入都是一样的，类似于把一个通道的数据复制多次。</p>
<p>多头注意力的计算过程类似深度可分离卷积，把通道分开计算，再融合到一起. 使用多头的好处有:</p>
<ol>
<li><strong>捕获多种类型的信息</strong>：在自然语言处理任务中，一个单词的含义可能会受到其上下文中的多个其他单词的影响。通过多头注意力，模型可以学习到如何根据不同的上下文关注不同的单词。例如，对于句子"我喜欢吃苹果，因为它很甜。"，在处理"甜"这个词时，模型可能需要同时关注"吃"和"苹果"这两个词。通过多头注意力，模型可以在一个头中关注"吃"，在另一个头中关注"苹果"。</li>
<li><strong>在不同的表示子空间中学习</strong>：在多头注意力中，每个头都有自己的权重矩阵，这些矩阵会在训练过程中进行学习。这意味着每个头都可以在不同的表示子空间中捕获输入的不同特征。这使得模型能够捕获更丰富的信息，从而提高模型的表达能力。</li>
<li><strong>并行计算</strong>：多头注意力的设计也使得并行计算成为可能。每个头的计算都是独立的，因此可以在硬件允许的情况下并行进行，从而提高计算效率。</li>
</ol>
<p>因为 QKV 就是简单的“问题—答案”结构，这个可以应用在一切问题上，图片，语音，文字，或者相互关联相互变化，都可以。</p>
<p>所以现在如果我回过头来看的话，整个&nbsp;transformers里面的重点就是它把基本的单元从原来神经元之间的连接结构变成了问答的(QKV)attention 结构。</p>
<p>同时Transformer的复杂度不高.<br>
<img alt="mRNF9EhWkDz3noZ.png" src="https://cdn.sa.net/2024/04/12/mRNF9EhWkDz3noZ.png" referrerpolicy="no-referrer"></p></div></div></div><div><p><img src="https://cdn.sa.net/2024/04/05/7LtbMfFcA8Va2eY.png" referrerpolicy="no-referrer"><br>
<img src="https://picx.zhimg.com/v2-162b5bc0cff35cb5368e8ac58ee85845_r.jpg?source=1def8aca" referrerpolicy="no-referrer"></p></div><div><p><img alt="y54wRbdZvFECisc.png" src="https://cdn.sa.net/2024/04/10/y54wRbdZvFECisc.png" referrerpolicy="no-referrer"></p></div><div class="admonition-parent admonition-tip-parent"><div class="callout admonition admonition-tip admonition-plugin " style="--callout-color: 0, 191, 165;" data-callout="tip" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="fire" class="svg-inline--fa fa-fire fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M216 23.86c0-23.8-30.65-32.77-44.15-13.04C48 191.85 224 200 224 288c0 35.63-29.11 64.46-64.85 63.99-35.17-.45-63.15-29.77-63.15-64.94v-85.51c0-21.7-26.47-32.23-41.43-16.5C27.8 213.16 0 261.33 0 320c0 105.87 86.13 192 192 192s192-86.13 192-192c0-170.29-168-193-168-296.14z"></path></svg></div><div class="callout-title-inner admonition-title-content">Tip</div></div><div class="callout-content admonition-content"><p>历史</p>
<ul>
<li>注意力机制最早由Google团队在2014年<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1406.6247" rel="noopener" class="external-link" href="https://arxiv.org/abs/1406.6247" target="_blank"> Recurrent Models of Visual Attention</a>论文中提出, 使用RNN + 注意力机制对图片进行分类.</li>
<li><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1409.0473" rel="noopener" class="external-link" href="https://arxiv.org/abs/1409.0473" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a>  在同年, 将注意力机制引入NLP, 在机器翻译任务上将翻译和对其同时进行. 点燃了注意力机制在基于RNN/CNN的NLP任务的大放异彩. </li>
<li>2017年Google团队发表 Attention is all you need, 首次提出自注意力机制, 允许模型在序列中的不同位置之间建立动态关系</li>
<li>自2017年Transformer提出以后，关于Transformer模型结构的改进层出不穷，比如语音识别的Conformer、Branchformer、E-Branchformer、R-Transformer 等等</li>
<li>目前(2023), Transformer已经成为了自然语言处理领域绝对主流的网络架构</li>
<li></li>
</ul></div></div></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="faq" class="callout drop-shadow"><div class="callout-title"><div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-help-circle"><circle cx="12" cy="12" r="10"></circle><path d="M9.09 9a3 3 0 0 1 5.83 1c0 2-3 3-3 3"></path><path d="M12 17h.01"></path></svg></div><div class="callout-title-inner">Transformer vs. RNN</div></div><div class="callout-content">
<p><strong>相比于RNN, Transformer 的优势(非常明显)主要来自于</strong> </p>
<ul>
<li data-line="2">由于注意力机制将序列中所有位置的信息都一视同仁地看待（除了表征位置信息的位置编码以外）, 而带来的 <span style="background:#fff88f">全局建模能力</span></li>
<li data-line="3">由于自注意力机制中的点积运算更加适合 GPU训练和Transformer 上下参数之间没有依赖, 所导致的能够<span style="background:#fff88f">更好提高训练效率</span>和<span style="background:#fff88f">训练参数的scalability </span></li>
<li data-line="4">不存在RNN中的递归结构, 非常不容易出现训练时的梯度爆炸/消失.</li>
</ul>
</div></div></div><div class="admonition-parent admonition-faq-parent"><div class="callout admonition admonition-faq admonition-plugin " style="--callout-color: 100, 221, 23;" data-callout="faq" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="question-circle" class="svg-inline--fa fa-question-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zM262.655 90c-54.497 0-89.255 22.957-116.549 63.758-3.536 5.286-2.353 12.415 2.715 16.258l34.699 26.31c5.205 3.947 12.621 3.008 16.665-2.122 17.864-22.658 30.113-35.797 57.303-35.797 20.429 0 45.698 13.148 45.698 32.958 0 14.976-12.363 22.667-32.534 33.976C247.128 238.528 216 254.941 216 296v4c0 6.627 5.373 12 12 12h56c6.627 0 12-5.373 12-12v-1.333c0-28.462 83.186-29.647 83.186-106.667 0-58.002-60.165-102-116.531-102zM256 338c-25.365 0-46 20.635-46 46 0 25.364 20.635 46 46 46s46-20.636 46-46c0-25.365-20.635-46-46-46z"></path></svg></div><div class="callout-title-inner admonition-title-content">Faq</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/644491667/answer/3398148139" rel="noopener" class="external-link" href="https://www.zhihu.com/question/644491667/answer/3398148139" target="_blank">为什么transformer在图像的效果比CNN好？ - 知乎</a></p>
<p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/531529633" rel="noopener" class="external-link" href="https://www.zhihu.com/question/531529633" target="_blank">在CV界，传统卷积已经彻底输给Transformer了吗？ - 知乎</a></p>
<p>至少在CV领域, Transformers 的效果不一定有CNN好, 尤其是在数据量较少的时候.</p></div></div></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="note" class="callout drop-shadow"><div class="callout-title"><div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-pencil"><path d="M17 3a2.85 2.83 0 1 1 4 4L7.5 20.5 2 22l1.5-5.5Z"></path><path d="m15 5 4 4"></path></svg></div><div class="callout-title-inner"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/676892576" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/676892576" target="_blank">Transformer架构的局限已凸显，被取代还有多久？ - 知乎</a></div></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=oe59jBeOFL4" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=oe59jBeOFL4" target="_blank">iTransformer讲了一个什么改进模型的思路？ - YouTube</a></p>
<p><img src="https://cdn.sa.net/2024/04/15/6doERMsal4veXC2.png" referrerpolicy="no-referrer"><br>
<img src="https://cdn.sa.net/2024/04/15/qlOrbzJUfvus4Dw.png" referrerpolicy="no-referrer"></p>
<p>蚂蚁集团出的一个维度倒置Transformer, 用于时间信息的注意力机制. </p>
<ul>
<li>可以帮助扩大模型窗口</li>
<li>实现方法主要是改变数据处理的方法</li>
</ul></div></div></div><div><p><strong>"Attention is All You Need"</strong> 是一篇由Google Brain团队的研究员在2017年撰写的论文，这篇论文首次提出了 Transformer 模型, 使用自注意力机制(Self-attention将序列问题转换为全连接层问题, 使模型能够在不同位置之间建立动态的关系，避免了传统循环神经网络（RNN）和长短时记忆网络（LSTM）的局限(难以捕捉长序列之间关系, 容易梯度爆炸/消失, 序列结构难以并行, 模型参数随序列长度指数增长, 难以批处理).  强力推动了自然语言处理, 计算机视觉等多</p></div><div><p>这是 Transformer - model architecture, 其中有两个最关键关键组件. 多头注意力机制, 位置掩码.</p></div><div><p><img alt="image.png" src="https://cdn.sa.net/2024/04/14/Dw4yhfKm5lctjg1.png" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p>Embedding 输入</p>
<ul>
<li data-line="1">经过Tokenizer之后的RAW数据变为Embedding</li>
<li data-line="2">为了让模型批量处理不同长度的音频，我们将同一个批次中的输入填充 (padding) 到同样长度</li>
</ul>
</li>
<li data-line="3"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>位置掩码(Positional Encoding)</strong></p>
<ul>
<li data-line="4">由于Transfomer没有像RNN那样隐含顺序信息,  所以位置掩码被添加到Embedding, 允许模型学到每个Embedding的位置和相对位置(无序数据)信息. 在这之前, 一般需要将相对位置和绝对位置直接注入到模型</li>
<li data-line="5"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>位置掩码和Embedding的维度相同, 两者直接相加. 具体的掩盖设计有如下两种
<ul>
<li data-line="6">在模型输入位置添加一个可训练的层</li>
<li data-line="7">或者更加常用的是使用三角函数来编码位置信息<br>
<span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msup><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-script style="vertical-align: 0.53em;"><mjx-mfrac size="s"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msub size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.34em;"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-script></mjx-msup></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span><span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msup><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-script style="vertical-align: 0.53em;"><mjx-mfrac size="s"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msub size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.34em;"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-script></mjx-msup></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span>其中, pos是输入序列最长长度, i 是序列中位置, d_model是Embedding的维度</li>
</ul>
</li>
<li data-line="11"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>位置掩码扩容方法
<ul>
<li data-line="12">简单外推(Extrapolation): 提前预留维度并设为0. 但是可能导致外推维度的训练不充分, 导致外推位置启用后模型性能严重下降.  </li>
<li data-line="13">线性内推: 将数和输之间的区间变小(e.g. 1,2,3.... 1.5,2.2.5,3). 模型学习的特征不一样了, 需要微调让模型重新学习拥挤的映射关系.</li>
<li data-line="14">进制转换: 比如将10进制变为16进制, 表示数量范围变大. 数值范围的天花板变大, 模型一般有泛化能力. 或者将更低进制减少变化程度. </li>
</ul>
</li>
</ul>
</li>
<li data-line="15"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>自注意力机制(Self-attention)</strong> 是整个架构的基础 <span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munderover space="4"><mjx-over style="padding-bottom: 0.192em; padding-left: 0.279em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-over><mjx-box><mjx-munder><mjx-row><mjx-base><mjx-mo class="mjx-lop"><mjx-c class="mjx-c2211 TEX-S2"></mjx-c></mjx-mo></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.148em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-under></mjx-row></mjx-munder></mjx-box></mjx-munderover><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> <img alt="image.png" src="https://cdn.sa.net/2024/01/23/MdOrH9oNbQRCKZz.png" referrerpolicy="no-referrer"></p>
<ul>
<li data-line="17" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> Embedding 经过 Postion Encoding的修饰之后输入到自注意力机制当中</span></li>
<li data-line="18" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 输入经过三个不同的全连接层 <code>nn.Linear</code> 获得和Embedding维度相同的 <code>query</code>, <code>key</code>, <code>value</code> 三个矩阵. 
</span><ul>
<li data-line="19">使用三个不同全连接层是因为独立层能够使得QKV保持独立, 使得更它们更有效地捕捉不同类型的信息和关系</li>
<li data-line="20" class="lc-list-callout" data-callout="&amp;" style="--lc-callout-color: 255, 214, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">&amp;</span> QK必须相同维度, 因为要点积. V不一定</span></li>
<li data-line="21">&nbsp;注意力机制本身并没有对&nbsp;QKV 的内容做出任何限制我们可以这样理解. 我们希望用Q 把 V 中的东西找出来, 而 K 是 V 的钥匙. 如果 Q和K的匹配度越高, 那么就可以在 K 这个位置对应的 V 中找出更多Q要在V中查询的信息. </li>
<li data-line="22">比如，我们现在希望计算音频和文本之间的注意力，或者说希望从音频特征中提取和文本相关的信息，那么这个时候应该将文本特征作为&nbsp;Q&nbsp;，音频特征作为&nbsp;K&nbsp;和&nbsp;V&nbsp;（<strong>交叉注意力机制</strong>）；又比如，我们希望计算文本和文本自身的注意力，那么就应该将文本特征同时作为&nbsp;，QKV.&nbsp;（<strong>自注意力机制</strong>）</li>
</ul>
</li>
<li data-line="23" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 计算 <code>query</code> 和 <code>key</code> 之间的相似度(点积、拼接、感知机). 以下以点积为例.  <img alt="image.png" src="https://cdn.sa.net/2024/01/22/z5lTBPSoeMfd12j.png" referrerpolicy="no-referrer">
</span><ul>
<li data-line="24"><code>query</code> 先和 <code>key</code> 点积用于衡量每个词（查询）与句子中每个其他词（键）的相关性.</li>
<li data-line="25">然后，应用缩放因子对这些点积进行缩放。通常，这个缩放因子是键向量维度的平方根的倒数</li>
<li data-line="26">最后, 使用softmax函数将其归一化为概率. 这个softmax之后的概率表示该个Embedding在该位置的重要程度.</li>
<li data-line="27" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> 矩阵点积为什么可以计算相似度? 因为两矩阵方向相同, 点积较大, 方向越垂直, 点积越小. 在几何上等同于计算两个向量的长度乘积和它们之间夹角余弦的乘积. 同时考虑了矩阵的长度和方向. 同时在GPU运算更加高效</span></li>
<li data-line="28" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 自注意力机制本质上是求一个离散概率的数学期望. <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munderover space="4" limits="false"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2211 TEX-S1"></mjx-c></mjx-mo><mjx-script style="vertical-align: -0.285em; margin-left: 0px;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom><mjx-spacer style="margin-top: 0.291em;"></mjx-spacer><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-munderover><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.024em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span> ; 离散分布P是softmax之后的值, X 是 V. </span></li>
</ul>
</li>
<li data-line="29" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 最后, softmax后的值 和 <code>value</code> 相乘. 用于增强该位置Embedding重要性或者减少重要性</span></li>
<li data-line="30"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>🌰 : “I eat pizza today”
<ul>
<li data-line="31">每个Token(I, eat, pizza, today)先转换为 Embedding</li>
<li data-line="32">对于每个Token我们使用三个不同的全连接层来生成QKV. 例如，“I”会生成一个Q向量、一个K向量和一个V向量。</li>
<li data-line="33">接下来，我们计算每个词的Q向量与所有词的K向量的点积。这相当于在评估句子中的每个词与其他每个词之间的关系。例如，计算“I”的Q向量与“I”，“eat”，“pizza”，“today”的K向量的点积</li>
<li data-line="34">之后对点积应用缩放因子，并使用softmax函数进行归一化，得到注意力权重。</li>
<li data-line="35">每个Token的Softmax值的V向量进行加权求和。这个加权和代表了考虑到了整个句子上下文的当前词的表示。例如，对于“I”，我们将它的注意力权重与所有词的V向量相乘，并加总起来，得到一个新的加权向量，这个向量就是考虑了整个句子上下文的“I”的表示。</li>
</ul>
</li>
<li data-line="36" class="lc-list-callout" data-callout="$" style="--lc-callout-color: 0, 200, 83;"><span class="lc-li-wrapper"><span class="lc-list-marker">$</span> 根据具体任务的需要, 我们会添加掩码(Mask). 其作用是通过修改注意力权重, 以避免不不要或不需要的信息传递到模型中. 比如, 在在翻译任务中我们不希望Encoder访问到句子后面的内容, 我们就可以用Mask遮蔽避免泄漏. 或者在处理变长序列中补全长度(padding)</span></li>
</ul>
</li>
<li data-line="37"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>多头注意力机制(Multi-head Attention)</strong><img alt="image.png" src="https://cdn.sa.net/2024/01/22/9teJRY7VQnXATIE.png" referrerpolicy="no-referrer"></p>
<ul>
<li data-line="38">多头注意力机制允许模型在不同子空间中学习到相关信息, 以捕捉数据的不同方面. 提高并行度并简化模型的计算复杂度. </li>
<li data-line="39"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>将 QKV 分别用h个不同投影矩阵投影h次, 然后分别做点积注意力, 最后将每个头计算出的加权和简单拼接回一个完整的向量后通过一个全连接层产生最终的输出
<ul>
<li data-line="40">相当于人从不同的视角观察一个物体</li>
</ul>
</li>
<li data-line="41" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 《Are sixteen heads really better than one?》多头自注意力机制不一定比单头好</span></li>
<li data-line="42" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 《Low-Rank Bottleneck in Multi-head Attention Models》指出Multi-Head Attention的表达能力瓶颈，并提出增大key_size来缓解。
</span><ul>
<li data-line="43">我们将 QK维度称为 key_size, V的维度为 head_size</li>
<li data-line="44">h 是 注意力头的数量. 一般实际是将原始d维度的QKV投影到 d/h)维中, 单独计算后输出 d/h 的结果. 然后将 h*d/h 拼接起来获得最终注意力值</li>
<li data-line="45"><span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msqrt size="s"><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.082em;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span> 可以看作一个二元联合分布. 假设序列长度为 n. 因为 QK维度一样所以有这个分布有 n*n = <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container></span> 的值</li>
<li data-line="46">但是将QK投影之后, 各自分布只有 n*(d/h) 的. 总的参数量为 2nd/h &lt;&lt; n^2. 参数量的减少导致表达能力的削弱. 这就是Low-rank bottleneck, 尤其是 h 较多时.这个叫低秩瓶颈. </li>
<li data-line="47">我们可以增加 d 的维度, 但是这会增加模型的复杂性. 或者减少 h, 但是多头本身就可以增加模型的表达能力</li>
<li data-line="48">我们可以通过只增加 key_size 以增加模型的表达能力, 而尽可能不增加模型的复杂性</li>
</ul>
</li>
<li data-line="49" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 《Talking-head attention》, 将多头的低秩分布叠加(在softmax之前叠加, e.g.加权平均)增强模型的表达能力. 原理是多个高斯分布(GMM)叠加的数量够多, 就可以逼近任意概率分布. </span></li>
</ul>
</li>
<li data-line="50" class="lc-list-callout" data-callout="~" style="--lc-callout-color: 124, 77, 255;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">~</span> 爱因斯坦求和约定（Einstein Summation Convention）<code>torch.ensum</code> 是一种表达多重求和（比如矩阵乘法、点积、批量矩阵乘法等）和某些类型的数据重排操作的简介写法. 比如</p>
</span><ul>
<li data-line="51"><strong>矩阵乘法：</strong> 例如，矩阵乘法 <code>AB</code> 可以表示为 <code>torch.einsum('ij,jk-&gt;ik', [A, B])</code>，其中 <code>A</code> 是一个形状为 <code>(i, j)</code> 的矩阵，<code>B</code> 是一个形状为 <code>(j, k)</code> 的矩阵。</li>
<li data-line="52"><strong>批量矩阵乘法：</strong> 对于批量矩阵乘法，比如有批次的两个矩阵 <code>A</code> 和 <code>B</code>，其操作可以表示为 <code>torch.einsum('bij,bjk-&gt;bik', [A, B])</code>。</li>
<li data-line="53"><strong>求和操作：</strong> 如果你想对一个矩阵的行进行求和，可以使用 <code>torch.einsum('ij-&gt;i', A)</code>。</li>
</ul>
</li>
<li data-line="54" class="lc-list-callout" data-callout="$" style="--lc-callout-color: 0, 200, 83;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">$</span> 这里用序列重新梳理一下Attention的公式<img alt="gpt-transformer.png" src="https://cdn.sa.net/2024/01/24/J7fCRGVWFL5EabS.png" referrerpolicy="no-referrer"></p>
</span><ul>
<li data-line="55"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>令 <code>F[t]</code> 为 t时刻的系统状态(高维状态); 令 <code>x[t]</code> 为 t 时刻的外部输入信息状态; 
<ul>
<li data-line="56">预测 <code>F[t+1]</code> 时，需考虑 <code>F[0]</code>,<code>F[1]</code>, .. <code>F[t]</code>。因此，生成长度 T 的序列，需 <code>O(T^2)</code> 复杂度。</li>
</ul>
</li>
<li data-line="57"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>那么Attention可以简化为 <img alt="image.png" src="https://cdn.sa.net/2024/01/25/kvG1ogqlYeRINpH.png" referrerpolicy="no-referrer">
<ul>
<li data-line="58">每个状态 &nbsp;i 对于后续的潜在贡献是&nbsp;<strong>V</strong>F[i] </li>
<li data-line="59">用&nbsp;<strong>Q</strong>x[t] 矢量，与此前的所有&nbsp;<strong>K</strong>F[i] 矢量分别做点乘，再 exp，得到 x[t] 与之前各个 F[i] 状态的匹配度。</li>
<li data-line="60">如果匹配度&nbsp;exp⁡(Q x[t]∗ K F[i])&nbsp;越大，<strong>V</strong>F[i] 的权重越大</li>
<li data-line="61">分母为归一化因子。</li>
<li data-line="62" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 公式中没有显式出现 t 与 i 的距离信息。我们会采用其它方式（例如位置编码）将其注入系统。</span></li>
</ul>
</li>
</ul>
</li>
</ul></div><div><p>我们这里实现一个带Mask和多头的 <code>SelfAttention</code> 的 pytorch示例代码:</p></div><div><p>编码器-解码器相关论文起源于 <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1409.3215" rel="noopener" class="external-link" href="https://arxiv.org/abs/1409.3215" target="_blank">Sequence to Sequence Learning with Neural Networks</a><br>
Transformer 完全基于注意力机制, 拥有编码器(Encoder)和解码器(Decoder)两个部分, 两者时常搭配使用, 但用途和模型结构相互独立, 也没有规定网络架构. 你可以编码器用Transformer解码器用LSTM...<br>
<img src="https://pic3.zhimg.com/v2-c7d7c326d453fa1d143c35abc543cb3a_r.jpg" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0"><strong>编码器(Encoder)</strong> 负责特征编码，即从原始的、比较底层的输入序列信号提取出抽象的、具有明显语义特征的特征信息. 由图可知是由  特征编码、位置编码、和若干个 TransformerBlock 组成(注意力+MLP+*归一化+*残差)</li>
<li data-line="1"><strong>解码器(Decoder)</strong> 负责从编码器得到的原始序列的特征信息中"破译"出目标序列的内容（比如从音频序列的特征中"破译"其对应的文本信息). </li>
<li data-line="2" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 这种架构提出的原因是因为, 类似RNN和LSTM这样的模型输入-输出是对应的. 但在有的任务中我们不希望输入-输出的长度相同. </span></li>
<li data-line="3"><strong>信息流</strong>：输入序列首先进入编码器，经过一系列变换后，转换为一组高维表示。这些表示然后被传递到解码器，解码器利用这些信息以及自身的前一步输出来逐步生成最终的输出序列。</li>
</ul></div><div><p>我们在这里结合之前的<code>SelfAttention</code>代码, 构建一个 <code>TransformerBlock</code> , 后搭建Encoder和Decoder. </p></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="faq" class="callout drop-shadow"><div class="callout-title"><div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-help-circle"><circle cx="12" cy="12" r="10"></circle><path d="M9.09 9a3 3 0 0 1 5.83 1c0 2-3 3-3 3"></path><path d="M12 17h.01"></path></svg></div><div class="callout-title-inner">为什么GPT要使用Decoder-only架构 而非 BERT的Encoder-only 架构?</div></div><div class="callout-content">
<p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/588325646/answer/2929224109" rel="noopener" class="external-link" href="https://www.zhihu.com/question/588325646/answer/2929224109" target="_blank">为什么现在的LLM都是Decoder only的架构？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/608929992/answer/3119081297" rel="noopener" class="external-link" href="https://www.zhihu.com/question/608929992/answer/3119081297" target="_blank">decoder-only和encoder-decoder transformer在应用时最大的区别是？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/642923989" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/642923989" target="_blank">LLM的3种架构：Encoder-only、Decoder-only、encode-decode - 知乎</a></p>
<ul>
<li data-line="4"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Encoder-Decoder架构 
<ul>
<li data-line="5"><strong>适用任务</strong>: 这种架构非常适合于那些需要理解输入和生成输出的任务，如机器翻译、文本摘要等。在这些任务中，模型需要首先理解输入文本（通过Encoder部分），然后基于这个理解生成新的文本（通过Decoder部分）。</li>
<li data-line="6"><strong>代表性模型</strong>: 最著名的例子是原始的Transformer模型，它首次在论文《Attention is All You Need》中被提出。这个模型在机器翻译任务中取得了显著的成效。</li>
</ul>
</li>
<li data-line="7"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Encoder-only架构
<ul>
<li data-line="8"><strong>适用任务</strong>: 这种架构适用于只需理解输入文本的任务，比如文本分类、情感分析、命名实体识别等。在这些任务中，模型的目标是分析和理解输入，而不需要生成任何新的文本。</li>
<li data-line="9"><strong>代表性模型</strong>: BERT（Bidirectional Encoder Representations from Transformers）是这一类架构中最著名的例子。通过预训练一个大型的语料库，BERT学会了理解语言的上下文，可以被用于各种不同的自然语言处理任务</li>
</ul>
</li>
<li data-line="10"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Decoder-only架构
<ul>
<li data-line="11"><strong>适用任务</strong>: Decoder-only架构特别适合于文本生成任务，如语言模型训练、文本生成、代码生成等。这种架构通常会被训练来预测接下来的单词或者序列。</li>
<li data-line="12"><strong>代表性模型</strong>: GPT（Generative Pre-trained Transformer）系列是这一类架构中最知名的例子。GPT通过大量的预训练，能够生成连贯、相关的文本，可以用于各种生成型任务。</li>
</ul>
</li>
</ul>
</div></div></div><div><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=bQ5BoolX9Ag&amp;t=13s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=bQ5BoolX9Ag&amp;t=13s" target="_blank">StatQuest - Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly Explained!!!</a></p></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="check" class="callout drop-shadow"><div class="callout-title"><div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-check"><path d="M20 6 9 17l-5-5"></path></svg></div><div class="callout-title-inner">加速Transformer的有关尝试, 可以分为两个方向</div></div><div class="callout-content">
<ul>
<li data-line="1">降低Attention运算的时间复杂度从 O(N^2) 到 O(N), 比如 Linear Transformer 和 Linformer, 一般是用近似技巧, 但是性能会有所下降</li>
<li data-line="2">在不改变attention理论时间复杂度的前提下，尽可能加速attention的运算, 比如 Flash Attention. 原理基本上是减少了GPU的SRAM (<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Static_random-access_memory" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Static_random-access_memory" target="_blank">Static Random Access Memory</a>) 和HBM (<a data-tooltip-position="top" aria-label="https://semiwiki.com/wikis/semiconductor-ip-wikis/high-bandwidth-memory-hbm-wiki/#:~:text=High%20Bandwidth%20Memory%20%28HBM%29%20is%20a%20high-performance%20RAM,HBM%20were%20the%20AMD%20Fiji%20GPUs%20in%202015" rel="noopener" class="external-link" href="https://semiwiki.com/wikis/semiconductor-ip-wikis/high-bandwidth-memory-hbm-wiki/#:~:text=High%20Bandwidth%20Memory%20%28HBM%29%20is%20a%20high-performance%20RAM,HBM%20were%20the%20AMD%20Fiji%20GPUs%20in%202015" target="_blank">High Bandwidth Memory</a>)之间的通信开销</li>
</ul>
</div></div></div><div><p>一个很好的大模型可视化项目<br>
<a data-tooltip-position="top" aria-label="https://bbycroft.net/llm" rel="noopener" class="external-link" href="https://bbycroft.net/llm" target="_blank">LLM Visualization</a>#### Vision Transformer(ViT)</p></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="check" class="callout drop-shadow"><div class="callout-title"><div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-check"><path d="M20 6 9 17l-5-5"></path></svg></div><div class="callout-title-inner">Check</div></div><div class="callout-content">
<p><img src="https://cdn.sa.net/2024/04/05/25mqLdI7Fth491z.png" referrerpolicy="no-referrer"><br>
<a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV13N4y1J7BD/?spm_id_from=333.1007.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV13N4y1J7BD/?spm_id_from=333.1007.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">论文研读之简化版Transformer：Simplifying Transformer Blocks_哔哩哔哩_bilibili</a><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/677511164" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/677511164" target="_blank">【论文详解】简化版Transformer：Simplifying Transformer Blocks - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/451568838" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/451568838" target="_blank"># Vison Transformer学习</a></p>
</div></div></div><div class="heading-wrapper"><h3 data-heading="ViT" class="heading" id="ViT"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>ViT</h3><div class="heading-children"><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=yRPVQKJaStw" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=yRPVQKJaStw" target="_blank">视觉Transformer是什么？ViT为什么是现在使用最广泛的模型？以及LLM的前世今生！ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2010.11929" rel="noopener" class="external-link" href="https://arxiv.org/abs/2010.11929" target="_blank">[2010.11929] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p>
<p>介绍了ViT , 一个现今最流行的模型. GPT, CLIP等著名项目都适用的模型Backbone. </p>
<p>ViT 把 CNN模块全部丢弃, 发现在图像分类任务上准确率和效率上都很好. </p>
<p>原因是:</p>
<ol>
<li>Self-Attention的全注意力机制更加容易在GPU上并行计算</li>
<li>使用固定大小的Patch替代CNN卷积, 复杂度更低</li>
<li>没有卷积层的ViT参数更少</li>
</ol>
<p>讲了</p>
<ul>
<li>Transformer 具有局部感受域和CNN的kernel看一个Size不一样</li>
</ul>
<p><img src="https://cdn.sa.net/2024/04/10/64YDWzTCkNELalb.png" referrerpolicy="no-referrer"></p>
<p>ViT 处理图像的方法</p>
<ul>
<li>Position Encoding 是 1D, 2D 效果都差不多</li>
<li>QKV 维度都是 197 * 768</li>
</ul>
<p>ViT 提取到了更加抽象的特征相比AlexNet<br>
ViT 训练调参敏感</p></div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=PCKnpM2g4r4" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=PCKnpM2g4r4" target="_blank">用MLP做Transformer？异想天开！其实注意力机制很简单！ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2105.01601" rel="noopener" class="external-link" href="https://arxiv.org/abs/2105.01601" target="_blank">[2105.01601] MLP-Mixer: An all-MLP Architecture for Vision - 2021</a></p>
<p>Google做的ViT的后续扩展. 使用纯MLP替代Transformer中Attention机制. </p>
<p>这篇文章介绍了一个全新的视觉模型架构一一MLP-Mixer,它完全基于多层感知机(MLPs)。与<br>
传统依赖于卷积和自注意力的模型不同，MLP-Mixr通过两种类型的MLP层来处理图像，一种在图<br>
像块(patches)上独立应用（即特征混合），另一种跨图像块应用（即空间信息混合）。通过在<br>
大规模数据集上训练或使用现代正则化方案，MLP-Mⅸr能够在图像分类基准测试中达到与最先进<br>
模型相当的性能，同时保持与它们相近的预训练和推断成本。作者希望这一结果能激发进一步的研<br>
究，超越目前基于卷积神经网络(CNNs)和Transformers的范畴。</p>
<p>MLP-Mixer 的主要优点在于其架构的简洁性和对大规模数据集的良好适应性。相比传统的CNN和基<br>
于注意力的模型，MLP-Mixer 不依赖于卷积操作或自注意力机制，而是仅通过基本的矩阵乘法、数<br>
据重排（如重塑和转置）、以及非线性激活函数实现。这种设计简化了模型的结构，同时在一些情<br>
况下，如在非常大的数据集上训练时，能够达到与最先进模型相似甚至更好的性能。此外，由于其<br>
简单的矩阵运算基础，MLP-Mxr在某些应用场景下可能更易于优化和扩展。</p>
<p><img alt="YaMf5AvRg6EWPXi.png" src="https://cdn.sa.net/2024/04/11/YaMf5AvRg6EWPXi.png" referrerpolicy="no-referrer"></p>
<p>在论文中，图1（Figure 1）展示了MLP-Mixer架构的宏观结构。这个结构接受一系列线性投影的图像块（也称为tokens）作为输入，这些图像块被组织成一个“patches × channels”的表格。MLP-Mixer的输入维度保持不变，即输入的图像块数量（S）和通道数（C）。</p>
<p>图1中的关键组件包括：</p>
<ol>
<li>
<p><strong>Patches</strong>: 输入图像被分割成不重叠的小块，每个小块都被线性投影到一个期望的隐藏维度C。如果原始输入图像的分辨率为(H, W)，每个图像块的分辨率为(P, P)，那么图像块的数量S = HW/P^2。</p>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>Mixer Layer</strong>: 这是MLP-Mixer的核心组件，包含两种类型的MLP层：token-mixing MLP和channel-mixing MLP。这两种MLP层交替出现，以实现输入的两个维度（空间位置和通道）之间的交互。</p>
<ul>
<li><strong>Token-mixing MLP</strong>: 这种类型的MLP作用于输入表格的列（即它应用于转置后的输入表格X^T），并独立地处理每个通道，从而允许不同空间位置（tokens）之间的信息交流。<br>
</li>
<li><strong>Channel-mixing MLP</strong>: 这种类型的MLP作用于输入表格的行，允许不同通道之间的信息交流。<br>
</li>
</ul>
</li>
<li>
<p><strong>Fully-connected</strong>: 这是MLP层的基本组成部分，每个MLP层包含两个全连接层（fully-connected layers）和一个GELU非线性激活函数。</p>
</li>
<li>
<p><strong>Layer Norm</strong>: 在每个MLP层之后，使用层归一化（Layer Normalization）来稳定训练过程。</p>
</li>
<li>
<p><strong>Skip-connections</strong>: 这些是现代深度学习架构中常用的组件，有助于信息在网络中流动，并减少梯度消失的问题。</p>
</li>
<li>
<p><strong>Global Average Pooling</strong>: 在所有MLP层之后，使用全局平均池化（Global Average Pooling）来聚合空间信息，为分类头（classifier head）做准备。</p>
</li>
<li>
<p><strong>Per-patch Fully-connected</strong>: 每个图像块都有一个全连接层，用于最终的分类任务。</p>
</li>
<li>
<p><strong>Class Layer</strong>: 最后，使用一个线性分类器（通常是一个全连接层）来输出最终的分类结果。</p>
</li>
</ol>
<p>图1还展示了MLP-Mixer中的跳跃连接（skip-connections）和层归一化（layer norm on the channels），这些都是帮助模型训练和泛化的重要组件。</p>
<p><strong>添加了token-mixing MLP</strong> </p>
<p><strong>添加了channel-mixing MLP</strong></p>
<p>自己看图, 有 layer features 的可视化</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="R-Transformer" class="heading" id="R-Transformer"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>R-Transformer</h3><div class="heading-children"><div><ul>
<li data-line="0">针对高维嵌入导致位置编码失效的解决方案</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="Mamba" class="heading" id="Mamba"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Mamba</h3><div class="heading-children"><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2312.00752" rel="noopener" class="external-link" href="https://arxiv.org/abs/2312.00752" target="_blank">[2023.12] Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a><br>
<a data-tooltip-position="top" aria-label="https://36kr.com/p/2547379574166664" rel="noopener" class="external-link" href="https://36kr.com/p/2547379574166664" target="_blank">颠覆Transformer霸权，CMU普林斯顿推Mamba新架构，解决致命bug推理速度暴增5倍-36氪</a></p>
<ul>
<li>Mamba 为了解决Transformer核心注意力层上下文长度无法很好scale计算资源的局限(在大模型基础热门的背景). 为此提出了结构化空间状态模型(SSM, structured state space models). 实验显示Mamba具有参数线性扩展性(可最高百万Token级别), 和五倍的推理速度. 在多项任务中都达到SOTA. 比如Mamba-3B模型表現和两倍参数量的Transformer模型相当. 向我们展示了一个非常有希望的 Transformer 替代. </li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>研究背景
<ul>
<li>在深度学习领域，Transformer架构及其核心的注意力机制已经成为处理各种序列数据的强大工具。然而，Transformer在处理长序列数据时存在计算效率低下的问题，这限制了其在某些应用场景中的使用。为了解决这个问题，研究者们提出了多种子二次时间复杂度的架构，如线性注意力、门控卷积和循环模型等，但这些模型在处理语言等重要模态数据时表现并不如注意力机制。</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>过去的方案及问题
<ul>
<li>尽管这些子二次时间复杂度的模型在计算效率上有所提升，但它们在处理离散模态数据（如文本）时，往往无法有效地进行基于内容的推理。这些模型的一个关键弱点是它们无法根据当前输入有选择地传播或遗忘信息。此外，尽管这些模型试图通过高效的卷积操作来实现，但这种变化阻止了它们使用高效的并行计算，这在硬件层面上带来了挑战。</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>本文方案及具体步骤： 
<ul>
<li>本文提出了一种新的选择性状态空间模型（Selective State Space Models，简称SSMs），它通过以下几个关键步骤来提高模型的性能：</li>
<li><strong>选择机制</strong>：通过将SSM参数设置为输入的函数，模型能够根据当前的输入选择性地传播或遗忘信息，从而在序列长度维度上动态地处理信息。在实现 Transformer 质量的性能，同时线性缩放序列长度</li>
<li><strong>硬件感知算法</strong>：为了解决选择性SSMs在计算上的挑战，研究者设计了一种硬件感知的并行算法，该算法以递归模式运行模型，并通过扫描操作而不是卷积来计算，同时避免了在GPU内存层次结构之间进行不必要的IO访问。</li>
<li><strong>简化的架构设计</strong>：将SSMs与Transformer的MLP块结合，形成了一个简化的、同质的架构（Mamba），这个架构在不使用注意力或MLP块的情况下，通过选择性状态空间实现了快速推理和线性序列长度扩展。</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>贡献
<ul>
<li>在多种模态（语言、音频和基因组学）上都取得SOTA性能，成为跨模态通用序列模型主干的有力候选者。</li>
</ul>
</li>
</ul></div></div></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="VAE(变分自编码器)" class="heading" id="VAE(变分自编码器)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>VAE(变分自编码器)</h2><div class="heading-children"><div><blockquote>
<p>生成模型<br>
<img alt="sXVMmcQZrtYpqn6.png" src="https://cdn.sa.net/2024/04/16/sXVMmcQZrtYpqn6.png" referrerpolicy="no-referrer"></p>
</blockquote></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p>Autoencoder + 概率模型</p>
<p>概率图 + 变分贝叶斯</p></div></div></div><div><p>AutoEncoder</p></div></div></div><div class="heading-wrapper"><h2 data-heading="GAN" class="heading" id="GAN"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>GAN</h2><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><img src="https://cdn.sa.net/2024/04/16/spOkPS5qRoUnYa7.png" referrerpolicy="no-referrer"></p>
<ul>
<li><strong>原理</strong>: 基于博弈论**</li>
<li><strong>优点</strong></li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>缺点</strong>
<ul>
<li>高分辩率的图片生成困难</li>
<li>难以训练</li>
<li>容易被恶意攻击</li>
<li>很难和其他 Control 方法结合</li>
</ul>
</li>
</ul>
<p>Generator 和 Discriminator. </p></div></div></div><div><p>GAN (生成对抗网络)、CGAN、DCGAN、WGAN (Wasserstein GAN)、StyleGAN、CycleGAN</p></div><div><p>RealESRGAN<br>
ESRGAN<br>
GFPGAN</p></div></div></div><div class="heading-wrapper"><h2 data-heading="Stable Diffusion" class="heading" id="Stable_Diffusion"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Stable Diffusion</h2><div class="heading-children"><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p>这是原来的Diffusion model, 原来效果很差. 我们现在加一个CLIP控制模型的生成.然后将这个 加噪去噪的过程重复几次, 效果就发现很不错. 降噪的backbone 是 U-Net. 我们叫这个新的模型是Stable Diffusion.</p>
<p>后面有更多的更多控制生成的控制机制, 比如ControlNet.</p></div></div></div><div><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/634573765" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/634573765" target="_blank">stable diffusion原理解读通俗易懂，史诗级万字爆肝长文，喂到你嘴里 - 知乎</a><br>
<img src="https://pic1.zhimg.com/v2-34de1acf7e698b2ef5e37f9562ed15dc_r.jpg" referrerpolicy="no-referrer"></p></div><div><p><img src="https://pic1.zhimg.com/v2-9827eeb683a871ca5f08e64df6411294_r.jpg" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0">Stable Diffusion的原理可以抽象看作forward add noise 和 backward denoising 两部分</li>
<li data-line="1">VAE 首先将猫猫图片, 比如 512*512*3, 从像素空间转换到 latent space, 大小可能是4x64x64. 完成对输入图像的压缩. </li>
<li data-line="2"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>forward diffusion
<ul>
<li data-line="3"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>对 latent space中的猫猫照片(<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container></span>)一步一步的添加高斯噪声, 直到噪声布满整个猫猫(<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span>)
<ul>
<li data-line="4">由于添加的噪声是高斯噪声, 所以不用顺序的加噪. 可以直接累加.</li>
</ul>
</li>
</ul>
</li>
<li data-line="5"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>backward denoising (已知 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span>  -&gt; <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container></span>)
<ul>
<li data-line="6">一个使用ResNet作为主干的U-Net 进行 backward 降噪. </li>
<li data-line="7">输入: Text Embedding + 噪声图片</li>
<li data-line="8">输出: 一个去噪声潜特征空间表达</li>
<li data-line="9">因为满足高斯分布, 可以使用贝叶斯公式连立公式反转马尔可夫链.  只有噪声是未知的. </li>
<li data-line="10">网络输入为 当前时刻的分布<strong>Xt</strong>和时刻<strong>t</strong>，还有之前的<strong>文本向量</strong>. 输出预测噪声</li>
</ul>
</li>
</ul></div><div><p><img src="https://pic1.zhimg.com/v2-8f0baddac9b7893b4cc35cef7f606f58_r.jpg" referrerpolicy="no-referrer"></p></div><div><p>上面的Algorithm 1是训练过程，</p></div><div><p>其中第二步表示取数据，一般来说都是一类猫，狗什么的，或者一类风格的图片，不能乱七八糟什么图片都来，那模型学不了。</p></div><div><p>第三步是说每个图片随机赋予一个时刻的噪声（上面说过），</p></div><div><p>第四步，噪声符合高斯分布，</p></div><div><p>第五步，真实的噪声和预测的噪声算损失（DDPM输入没有文本向量，所有没有写，你就理解为多加了一个输入），更新参数。直到训练的输出的噪声和真实噪声相差很小，Unet模型训练完毕</p></div><div><p>下面我们来到Algorithm2采样(马尔可夫)过程</p></div><div><ol>
<li data-line="0">不就是说Xt符合高斯分布嘛</li>
<li data-line="1">执行T次，依次求Xt-1到X0，不是T个时刻嘛(两步之间可以预测)</li>
<li data-line="2">Xt-1不就是我们逆向扩散推出的公式，Xt-1=μ+σZ，均值和方差都是已知的，唯一的未知噪声Z被Unet模型预测出来，<strong>εθ</strong>这个是指已经训练好的UNet</li>
</ol></div><div><blockquote>
<p>Prompt -&gt; CLIP -&gt; Text Embedding -&gt; 借由 Cross-Attention 暴露给去噪U- Net</p>
</blockquote></div><div><pre><code>- 其他语义也可以借此串联的进行Control. e.g. 语义图、 图像、 Inpaint
</code><button class="copy-code-button">Copy</button></pre></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=CSDgyma9RUo" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=CSDgyma9RUo" target="_blank">【博士Vlog】Diffusion生成式模型新思路，用时间序列来降噪 - YouTube</a></p>
<ul>
<li>介绍了使用Diffusion生成时间序列的论文</li>
<li>把UNet改成RNN</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=Uav0zrV6A9U" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=Uav0zrV6A9U" target="_blank">【博士详解】Diffusion和GAN是怎么回事？各自有什么优缺点？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2105.05233" rel="noopener" class="external-link" href="https://arxiv.org/abs/2105.05233" target="_blank">[2105.05233] Diffusion Models Beat GANs on Image Synthesis</a></p>
<ul>
<li>介绍了这篇论文, </li>
</ul>
<p>Diffusion 和 GAN 各自有什么优缺点?</p>
<p> Diffusion模型和GAN（生成对抗网络）都是深度学习中广泛使用的生成模型，它们各自有着不同的优缺点。</p>
<p>Diffusion模型：<br>
优点：</p>
<ol>
<li>理论依据强：Diffusion模型基于物理学中的扩散过程，有着坚实的理论依据。</li>
<li>可解释性强：相比于GAN，Diffusion模型的生成过程更容易解释和理解。</li>
<li>不依赖于对抗训练：Diffusion模型不需要像GAN那样进行对抗训练，训练过程更稳定。</li>
<li>Backward denoising 过程可以借由 cross-attention 机制, 增加各种形式的control</li>
</ol>
<p>缺点：</p>
<ol>
<li>生成速度慢：由于需要进行多步的扩散过程，Diffusion模型生成样本的速度相比于GAN要慢。</li>
<li>需要更多的计算资源：Diffusion模型通常需要更多的计算资源和存储空间。</li>
</ol>
<p>GAN：<br>
优点：</p>
<ol>
<li>生成质量高：GAN通常能够生成高质量、高分辨率的样本。</li>
<li>生成速度快：一旦训练完成，GAN生成样本的速度非常快。</li>
</ol>
<p>缺点：</p>
<ol>
<li>训练不稳定：GAN的训练过程常常不稳定，可能会遇到梯度消失、模式崩溃等问题。</li>
<li>对抗训练难以优化：GAN的对抗训练过程是一个最小化最大化问题，这使得优化过程非常困难。(熊猫_img + 线虫_img = 丹顶鹤_img ????) -&gt; 容易被攻击</li>
<li>可解释性差：GAN的生成过程是一个黑盒过程，可解释性较差。</li>
<li></li>
</ol></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="深度学习的著名应用" class="heading" id="深度学习的著名应用"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>深度学习的著名应用</h2><div class="heading-children"><div class="heading-wrapper"><h3 data-heading="GPT &amp; BERT" class="heading" id="GPT_&amp;_BERT"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>GPT &amp; BERT</h3><div class="heading-children"><div><p>GPT 论文<br>
<strong>"Improving Language Understanding by Generative Pre-Training" by Alec Radford et al. (2018)</strong></p></div><div><p><strong>"Language Models are Unsupervised Multitask Learners" by Alec Radford et al. (2019)</strong></p></div><div><p><strong>"Language Models are Few-Shot Learners" by Tom B. Brown et al. (2020)</strong></p></div><div><p>BERT:<br>
Pre-training of Deep Bidirectional Transformers for Language Understanding" by Jacob Devlin et al. (2018)</p></div><div class="admonition-parent admonition-quote-parent"><div class="callout admonition admonition-quote admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="quote" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Quote</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2403.07183" rel="noopener" class="external-link" href="https://arxiv.org/abs/2403.07183" target="_blank">[2403.07183] Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews</a></p>
<p>Standford出品。发现在各大会议的同行评审中由解决15%的内容由LLM生成。</p>
<p>判断文本的来源的方法是创建一个最大似然MLE的统计学方法。（判断AI味有多浓）</p>
<p>捕捉到了同行评审同质化的倾向(用词倾向多样化降低)。解释了滥用ChatGPT的乱象。</p>
<p>在拒稿的论文中，发现不太回应作者反驳的评审中，使用LLM的比例较高。</p>
<p>截稿三天之前，AI味明显增加。</p>
<p>AI润色和直接写有区别。发现很多review是直接生成的。</p>
<p><img src="https://cdn.sa.net/2024/04/03/DVaiTnpkxsAt7BC.png" referrerpolicy="no-referrer"></p></div></div></div><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=t6qBKPubEEo" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=t6qBKPubEEo" target="_blank">什么是大语言模型LLM？ChatGPT、LLAMA各自有什么优势？ - YouTube</a></p>
<p><img src="https://pbs.twimg.com/media/Fuw9fv9akAA_h0q.jpg" referrerpolicy="no-referrer"><br>
<a data-tooltip-position="top" aria-label="https://twitter.com/ylecun/status/1651762787373428736?lang=en" rel="noopener" class="external-link" href="https://twitter.com/ylecun/status/1651762787373428736?lang=en" target="_blank">Yann LeCun on X: "A survey of LLMs with a practical guide and evolutionary tree. Number of LLMs from Meta = 7 Number of open source LLMs from Meta = 7 The architecture nomenclature for LLMs is somewhat confusing and unfortunate. What's called "encoder only" actually has an encoder and a decoder… https://t.co/OpcoinvfDX" / X</a></p>
<p><a data-tooltip-position="top" aria-label="https://github.com/Mooler0410/LLMsPracticalGuide" rel="noopener" class="external-link" href="https://github.com/Mooler0410/LLMsPracticalGuide" target="_blank">GitHub - Mooler0410/LLMsPracticalGuide: A curated list of practical guide resources of LLMs (LLMs Tree, Examples, Papers)</a></p>
<p>2018<br>
Word2Vect 时代</p>
<p>2019<br>
LSTM 时代</p>
<p>2020<br>
Transformer 时代</p>
<ul>
<li>encoder-decoder : BERT -&gt; 理解</li>
<li>decoder : GPT -&gt; 生成</li>
</ul>
<p>2020之后</p>
<p>decoder 效果太好了, 各种下游应用全部爆发了</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="AlphaFold" class="heading" id="AlphaFold"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>AlphaFold</h3><div class="heading-children"><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=LEmNu0UJaXc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=LEmNu0UJaXc" target="_blank">Nature上最重要的论文AlphaFold讲了什么？作为一个打工人，咱们需要会些什么才能进公司？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://pubmed.ncbi.nlm.nih.gov/34265844/" rel="noopener" class="external-link" href="https://pubmed.ncbi.nlm.nih.gov/34265844/" target="_blank">Highly accurate protein structure prediction with AlphaFold(2021) - PubMed</a></p>
<p><img src="https://cdn.sa.net/2024/04/11/rGUN9nmDtq1alKe.png" referrerpolicy="no-referrer"></p>
<p><img src="https://cdn.sa.net/2024/04/11/AiKFTORmXEzcL7n.png" referrerpolicy="no-referrer"></p>
<p><strong>AlphaFold v2</strong> 是一种由DeepMind开发的人工智能算法，它的主要目标是解决蛋白质折叠预测的问题。蛋白质折叠预测是一个非常重要的生物学问题，因为一个蛋白质的形状（即它如何折叠）决定了它在生物体内的功能。如果我们能够准确预测蛋白质的结构，那么我们就能更好地理解疾病的发生机制，甚至设计出新的药物。</p>
<p>AlphaFold的输入是蛋白质的氨基酸序列，这就像是蛋白质的"配方"。蛋白质是由20种不同的氨基酸以特定的顺序链接起来形成的。AlphaFold从这个序列中预测出蛋白质在三维空间中的结构。</p>
<p>AlphaFold的输出是每个氨基酸在三维空间中的位置，以及氨基酸之间的相对距离和角度。这就构成了蛋白质的三维结构模型。</p>
<p>AlphaFold2的模型主要由两个部分组成：卷积神经网络（CNN）和Transformer。首先，卷积神经网络用于处理输入的一维氨基酸序列和二维的配对势能图（包含了序列中两个氨基酸之间的相互作用信息），并将这些信息转换为一种更高级的内部表示。然后，这个内部表示被输入到一个Transformer模型中。</p>
<p>Transformer模型的关键特性是其自注意力机制，这使得模型能够考虑到输入序列中的所有位置之间的相互关系。在AlphaFold2中，这意味着模型可以考虑到蛋白质中的所有氨基酸对的相互作用，这对于预测蛋白质的三维结构是非常重要的。</p>
<p>AlphaFold的工作过程可以分为两个步骤。首先，它生成一个称为"距离图"的东西，这是一个二维图，这些图描述了蛋白质中的氨基酸对之间的相对位置和取向。然后，AlphaFold2使用一个叫做分子动力学的过程，从这些预测的距离和角度图生成蛋白质的三维结构。</p>
<p>AlphaFold的性能非常出色，它在2020年的蛋白质结构预测竞赛（CASP）中大放异彩，被誉为解决了蛋白质折叠问题。然而，虽然AlphaFold的预测非常准确，但它仍然有一些局限性，例如它对于某些类型的蛋白质结构（如膜蛋白）的预测能力有限。</p>
<p>用 V100 32G <em>128, (EPYC </em>2, 512G RAM) 8 * 16 Nodes,  训练了三个月.</p>
<p>额外(主要)介绍了: </p>
<ol>
<li>打工人需要对各种算法和技术原理需要有一定的理解和涉猎("面试造火箭"); 最好不要做模型人少大厂. </li>
<li>BPS和SPAM的HPC部署脚本</li>
<li>简单介绍了分布式式训练, 分布式方法, 节点离线处理方法.</li>
</ol></div></div></div><div><p><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1zC411474Z/?spm_id_from=333.1007.top_right_bar_window_history.content.click" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1zC411474Z/?spm_id_from=333.1007.top_right_bar_window_history.content.click" target="_blank">【吹爆！】强强联手！科大讯飞和中科院终于把【多模态大模型】给讲通透了！CLIP、blip、blip2三种模型原理一次性学透！全程干货分享无废话！_哔哩哔哩_bilibili</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/661854155" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/661854155" target="_blank">视觉大模型 - 知乎</a></p></div></div></div><div class="heading-wrapper"><h3 data-heading="CLIP" class="heading" id="CLIP"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>CLIP</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2103.00020" rel="noopener" class="external-link" href="https://arxiv.org/abs/2103.00020" target="_blank">[2103.00020] Learning Transferable Visual Models From Natural Language Supervision</a><br>
<a data-tooltip-position="top" aria-label="https://blog.csdn.net/weixin_44031582/article/details/120469669" rel="noopener" class="external-link" href="https://blog.csdn.net/weixin_44031582/article/details/120469669" target="_blank">CLIP论文笔记--《Learning Transferable Visual Models From Natural Language Supervision》_visual n-grams模型-CSDN博客</a></p></div><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=xkSVUjbDI6I" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=xkSVUjbDI6I" target="_blank">【博士Vlog】OpenAI最新模型CLIP，想法极其简单？但为什么咱们做不了？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://github.com/openai/CLIP" rel="noopener" class="external-link" href="https://github.com/openai/CLIP" target="_blank">GitHub - openai/CLIP: CLIP (Contrastive Language-Image Pretraining), Predict the most relevant text snippet given an image</a></p>
<p><img alt="v4yTcC7odluDjHG.png" src="https://cdn.sa.net/2024/04/10/v4yTcC7odluDjHG.png" referrerpolicy="no-referrer"></p>
<ul>
<li>对角线的值，关联度最高，应当为一，作为Goal, 训练两个encoder使其优化到Goal</li>
<li>训练的数据量极大，图片数据大概100T.(ImageNet是1400万)</li>
<li>对比了在ResNet-50和ViT上测试了效果，ViT效果好训练时间短</li>
</ul>
<ul>
<li>在有名词描述的数据集上效果好，动词描述的数据集效果差</li>
</ul>
<ul>
<li>输入 text 输出 77 token embedding(每个token有768维度)</li>
</ul></div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=SImpr4oEezg" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=SImpr4oEezg" target="_blank">深度学习艺术品鉴赏大师？OpenAI的CLIP崭露头角！ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2204.14244" rel="noopener" class="external-link" href="https://arxiv.org/abs/2204.14244" target="_blank">[2204.14244] CLIP-Art: Contrastive Pre-training for Fine-Grained Art Classification</a></p>
<p>介绍了CLIP-Art 这篇论文。</p>
<p>艺术品没有很好的标签，且艺术品的分类精细复杂，原有的CLIP不够用。</p>
<p>作者使用了一个带标签的艺术品数据集对CLIP模型进行微调。<br>
在IMG_ENCODER, TEXT_ENCODER， 通过使用最小化对比预训练过程中的InfoNCE, 进行了联合嵌入。</p>
<p><img alt="9CbtHiDMVGzxU2L.png" src="https://cdn.sa.net/2024/04/10/9CbtHiDMVGzxU2L.png" referrerpolicy="no-referrer"></p>
<p>做了一个消融实验发现比 CLIP 要好。且Label的确有帮助。</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p>Alpha-CLIP</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p>MoCo</p></div></div></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin " style="--callout-color: 0, 200, 83;" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/688152203" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/688152203" target="_blank">CVPR 2024 | 医学异常检测新工作！采用VLM进行医学图像中的通用异常检测 - 知乎</a></p>
<p>介绍了</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="PointNet(2016)" class="heading" id="PointNet(2016)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>PointNet(2016)</h3><div class="heading-children"><div><blockquote>
<p><span style="background:#fff88f">处理3D 图像分割/分类的基本方向</span> 斯坦福🐰<br>
<img src="https://cdn.sa.net/2024/04/15/W6h71wAIHFcnd2e.png" referrerpolicy="no-referrer"></p>
</blockquote></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=2gTq3UZ9mmw" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=2gTq3UZ9mmw" target="_blank">经典网络PointNet详细解读，看看斯坦福如何处理点云数据？ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1612.00593" rel="noopener" class="external-link" href="https://arxiv.org/abs/1612.00593" target="_blank">[1612.00593] PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</a></p>
<p><img alt="zbjpxCoItJhZi27.png" src="https://cdn.sa.net/2024/04/15/zbjpxCoItJhZi27.png" referrerpolicy="no-referrer"></p>
<p>介绍了PointNet, 用于处理电云数据, 而不用转换成切片然后喂入CNN. 尊重了点云是物体的集合这一事实.</p>
<p>文章介绍了一个名为PointNet的深度学习网络，这个网络可以直接处理点云数据，用于3D分类和分<br>
割任务。文章的主要创新点包括：</p>
<ol>
<li><strong>新的神经网络结构</strong>：设计了一种新型的神经网络(T-Net)，能够直接消化无序的点集，而不需要将它们转换为规则的3D体素网格或图像集合(网格化)。这种设计大大减少了数据量，并避免了由于量化造成的失真问题。</li>
<li><strong>对输入的置换不变性</strong>：PointNeti尊重点云只是点的集合这一事实，因此对其成员的排列顺序具有不变性，通过网络计算中的某些对称操作来实现这一点。</li>
<li><strong>对刚性运动的不变性</strong>：在点云数据的表示学习中，网络能够对某些变换（例如整体旋转和平移）保持不变性。</li>
<li><strong>全局和局部特征学习</strong>：网络使用一个对称函数（最大池化）来从每个点中聚合信息，然后将这些信息编码为对整个形状的全局描述，或者用于预测每个点的部分标签。</li>
<li><strong>理论分析</strong>：提供了对PointNet能够逼近任意连续集合函数的理论分析，并且通过实验证明了网络对输入点的小扰动以及点的插入（异常值）或删除（缺失数据）具有很强的鲁棒性。</li>
<li><strong>实验验证</strong>：在多个基准数据集上，PointNet不仅在速度上比现有方法更快，而且在形状分类、部分分割和场景分割任务中达到了最先进的性能。</li>
<li><strong>直观解释</strong>：通过网络中选定神经元计算的3D特征的可视化，开发出对其性能的直观解释。</li>
</ol>
<p>综上所述，PointNet为3D数据的深度学习提供了一种有效的方法，并且在多项任务上都展现出了很<br>
强的性能和鲁棒性。</p>
<p><img src="https://cdn.sa.net/2024/04/15/NREHiej52UKmhJn.png" referrerpolicy="no-referrer"></p>
<p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=CPKkqpiC4PA" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=CPKkqpiC4PA" target="_blank">全面升级！斯坦福的点云识别系统什么原理？ - YouTube</a></p>
<ul>
<li>提出了一个名为 Point++ 的网络</li>
<li>多了 Skip Connection</li>
<li>引入网络分层, 增加上下文理解, 增加学习局部特征的能力</li>
</ul>
<p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=hISreorZoV4&amp;t=6s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=hISreorZoV4&amp;t=6s" target="_blank">世界上最有名的茶壶是哪一只？有什么有趣的历史？ - YouTube</a> &lt;- 犹它茶壶</p>
<ul>
<li>介绍了斯坦福兔子、犹它茶壶、康奈尔盒子</li>
<li>介绍了光栅渲染和RealTracing渲染</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=v9-6_PoDzhE" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=v9-6_PoDzhE" target="_blank">Transformer火出圈了，做点云分类和分割如何？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Point_Transformer_ICCV_2021_paper.pdf" rel="noopener" class="external-link" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Point_Transformer_ICCV_2021_paper.pdf" target="_blank">ICCV2021 Point_Transformer_ICCV_2021_paper.pdf</a></p>
<p><img src="https://cdn.sa.net/2024/04/14/Evz4j2AZlKY5naO.png" referrerpolicy="no-referrer"><br>
<img src="https://cdn.sa.net/2024/04/15/bBncrQFA97GyZWl.png" referrerpolicy="no-referrer"></p>
<p>这篇文章介绍了一个新的框架一一点云变换器(Point Cloud Transformer,,PcT),用于点云学习. 点云是3D空间中的一组点，表示物体的表面。与自然语言处理和图像处理中广泛使用的Transformer框架相似，PcT利用Transformer的原理来处理点云数据。由于点云的无序性和不规则性，设计深度神经网络来处理点云一直是一个挑战。PcT通过引入Transformer,使得模型能够自然地处理点序列，且不受点序列排列顺序的影响，因而非常适合于点云学习。</p>
<p><strong>主要贡献</strong></p>
<ul>
<li>
<p>提出了PcT框架：这是一个基于Transformer的点云学习框架，通过自注意力机制学习点云的全局特征，适用于无序且不规则的点云数据。</p>
</li>
<li>
<p>坐标基输入嵌入模块：考虑到点云没有固定的顺序，PCT将原始的位置编码和输入嵌入合并为一个基于坐标的输入嵌入模块，通过每个点的独特坐标生成可区分的特征。</p>
</li>
<li>
<p>优化的偏移注意力模块：P℃T提出了偏移注意力模块，通过计算自注意力特征与输入特征之间的偏移来提取特征，增强了模型对点云学习的适应性。邻域嵌入模块：为了更好地捕获点云中的局部上下文信息，P℃T使用了邻域嵌入策略，通过考虑点与其局部邻域内其他点的关系来增强点特征表示。</p>
</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=rqm8vmCig2E&amp;t=26s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=rqm8vmCig2E&amp;t=26s" target="_blank">Transformer火出圈了！第一篇用Transformer处理点云的文章讲了什么？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Point_Transformer_ICCV_2021_paper.pdf" rel="noopener" class="external-link" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Point_Transformer_ICCV_2021_paper.pdf" target="_blank">Point Transformer ICCV 2021 paper.pdf</a></p>
<p><img src="https://cdn.sa.net/2024/04/14/YUJrsxSmizRV31o.png" referrerpolicy="no-referrer"></p>
<p>第一篇用 Transformer 作为主干的 PointNet.</p>
<p>基本上就是用 Attention 取代 ConV. </p>
<p>搭建的模式基本上和 CNN 的搭建模式一致.</p>
<p>上面的网络类似 U-Net<br>
下面的网络类似 VGG-16</p>
<p>这篇文章介绍了点云变换器(Point Transformer)，这是一种用于3D点云处理的新型神经网络结<br>
构。文章的核心贡献在于将自注意力机制应用于点云数据，以此来改进语义场景分割、对像部分分<br>
割和对象分类等3D理解任务的性能。</p>
<p>主要贡献和工作原理:</p>
<ul>
<li>设计了用于点云处理的自注意力层：点云变换器通过自注意力层来处理点云数据，这种结构特别适用于点云这种结构，因为点云本质上是在连续空间中嵌入的点集，自注意力运算本质上是一种集合操作，对输入元素的排列和数量变。</li>
<li>提出了点云变换器网络结构：基于自注意力层，构建了用于分类和密集预测的点云变换器网络。这些网络可以作3D场景理解的通用骨架。</li>
<li>在多个领域和数据集上进行了广泛的实验：通过在不同的3D理解任务上的实验，点云变换器展现了其卓越的性能，尤其在大规模语义分割（如S3DIS数据集上以70.4%的loU首次突破了70%的门槛)、形状分类（在ModelNet-40数据集上以937%的总体准确率）和对象部分分割（在ShapeNetPart数据集上以86.6%的实例mloU)上设置了新的最佳性能记录。</li>
</ul>
<p>与以往工作的不同:</p>
<p>点云变换器与以往的工作相比，最主要的不同在于它完全基于自注意力和逐点操作，而不是传统的<br>
卷积处理方法。这种设计使其能够更自然地处理点云的无序和不规则性，从而更好地学习点云中的<br>
全局特征和局部上下文信息。此外，通过自注意力机制，点云变换器能够更有效地捕获点与点之间<br>
的复杂关系，这对于3D点云分析至关重要。</p>
<p>实际应用和影响:</p>
<p>点云变换器的提出，为3D点云分析提供了一种新的强大工具，尤其适用于自动驾驶、增强现实和<br>
机器人等领域，这些领域中经常需要对3D空间数据进行高效和准确的理解。此外，文章希望这项<br>
工作能激发更多关于点云变换器的研究，包括开发新的操作和网络设计，以及将变换器应用到3D<br>
对象检测等其他任务上。</p></div></div></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="双流网络(Swing Network)" class="heading" id="双流网络(Swing_Network)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>双流网络(Swing Network)</h2><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/search?q=%E5%8F%8C%E6%B5%81%E7%BD%91%E7%BB%9C&amp;search_source=Suggestion&amp;utm_content=search_suggestion&amp;type=content" rel="noopener" class="external-link" href="https://www.zhihu.com/search?q=%E5%8F%8C%E6%B5%81%E7%BD%91%E7%BB%9C&amp;search_source=Suggestion&amp;utm_content=search_suggestion&amp;type=content" target="_blank">双流网络 - 搜索结果 - 知乎</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/516956247/answer/3310533088" rel="noopener" class="external-link" href="https://www.zhihu.com/question/516956247/answer/3310533088" target="_blank">医学图像的深度学习该怎么做? - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/548613974/answer/3229770898" rel="noopener" class="external-link" href="https://www.zhihu.com/question/548613974/answer/3229770898" target="_blank">关于U-Net的魔改到了什么程度了？ - 知乎</a></p></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content heading-wrapper"><h2 data-heading="双流网络 (Two-Stream Network) 详解" class="heading" id="双流网络_(Two-Stream_Network)_详解"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>双流网络 (Two-Stream Network) 详解</h2>
<p>双流网络是一种在视频动作识别领域中非常流行的深度学习架构。它通过同时分析视频的空间和时间信息来进行动作识别，从而提高了识别精度。</p>
<p><strong>双流网络的核心思想是将视频信息分成两个独立的流进行处理：</strong></p>
<ul>
<li><strong>空间流 (Spatial Stream):</strong>&nbsp;该流处理视频帧，提取其中的静态外观信息，例如场景、物体和人物等。通常使用卷积神经网络 (CNN) 来进行特征提取。</li>
<li><strong>时间流 (Temporal Stream):</strong>&nbsp;该流处理视频帧之间的运动信息，例如光流 (Optical Flow)。光流描述了像素在相邻帧之间的运动方向和速度。通常使用 3D 卷积神经网络 (3D CNN) 或循环神经网络 (RNN) 来进行特征提取。</li>
</ul>
<p><strong>双流网络的优势：</strong></p>
<ul>
<li><strong>充分利用视频信息:</strong>&nbsp;通过同时分析空间和时间信息，双流网络可以更全面地理解视频内容，从而提高动作识别的准确性。</li>
<li><strong>鲁棒性强:</strong>&nbsp;即使视频中存在遮挡或光照变化等干扰因素，双流网络仍然能够有效地识别动作。</li>
</ul>
<p><strong>双流网络的应用:</strong></p>
<ul>
<li><strong>动作识别:</strong>&nbsp;这是双流网络最主要的应用领域，例如识别视频中的人物动作 (跑步、跳跃、游泳等)。</li>
<li><strong>行为分析:</strong>&nbsp;可以分析视频中人物的行为，例如打架、拥抱、握手等。</li>
<li><strong>视频描述:</strong>&nbsp;可以根据视频内容生成文本描述。</li>
</ul>
<p><strong>双流网络的局限性:</strong></p>
<ul>
<li><strong>计算量大:</strong>&nbsp;由于需要同时处理两个流的信息，双流网络的计算量较大，需要较高的计算资源。</li>
<li><strong>训练难度大:</strong>&nbsp;双流网络的训练过程比较复杂，需要大量的训练数据和调参技巧。</li>
</ul>
<p><strong>双流网络的发展趋势:</strong></p>
<ul>
<li><strong>更强大的特征提取网络:</strong>&nbsp;研究者们正在探索更强大的特征提取网络，例如基于 Transformer 的网络，以提高双流网络的性能。</li>
<li><strong>多模态融合:</strong>&nbsp;将双流网络与其他模态的信息 (例如音频) 进行融合，可以进一步提高动作识别的准确性。</li>
<li><strong>轻量化模型:</strong>&nbsp;研究者们正在研究如何降低双流网络的计算量，使其能够在移动设备上运行。</li>
</ul>
<p><strong>总而言之，双流网络是一种强大的视频动作识别模型，在视频理解领域具有广泛的应用前景。</strong></p>
<p>希望以上信息能帮助你了解双流网络。</p>
<p><strong>如果你想了解更多信息，可以参考以下资源:</strong></p>
<ul>
<li><strong>论文:</strong>&nbsp;Two-Stream Convolutional Networks for Action Recognition in Videos</li>
<li><strong>代码:</strong>&nbsp;<a rel="noopener" class="external-link" href="https://github.com/feichtenhofer/twostreamfusion" target="_blank">https://github.com/feichtenhofer/twostreamfusion</a></li>
</ul><div class="heading-children"></div></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=OOWZl4mx2fM" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=OOWZl4mx2fM" target="_blank">【博士Vlog】全网最全！十分钟看完所有双流神经网络，图片视频处理都在这里了！ - YouTube</a></p>
<p><img src="https://cdn.sa.net/2024/04/15/YWOvjX8w9GKulFb.png" referrerpolicy="no-referrer"><br>
<img src="https://cdn.sa.net/2024/04/15/BLhnCS2eKHw7lFv.png" referrerpolicy="no-referrer"></p>
<ul>
<li>提出了一个新的数据集</li>
<li>介绍了使用双流神经网路进行视频预测, 将2D ConvNet 膨胀到 双流 3D ConvNet(网络E)</li>
<li>“Two Stream Inflated 3D ConvNet(I3D)” (Inception, VGG16, ResNet)</li>
<li>现在已经成为Fondation model</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/692173683?utm_campaign=&amp;utm_medium=social&amp;utm_psn=1763139418939244544&amp;utm_source=io.raindrop.raindropio" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/692173683?utm_campaign=&amp;utm_medium=social&amp;utm_psn=1763139418939244544&amp;utm_source=io.raindrop.raindropio" target="_blank">结合创新！ResNet+Transformer，高性能低参数，准确率达99.12％ - 知乎</a></p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="模型可解释" class="heading" id="模型可解释"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>模型可解释</h2><div class="heading-children"><div class="admonition-parent admonition-failure-parent"><div class="callout admonition admonition-failure admonition-plugin " style="--callout-color: 255, 82, 82;" data-callout="failure" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times-circle" class="svg-inline--fa fa-times-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3 0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3 0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3 0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3 0 17L312 256l65.6 65.1z"></path></svg></div><div class="callout-title-inner admonition-title-content">Failure</div></div><div class="callout-content admonition-content"><p>模型解释性很重要，特别是医疗和金融领域。</p>
<p>主要的理由有：</p>
<ol>
<li><strong>可信任性</strong>：如果模型的预测能够被解释和理解，那么人们更可能信任模型，特别是在影响重大的决策场景，如医疗诊断和金融信贷中。</li>
<li><strong>模型调试</strong>：可解释性可以帮助我们理解模型在某些情况下为何预测错误，并可以指导我们如何改进模型。</li>
<li><strong>公平性和偏见检查</strong>：如果我们能理解模型的决策过程，那么我们可以更容易地检测到模型是否有不公平的偏见，例如预测是否受到某些不相关因素（如性别或种族）的影响。</li>
<li><strong>符合监管要求</strong>：在某些行业，如金融和医疗，预测模型可能需要满足监管机构的可解释性要求。</li>
<li><strong>特征工程和选择</strong>：解释性模型能够显示哪些特征对预测结果有重大影响，这有助于我们进行更有效的特征选择和工程</li>
</ol></div></div></div><div style="overflow-x: auto;"><table>
<thead>
<tr>
<th>方法</th>
<th>可解释性水平(0-100)</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td>线性回归/逻辑回归</td>
<td>90</td>
<td>输出可以清晰地用输入特征的线性组合表示，且模型权重可解释为各特征的影响大小和方向</td>
</tr>
<tr>
<td>决策树</td>
<td>95</td>
<td>每一个决策都基于清晰的规则，规则基于特征值，轻松理解每步决策</td>
</tr>
<tr>
<td>随机森林</td>
<td>80</td>
<td>尽管是多个决策树的集合，使得整体解释性相比单个树略弱，但每棵树都基于明确的规则，且可以通过特征重要性指标理解特征对模型的影响</td>
</tr>
<tr>
<td>支持向量机（线性核）</td>
<td>85</td>
<td>找到线性界限将特征空间分割为不同类别，权重向量可理解为特征影响，但分类决策通常只受支持向量影响</td>
</tr>
<tr>
<td>支持向量机（非线性核，如RBF）</td>
<td>40</td>
<td>通过核函数映射到高维空间，解释性变得复杂，难以明确理解特征如何影响分类界限</td>
</tr>
<tr>
<td>神经网络</td>
<td>20</td>
<td>隐藏层和参数众多，难以理解特征如何直接影响输出，通常被视为"黑箱"，尽管有一些可视化、解释工具尝试提高解释性</td>
</tr>
<tr>
<td>K-近邻</td>
<td>75</td>
<td>预测结果由最靠近的K个样本投票决定，直观易懂，但不提供特征权重或影响程度信息</td>
</tr>
<tr>
<td>LIME</td>
<td>80</td>
<td>LIME通过拟合一个局部线性模型来解释黑盒模型的预测。它可以对任何模型进行解释，并为每个预测指出了重要的局部特征，但结果可能依赖于随机样本和选择的超参数</td>
</tr>
<tr>
<td>SHAP</td>
<td>85</td>
<td>SHAP基于博弈论的概念，计算出每个特征对预测的平均贡献，甚至能够对复杂模型如XGBoost、神经网络等提供公平、准确的特征贡献解释。但它的计算复杂度较高，扩展到大型或深度模型需要特定的优化版本</td>
</tr>
<tr>
<td>GradCAM</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div><div class="admonition-parent admonition-attention-parent"><div class="callout admonition admonition-attention admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="attention" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Attention</div></div><div class="callout-content admonition-content"><p>有多种方法可以可视化神经网络的隐藏层。以下是一些常见的方法：</p>
<ol>
<li>
<p><strong>激活映射（Activation Maps）</strong>：对于卷积神经网络（CNN），我们可以直接可视化特定层的激活。这可以帮助我们理解网络在哪些区域和特征上被激活。</p>
</li>
<li>
<p><strong>过滤器可视化（Filter Visualization）</strong>：对于CNN的卷积层，我们可以可视化学习到的过滤器。对于第一层，这些过滤器可能会显示出一些简单的特征，如边缘和颜色斑块。对于更深的层，过滤器可能会表示更复杂的特征。</p>
</li>
<li>
<p><strong>类激活映射（Class Activation Mapping, CAM）</strong>：CAM是一种可视化技术，可以显示出网络在进行特定类别预测时关注的图像区域。</p>
</li>
<li>
<p><strong>特征反演（Feature Inversion）</strong>：这种方法试图找到一个输入，其通过网络传播后在特定层产生的激活最接近给定的激活。这可以用于理解特定层的激活表示什么。</p>
</li>
<li>
<p><strong>深度梦境（DeepDream）</strong>：DeepDream是一种使用神经网络自身来生成图像的技术。它通过选择一个层，然后尽可能地激活那一层来修改输入图像。</p>
</li>
<li>
<p><strong>神经网络去卷积（Deconvolutional Networks）</strong>：这种方法试图将特定层的激活映射回输入空间，以理解网络学习的特征。</p>
</li>
<li>
<p><strong>梯度上升（Gradient Ascent）</strong>：这种方法通过优化输入图像以最大化特定层的激活来理解该层的功能。</p>
</li>
<li>
<p><strong>t-SNE 和 PCA</strong>：这些降维技术可以用于可视化高维特征空间。</p>
</li>
</ol>
<p>每种方法都有其优点和缺点，适用于解决特定的问题。所以，选择哪种方法取决于你的具体需求和你想要解答的问题。</p>
<hr>
<p><strong>Grad-CAM</strong>，全称Gradient-weighted Class Activation Mapping，是一种可视化卷积神经网络决策的技术。它是类激活映射（Class Activation Mapping, CAM）的一种扩展，可以用于任何CNN架构，而不仅仅是那些有全局平均池化层的网络。</p>
<p>Grad-CAM的主要思想是使用目标类别相对于最后一个卷积层的梯度来生成一个粗糙的定位图（也称为热图）。这个热图突出显示了对分类决策最重要的图像区域。</p>
<p>下面是Grad-CAM的基本步骤：</p>
<ol>
<li>选择网络的某个卷积层。</li>
<li>对给定的输入和类别，运行前向传播。</li>
<li>使用类别的得分计算相对于选定卷积层的梯度。</li>
<li>对该层的每个特征映射的梯度进行全局平均池化，得到权重系数。</li>
<li>将这些权重应用于相应的特征映射，然后对结果进行求和，得到定位图。</li>
<li>对定位图应用ReLU激活函数以保留仅对目标类别有用的特征。</li>
<li>将定位图上采样（比如，使用双线性插值）到输入图像的大小，以产生热图。</li>
</ol>
<p>通过这种方式，Grad-CAM提供了一种可视化方法，可以清楚地看到模型在做出决策时关注的图像区域。这对于模型的解释性和透明度非常有帮助。</p>
<hr>
<p><strong>激活映射（Activation Maps）</strong>是一种可视化技术，用于显示卷积神经网络（CNN）中特定层的激活。这种方法通常用于理解网络在处理特定输入时，各层是如何响应的。以下是激活映射的主要原理、优势和劣势。</p>
<p><strong>原理</strong>：</p>
<p>在CNN中，每个卷积层都会生成一组特征映射（或激活映射），这些映射表示网络在特定层级捕获的特征。例如，第一层可能会捕获简单的边缘和纹理，而更深的层可能会捕获更复杂的特征，如对象的部分或整体。通过可视化这些激活映射，我们可以直观地理解网络在处理输入时的行为。</p>
<p><strong>优势</strong>：</p>
<ol>
<li>
<p><strong>直观理解</strong>：激活映射可以直观地揭示网络在处理特定输入时的内部行为，有助于理解网络如何识别和抽取特征。</p>
</li>
<li>
<p><strong>故障诊断</strong>：如果网络的性能不佳，检查激活映射可以帮助发现问题。例如，如果某些特征映射始终未激活，或者激活的模式看起来不正确，那可能就是网络配置或训练过程中存在问题。</p>
</li>
</ol>
<p><strong>劣势</strong>：</p>
<ol>
<li>
<p><strong>高维复杂性</strong>：对于具有大量特征映射的深度网络，激活映射可能会非常复杂，难以一次性全部理解。</p>
</li>
<li>
<p><strong>缺乏高级解释</strong>：虽然激活映射可以显示网络的内部行为，但它们并不能直接解释网络如何做出特定的预测。例如，它们不能告诉我们网络为什么将某个图像分类为猫而不是狗。</p>
</li>
<li>
<p><strong>难以解释抽象特征</strong>：在深层网络中，激活映射可能会表示非常抽象的概念，这些概念可能难以理解或解释。</p>
</li>
</ol>
<hr>
<p>t-SNE（t-Distributed Stochastic Neighbor Embedding）和PCA（Principal Component Analysis）是两种常用的降维技术，可以用于可视化神经网络的隐藏层。</p>
<ol>
<li>
<p><strong>PCA（主成分分析）</strong>：PCA是一种线性降维技术，它通过找到数据中的主要变化方向（即主成分）来减少数据的维数。在神经网络的隐藏层可视化中，PCA可以用来将高维的激活映射投影到2D或3D空间，以便我们可以在图形中看到它们。PCA的主要优点是计算效率高，但由于它是线性的，所以可能无法捕捉到数据中的非线性结构。</p>
</li>
<li>
<p><strong>t-SNE（t-分布随机邻域嵌入）</strong>：t-SNE是一种非线性降维技术，它通过尝试保持原始高维空间中的邻域关系来将高维数据映射到低维空间。在神经网络的隐藏层可视化中，t-SNE可以用来揭示高维激活映射中的结构和模式。t-SNE的优点是能够捕捉到数据中的非线性结构，但计算效率较低，且超参数调整可能会影响结果。</p>
</li>
</ol>
<p>这两种方法都可以用来理解和解释神经网络的行为。例如，你可以使用它们来查看网络在处理不同类型的输入时，隐藏层的激活是否有明显的区别。这可以帮助你理解网络是否成功地学习到了有用的特征，或者是否存在可能的问题（例如，所有的输入都映射到同一区域，可能表明网络没有成功地区分不同的输入）。</p></div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=5XiFThIeo2U" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=5XiFThIeo2U" target="_blank">【博士Vlog】模型解释哪家强？一篇文章节省你三个月时间！</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2103.10689" rel="noopener" class="external-link" href="https://arxiv.org/abs/2103.10689" target="_blank">[2103.10689] Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond</a></p>
<p>介绍了一个模型可解释性的Survey。<br>
对比了 Human label, LIME, GradCAM, SmoothGrad 对图片label的标记水平。</p>
<p><img alt="sgeX82wc4EobN5F.png" src="https://cdn.sa.net/2024/04/08/sgeX82wc4EobN5F.png" referrerpolicy="no-referrer"></p>
<ol>
<li>
<p><strong>闭式(Closed-Form)</strong>：线性回归就是一个闭式的例子。线性回归模型的每个特征都有一个系<br>
数，这些系数直接表明了特征对于模型预测的贡献度。由于模型结构简单且系数直接对应于输入<br>
特征的重要性，所以可以直接将这些系数视为模型的解释。</p>
</li>
<li>
<p><strong>组合(Composition)</strong>:决策树模型是组合关系的一个例子。决策树在做出预测时，可以直接展示<br>
其决策路径，这个路径可以视为模型的解释。例如，在银行贷款批准的决策树中，每个决策节点<br>
(如信用评分、收入等)和它们的阈值就构成了预测的解释。</p>
</li>
<li>
<p><strong>依赖(Dependence)</strong>:深度学习模型中的特征重要性图（如Grad-CAM)就是依赖关系的一个例<br>
子。例如，在一个用于图像识别的卷积神经网络中，Grad-CAM使用模型的梯度信息来突出显示对<br>
于预测类别最重要的图像区域，这些区域的突出显示依赖于模型内部的特定层次。</p>
</li>
<li>
<p><strong>代理(Proxy)</strong>：LME(局部可解释模型不透明度解释)算法是代理关系的一个例子。LIME通过<br>
在原始复杂模型（如随机森林或深度神经网络）的预测周围采样，并用这些样本来训练一个简单<br>
的线性模型作为代理。这个简单的线性模型（如线性回归）然后用来解释原始模型在局部区域的<br>
行为。</p>
</li>
</ol></div></div></div><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=hdDE676jJU4" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=hdDE676jJU4" target="_blank">【博士Vlog】如何解释机器学习深度学习？LIME和SHAP方法介绍 - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1602.04938" rel="noopener" class="external-link" href="https://arxiv.org/abs/1602.04938" target="_blank">[1602.04938] "Why Should I Trust You?": Explaining the Predictions of Any Classifier</a></p>
<p>介绍了一篇论文关于<strong>LIME</strong>（Local Interpretable Model-Agnostic Explanations）模型解释技术的论文，用于任何分类器的预测。</p>
<p>在随机森林和神经网络证明解释性是可行的。</p>
<p><img alt="tU1hkHMvgICK3oZ.png" src="https://cdn.sa.net/2024/04/08/tU1hkHMvgICK3oZ.png" referrerpolicy="no-referrer"></p>
<p><img alt="joRClJi5gWaKL7p.png" src="https://cdn.sa.net/2024/04/08/joRClJi5gWaKL7p.png" referrerpolicy="no-referrer"></p>
<p>用一个可解释的模型(e.g.SVM,Random Forest) 来拟合模型局部在一个特定实例下的行为。</p>
<p>这个图是LIME一个简化的示例，用来解释LME(局部可解释模型-不透明度解释)的直观概念。在图<br>
中，有一个复杂的决策边界（由蓝色和粉色区域表示），它代表一个黑盒模型的决策函数：，LME<br>
并不了解这个函数。这个决策边界不是线性的，也就是说，它不能被一个简单的线性模型很好地近<br>
似。<br>
在这个决策空间中，有两类实例，用加号和圆点标记。红色粗十字代表需要解释的实例，也就是<br>
说，我们想要了解模型为何将这个特定的实例分类为当前的类别。<br>
LME通过在决策边界附近采样新的实例，并使用原模型来对这些新实例做出预测。这些预测然后<br>
根据它们与需要解释的实例的近似程度被加权（在图中以大小表示）。最后，LME会学习一个简<br>
单的模型（在图中用虚线表示），这个模型仅在局部是准确的，也就是说它在被解释实例的周围是<br>
可靠的，尽管在整个决策空间中不一定可靠。这样，即使原始的黑盒模型很复杂和不可解释，</p>
<p><strong>LME也能提供对<font color="#ffff00">单个</font>预测决策的直观理解。</strong></p>
<p><span style="background:#fff88f">LIME 库已停止维护</span></p></div></div></div><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=JT_9zozHCDM" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=JT_9zozHCDM" target="_blank">Lime和SHAP哪个更好？这个文章做了个对比！ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://www.researchgate.net/publication/378171944_E-XAI_Evaluating_Black-Box_Explainable_AI_Frameworks_for_Network_Intrusion_Detection" rel="noopener" class="external-link" href="https://www.researchgate.net/publication/378171944_E-XAI_Evaluating_Black-Box_Explainable_AI_Frameworks_for_Network_Intrusion_Detection" target="_blank">(PDF) E-XAI: Evaluating Black-Box Explainable AI Frameworks for Network Intrusion Detection</a></p>
<p>这篇论文介绍了网络安全领域AI模型的可解释性。</p>
<p>有一个E-XAI的一个黑盒用于检测黑客是否入侵，需要提供这个AI的可解释性。</p>
<p>作者提出了一个XAI的框架用于评估，使用了LIME和SHAP进行模型可视化和六个指标的评估(准确性稀疏性鲁棒性稳定性效率完整性)。</p>
<p><img src="https://cdn.sa.net/2024/04/08/EO4ughnR3jH7IMz.png" referrerpolicy="no-referrer"></p>
<p><img src="https://cdn.sa.net/2024/04/08/mVYvHPWbdZCFhz2.png" referrerpolicy="no-referrer"></p>
<p>对比了LIME和SHAP的可视化结果，发现LIME和SHAP可解释性差不多。文章说在他们的数据集，SHAP效果比较好。</p>
<p>这里有医疗AI模型的对比<br>
<a data-tooltip-position="top" aria-label="https://www.mdpi.com/2075-4418/12/2/237" rel="noopener" class="external-link" href="https://www.mdpi.com/2075-4418/12/2/237" target="_blank">Diagnostics | Free Full-Text | Applications of Explainable Artificial Intelligence in Diagnosis and Surgery</a></p>
<p><img alt="eUdzFsShQgtyrCK.png" src="https://cdn.sa.net/2024/04/08/eUdzFsShQgtyrCK.png" referrerpolicy="no-referrer"></p></div></div></div><div class="admonition-parent admonition-info-parent"><div class="callout admonition admonition-info admonition-plugin " style="--callout-color: 0, 184, 212;" data-callout="info" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="info-circle" class="svg-inline--fa fa-info-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"></path></svg></div><div class="callout-title-inner admonition-title-content">Info</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://shap.readthedocs.io/en/latest/" rel="noopener" class="external-link" href="https://shap.readthedocs.io/en/latest/" target="_blank">Welcome to the SHAP documentation — SHAP latest documentation</a></p>
<p><img alt="ByNfczr57LWdCb2.png" src="https://cdn.sa.net/2024/04/08/ByNfczr57LWdCb2.png" referrerpolicy="no-referrer"></p>
<p><strong>SHAP（SHapley Additive exPlanations）</strong>是一种用于解释机器学习模型的算法。它基于博弈论中的Shapley值，这是一种用于分配合作游戏收益的公平方式。在机器学习模型解释的上下文中，SHAP用于分配每个特征对预测结果的贡献。</p>
<p>SHAP值的计算过程涉及到所有可能的特征组合，这使得它在理论上是最公平的分配方式。这也意味着SHAP值满足了一些重要的属性，例如效率（所有特征的SHAP值之和等于预测结果）和对称性（如果两个特征对预测结果的贡献相同，那么他们的SHAP值应该相同）。</p>
<p>SHAP的主要优点是它提供了一种全局和局部的模型解释方式。全局解释是通过平均所有实例的SHAP值来得到的，它可以给出每个特征对模型预测结果的平均贡献。局部解释是通过计算单个实例的SHAP值来得到的，它可以给出在该实例中每个特征对预测结果的贡献。</p>
<p>然而，SHAP的计算过程可能会非常复杂和计算密集，尤其是在有大量特征的情况下。为了解决这个问题，有一些近似的方法被提出，例如KernelSHAP和TreeSHAP，它们在保持解释性的同时减少了计算复杂性。</p>
<p><strong>比如</strong>，一个简单的二元分类问题为例，假设我们有一个机器学习模型，它使用年龄，性别，和体重三个特征来预测一个人是否会患有某种疾病。给定一个具体的个体，例如一个40岁的男性，体重为75公斤，我们的模型预测他有70%的概率会患病。</p>
<p>我们可以使用SHAP来解释这个预测结果。首先，我们需要计算每个特征的SHAP值。这涉及到计算所有可能的特征组合，例如只有年龄，只有性别，只有体重，年龄和性别，年龄和体重，性别和体重，以及所有三个特征。对于每一种组合，我们都需要计算出该特征组合对预测结果的贡献，然后按照Shapley值的公式分配给每个特征。</p>
<p>假设我们得到的SHAP值是：年龄为0.3，性别为0.1，体重为0.3。这意味着年龄和体重对预测结果的贡献最大，每个都贡献了30%的可能性，性别贡献了10%的可能性。这些SHAP值的总和（0.3 + 0.1 + 0.3 = 0.7）等于模型的预测结果（70%的概率会患病），这满足了SHAP的效率属性。</p>
<p>这样，我们就可以更好地理解模型的预测结果了。例如，我们可以看到年龄和体重是最重要的因素，而性别的影响相对较小。这可以帮助我们理解模型的工作原理，以及如何改进模型的性能。</p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="知识蒸馏" class="heading" id="知识蒸馏"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>知识蒸馏</h2><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=Kqr4jgkccD8" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=Kqr4jgkccD8" target="_blank">必读论文：知识蒸馏的奠基性工作，Label Smoothing讲了什么？ - YouTube</a></p>
<p>[[1503.02531] Distilling the Knowledge in a Neural Network ] (<a rel="noopener" class="external-link" href="https://arxiv.org/abs/1503.02531" target="_blank">https://arxiv.org/abs/1503.02531</a>)</p>
<p><strong>Geoffrey Hinton 2015年一作, 是知识蒸馏领域的奠基性论文.</strong> </p>
<p>这篇文章提出了一个 Label Smoothing 通过改变 损失函数中 class_index 的置信程度(e.g.比如原先是1现在改小一点为0.923), 来提高模型的泛化能力和过拟合风险. 这个改的方法由一个数学公式支持.</p>
<p>在知识蒸馏任务中，标签平滑可以有以下的贡献：</p>
<ol>
<li><strong>更好的泛化能力</strong>：由于标签平滑可以防止模型过拟合，因此使用标签平滑的学生模型通常可以在测试数据上获得更好的性能。</li>
<li><strong>更好的知识蒸馏效果</strong>：在知识蒸馏中，我们希望学生模型能够复制教师模型的输出概率分布。由于标签平滑可以使模型对每个样本的类别有一定的不确定性，因此使用标签平滑的学生模型可能可以更好地复制教师模型的输出概率分布。</li>
<li><strong>更稳定的训练过程</strong>：标签平滑可以使得模型的训练过程更稳定，因为它避免了模型对每个样本的类别过于确定，这可能导致模型在训练过程中产生大的梯度，从而导致训练过程不稳定。</li>
</ol>
<p>Inception v3 加入了这个模块. </p>
<p><img src="https://cdn.sa.net/2024/04/13/ucUvtyClaRb2QsB.png" referrerpolicy="no-referrer"></p>
<p>让轻量的学生网络学习教师模型的行为. 这样我们不仅可以获得一个轻量的网络, 还可以选择学习什么样的内容.</p>
<p>CLIP2 使用 ResNet 作为教师网络, 让 Transformer 学习 ResNet 的特征.</p>
<p>现在有很多人想 “偷” GPT4的模型行为. </p></div></div></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=DqE8tVjzqqY" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=DqE8tVjzqqY" target="_blank">模型压缩的开山之作：谷歌的《知识蒸馏》讲了什么？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1503.02531" rel="noopener" class="external-link" href="https://arxiv.org/abs/1503.02531" target="_blank">Distilling the Knowledge in a Neural Network - 2015</a></p>
<ul>
<li>三个大牛在2015年的论文, 讨论很general的如何进行知识蒸馏(模型压缩)的内容. </li>
<li>动机是把多个模型压缩到一个模型中, 进行知识迁移或者压缩</li>
<li>用到了 label smoothing, soft target. 就是不要让 student teacher 单纯傻记答案.  </li>
</ul>
<p><img alt="6HCpMuxFT5ZeB28.png" src="https://cdn.sa.net/2024/04/16/6HCpMuxFT5ZeB28.png" referrerpolicy="no-referrer"></p>
<p><strong>Distilling the Knowledge in a Neural Network 这篇论文? 主要探讨了什么问题? 有什么贡献?</strong></p>
<p> 这篇论文的主要研究方向是神经网络的知识蒸馏。知识蒸馏是一种将大型、复杂模型（教师模型）的知识转移到小型、简单模型（学生模型）的技术。这种方法可以减少模型的计算复杂度和存储需求，同时保持较高的性能。</p>
<p>论文的主要贡献包括：</p>
<ol>
<li>
<p>提出了一种新的训练方法，即通过训练一个简单的模型（学生模型）来模拟复杂模型（教师模型）的行为，从而达到压缩模型和提高效率的目的。</p>
</li>
<li>
<p>提出了一种新的软目标训练方法，即不仅使用原始硬目标（类别标签），还使用教师模型的软目标（类别概率分布）进行训练。这种方法可以使学生模型学习到教师模型的一些隐含知识，从而提高其泛化能力。</p>
</li>
<li>
<p>通过实验验证了知识蒸馏的有效性。结果表明，使用知识蒸馏训练的小模型可以达到与原始大模型相近的性能，且计算复杂度和存储需求大大降低。</p>
</li>
</ol></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="调优指南" class="heading" id="调优指南"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>调优指南</h2><div class="heading-children"><div><ul>
<li data-line="0">项目一开始，尽量使用一个完善有效的模型，尽快先让代码跑起来</li>
<li data-line="1">Quasi-Random-Search 通过在搜索空间均匀的生成样本点来进行搜索，在优化探索阶段很好用</li>
<li data-line="2">初始超参配置: 1) 模型配置 2) 优化器 3) 训练步数</li>
<li data-line="3">先使用常见的优化器(e.g. Adam, NAdam, SGD with momentum)</li>
<li data-line="4">选用硬件可支持的最大Batch Size, 以减少训练时间， 测试更多新想法。e.g.(batch size = 2^n, 1,2....,n)</li>
<li data-line="5">大多超参最优值对batch size 敏感， 特别是优化器(学习率， 动量)和正则化超参</li>
<li data-line="6"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>提高模型性能遵循增量调优策略
<ul>
<li data-line="7"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<ol>
<li data-line="7">设定下一轮实验的目标(小步快跑)</li>
</ol>
</li>
<li data-line="8"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<ol start="2">
<li data-line="8">设计展开实验(识别目标, 冗余, 固定超参)</li>
</ol>
</li>
<li data-line="9"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<ol start="3">
<li data-line="9">从实验中获得经验</li>
</ol>
</li>
<li data-line="10"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<ol start="4">
<li data-line="10">考虑是否使用新配置</li>
</ol>
</li>
</ul>
</li>
<li data-line="11">如果最佳点在搜索空间的边界， 那么该模型很可能并不稳定</li>
<li data-line="12">训练函数的val error在某个时刻增加时， 就会发生over fitting. 缓解手段有e.g. dropout、标签平滑化、权重衰减</li>
<li data-line="13">isolation 图可以帮助检测更改是否有效</li>
<li data-line="14">贝叶斯优化工具可以在完成“好”的搜索空间探索后，继续精准优化超参</li>
<li data-line="15">在训练误差无限改善时，多试几个参数可能更有帮助</li>
<li data-line="16"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>对于多轮测试， 可以在第一轮进行短时间训练获取最佳模型和优化器参数。第二轮，在较佳超参数上
<ul>
<li data-line="17"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>在第一轮获得超参泛化到更长epochs可能是
<ul>
<li data-line="18">warmup 时长，模型参数初始值</li>
<li data-line="19">优化算法/优化器超参数</li>
<li data-line="20">数据增强方法</li>
<li data-line="21">正则化</li>
</ul>
</li>
<li data-line="22"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>不可能转移
<ul>
<li data-line="23">学习率衰减schedule</li>
</ul>
</li>
</ul>
</li>
<li data-line="24"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>pipeline性能受阻的常见原因
<ul>
<li data-line="25">数据未与训练进程存放在同一位置，导致I/O延迟</li>
<li data-line="26">开销很大的在线预处理</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h2 data-heading="阅读笔记" class="heading" id="阅读笔记"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>阅读笔记</h2><div class="heading-children"><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://zh.d2l.ai/" rel="noopener" class="external-link" href="https://zh.d2l.ai/" target="_blank">《动手学深度学习》 — 动手学深度学习 2.0.0 documentation</a></p>
<p>建议直接跑一边它们的notebook</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=hnQyjGrGXfU" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=hnQyjGrGXfU" target="_blank">CVPR2023最新文章：拓扑生成网络 - YouTube</a><br>
<img src="https://cdn.sa.net/2024/04/15/tlYFVSJQa8zOCur.png" referrerpolicy="no-referrer"></p>
<p>介绍了一片使用拓扑网络生成细胞切片用于<strong>数据增强</strong>论文.<br>
在GAN的网络中, 添加了一些控制因子, 着重注意细胞周围的上下文. </p></div></div></div><div class="mod-footer"></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#深度学习里程碑"><div class="tree-item-contents heading-link" heading-name="深度学习里程碑"><span class="tree-item-title">深度学习里程碑</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#深度学习名人堂"><div class="tree-item-contents heading-link" heading-name="深度学习名人堂"><span class="tree-item-title">深度学习名人堂</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#基本算法"><div class="tree-item-contents heading-link" heading-name="基本算法"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">基本算法</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#反向传播_&amp;_梯度优化"><div class="tree-item-contents heading-link" heading-name="反向传播 &amp; 梯度优化"><span class="tree-item-title">反向传播 &amp; 梯度优化</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#模块"><div class="tree-item-contents heading-link" heading-name="模块"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">模块</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#优化器"><div class="tree-item-contents heading-link" heading-name="优化器"><span class="tree-item-title">优化器</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#激活函数"><div class="tree-item-contents heading-link" heading-name="激活函数"><span class="tree-item-title">激活函数</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#正则化"><div class="tree-item-contents heading-link" heading-name="正则化"><span class="tree-item-title">正则化</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#数据增强"><div class="tree-item-contents heading-link" heading-name="数据增强"><span class="tree-item-title">数据增强</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#CNN(卷积神经网络)"><div class="tree-item-contents heading-link" heading-name="CNN(卷积神经网络)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">CNN(卷积神经网络)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#LeNet(鼻祖)_-_1998"><div class="tree-item-contents heading-link" heading-name="LeNet(鼻祖) - 1998"><span class="tree-item-title">LeNet(鼻祖) - 1998</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#LeNet_是什么?_对深度学习的发展有什么贡献?"><div class="tree-item-contents heading-link" heading-name="LeNet 是什么? 对深度学习的发展有什么贡献?"><span class="tree-item-title">LeNet 是什么? 对深度学习的发展有什么贡献?</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#AlexNet(重启深度学习)_-_2012"><div class="tree-item-contents heading-link" heading-name="AlexNet(重启深度学习) - 2012"><span class="tree-item-title">AlexNet(重启深度学习) - 2012</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#AlexNet_是什么?_对深度学习的发展有什么贡献?"><div class="tree-item-contents heading-link" heading-name="AlexNet 是什么? 对深度学习的发展有什么贡献?"><span class="tree-item-title">AlexNet 是什么? 对深度学习的发展有什么贡献?</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#ZfNet(第一次可视化隐藏层)_-_2013"><div class="tree-item-contents heading-link" heading-name="ZfNet(第一次可视化隐藏层) - 2013"><span class="tree-item-title">ZfNet(第一次可视化隐藏层) - 2013</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#R-CNN(分割网络开山之作)_-_2013"><div class="tree-item-contents heading-link" heading-name="R-CNN(分割网络开山之作) - 2013"><span class="tree-item-title">R-CNN(分割网络开山之作) - 2013</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#历史上的分割网路"><div class="tree-item-contents heading-link" heading-name="历史上的分割网路"><span class="tree-item-title">历史上的分割网路</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#R-CNN_是什么?_对深度学习的发展有什么贡献?"><div class="tree-item-contents heading-link" heading-name="R-CNN 是什么? 对深度学习的发展有什么贡献?"><span class="tree-item-title">R-CNN 是什么? 对深度学习的发展有什么贡献?</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#VGG(纯CNN最深网络)_-_2014"><div class="tree-item-contents heading-link" heading-name="VGG(纯CNN最深网络) - 2014"><span class="tree-item-title">VGG(纯CNN最深网络) - 2014</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#GoogLeNet_/_Inception(探索各种trick)_-_2014"><div class="tree-item-contents heading-link" heading-name="GoogLeNet / Inception(探索各种trick) - 2014"><span class="tree-item-title">GoogLeNet / Inception(探索各种trick) - 2014</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#ResNet(网络变深)_-_2015"><div class="tree-item-contents heading-link" heading-name="ResNet(网络变深) - 2015"><span class="tree-item-title">ResNet(网络变深) - 2015</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#MobileNet(网络做小)_-_2016"><div class="tree-item-contents heading-link" heading-name="MobileNet(网络做小) - 2016"><span class="tree-item-title">MobileNet(网络做小) - 2016</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#Xception(常用)_-_2016"><div class="tree-item-contents heading-link" heading-name="Xception(常用) - 2016"><span class="tree-item-title">Xception(常用) - 2016</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#U-Net(常用)"><div class="tree-item-contents heading-link" heading-name="U-Net(常用)"><span class="tree-item-title">U-Net(常用)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#摘要"><div class="tree-item-contents heading-link" heading-name="摘要"><span class="tree-item-title">摘要</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#引言"><div class="tree-item-contents heading-link" heading-name="引言"><span class="tree-item-title">引言</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#网络架构"><div class="tree-item-contents heading-link" heading-name="网络架构"><span class="tree-item-title">网络架构</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#训练"><div class="tree-item-contents heading-link" heading-name="训练"><span class="tree-item-title">训练</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#数据增强"><div class="tree-item-contents heading-link" heading-name="数据增强"><span class="tree-item-title">数据增强</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#实验"><div class="tree-item-contents heading-link" heading-name="实验"><span class="tree-item-title">实验</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#结论"><div class="tree-item-contents heading-link" heading-name="结论"><span class="tree-item-title">结论</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#后续改进"><div class="tree-item-contents heading-link" heading-name="后续改进"><span class="tree-item-title">后续改进</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#EfficientNet_(终结网络scale经验法则)_-_2019"><div class="tree-item-contents heading-link" heading-name="EfficientNet (终结网络scale经验法则) - 2019"><span class="tree-item-title">EfficientNet (终结网络scale经验法则) - 2019</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#BiT_(最大有监督CNN)"><div class="tree-item-contents heading-link" heading-name="BiT (最大有监督CNN)"><span class="tree-item-title">BiT (最大有监督CNN)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#VanillaNet_?"><div class="tree-item-contents heading-link" heading-name="VanillaNet ?"><span class="tree-item-title">VanillaNet ?</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#RNN(循环神经网络)"><div class="tree-item-contents heading-link" heading-name="RNN(循环神经网络)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">RNN(循环神经网络)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#LSTM(长短期记忆网络)_&amp;_GRU(门控循环单元)"><div class="tree-item-contents heading-link" heading-name="LSTM(长短期记忆网络) &amp; GRU(门控循环单元)"><span class="tree-item-title">LSTM(长短期记忆网络) &amp; GRU(门控循环单元)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#RWKV"><div class="tree-item-contents heading-link" heading-name="RWKV"><span class="tree-item-title">RWKV</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#Transformer_(2017)"><div class="tree-item-contents heading-link" heading-name="Transformer (2017)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">Transformer (2017)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#Transformer_普遍拥有什么特性?"><div class="tree-item-contents heading-link" heading-name="Transformer 普遍拥有什么特性?"><span class="tree-item-title">Transformer 普遍拥有什么特性?</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#Transformer_的_感受域_和_CNN_比有什么异同?"><div class="tree-item-contents heading-link" heading-name="Transformer 的 感受域 和 CNN 比有什么异同?"><span class="tree-item-title">Transformer 的 感受域 和 CNN 比有什么异同?</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#ViT"><div class="tree-item-contents heading-link" heading-name="ViT"><span class="tree-item-title">ViT</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#R-Transformer"><div class="tree-item-contents heading-link" heading-name="R-Transformer"><span class="tree-item-title">R-Transformer</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#Mamba"><div class="tree-item-contents heading-link" heading-name="Mamba"><span class="tree-item-title">Mamba</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#VAE(变分自编码器)"><div class="tree-item-contents heading-link" heading-name="VAE(变分自编码器)"><span class="tree-item-title">VAE(变分自编码器)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#GAN"><div class="tree-item-contents heading-link" heading-name="GAN"><span class="tree-item-title">GAN</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#Stable_Diffusion"><div class="tree-item-contents heading-link" heading-name="Stable Diffusion"><span class="tree-item-title">Stable Diffusion</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#深度学习的著名应用"><div class="tree-item-contents heading-link" heading-name="深度学习的著名应用"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">深度学习的著名应用</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#GPT_&amp;_BERT"><div class="tree-item-contents heading-link" heading-name="GPT &amp; BERT"><span class="tree-item-title">GPT &amp; BERT</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#AlphaFold"><div class="tree-item-contents heading-link" heading-name="AlphaFold"><span class="tree-item-title">AlphaFold</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#CLIP"><div class="tree-item-contents heading-link" heading-name="CLIP"><span class="tree-item-title">CLIP</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#PointNet(2016)"><div class="tree-item-contents heading-link" heading-name="PointNet(2016)"><span class="tree-item-title">PointNet(2016)</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#双流网络(Swing_Network)"><div class="tree-item-contents heading-link" heading-name="双流网络(Swing Network)"><span class="tree-item-title">双流网络(Swing Network)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#双流网络_(Two-Stream_Network)_详解"><div class="tree-item-contents heading-link" heading-name="双流网络 (Two-Stream Network) 详解"><span class="tree-item-title">双流网络 (Two-Stream Network) 详解</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#模型可解释"><div class="tree-item-contents heading-link" heading-name="模型可解释"><span class="tree-item-title">模型可解释</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#知识蒸馏"><div class="tree-item-contents heading-link" heading-name="知识蒸馏"><span class="tree-item-title">知识蒸馏</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#调优指南"><div class="tree-item-contents heading-link" heading-name="调优指南"><span class="tree-item-title">调优指南</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/深度学习里程碑.html#阅读笔记"><div class="tree-item-contents heading-link" heading-name="阅读笔记"><span class="tree-item-title">阅读笔记</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>