<!DOCTYPE html> <html><head>
		<title>4_深度学习</title>
		<base href="../">
		<meta id="root-path" root-path="../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="🌱 Digital-Garden - 4_深度学习">
		<meta property="og:title" content="4_深度学习">
		<meta property="og:description" content="🌱 Digital-Garden - 4_深度学习">
		<meta property="og:type" content="website">
		<meta property="og:url" content="💾-科技工程/4_深度学习.html">
		<meta property="og:image" content="https://cdn.sa.net/2024/06/05/sJ7x6MuVtqg5rjR.gif">
		<meta property="og:site_name" content="🌱 Digital-Garden">
		<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.4.0/pixi.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="https://cdn.jsdelivr.net/npm/minisearch@6.3.0/dist/umd/index.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="preload" href="lib/styles/other-plugins.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/other-plugins.css"></noscript><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript></head><body class="publish css-settings-manager native-scrollbars theme-light show-inline-title show-ribbon"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles">mjx-munderover { display: inline-block; text-align: left; }
mjx-munderover:not([limits="false"]) { padding-top: 0.1em; }
mjx-munderover:not([limits="false"]) > * { display: block; }
mjx-msubsup { display: inline-block; text-align: left; }
mjx-script { display: inline-block; padding-right: 0.05em; padding-left: 0.033em; }
mjx-script > mjx-spacer { display: block; }
mjx-mtable { display: inline-block; text-align: center; vertical-align: 0.25em; position: relative; box-sizing: border-box; border-spacing: 0px; border-collapse: collapse; }
mjx-mstyle[size="s"] mjx-mtable { vertical-align: 0.354em; }
mjx-labels { position: absolute; left: 0px; top: 0px; }
mjx-table { display: inline-block; vertical-align: -0.5ex; box-sizing: border-box; }
mjx-table > mjx-itable { vertical-align: middle; text-align: left; box-sizing: border-box; }
mjx-labels > mjx-itable { position: absolute; top: 0px; }
mjx-mtable[justify="left"] { text-align: left; }
mjx-mtable[justify="right"] { text-align: right; }
mjx-mtable[justify="left"][side="left"] { padding-right: 0px !important; }
mjx-mtable[justify="left"][side="right"] { padding-left: 0px !important; }
mjx-mtable[justify="right"][side="left"] { padding-right: 0px !important; }
mjx-mtable[justify="right"][side="right"] { padding-left: 0px !important; }
mjx-mtable[align] { vertical-align: baseline; }
mjx-mtable[align="top"] > mjx-table { vertical-align: top; }
mjx-mtable[align="bottom"] > mjx-table { vertical-align: bottom; }
mjx-mtable[side="right"] mjx-labels { min-width: 100%; }
mjx-mtr { display: table-row; text-align: left; }
mjx-mtr[rowalign="top"] > mjx-mtd { vertical-align: top; }
mjx-mtr[rowalign="center"] > mjx-mtd { vertical-align: middle; }
mjx-mtr[rowalign="bottom"] > mjx-mtd { vertical-align: bottom; }
mjx-mtr[rowalign="baseline"] > mjx-mtd { vertical-align: baseline; }
mjx-mtr[rowalign="axis"] > mjx-mtd { vertical-align: 0.25em; }
mjx-mtd { display: table-cell; text-align: center; padding: 0.215em 0.4em; }
mjx-mtd:first-child { padding-left: 0px; }
mjx-mtd:last-child { padding-right: 0px; }
mjx-mtable > * > mjx-itable > :first-child > mjx-mtd { padding-top: 0px; }
mjx-mtable > * > mjx-itable > :last-child > mjx-mtd { padding-bottom: 0px; }
mjx-tstrut { display: inline-block; height: 1em; vertical-align: -0.25em; }
mjx-labels[align="left"] > mjx-mtr > mjx-mtd { text-align: left; }
mjx-labels[align="right"] > mjx-mtr > mjx-mtd { text-align: right; }
mjx-mtd[extra] { padding: 0px; }
mjx-mtd[rowalign="top"] { vertical-align: top; }
mjx-mtd[rowalign="center"] { vertical-align: middle; }
mjx-mtd[rowalign="bottom"] { vertical-align: bottom; }
mjx-mtd[rowalign="baseline"] { vertical-align: baseline; }
mjx-mtd[rowalign="axis"] { vertical-align: 0.25em; }
mjx-munder { display: inline-block; text-align: left; }
mjx-over { text-align: left; }
mjx-munder:not([limits="false"]) { display: inline-table; }
mjx-munder > mjx-row { text-align: left; }
mjx-under { padding-bottom: 0.1em; }
mjx-mspace { display: inline-block; text-align: left; }
mjx-c.mjx-c7C::before { padding: 0.75em 0.278em 0.249em 0px; content: "|"; }
mjx-c.mjx-c1D464.TEX-I::before { padding: 0.443em 0.716em 0.011em 0px; content: "w"; }
mjx-c.mjx-c1D6F4.TEX-I::before { padding: 0.683em 0.806em 0px 0px; content: "Σ"; }
mjx-c.mjx-c1D456.TEX-I::before { padding: 0.661em 0.345em 0.011em 0px; content: "i"; }
mjx-c.mjx-c3A::before { padding: 0.43em 0.278em 0px 0px; content: ":"; }
mjx-c.mjx-c5B::before { padding: 0.75em 0.278em 0.25em 0px; content: "["; }
mjx-c.mjx-c30::before { padding: 0.666em 0.5em 0.022em 0px; content: "0"; }
mjx-c.mjx-c5D::before { padding: 0.75em 0.278em 0.25em 0px; content: "]"; }
mjx-c.mjx-c2192::before { padding: 0.511em 1em 0.011em 0px; content: "→"; }
mjx-c.mjx-c211D.TEX-A::before { padding: 0.683em 0.722em 0px 0px; content: "R"; }
mjx-c.mjx-c1D45E.TEX-I::before { padding: 0.442em 0.46em 0.194em 0px; content: "q"; }
mjx-c.mjx-c1D44B.TEX-I::before { padding: 0.683em 0.852em 0px 0px; content: "X"; }
mjx-c.mjx-c1D465.TEX-I::before { padding: 0.442em 0.572em 0.011em 0px; content: "x"; }
mjx-c.mjx-c2026::before { padding: 0.12em 1.172em 0px 0px; content: "…"; }
mjx-c.mjx-c2211.TEX-S2::before { padding: 0.95em 1.444em 0.45em 0px; content: "∑"; }
mjx-c.mjx-c28.TEX-S4::before { padding: 1.75em 0.792em 1.249em 0px; content: "("; }
mjx-c.mjx-c29.TEX-S4::before { padding: 1.75em 0.792em 1.249em 0px; content: ")"; }
mjx-c.mjx-c1D713.TEX-I::before { padding: 0.694em 0.651em 0.205em 0px; content: "ψ"; }
mjx-c.mjx-c1D44E.TEX-I::before { padding: 0.441em 0.529em 0.01em 0px; content: "a"; }
mjx-c.mjx-cD7::before { padding: 0.491em 0.778em 0px 0px; content: "×"; }
mjx-c.mjx-c221A.TEX-S1::before { padding: 0.85em 1.02em 0.35em 0px; content: "√"; }
mjx-c.mjx-c1D461.TEX-I::before { padding: 0.626em 0.361em 0.011em 0px; content: "t"; }
mjx-c.mjx-c4D::before { padding: 0.683em 0.917em 0px 0px; content: "M"; }
mjx-c.mjx-c75::before { padding: 0.442em 0.556em 0.011em 0px; content: "u"; }
mjx-c.mjx-c48::before { padding: 0.683em 0.75em 0px 0px; content: "H"; }
mjx-c.mjx-c64::before { padding: 0.694em 0.556em 0.011em 0px; content: "d"; }
mjx-c.mjx-c43::before { padding: 0.705em 0.722em 0.021em 0px; content: "C"; }
mjx-c.mjx-c63::before { padding: 0.448em 0.444em 0.011em 0px; content: "c"; }
mjx-c.mjx-c68::before { padding: 0.694em 0.556em 0px 0px; content: "h"; }
mjx-c.mjx-c1D44A.TEX-I::before { padding: 0.683em 1.048em 0.022em 0px; content: "W"; }
mjx-c.mjx-c50::before { padding: 0.683em 0.681em 0px 0px; content: "P"; }
mjx-c.mjx-c45::before { padding: 0.68em 0.681em 0px 0px; content: "E"; }
mjx-c.mjx-c1D460.TEX-I::before { padding: 0.442em 0.469em 0.01em 0px; content: "s"; }
mjx-c.mjx-c28.TEX-S3::before { padding: 1.45em 0.736em 0.949em 0px; content: "("; }
mjx-c.mjx-c29.TEX-S3::before { padding: 1.45em 0.736em 0.949em 0px; content: ")"; }
mjx-c.mjx-c2B::before { padding: 0.583em 0.778em 0.082em 0px; content: "+"; }
mjx-c.mjx-c1D703.TEX-I::before { padding: 0.705em 0.469em 0.01em 0px; content: "θ"; }
mjx-c.mjx-c1D443.TEX-I::before { padding: 0.683em 0.751em 0px 0px; content: "P"; }
mjx-c.mjx-c1D438.TEX-I::before { padding: 0.68em 0.764em 0px 0px; content: "E"; }
mjx-c.mjx-c35::before { padding: 0.666em 0.5em 0.022em 0px; content: "5"; }
mjx-c.mjx-c70::before { padding: 0.442em 0.556em 0.194em 0px; content: "p"; }
mjx-c.mjx-c4C::before { padding: 0.683em 0.625em 0px 0px; content: "L"; }
mjx-c.mjx-c79::before { padding: 0.431em 0.528em 0.204em 0px; content: "y"; }
mjx-c.mjx-c72::before { padding: 0.442em 0.392em 0px 0px; content: "r"; }
mjx-c.mjx-c4E::before { padding: 0.683em 0.75em 0px 0px; content: "N"; }
mjx-c.mjx-c53::before { padding: 0.705em 0.556em 0.022em 0px; content: "S"; }
mjx-c.mjx-c62::before { padding: 0.694em 0.556em 0.011em 0px; content: "b"; }
mjx-c.mjx-c46::before { padding: 0.68em 0.653em 0px 0px; content: "F"; }
mjx-c.mjx-c1D44F.TEX-I::before { padding: 0.694em 0.429em 0.011em 0px; content: "b"; }
mjx-c.mjx-c1D45A.TEX-I::before { padding: 0.442em 0.878em 0.011em 0px; content: "m"; }
mjx-c.mjx-c1D45F.TEX-I::before { padding: 0.442em 0.451em 0.011em 0px; content: "r"; }
mjx-c.mjx-c1D439.TEX-I::before { padding: 0.68em 0.749em 0px 0px; content: "F"; }
mjx-c.mjx-c5B.TEX-S3::before { padding: 1.45em 0.528em 0.949em 0px; content: "["; }
mjx-c.mjx-c5D.TEX-S3::before { padding: 1.45em 0.528em 0.949em 0px; content: "]"; }
mjx-c.mjx-c22A4::before { padding: 0.668em 0.778em 0px 0px; content: "⊤"; }
mjx-c.mjx-c2299::before { padding: 0.583em 0.778em 0.083em 0px; content: "⊙"; }
mjx-c.mjx-c2297::before { padding: 0.583em 0.778em 0.083em 0px; content: "⊗"; }
mjx-c.mjx-c1D70E.TEX-I::before { padding: 0.431em 0.571em 0.011em 0px; content: "σ"; }
mjx-c.mjx-c1D463.TEX-I::before { padding: 0.443em 0.485em 0.011em 0px; content: "v"; }
mjx-c.mjx-c77::before { padding: 0.431em 0.722em 0.011em 0px; content: "w"; }
mjx-c.mjx-c6B::before { padding: 0.694em 0.528em 0px 0px; content: "k"; }
mjx-c.mjx-c1D6FC.TEX-I::before { padding: 0.442em 0.64em 0.011em 0px; content: "α"; }
mjx-c.mjx-c1D457.TEX-I::before { padding: 0.661em 0.412em 0.204em 0px; content: "j"; }
mjx-c.mjx-c1D410.TEX-B::before { padding: 0.696em 0.864em 0.193em 0px; content: "Q"; }
mjx-c.mjx-c1D40A.TEX-B::before { padding: 0.686em 0.901em 0px 0px; content: "K"; }
mjx-c.mjx-c1D413.TEX-B::before { padding: 0.675em 0.8em 0px 0px; content: "T"; }
mjx-c.mjx-c1D452.TEX-I::before { padding: 0.442em 0.466em 0.011em 0px; content: "e"; }
mjx-c.mjx-c1D415.TEX-B::before { padding: 0.686em 0.869em 0.007em 0px; content: "V"; }
mjx-c.mjx-c1D405.TEX-B::before { padding: 0.68em 0.724em 0px 0px; content: "F"; }
mjx-c.mjx-c1D53C.TEX-A::before { padding: 0.683em 0.667em 0px 0px; content: "E"; }
mjx-c.mjx-c1D466.TEX-I::before { padding: 0.442em 0.49em 0.205em 0px; content: "y"; }
mjx-c.mjx-c223C::before { padding: 0.367em 0.778em 0px 0px; content: "∼"; }
mjx-c.mjx-c44.TEX-C::before { padding: 0.683em 0.771em 0px 0px; content: "D"; }
mjx-c.mjx-c1D70B.TEX-I::before { padding: 0.431em 0.57em 0.011em 0px; content: "π"; }
mjx-c.mjx-c1D43F.TEX-I::before { padding: 0.683em 0.681em 0px 0px; content: "L"; }
mjx-c.mjx-c1D6FD.TEX-I::before { padding: 0.705em 0.566em 0.194em 0px; content: "β"; }
mjx-c.mjx-c1D446.TEX-I::before { padding: 0.705em 0.645em 0.022em 0px; content: "S"; }
mjx-c.mjx-c1D6FE.TEX-I::before { padding: 0.441em 0.543em 0.216em 0px; content: "γ"; }
mjx-c.mjx-c5B.TEX-S2::before { padding: 1.15em 0.472em 0.649em 0px; content: "["; }
mjx-c.mjx-c5D.TEX-S2::before { padding: 1.15em 0.472em 0.649em 0px; content: "]"; }
mjx-c.mjx-c1D450.TEX-I::before { padding: 0.442em 0.433em 0.011em 0px; content: "c"; }
mjx-c.mjx-c2248::before { padding: 0.483em 0.778em 0px 0px; content: "≈"; }
mjx-c.mjx-c2211.TEX-S1::before { padding: 0.75em 1.056em 0.25em 0px; content: "∑"; }
mjx-c.mjx-c1D445.TEX-I::before { padding: 0.683em 0.759em 0.021em 0px; content: "R"; }
mjx-c.mjx-c1D45D.TEX-I::before { padding: 0.442em 0.503em 0.194em 0px; content: "p"; }
mjx-mn { display: inline-block; text-align: left; }
mjx-c.mjx-c1D442.TEX-I::before { padding: 0.704em 0.763em 0.022em 0px; content: "O"; }
mjx-c.mjx-c1D45B.TEX-I::before { padding: 0.442em 0.6em 0.011em 0px; content: "n"; }
mjx-c.mjx-c32::before { padding: 0.666em 0.5em 0px 0px; content: "2"; }
mjx-c.mjx-c31::before { padding: 0.666em 0.5em 0px 0px; content: "1"; }
mjx-c.mjx-c2212::before { padding: 0.583em 0.778em 0.082em 0px; content: "−"; }
mjx-c.mjx-c22C5::before { padding: 0.31em 0.278em 0px 0px; content: "⋅"; }
mjx-c.mjx-c210E.TEX-I::before { padding: 0.694em 0.576em 0.011em 0px; content: "h"; }
mjx-c.mjx-c2E::before { padding: 0.12em 0.278em 0px 0px; content: "."; }
mjx-c.mjx-c33::before { padding: 0.665em 0.5em 0.022em 0px; content: "3"; }
mjx-c.mjx-c6C::before { padding: 0.694em 0.278em 0px 0px; content: "l"; }
mjx-c.mjx-c67::before { padding: 0.453em 0.5em 0.206em 0px; content: "g"; }
mjx-c.mjx-c2061::before { padding: 0px; content: ""; }
mjx-mtext { display: inline-block; text-align: left; }
mjx-mfrac { display: inline-block; text-align: left; }
mjx-frac { display: inline-block; vertical-align: 0.17em; padding: 0px 0.22em; }
mjx-frac[type="d"] { vertical-align: 0.04em; }
mjx-frac[delims] { padding: 0px 0.1em; }
mjx-frac[atop] { padding: 0px 0.12em; }
mjx-frac[atop][delims] { padding: 0px; }
mjx-dtable { display: inline-table; width: 100%; }
mjx-dtable > * { font-size: 2000%; }
mjx-dbox { display: block; font-size: 5%; }
mjx-num { display: block; text-align: center; }
mjx-den { display: block; text-align: center; }
mjx-mfrac[bevelled] > mjx-num { display: inline-block; }
mjx-mfrac[bevelled] > mjx-den { display: inline-block; }
mjx-den[align="right"], mjx-num[align="right"] { text-align: right; }
mjx-den[align="left"], mjx-num[align="left"] { text-align: left; }
mjx-nstrut { display: inline-block; height: 0.054em; width: 0px; vertical-align: -0.054em; }
mjx-nstrut[type="d"] { height: 0.217em; vertical-align: -0.217em; }
mjx-dstrut { display: inline-block; height: 0.505em; width: 0px; }
mjx-dstrut[type="d"] { height: 0.726em; }
mjx-line { display: block; box-sizing: border-box; min-height: 1px; height: 0.06em; border-top: 0.06em solid; margin: 0.06em -0.1em; overflow: hidden; }
mjx-line[type="d"] { margin: 0.18em -0.1em; }
mjx-mrow { display: inline-block; text-align: left; }
mjx-msup { display: inline-block; text-align: left; }
mjx-msqrt { display: inline-block; text-align: left; }
mjx-root { display: inline-block; white-space: nowrap; }
mjx-surd { display: inline-block; vertical-align: top; }
mjx-sqrt { display: inline-block; padding-top: 0.07em; }
mjx-sqrt > mjx-box { border-top: 0.07em solid; }
mjx-sqrt.mjx-tall > mjx-box { padding-left: 0.3em; margin-left: -0.3em; }
mjx-c.mjx-c41::before { padding: 0.716em 0.75em 0px 0px; content: "A"; }
mjx-c.mjx-c74::before { padding: 0.615em 0.389em 0.01em 0px; content: "t"; }
mjx-c.mjx-c65::before { padding: 0.448em 0.444em 0.011em 0px; content: "e"; }
mjx-c.mjx-c6E::before { padding: 0.442em 0.556em 0px 0px; content: "n"; }
mjx-c.mjx-c69::before { padding: 0.669em 0.278em 0px 0px; content: "i"; }
mjx-c.mjx-c6F::before { padding: 0.448em 0.5em 0.01em 0px; content: "o"; }
mjx-c.mjx-c1D444.TEX-I::before { padding: 0.704em 0.791em 0.194em 0px; content: "Q"; }
mjx-c.mjx-c2C::before { padding: 0.121em 0.278em 0.194em 0px; content: ","; }
mjx-c.mjx-c1D43E.TEX-I::before { padding: 0.683em 0.889em 0px 0px; content: "K"; }
mjx-c.mjx-c1D449.TEX-I::before { padding: 0.683em 0.769em 0.022em 0px; content: "V"; }
mjx-c.mjx-c3D::before { padding: 0.583em 0.778em 0.082em 0px; content: "="; }
mjx-c.mjx-c73::before { padding: 0.448em 0.394em 0.011em 0px; content: "s"; }
mjx-c.mjx-c66::before { padding: 0.705em 0.372em 0px 0px; content: "f"; }
mjx-c.mjx-c6D::before { padding: 0.442em 0.833em 0px 0px; content: "m"; }
mjx-c.mjx-c61::before { padding: 0.448em 0.5em 0.011em 0px; content: "a"; }
mjx-c.mjx-c78::before { padding: 0.431em 0.528em 0px 0px; content: "x"; }
mjx-c.mjx-c1D447.TEX-I::before { padding: 0.677em 0.704em 0px 0px; content: "T"; }
mjx-c.mjx-c221A::before { padding: 0.8em 0.853em 0.2em 0px; content: "√"; }
mjx-c.mjx-c1D458.TEX-I::before { padding: 0.694em 0.521em 0.011em 0px; content: "k"; }
mjx-c.mjx-c3A6::before { padding: 0.683em 0.722em 0px 0px; content: "Φ"; }
mjx-c.mjx-c1D719.TEX-I::before { padding: 0.694em 0.596em 0.205em 0px; content: "ϕ"; }
mjx-container[jax="CHTML"] { line-height: 0; }
mjx-container [space="1"] { margin-left: 0.111em; }
mjx-container [space="2"] { margin-left: 0.167em; }
mjx-container [space="3"] { margin-left: 0.222em; }
mjx-container [space="4"] { margin-left: 0.278em; }
mjx-container [space="5"] { margin-left: 0.333em; }
mjx-container [rspace="1"] { margin-right: 0.111em; }
mjx-container [rspace="2"] { margin-right: 0.167em; }
mjx-container [rspace="3"] { margin-right: 0.222em; }
mjx-container [rspace="4"] { margin-right: 0.278em; }
mjx-container [rspace="5"] { margin-right: 0.333em; }
mjx-container [size="s"] { font-size: 70.7%; }
mjx-container [size="ss"] { font-size: 50%; }
mjx-container [size="Tn"] { font-size: 60%; }
mjx-container [size="sm"] { font-size: 85%; }
mjx-container [size="lg"] { font-size: 120%; }
mjx-container [size="Lg"] { font-size: 144%; }
mjx-container [size="LG"] { font-size: 173%; }
mjx-container [size="hg"] { font-size: 207%; }
mjx-container [size="HG"] { font-size: 249%; }
mjx-container [width="full"] { width: 100%; }
mjx-box { display: inline-block; }
mjx-block { display: block; }
mjx-itable { display: inline-table; }
mjx-row { display: table-row; }
mjx-row > * { display: table-cell; }
mjx-mtext { display: inline-block; }
mjx-mstyle { display: inline-block; }
mjx-merror { display: inline-block; color: red; background-color: yellow; }
mjx-mphantom { visibility: hidden; }
mjx-assistive-mml { top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); user-select: none; position: absolute !important; padding: 1px 0px 0px !important; border: 0px !important; display: block !important; width: auto !important; overflow: hidden !important; }
mjx-assistive-mml[display="block"] { width: 100% !important; }
mjx-math { display: inline-block; text-align: left; line-height: 0; text-indent: 0px; font-style: normal; font-weight: normal; font-size: 100%; letter-spacing: normal; border-collapse: collapse; overflow-wrap: normal; word-spacing: normal; white-space: nowrap; direction: ltr; padding: 1px 0px; }
mjx-container[jax="CHTML"][display="true"] { display: block; text-align: center; margin: 1em 0px; }
mjx-container[jax="CHTML"][display="true"][width="full"] { display: flex; }
mjx-container[jax="CHTML"][display="true"] mjx-math { padding: 0px; }
mjx-container[jax="CHTML"][justify="left"] { text-align: left; }
mjx-container[jax="CHTML"][justify="right"] { text-align: right; }
mjx-mi { display: inline-block; text-align: left; }
mjx-c { display: inline-block; }
mjx-utext { display: inline-block; padding: 0.75em 0px 0.2em; }
mjx-mo { display: inline-block; text-align: left; }
mjx-stretchy-h { display: inline-table; width: 100%; }
mjx-stretchy-h > * { display: table-cell; width: 0px; }
mjx-stretchy-h > * > mjx-c { display: inline-block; transform: scaleX(1); }
mjx-stretchy-h > * > mjx-c::before { display: inline-block; width: initial; }
mjx-stretchy-h > mjx-ext { overflow: clip visible; width: 100%; }
mjx-stretchy-h > mjx-ext > mjx-c::before { transform: scaleX(500); }
mjx-stretchy-h > mjx-ext > mjx-c { width: 0px; }
mjx-stretchy-h > mjx-beg > mjx-c { margin-right: -0.1em; }
mjx-stretchy-h > mjx-end > mjx-c { margin-left: -0.1em; }
mjx-stretchy-v { display: inline-block; }
mjx-stretchy-v > * { display: block; }
mjx-stretchy-v > mjx-beg { height: 0px; }
mjx-stretchy-v > mjx-end > mjx-c { display: block; }
mjx-stretchy-v > * > mjx-c { transform: scaleY(1); transform-origin: left center; overflow: hidden; }
mjx-stretchy-v > mjx-ext { display: block; height: 100%; box-sizing: border-box; border: 0px solid transparent; overflow: visible clip; }
mjx-stretchy-v > mjx-ext > mjx-c::before { width: initial; box-sizing: border-box; }
mjx-stretchy-v > mjx-ext > mjx-c { transform: scaleY(500) translateY(0.075em); overflow: visible; }
mjx-mark { display: inline-block; height: 0px; }
mjx-texatom { display: inline-block; text-align: left; }
mjx-msub { display: inline-block; text-align: left; }
mjx-c::before { display: block; width: 0px; }
.MJX-TEX { font-family: MJXZERO, MJXTEX; }
.TEX-B { font-family: MJXZERO, MJXTEX-B; }
.TEX-I { font-family: MJXZERO, MJXTEX-I; }
.TEX-MI { font-family: MJXZERO, MJXTEX-MI; }
.TEX-BI { font-family: MJXZERO, MJXTEX-BI; }
.TEX-S1 { font-family: MJXZERO, MJXTEX-S1; }
.TEX-S2 { font-family: MJXZERO, MJXTEX-S2; }
.TEX-S3 { font-family: MJXZERO, MJXTEX-S3; }
.TEX-S4 { font-family: MJXZERO, MJXTEX-S4; }
.TEX-A { font-family: MJXZERO, MJXTEX-A; }
.TEX-C { font-family: MJXZERO, MJXTEX-C; }
.TEX-CB { font-family: MJXZERO, MJXTEX-CB; }
.TEX-FR { font-family: MJXZERO, MJXTEX-FR; }
.TEX-FRB { font-family: MJXZERO, MJXTEX-FRB; }
.TEX-SS { font-family: MJXZERO, MJXTEX-SS; }
.TEX-SSB { font-family: MJXZERO, MJXTEX-SSB; }
.TEX-SSI { font-family: MJXZERO, MJXTEX-SSI; }
.TEX-SC { font-family: MJXZERO, MJXTEX-SC; }
.TEX-T { font-family: MJXZERO, MJXTEX-T; }
.TEX-V { font-family: MJXZERO, MJXTEX-V; }
.TEX-VB { font-family: MJXZERO, MJXTEX-VB; }
mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c { font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A !important; }
@font-face { font-family: MJXZERO; src: url("lib/fonts/mathjax_zero.woff") format("woff"); }
@font-face { font-family: MJXTEX; src: url("lib/fonts/mathjax_main-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-B; src: url("lib/fonts/mathjax_main-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-I; src: url("lib/fonts/mathjax_math-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-MI; src: url("lib/fonts/mathjax_main-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-BI; src: url("lib/fonts/mathjax_math-bolditalic.woff") format("woff"); }
@font-face { font-family: MJXTEX-S1; src: url("lib/fonts/mathjax_size1-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S2; src: url("lib/fonts/mathjax_size2-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S3; src: url("lib/fonts/mathjax_size3-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S4; src: url("lib/fonts/mathjax_size4-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-A; src: url("lib/fonts/mathjax_ams-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-C; src: url("lib/fonts/mathjax_calligraphic-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-CB; src: url("lib/fonts/mathjax_calligraphic-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-FR; src: url("lib/fonts/mathjax_fraktur-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-FRB; src: url("lib/fonts/mathjax_fraktur-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-SS; src: url("lib/fonts/mathjax_sansserif-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-SSB; src: url("lib/fonts/mathjax_sansserif-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-SSI; src: url("lib/fonts/mathjax_sansserif-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-SC; src: url("lib/fonts/mathjax_script-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-T; src: url("lib/fonts/mathjax_typewriter-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-V; src: url("lib/fonts/mathjax_vector-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-VB; src: url("lib/fonts/mathjax_vector-bold.woff") format("woff"); }
mjx-c.mjx-c1D459.TEX-I::before { padding: 0.694em 0.298em 0.011em 0px; content: "l"; }
mjx-c.mjx-c1D45C.TEX-I::before { padding: 0.441em 0.485em 0.011em 0px; content: "o"; }
mjx-c.mjx-c1D454.TEX-I::before { padding: 0.442em 0.477em 0.205em 0px; content: "g"; }
mjx-c.mjx-c28::before { padding: 0.75em 0.389em 0.25em 0px; content: "("; }
mjx-c.mjx-c1D441.TEX-I::before { padding: 0.683em 0.888em 0px 0px; content: "N"; }
mjx-c.mjx-c2F::before { padding: 0.75em 0.5em 0.25em 0px; content: "/"; }
mjx-c.mjx-c1D451.TEX-I::before { padding: 0.694em 0.52em 0.01em 0px; content: "d"; }
mjx-c.mjx-c1D453.TEX-I::before { padding: 0.705em 0.55em 0.205em 0px; content: "f"; }
mjx-c.mjx-c29::before { padding: 0.75em 0.389em 0.25em 0px; content: ")"; }
</style><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="4_深度学习"><p dir="auto">4_深度学习</p></h1><div><p dir="auto"><a href="?query=tag:%E8%AE%A1%E7%AE%97%E6%9C%BA" class="tag" target="_blank" rel="noopener">#计算机</a> <a href="?query=tag:%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD" class="tag" target="_blank" rel="noopener">#人工智能</a> <a href="?query=tag:%E5%AE%9E%E7%94%A8" class="tag" target="_blank" rel="noopener">#实用</a> <a href="?query=tag:%E5%B0%8F%E8%AE%B0" class="tag" target="_blank" rel="noopener">#小记</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/638660013/answer/3422970337" rel="noopener" class="external-link" href="https://www.zhihu.com/question/638660013/answer/3422970337" target="_blank">2024年，深度学习，你心目中的top10算法是什么？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://zh.d2l.ai/index.html" rel="noopener" class="external-link" href="https://zh.d2l.ai/index.html" target="_blank">《动手学深度学习》 — 动手学深度学习 2.0.0 documentation</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/mli/paper-reading" rel="noopener" class="external-link" href="https://github.com/mli/paper-reading" target="_blank">GitHub - mli/paper-reading: 深度学习经典、新论文逐段精读</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap" rel="noopener" class="external-link" href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap" target="_blank">GitHub - floodsung/Deep-Learning-Papers-Reading-Roadmap: Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/exacity/deeplearningbook-chinese" rel="noopener" class="external-link" href="https://github.com/exacity/deeplearningbook-chinese" target="_blank">GitHub - exacity/deeplearningbook-chinese: Deep Learning Book Chinese Translation（花书）</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 深度学习的核心是反向传播和梯度下降. SGD yyds Adam yyds </span></li>
<li data-line="1" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 如果把 DL 放在原有 ML 的pipeline 当中，DL 真正革命的是自动化的another word端到端的特征提取和特征筛选。两者结合在一起形成表征学习(representation learning)。 e.g.卷积核，注意力权重</span></li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> DL 的最重要的特点就是可以提取 hidden features. 通过hidden features 提取然后用 MLP 超强的建模拟合能力完成各种抽象程度超高的任务. 完成一种表征空间映射到另一个空间的建模。 e.g.理解语言图片生成图片视频</span></li>
</ul></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=hvmTCesa30c" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=hvmTCesa30c" target="_blank"># 深度学习简史，从感知机到lenet到vgg到resnet到clip</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1SZ421a7v3/?spm_id_from=333.1365.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1SZ421a7v3/?spm_id_from=333.1365.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">何恺明教授在MIT的第一课：卷积神经网络_哔哩哔哩_bilibili</a></p>
<ul>
<li dir="auto">1957 - 细胞感知机(Perceptron) 只能处理线性问题</li>
<li dir="auto">1969 - 发现感知机不能处理XOR问题，陷入第一次寒冬</li>
<li dir="auto">1974 - 第一次发明了反向传播算法，但没有引起注意</li>
<li dir="auto">1986 - 重新发明了反向传播算法，引发深度学习的热潮</li>
<li dir="auto">1990 - SVM的发明和深度学习的一些问题(e.g.梯度爆炸)，陷入第二次寒冬</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>1998 - LeNet成功运用在MINST手写数据集, 
<ul>
<li dir="auto">introduced convolution, pooling, full-connected layer, train by backprop end to end</li>
</ul>
</li>
<li dir="auto">2009 - ImageNet第一次举办</li>
<li dir="auto">2012 - AlexNet在ImageNet上大放异彩</li>
<li dir="auto">2014 - VGG证明网络越深越好</li>
<li dir="auto">2014 - GoogLeNet 发明Inception 发明多个并行的卷积层和1x1卷积</li>
<li dir="auto">2015 - ResNet </li>
<li dir="auto">2016 - DenseNet 每两个Block一个Skip Connection</li>
<li dir="auto">2017 - Xception改进了Inception将Max Pooling放在网络后部分</li>
<li dir="auto">2017 - ResNext引入分组卷积(多头机制)</li>
<li dir="auto">2017 - MobileNets 引入了深度可变卷积</li>
<li dir="auto">2018 - NasNet 引入了可变多头机制</li>
<li dir="auto">2019 - EfficentNet 可以灵活改变网络的深度宽度和channel</li>
<li dir="auto">2021 - CLIP引入对比学习鼓励模型将图像和描述该图像的文本表示靠近</li>
</ul></div></div></div><div class="heading-wrapper"><h2 data-heading="深度学习名人堂" dir="auto" class="heading" id="深度学习名人堂"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>深度学习名人堂</h2><div class="heading-children"><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto">谁是何恺明？他对深度学习做出了什么贡献？</p>
<p dir="auto"> <strong>何恺明（Kaiming He）</strong>是一位在计算机视觉和深度学习领域具有重要影响的科学家。他在深度学习，尤其是在计算机视觉方面的贡献，对推动该领域的发展起到了重要作用。</p>
<p dir="auto">何恺明最为人熟知的工作之一是他对卷积神经网络（CNNs）结构改进的贡献。他参与开发了几种重要的网络结构和训练方法，包括：</p>
<ol>
<li dir="auto">
<p><strong>残差网络（Residual Networks, ResNets）</strong>：在2015年的《深度残差网络》论文中，何恺明及其合作者提出了残差网络架构。这一架构通过引入“残差块”来解决深层网络训练难题，使得可以训练出比以往更深的神经网络而不会出现性能下降问题。ResNet极大地推动了图像识别、物体检测等领域的发展，并被广泛应用于商业和研究中。</p>
</li>
<li dir="auto">
<p><strong>特征金字塔网络（Feature Pyramid Networks, FPNs）</strong>：在目标检测任务中，特征金字塔结构可以有效地处理不同尺寸的物体。FPN通过高效地结合不同层次的特征表示，显著提高了目标检测和分割任务的性能。</p>
</li>
<li dir="auto">
<p><strong>Mask R-CNN</strong>：这是一种用于目标检测和实例分割任务的框架，基于Faster R-CNN而来，并加入了一个分支以输出物体掩码。Mask R-CNN简单、灵活且性能强大，对实例分割领域产生了深远影响。</p>
</li>
</ol>
<p dir="auto">除此之外，何恺明还参与了其他多项重要研究工作，并且常常在顶级会议和期刊如CVPR、ICCV、NeurIPS、ECCV等发表论文。他所参与开发的技术已被广泛应用于多个领域，并促进了计算机视觉技术在实际应用中的落地。</p>
<p dir="auto">何恺明目前是Facebook AI Research（FAIR）团队的一员，在业界享有很高声誉。通过持续地贡献高质量研究成果，何恺明为推动人工智能和机器学习技术向前发展做出了显著贡献。</p></div></div></div><div><p dir="auto">深度学习领域有什么奠基性的学者? 他们各自都做了什么奠基性的工作?</p></div><div><p dir="auto"> 深度学习领域有很多奠基性的学者，以下是其中的一部分：</p></div><div><ol>
<li data-line="0" dir="auto">
<p>杨立昆（Yann LeCun）：现任Facebook首席人工智能科学家，他在卷积神经网络（Convolutional Neural Networks, CNN）的研发上做出了巨大贡献，这种网络是目前图像识别等领域广泛使用的工具。他的这项工作也为他赢得了2018年的图灵奖。</p>
</li>
<li data-line="2" dir="auto">
<p>吴恩达（Andrew Ng）：吴恩达是斯坦福大学的教授，他的在线课程“机器学习”引导了数百万人进入了这个领域。他还是Google大脑（Google Brain）项目的创始人，该项目对深度学习的商业应用做出了巨大贡献。此外，他还是Coursera的创始人之一。</p>
</li>
<li data-line="4" dir="auto">
<p>杰弗里·辛顿（Geoffrey Hinton）：被誉为“深度学习之父”，他在神经网络和深度学习领域的研究为当前的人工智能热潮打下了基础。他的研究包括反向传播、深度信念网络和长短期记忆网络（LSTM）等。</p>
</li>
<li data-line="6" dir="auto">
<p>Yoshua Bengio：他是加拿大蒙特利尔大学的教授，也是深度学习领域的先驱者之一。他的研究包括反向传播、深度学习和人工神经网络等。</p>
</li>
<li data-line="8" dir="auto">
<p>Ian Goodfellow：他是深度学习领域的重要人物，是生成对抗网络（Generative Adversarial Networks, GANs）的创造者，这是一种能够生成逼真图像的人工智能模型。</p>
</li>
<li data-line="10" dir="auto">
<p>Juergen Schmidhuber：他在循环神经网络（Recurrent Neural Networks, RNNs）和长短期记忆网络（Long Short-Term Memory, LSTM）的研发上做出了重要贡献。</p>
</li>
</ol></div><div><p dir="auto">以上这些学者的工作都对深度学习领域产生了深远的影响，推动了人工智能的发展。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="基本算法" dir="auto" class="heading" id="基本算法"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>基本算法</h2><div class="heading-children"><div class="heading-wrapper"><h3 data-heading="梯度下降 &amp; 反向传播" dir="auto" class="heading" id="梯度下降_&amp;_反向传播"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>梯度下降 &amp; 反向传播</h3><div class="heading-children"><div><img src="https://cdn.sa.net/2024/06/05/sJ7x6MuVtqg5rjR.gif" referrerpolicy="no-referrer"></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;t=322s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;t=322s" target="_blank">The spelled-out intro to neural networks and backpropagation: building micrograd - YouTube</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=i7V1ci9dkxg" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=i7V1ci9dkxg" target="_blank">梯度下降是什么？为什么你不用知道具体原理？ - YouTube</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html" rel="noopener" class="external-link" href="https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html" target="_blank">4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0 documentation</a></p></div><div><ul>
<li data-line="0" dir="auto">前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。</li>
<li data-line="1" dir="auto">反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。</li>
<li data-line="2" dir="auto">在训练深度学习模型时，前向传播和反向传播是相互依赖的。</li>
<li data-line="3" dir="auto">训练比预测需要更多的内存。</li>
<li data-line="4" dir="auto">(现在的)反向传播算法是由保罗·韦贝罗斯在1986年发明的。</li>
<li data-line="5" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 深度学习试图解决的是一个最优化问题，数学里叫<del>凸优化(Convex Optimization)</del>
</span><ul>
<li data-line="6" dir="auto">凸优化是一种特殊类型的优化问题，其中目标函数和约束都是凸的。在数学中，一个函数被称为凸函数，如果其定义域是一个凸集，并且对于定义域中的任何两点，函数在这两点之间的线段上的值总是小于或等于线段端点处的值。这种性质使得凸优化问题相对易于解决，因为它们有一个全局最优解，而且我们可以使用有效的算法找到这个解。</li>
</ul>
</li>
<li data-line="7" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 梯度下降方法是一种常见的用于解决优化问题的迭代方法，特别是在机器学习和深度学习中。在每一步，梯度下降方法都会沿着目标函数的负梯度方向（即函数下降最快的方向）更新参数，直到找到函数的最小值</span></li>
<li data-line="8" dir="auto">对于凸优化问题，梯度下降是一种非常有效的方法，因为我们知道只有一个全局最小值，而且梯度下降总是朝向这个最小值移动。然而，对于非凸函数，梯度下降可能会陷入局部最小值，并不能找到全局最小值。</li>
<li data-line="9" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 反向传播的基本思想是使用链式法则来计算损失函数关于网络参数的梯度。</span></li>
<li data-line="10" dir="auto">训练神经网络的目标通常是通过调整网络的参数（即神经元之间的权重和偏置）来最小化某个损失函数。损失函数衡量的是网络的输出与实际目标值之间的差距。为了找到使损失函数最小化的参数，我们需要知道每个参数的变化如何影响损失函数的值，这就需要计算损失函数关于每个参数的梯度。</li>
<li data-line="11" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>反向传播算法的步骤大致如下：
<ol>
<li data-line="12" dir="auto"><strong>前向传播</strong>：输入样本传递通过网络，生成预测输出，并计算出损失函数的值。</li>
<li data-line="13" dir="auto"><strong>反向传播</strong>：从输出层开始，计算损失函数关于每一层的梯度，并将这些梯度传递到前一层。这就是“反向传播”名字的来源。</li>
<li data-line="14" dir="auto"><strong>权重更新</strong>：使用这些梯度来更新网络的权重和偏置，通常使用一种优化算法，如梯度下降。<br>
这个过程在每个训练样本或者每个训练批次上重复进行，直到网络的性能达到满意的水平。</li>
</ol>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="过拟合 &amp; 欠拟合" dir="auto" class="heading" id="过拟合_&amp;_欠拟合"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>过拟合 &amp; 欠拟合</h3><div class="heading-children"><div><p dir="auto"><img alt="Fetching Title#1050" src="https://zh.d2l.ai/_images/capacity-vs-error.svg" referrerpolicy="no-referrer"></p></div></div></div><div class="heading-wrapper"><h3 data-heading="数值稳定性和参数初始化" dir="auto" class="heading" id="数值稳定性和参数初始化"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>数值稳定性和参数初始化</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html" rel="noopener" class="external-link" href="https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html" target="_blank">4.8. 数值稳定性和模型初始化 — 动手学深度学习 2.0.0 documentation</a></p></div><div><ul>
<li data-line="0" dir="auto">梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。</li>
<li data-line="1" dir="auto">需要用启发式的初始化方法来确保初始梯度既不太大也不太小。</li>
<li data-line="2" dir="auto">ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</li>
<li data-line="3" dir="auto">随机初始化是保证在进行优化前打破对称性的关键。</li>
<li data-line="4" dir="auto">Xavier初始化表明，对于每一层，输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="优化器" dir="auto" class="heading" id="优化器"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>优化器</h3><div class="heading-children"><div><blockquote dir="auto">
<p>直接 Adma + Cos LR Schedule</p>
</blockquote></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 梯度下降</p>
</span></li>
<li data-line="1" dir="auto">
<p>简单的梯度下降无法处理很多常见，比如损失函数的鞍部</p>
</li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> 额外技巧</p>
</span></li>
</ul></div><div dir="ltr" style="overflow-x: auto;"><table>
<thead>
<tr>
<th dir="ltr">优化器</th>
<th dir="ltr">简单原理</th>
<th dir="ltr">参数</th>
<th dir="ltr">额外技巧</th>
<th dir="ltr">适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td dir="ltr">SGD</td>
<td dir="ltr">随机挑一部分数据(现在一般是随机挑一个 batch所以也称为mini-batch方法)，按梯度反方向更新参数，固定学习率.</td>
<td dir="ltr">学习率 <code>η</code></td>
<td dir="ltr">bootstrap</td>
<td dir="ltr">小规模数据集，稀疏数据</td>
</tr>
<tr>
<td dir="ltr">Momentum</td>
<td dir="ltr">在SGD基础上考虑历史梯度，加速收敛</td>
<td dir="ltr">学习率 <code>η</code>，动量 <code>γ</code></td>
<td dir="ltr">动量技术，累积历史梯度</td>
<td dir="ltr">大规模深度学习，适用于大多数场景</td>
</tr>
<tr>
<td dir="ltr">Nesterov</td>
<td dir="ltr">在动量基础上提前计算梯度，预知下一步位置</td>
<td dir="ltr">学习率 <code>η</code>，动量 <code>γ</code></td>
<td dir="ltr">Nesterov加速梯度</td>
<td dir="ltr">神经网络，需要快速收敛</td>
</tr>
<tr>
<td dir="ltr">Adagrad</td>
<td dir="ltr">自适应调整各参数的学习率，便于稀疏数据</td>
<td dir="ltr">学习率 <code>η</code>，累加梯度平方</td>
<td dir="ltr">自适应学习率</td>
<td dir="ltr">处理稀疏数据的问题</td>
</tr>
<tr>
<td dir="ltr">Adadelta</td>
<td dir="ltr">RMSprop的一个扩展，它在计算学习率的时候不仅考虑了过去所有梯度的平方的平均值，还对参数更新的变化量也进行平均，从而减少学习率的急剧下降。</td>
<td dir="auto"></td>
<td dir="ltr">自适应学习率</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">RMSprop</td>
<td dir="ltr">改进Adagrad，滑动平均梯度平方，防止学习率急剧下降</td>
<td dir="ltr">学习率 <code>η</code>，衰减率 <code>β</code></td>
<td dir="ltr">移动平均梯度平方</td>
<td dir="ltr">神经网络，非稳定梯度的问题</td>
</tr>
<tr>
<td dir="ltr">Adam</td>
<td dir="ltr">结合Momentum和RMSprop，做一阶和二阶矩估计的自适应学习率</td>
<td dir="ltr">学习率 <code>η</code>，动量 <code>β1</code>，衰减率 <code>β2</code></td>
<td dir="ltr">动量和自适应学习率</td>
<td dir="ltr">深度学习中的多数场景</td>
</tr>
<tr>
<td dir="ltr">BFGS</td>
<td dir="ltr">使用梯度信息和近似海森矩阵，更新参数</td>
<td dir="ltr">近似海森矩阵</td>
<td dir="ltr">拟牛顿方法</td>
<td dir="ltr">函数优化，小规模数据</td>
</tr>
<tr>
<td dir="ltr">Newton's Method</td>
<td dir="ltr">使用二阶导数信息（海森矩阵），实现快速收敛</td>
<td dir="ltr">海森矩阵</td>
<td dir="ltr">二阶方法</td>
<td dir="ltr">函数优化，凸问题</td>
</tr>
<tr>
<td dir="ltr">Genetic Algorithm（GA）</td>
<td dir="ltr">启用自然选择原理，通过交叉突变等实现全局搜索</td>
<td dir="ltr">种群大小，交叉率，变异率</td>
<td dir="ltr">演化算法</td>
<td dir="ltr">复杂优化问题，全局搜索</td>
</tr>
<tr>
<td dir="ltr">Particle Swarm Optimization（PSO）</td>
<td dir="ltr">粒子群模拟鸟群狩猎行为，搜索最优解</td>
<td dir="ltr">粒子数，个体和社会学习因子</td>
<td dir="ltr">群体智能方法</td>
<td dir="ltr">多峰值问题，全局最优</td>
</tr>
</tbody>
</table></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/323747423/answer/2576604040" rel="noopener" class="external-link" href="https://www.zhihu.com/question/323747423/answer/2576604040" target="_blank">如何理解Adam算法(Adaptive Moment Estimation)？ - 知乎</a></p></div><div><p dir="auto"><img alt="lr.png" src="https://cdn.sa.net/2024/04/12/jUk6e3tn9Tq5uLA.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto"><img alt="local_minima.png" src="https://cdn.sa.net/2024/04/12/gGlHIW1atNzVdb3.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto"><img alt="gradient.png" src="https://cdn.sa.net/2024/04/12/47KZPnfkexdgSVJ.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto"><img alt="batch.png" src="https://cdn.sa.net/2024/04/12/xVZaYLTvUhuPDXJ.png" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0" dir="auto">梯度下降</li>
<li data-line="1" dir="auto">SGD</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="线性层 + 激活函数" dir="auto" class="heading" id="线性层_+_激活函数"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>线性层 + 激活函数</h3><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p dir="auto"><img src="https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2424009416f21db643e21_Group%20807.jpg" referrerpolicy="no-referrer"></p>
<p dir="auto">神经网络中为什么要有激活函数?</p>
<p dir="auto"> 神经网络中的激活函数有两个主要作用：</p>
<ol>
<li dir="auto">
<p><strong>引入非线性因素</strong>：由于大多数真实世界的数据都是非线性的，所以我们需要激活函数来引入非线性因素，使得神经网络可以适应更复杂的数据，提高模型的预测能力。如果没有激活函数，无论神经网络有多少层，其输出都是输入的线性组合，这样的神经网络模型无法处理复杂的数据模式。</p>
</li>
<li dir="auto">
<p><strong>模拟生物神经元的阈值触发机制</strong>：神经元在接收到的信号强度超过某个阈值时才会被激活，这种机制通过激活函数得以在神经网络中实现。</p>
</li>
</ol>
<p dir="auto">总的来说，激活函数的存在使得神经网络更具有通用性和适应性，能够处理更复杂的问题。</p></div></div></div><div><p dir="auto"><img src="https://media.licdn.com/dms/image/D4D12AQH2F3GJ9wen_Q/article-cover_image-shrink_720_1280/0/1688885174323?e=2147483647&amp;v=beta&amp;t=gFWxErTLLWBc6iRWDxCBRxkdJ7ob24cmjWZAOuKN9o4" referrerpolicy="no-referrer"></p></div><div><blockquote dir="auto">
<p>下面所有的激活函数都是为了解决非线性问题，因为如果网络中所有层都是线性的，那么无论网络有多少层，最后都可以被简化为一个线性函数。</p>
</blockquote></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html" rel="noopener" class="external-link" href="https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html" target="_blank">4.1. 多层感知机 — 动手学深度学习 2.0.0 documentation</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> ReLU（Rectified Linear Unit) <span style="background:#fff88f">最常用</span></p>
</span><ul>
<li data-line="1" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>优势
<ul>
<li data-line="2" dir="auto">计算简单：ReLU函数只需要判断输入是否大于0，计算复杂度非常低。</li>
<li data-line="3" dir="auto">解决梯度消失问题：在反向传播过程中，ReLU函数的梯度要么为0，要么为1，不会出现梯度消失的问题，有助于深度神经网络的训练。</li>
<li data-line="4" dir="auto">稀疏激活性：ReLU会使部分神经元的输出为0，这样的结果使得神经网络的激活具有稀疏性，并且可以减少神经元之间的依赖。</li>
</ul>
</li>
<li data-line="5" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>劣势
<ul>
<li data-line="6" dir="auto">"死亡"ReLU问题：在训练过程中，有可能部分神经元始终不被激活，导致对应的参数无法更新(传递到激活函数的数值非常不符合正态分布)。一旦出现这种情况，这些神经元将对任何数据的预测都没有贡献，即出现了"死亡"ReLU问题。</li>
<li data-line="7" dir="auto">输出不以0为中心：ReLU函数的输出是非负的，这可能会影响模型的训练效果。</li>
</ul>
</li>
<li data-line="8" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>动机
<ul>
<li data-line="9" dir="auto">ReLU激活函数是为了解决神经网络训练过程中的一些问题而提出的。在ReLU之前，常用的激活函数如sigmoid或tanh，它们在输入值较大或较小时的梯度接近于0，导致梯度下降训练非常缓慢，这就是所谓的梯度消失问题。ReLU激活函数的提出，有效地解决了这个问题。</li>
</ul>
</li>
</ul>
</li>
<li data-line="10" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> Leaky ReLU</p>
</span><ul>
<li data-line="11" dir="auto">原理：Leaky ReLU是对ReLU的改进，解决了神经元“死亡”的问题，当x&lt;0时，有一个小的梯度，而不是完全的0。</li>
<li data-line="12" dir="auto">优点：解决了ReLU的神经元“死亡”问题；相比于ReLU函数，Leaky ReLU全程都有梯度，训练时的收敛速度会比较快。</li>
<li data-line="13" dir="auto">缺点：Leaky ReLU的参数通常都是经验设置，可能需要通过交叉验证来进行选择。</li>
</ul>
</li>
<li data-line="15" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> Parameterized ReLU (pReLU)</p>
</span><ul>
<li data-line="16" dir="auto"><a data-tooltip-position="top" aria-label="https://zh.d2l.ai/chapter_references/zreferences.html#id59" rel="noopener" class="external-link" title="He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). Delving deep into rectifiers: surpassing human-level performance on imagenet classification. Proceedings of the IEEE international conference on computer vision (pp. 1026–1034)." href="https://zh.d2l.ai/chapter_references/zreferences.html#id59" target="_blank">He&nbsp;<em>et al.</em>, 2015</a></li>
<li data-line="17" dir="auto">在负数部分添加了一个线性项</li>
</ul>
</li>
<li data-line="19" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> GELU（Gaussian Error Linear Units）</p>
</span><ul>
<li data-line="20" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>优势
<ul>
<li data-line="21" dir="auto">GELU激活函数在负数部分的处理上，相比ReLU等其他激活函数，提供了更为平滑的梯度，可以缓解梯度消失问题。</li>
<li data-line="22" dir="auto">GELU相比于其他激活函数，如sigmoid和tanh, 在正数部分提供近似线性的映射，这样可以减少模型的复杂度，也使得模型的训练更加稳定。</li>
<li data-line="23" dir="auto">在一些神经网络模型中，<span style="background:#fff88f">如Transformer等，GELU表现出了更好的性能</span>。</li>
</ul>
</li>
<li data-line="24" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>劣势
<ul>
<li data-line="25" dir="auto">GELU的计算复杂度相比ReLU等激活函数要高，因为它包含了tanh和指数运算。</li>
<li data-line="26" dir="auto">虽然GELU在一些模型上有更好的性能，但并非在所有情况下都是最优的。</li>
</ul>
</li>
<li data-line="27" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>动机
<ul>
<li data-line="28" dir="auto">GELU是为了解决ReLU等激活函数在处理负数输入时出现的梯度消失问题，同时保持在正数部分的近似线性映射，以此来提高模型的训练稳定性和性能。</li>
</ul>
</li>
</ul>
</li>
</ul></div><div><blockquote dir="auto">
<p>Sigmoid和Tanh函数都是早期神经网络中最常用的激活函数，它们的主要目的是为了解决线性函数无法处理的复杂问题，如异或问题（XOR）。同时，由于它们都能将输入值压缩到一定范围内，因此在处理一些需要输出在特定范围内的问题时非常有用。</p>
</blockquote></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">%</span> Sigmoid函数：
</span><ul>
<li data-line="1" dir="auto">原理：Sigmoid函数可以将任何实数映射到（0，1）之间，其输出可以看作概率，用于二分类问题。</li>
<li data-line="2" dir="auto">优点：输出的结果在0和1之间，适合用于输出层，值域范围有限，优化稳定。</li>
<li data-line="3" dir="auto">缺点：存在梯度消失问题，对于深层网络训练困难；计算复杂，训练时间长；输出不以0为中心。</li>
</ul>
</li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">%</span> Tanh函数
</span><ul>
<li data-line="5" dir="auto">原理：Tanh函数是Sigmoid函数的变体，将实数映射到（-1，1）之间。</li>
<li data-line="6" dir="auto">优点：输出以0为中心，解决了Sigmoid函数的这个问题。</li>
<li data-line="7" dir="auto">缺点：同样存在梯度消失问题，对于深度神经网络存在困难。</li>
</ul>
</li>
<li data-line="8" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">%</span> Softmax函数
</span><ul>
<li data-line="9" dir="auto">原理：Softmax函数可以看做是多分类的Sigmoid，将任意实数映射成为一个概率分布。</li>
<li data-line="10" dir="auto">优点：适合多分类问题。</li>
<li data-line="11" dir="auto">缺点：由于Softmax的输出是一个概率分布，因此其输出节点之间存在依赖关系，这使得模型的解释性变差。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="正则化" dir="auto" class="heading" id="正则化"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>正则化</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6" rel="noopener" class="external-link" href="https://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6" target="_blank">Different Normalization Layers in Deep Learning | by Nilesh Vijayrania | Towards Data Science</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>数据增强(Data Augmentation)</strong>
</span><ul>
<li data-line="1" dir="auto"><strong>原理</strong>：通过对训练数据进行一系列的变换，如旋转、缩放、剪切等，生成新的训练样本，以增加训练数据的多样性，并防止过拟合。</li>
<li data-line="2" dir="auto"><strong>优劣势</strong>：数据增强可以有效地防止过拟合，提高模型的泛化能力，但是数据增强需要花费额外的计算资源。</li>
<li data-line="3" dir="auto"><strong>适用场景</strong>：主要用于图像、音频等领域，尤其是当训练数据较少时。</li>
<li data-line="4" dir="auto"><strong>提出动机</strong>：为了增加训练数据的多样性，防止过拟合。</li>
<li data-line="5" dir="auto"><strong>实际效果</strong>：在实际应用中，数据增强可以有效地防止过拟合，提高模型的泛化能力。</li>
</ul>
</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/680237923?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1735297419670257664&amp;utm_source=io.raindrop.raindropio" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/680237923?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1735297419670257664&amp;utm_source=io.raindrop.raindropio" target="_blank"># L2 正则化比大多数人想象的更加神奇</a><br>
<a data-tooltip-position="top" aria-label="https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html" rel="noopener" class="external-link" href="https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html" target="_blank">4.5. 权重衰减 — 动手学深度学习 2.0.0 documentation</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">@</span>  <strong>L1/L2正则化</strong>
</span><ul>
<li data-line="1" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>原理</strong>
<ul>
<li data-line="2" dir="auto">L1和L2正则化是一种用于防止模型过拟合的技术。它们通过在损失函数中添加一个惩罚项来实现。这个惩罚项会随着模型复杂度的增加而增加，从而鼓励模型选择更简单的解释数据的方式。</li>
<li data-line="3" dir="auto">L1正则化: L1正则化的惩罚项是模型系数的绝对值之和，即<code>||w||_1 = Σ|wi|</code>。L1正则化会产生稀疏的权重，即许多权重为0，这使得模型更加简单，易于解释，也有助于特征选择。</li>
<li data-line="4" dir="auto">L2正则化: L2正则化的惩罚项是模型系数的平方和，即 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D6F4 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container></span>。L2正则化会产生较小的权重，但不会使它们为0，这使得模型更加稳定，对于噪声的抗性更强。</li>
</ul>
</li>
<li data-line="5" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>优劣势</strong>
<ul>
<li data-line="6" dir="auto">L1可以产生<strong>稀疏解</strong>，有特征选择作用，但是L1正则化的优化问题非光滑，难以求解。L2可以防止过拟合，但是不能产生稀疏解。</li>
<li data-line="7" dir="auto">L1正则化的优点是可以产生稀疏的模型，有助于特征选择。但是L1正则化的优化问题非光滑，难以求解, 导致可能会忽略一些有用的特征。</li>
<li data-line="8" dir="auto">L2正则化的优点是可以保留所有的特征，对于噪声的抗性更强。缺点是可能会让模型变得过于复杂，不易于解释。</li>
</ul>
</li>
<li data-line="9" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>适用场景</strong>：
<ul>
<li data-line="10" dir="auto">当特征数量很大，且有很多无关的特征时，可以使用L1正则化进行特征选择。<br>
-当特征都有一定的相关性，且希望模型对噪声有较强的抗性时，可以使用L2正则化。</li>
</ul>
</li>
<li data-line="12" dir="auto"><strong>提出动机</strong>：L1和L2正则化的提出是为了防止模型过拟合，通过添加一个惩罚项来限制模型的复杂度，使得模型更加稳健，对于噪声的抗性更强。</li>
<li data-line="13" dir="auto"><strong>实际效果</strong>：在实际应用中，L1/L2正则化可以有效地防止过拟合，提高模型的泛化能力。在某些任务中，L1正则化可以有效地进行特征选择，提高模型的解释性. 在其他任务中，L2正则化可以提高模型的稳定性和抗噪声能力</li>
</ul>
</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html" rel="noopener" class="external-link" href="https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html" target="_blank">4.6. 暂退法（Dropout） — 动手学深度学习 2.0.0 documentation</a></p></div><div><ul>
<li data-line="0" dir="auto">
<p>暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。</p>
</li>
<li data-line="1" dir="auto">
<p>暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。</p>
</li>
<li data-line="2" dir="auto">
<p>暂退法将活性值ℎ替换为具有期望值ℎ的随机变量。</p>
</li>
<li data-line="3" dir="auto">
<p>暂退法仅在训练期间使用。</p>
</li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>Dropout</strong></p>
</span><ul>
<li data-line="5" dir="auto"><strong>原理</strong>：在每次迭代训练过程中，随机地将部分神经元的输出设置为0。</li>
<li data-line="6" dir="auto"><strong>优劣势</strong>：Dropout可以有效地防止过拟合，提高模型的泛化能力，但是Dropout也会使得模型的收敛速度变慢。</li>
<li data-line="7" dir="auto"><strong>适用场景</strong>：用于防止神经网络的过拟合。</li>
<li data-line="8" dir="auto"><strong>提出动机</strong>：为了防止神经网络的过拟合，提高模型的泛化能力。</li>
<li data-line="9" dir="auto"><strong>实际效果</strong>：在实际应用中，Dropout可以有效地防止过拟合，提高模型的泛化能力。</li>
</ul>
</li>
<li data-line="12" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>池化层(Pooling layer)</strong></p>
</span><ul>
<li data-line="13" dir="auto">Max Pooling 最大池化, 可检测边缘</li>
<li data-line="14" dir="auto">Average Pooling, 平均池化可平滑图像</li>
<li data-line="15" dir="auto"><strong>原理</strong>：池化层是一种降采样操作，常见的有最大池化和平均池化。最大池化是取滑动窗口内的最大值作为输出，平均池化是取滑动窗口内的平均值作为输出。</li>
<li data-line="16" dir="auto"><strong>优劣势</strong>：池化层可以有效地降低数据的维度，防止过拟合，提高模型的泛化能力，但是也会丢失一部分信息。</li>
<li data-line="17" dir="auto"><strong>适用场景</strong>：用于卷积神经网络中，降低数据的维度，防止过拟合。</li>
<li data-line="18" dir="auto"><strong>提出动机</strong>：为了降低数据的维度，防止过拟合，提高模型的泛化能力。</li>
<li data-line="19" dir="auto"><strong>实际效果</strong>：在实际应用中，池化层可以有效地降低数据的维度，防止过拟合，提高模型的泛化能力。</li>
</ul>
</li>
<li data-line="21" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>Early stopping</strong></p>
</span><ul>
<li data-line="22" dir="auto"><strong>原理</strong>：在训练过程中，当验证集的损失函数值不再下降时，停止训练。</li>
<li data-line="23" dir="auto"><strong>优劣势</strong>：Early stopping可以有效地防止过拟合，提高模型的泛化能力，但是Early stopping的提前停止点的选择有一定的难度。</li>
<li data-line="24" dir="auto"><strong>适用场景</strong>：用于防止神经网络的过拟合。</li>
<li data-line="25" dir="auto"><strong>提出动机</strong>：为了防止神经网络的过拟合，提高模型的泛化能力。</li>
<li data-line="26" dir="auto"><strong>实际效果</strong>：在实际应用中，Early stopping可以有效地防止过拟合，提高模型的泛化能力。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="规范化(BN, LN, IN, GN, RMSNorm)" dir="auto" class="heading" id="规范化(BN,_LN,_IN,_GN,_RMSNorm)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>规范化(BN, LN, IN, GN, RMSNorm)</h3><div class="heading-children"><div>  <img src="https://cdn.sa.net/2024/05/21/x4bsKMrdv1PW8Lm.png" referrerpolicy="no-referrer" style="width: 800px; max-width: 100%;">
  <br>
  <i>Fig. Wu and He, "Group Normalization", ECCV 2018
<br></i></div><div><p dir="auto"><strong>内部协方差偏移(ICS)</strong></p></div><div><p dir="auto"><img src="https://cdn.sa.net/2024/06/05/3L6tfK95kREOVxv.png" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>批归一化(Batch normalization)</strong></p>
</span><ul>
<li data-line="1" dir="auto">Batch Normalization是由Sergey loffe和Christian Szegedy发明的。这项技术首次出现在2015年的一篇名为"Batch Normalization:Accelerating Deep Network Training by Reducing InternalCovariate Shift"的论文中。这篇论文彻底改变了深度学习训川练过程，特别是在训练非常深的神经网络时。</li>
<li data-line="2" dir="auto"><strong>原理</strong>：在每一层的输出上进行标准化操作，使得输出的均值为0，方差为1。减少所谓的内部协变量偏移(internal covariate shift)</li>
<li data-line="3" dir="auto"><strong>优劣势</strong>：Batch normalization可以加速神经网络的训练，同时也可以防止过拟合，但是Batch normalization对于小批量的样本效果不好。</li>
<li data-line="4" dir="auto"><strong>适用场景</strong>：用于加速神经网络的训练，防止过拟合。</li>
<li data-line="5" dir="auto"><strong>提出动机</strong>：为了加速神经网络的训练，防止过拟合。</li>
<li data-line="6" dir="auto"><strong>实际效果</strong>：在实际应用中，Batch normalization可以有效地加速神经网络的训练，防止过拟合。</li>
<li data-line="7" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 对不同序列中同一位置的数据进行归一化操作, 所有网络都常用</span></li>
</ul>
</li>
<li data-line="10" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>层归一化(Layer Normalization)</strong></p>
</span><ul>
<li data-line="11" dir="auto"><strong>原理</strong>：与批归一化类似，层归一化是对每一层的输出进行归一化，使得输出的均值为0，方差为1，但不同的是，层归一化是在每个样本内部进行，而不是在批次之间。</li>
<li data-line="12" dir="auto"><strong>优劣势</strong>：层归一化可以加速神经网络的训练，同时也可以防止过拟合，而且对于小批量的样本效果也好。</li>
<li data-line="13" dir="auto"><strong>适用场景</strong>：用于加速神经网络的训练，防止过拟合，尤其适用于批量较小的情况。</li>
<li data-line="14" dir="auto"><strong>提出动机</strong>：为了解决批归一化在小批量样本上的问题，提出了层归一化。</li>
<li data-line="15" dir="auto"><strong>实际效果</strong>：在实际应用中，层归一化可以有效地加速神经网络的训练，防止过拟合。</li>
<li data-line="16" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 对相同序列中不同位置的数据进行归一化操作, Transforme常用(便于处理不定长)</span></li>
</ul>
</li>
<li data-line="22" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>添加噪声(Noise Regularization)</strong></p>
</span><ul>
<li data-line="23" dir="auto">
<p><strong>原理</strong>：在训练过程中，将噪声添加到输入数据或网络权重中，以增强模型的鲁棒性，并防止过拟合。</p>
</li>
<li data-line="24" dir="auto">
<p><strong>优劣势</strong>：添加噪声可以有效地防止过拟合，提高模型的泛化能力，但是如果噪声添加过多，可能会影响模型的性能。</p>
</li>
<li data-line="25" dir="auto">
<p><strong>适用场景</strong>：用于防止神经网络的过拟合，尤其适用于在训练数据中存在噪声的情况。</p>
</li>
<li data-line="26" dir="auto">
<p><strong>提出动机</strong>：为了增强模型的鲁棒性，防止过拟合。</p>
</li>
<li data-line="27" dir="auto">
<p><strong>实际效果</strong>：在实际应用中，添加噪声可以有效地防止过拟合，提高模型的泛化能力。</p>
</li>
<li data-line="29" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>标签平滑（Label Smoothing）</strong> <span style="background:#fff88f">常用于分类任务和知识蒸馏</span></p>
</span><ul>
<li data-line="30" dir="auto"><strong>原理</strong>：标签平滑是一种正则化技术，旨在减少过拟合。在训练深度学习模型时，我们通常使用"硬"标签，例如，如果一个样本属于第i类，那么它的标签就是一个全为0的向量，只有第i位为1。标签平滑通过将这些"硬"标签转换为"软"标签，即在标签中引入一些噪声，使得模型不会过于自信，从而提高其泛化能力。</li>
<li data-line="31" dir="auto"><strong>优劣势</strong>：标签平滑的优点是可以防止模型过拟合，提高模型的泛化能力。此外，它还可以防止模型对于一些不确定或错误标签的过度反应。然而，它的缺点是可能会导致模型的训练速度变慢，因为它使得损失函数的优化变得更加复杂。</li>
<li data-line="32" dir="auto"><strong>适用场景</strong>：标签平滑主要用于深度学习领域，尤其是在处理分类问题时，如图像分类、文本分类等。</li>
<li data-line="33" dir="auto"><strong>提出动机</strong>：为了防止模型过拟合，提高模型的泛化能力。</li>
<li data-line="34" dir="auto"><strong>实际效果</strong>：在实际应用中，标签平滑已被证明可以提高模型的泛化能力，特别是在训练大规模深度学习模型时。</li>
</ul>
</li>
</ul>
</li>
<li data-line="40" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> <strong>集成方法（Ensemble Methods）</strong></p>
</span><ul>
<li data-line="41" dir="auto">训练多个模型，然后将它们的输出进行组合，以提高模型的泛化能力。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="数据增强" dir="auto" class="heading" id="数据增强"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>数据增强</h3><div class="heading-children"><div><p dir="auto">图像任务<br>
<img src="https://cdn.sa.net/2024/04/21/KqJvrHPoaintzkm.png" referrerpolicy="no-referrer"></p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="多层感知机器(MLP)" dir="auto" class="heading" id="多层感知机器(MLP)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>多层感知机器(MLP)</h2><div class="heading-children"><div><ul>
<li data-line="0" dir="auto">线性层 + 非线性激活函数</li>
<li data-line="1" dir="auto">多层网络拟合</li>
</ul></div><div class="heading-wrapper"><h3 data-heading="KAN (alternative)" dir="auto" class="heading" id="KAN_(alternative)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>KAN (alternative)</h3><div class="heading-children"><div class="admonition-parent admonition-hint-parent"><div class="callout admonition admonition-hint admonition-plugin " style="--callout-color: 0, 191, 165;" data-callout="hint" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="fire" class="svg-inline--fa fa-fire fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M216 23.86c0-23.8-30.65-32.77-44.15-13.04C48 191.85 224 200 224 288c0 35.63-29.11 64.46-64.85 63.99-35.17-.45-63.15-29.77-63.15-64.94v-85.51c0-21.7-26.47-32.23-41.43-16.5C27.8 213.16 0 261.33 0 320c0 105.87 86.13 192 192 192s192-86.13 192-192c0-170.29-168-193-168-296.14z"></path></svg></div><div class="callout-title-inner admonition-title-content">Hint</div></div><div class="callout-content admonition-content"><p dir="auto">1957 年提出的 Kolmogorov-Arnold 表示定理是类似泰勒展开傅里叶分解的函数分解定理，这个定理表明无论一个连续的多元函数是如何的复杂, 只要这个函数是从多维空间变化到实数的, 你总可以找到有限个一元函数复合来精准的表示原始的函数，但是这个定理没有如何构建这些一元函数的方法，需要通过反向传播拟合。这种用数学公式替代 MLP多层结构的好处是拟合能力有数学定理支撑；减少了网络的参数量；有极好的可解释性因为我们甚至可以写出网络的表示公式； 函数复合表示本身会带来极高的精度。但是我试着跑了一下官方的 Python包，可解释性是够了速度也太慢了。而且 scale 能力存疑，有理由怀疑未来只能应用在和物理数学强相关的领域。</p>
<p dir="auto">MLP学习画直线，然后求和进行非线性激活 KAN学习画曲线，然后求和 曲线表征能力远超直线，所以需要曲线的数量超低于直线的数量 学习曲线比学习直线要难，由于现在算力上去了，可以学习曲线了 由于曲线有更好的表征能力，以及拟合所需的数量可接受，所以解释性变强了</p></div></div></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2404.19756" rel="noopener" class="external-link" href="https://arxiv.org/abs/2404.19756" target="_blank">KAN:Kolmogorov-Arnold Networks</a><br>
<a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=OEvJE-O1R2k" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=OEvJE-O1R2k" target="_blank">超越谷歌DeepMind的最新大作：KAN全网最详细解读！ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://kindxiaoming.github.io/pykan/index.html" rel="noopener" class="external-link" href="https://kindxiaoming.github.io/pykan/index.html" target="_blank">Welcome to Kolmogorov Arnold Network (KAN) documentation! — Kolmogorov Arnold Network documentation</a> ⬅️  基于Pytorch 的 KAN python库(official)<br>
<a data-tooltip-position="top" aria-label="https://github.com/Blealtan/efficient-kan" rel="noopener" class="external-link" href="https://github.com/Blealtan/efficient-kan" target="_blank">GitHub - Blealtan/efficient-kan: An efficient pure-PyTorch implementation of Kolmogorov-Arnold Network (KAN).</a> ⬅️ 基于 Pytorch 基本原地替代 <code>nn.Linear</code> 为 <code>KanLinear</code></p></div><div><p dir="auto"><img src="https://kindxiaoming.github.io/pykan/_images/kan_plot.png" referrerpolicy="no-referrer"><br>
<img src="https://pbs.twimg.com/media/GMhpsTCbUAAP0iC?format=jpg&amp;name=medium" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> MLP 不就是 带学习参数的线性变换+非线性的激活函数不断复合起来的么？那直接用函数复合不就好了么？MLP 是图灵完备的？计算机也是图灵完备的？为什么要在计算机里再跑一个 MLP？其实关键是我们在实际任务中，数学证明一个东西可以干什么还不够，我们还要能在合理的时间内获得其解析解。主要的缺点是很难解释学习到的参数，很难手动调节，容易过拟合</p>
</span></li>
<li data-line="1" dir="auto">
<p>KAN和Mamba一样是数学代替原始模型. 减少了参数量; 利于更好的拟合; 更好的解释性. </p>
</li>
<li data-line="2" dir="auto">
<p>科尔莫戈罗夫-阿诺德表示定理由科尔莫戈罗夫和阿诺德在1957年提出, 证明了希尔伯特的第13个问题的否定答案, 即即七次以上的方程并不需要特殊的函数来解决，而是可以通过使用二元一次函数的复合来解决. </p>
</li>
<li data-line="3" dir="auto">
<p><strong>Kolmogorov-Arnold  表示定理</strong>是一个类似泰勒展开, 傅立叶分解的<strong>函数逼近定理</strong>. 这个表达定理表明<span style="background:#fff88f">任何连续的多元实值函数都可以表示成有限个单元量函数的叠加和复合形式, 但是这个定理没有说明如何构建这个一元函数</span>. 换句话说, 无论一个连续的多元函数是如何的复杂, 只要这个函数是从多维空间变化到实数的, 你总可以找到一些一元函数复合来精准的逼近原始的函数. </p>
</li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span>  Kolmogorov-Arnold 表示定理 可以运用在神经网络上, 因为每个神经元都可以看作一个简单的函数. 神经元的互相连接可以看作简单函数复合成一个多元函数的状况. 函数有多少输入变量就是神经网络的输入维度.  </p>
</span></li>
<li data-line="5" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 可以理解为将 MLP 上激活函数从点换到边上，将固定的可微的的激活函数用 B 样条函数取代，函数形式固定，具体的函数值用训练学习。</p>
</span></li>
<li data-line="6" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">%</span> Kolmogorov-Arnold 表示定理和万能逼近定理(1989)都可以描述神经网络具有强大拟合能力. 后者者表明任何一个前馈网络如果具有线性输出层和任何一个具有“挤压性质” 的隐藏层(阶跃函数), 只要这个隐藏层具有足够宽, 就可以拟合从一个有限维度到另一个有限维度的Borel 可测函数. 前者提供了理论框架, 后者提供了一个具体的实现路径.<br>
<img src="https://zh.d2l.ai/_images/blocks.svg" referrerpolicy="no-referrer"></p>
</span></li>
<li data-line="9" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> Kolmogorov-Arnold 表示定理的公式形式如下(否定了希尔伯特第十三个问题-是否存在七次方程以上方程的通用解)：</p>
</span></li>
<li data-line="10" dir="auto">
<p>对于任何连续的函数 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo><mjx-script style="vertical-align: 0.363em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom></mjx-math></mjx-container></span>，存在整数 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> 和连续的函数 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom></mjx-math></mjx-container></span> 和 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A6"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom></mjx-math></mjx-container></span>，都有：</p>
</li>
</ul></div><div><p dir="auto"><span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2026"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munderover space="4"><mjx-over style="padding-bottom: 0.192em; padding-left: 0.333em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-over><mjx-box><mjx-munder><mjx-row><mjx-base><mjx-mo class="mjx-lop"><mjx-c class="mjx-c2211 TEX-S2"></mjx-c></mjx-mo></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.108em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom></mjx-under></mjx-row></mjx-munder></mjx-box></mjx-munderover><mjx-msub space="2"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A6"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-s4"><mjx-c class="mjx-c28 TEX-S4"></mjx-c></mjx-mo><mjx-munderover><mjx-over style="padding-bottom: 0.192em; padding-left: 0.51em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-over><mjx-box><mjx-munder><mjx-row><mjx-base><mjx-mo class="mjx-lop"><mjx-c class="mjx-c2211 TEX-S2"></mjx-c></mjx-mo></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.092em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-under></mjx-row></mjx-munder></mjx-box></mjx-munderover><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-s4"><mjx-c class="mjx-c29 TEX-S4"></mjx-c></mjx-mo></mjx-mrow></mjx-math></mjx-container></span></p></div><div><ul>
<li data-line="0" dir="auto">这个定理没有给出构建 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span> 和 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D713 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math></mjx-container></span> 函数的具体方法。在实践中，我们通常使用神经网络来逼近这些函数。每个神经元可以看作是一个简单的函数，通过组合大量的神经元，我们可以逼近任何复杂的函数。</li>
<li data-line="1" dir="auto">KAN 使用的 <strong>B样条函数(Basis Spline functions)</strong>.拥有平滑数值稳定高度可控简洁的的递归定义，故常用于计算机图形学，曲线拟合，数据逼近等场景。</li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper"><span class="lc-list-marker">%</span> 类似展开函数回顾</span></li>
<li data-line="3" dir="auto"><strong>傅立叶分解</strong> : 将任何周期函数表示为正弦和余弦函数的无穷级数. 这些正弦和余弦函数的频率是基础频率的整数倍，基础频率由原始函数的周期决定</li>
<li data-line="4" dir="auto"><strong>泰勒展开</strong> : 将任何可微函数展开为无穷级数，这个级数的每一项都是函数在某一点的导数和&nbsp;<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-script style="vertical-align: 0.363em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math></mjx-container></span>&nbsp;的乘积</li>
<li data-line="5" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 特点
</span><ul>
<li data-line="6" dir="auto">拟合能力由数学证明支撑</li>
<li data-line="7" dir="auto">可解释性好. 甚至最后可以写出神经网络的公式. </li>
<li data-line="8" dir="auto">精度高。因为拟合函数本身</li>
<li data-line="9" dir="auto">参数效率高</li>
<li data-line="10" dir="auto">可以拟合周期函数</li>
<li data-line="11" dir="auto">要用的算力大（目前）- 关键不是网络大小而是forward的计算量</li>
</ul>
</li>
</ul></div><div><p dir="auto">⬇️ 其实你大概率不需要用KAN. 它训练贼慢, 也很难拟合一般的图像或者文本数据.(维度太高了!)<br>
<img src="https://cdn.sa.net/2024/05/07/GpdFXES6ogfj7eW.png" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span>  KAN 主要是用于 AI For Science. 比如 MLP 无法拟合周期函数。</span></li>
<li data-line="1" dir="auto">质疑： PINN（Physics Informed Neural Network）在网络物理计算领域有广泛应用的作者，美国科学院院士George Carnevale。他拟合了几个常见的数学方程，非常笃定的锤 KAN 的效果。</li>
<li data-line="2" dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Dr421j72d/?spm_id_from=333.1007.tianma.1-2-2.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Dr421j72d/?spm_id_from=333.1007.tianma.1-2-2.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">[中英字幕] KAN 论文一作 刘子鸣 亲自讲解 Kolmogorov-Arnold Networks_哔哩哔哩_bilibili</a></li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="CNN(卷积神经网络)" dir="auto" class="heading" id="CNN(卷积神经网络)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>CNN(卷积神经网络)</h2><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1SZ421a7v3/?spm_id_from=333.1365.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1SZ421a7v3/?spm_id_from=333.1365.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">何恺明教授在MIT的第一课：卷积神经网络_哔哩哔哩_bilibili</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/407763732/answer/3452197126" rel="noopener" class="external-link" href="https://www.zhihu.com/question/407763732/answer/3452197126" target="_blank">卷积神经网络中的特征到底是什么？它和传统图像处理中的比如轮廓、颜色等特征有什么异同？ - 知乎</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://ytzfhqs.github.io/AAAMLP-CN/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E5%88%86%E5%89%B2%E6%96%B9%E6%B3%95/" rel="noopener" class="external-link" href="https://ytzfhqs.github.io/AAAMLP-CN/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E5%88%86%E5%89%B2%E6%96%B9%E6%B3%95/" target="_blank">图像分类和分割方法 - AAAMLP 中译版</a><br>
里面有Padding 和 dilation 的说明.</p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 卷积神经网络的特点</span></li>
<li data-line="1" dir="auto">互相关操作</li>
<li data-line="2" dir="auto">局部感受域(local方法)</li>
<li data-line="3" dir="auto">权重共享</li>
<li data-line="4" dir="auto">平移不变性。 但是 CNN 对旋转敏感，所有数据增强有时要 <code>RandomRotate</code></li>
<li data-line="5" dir="auto">多层网络</li>
</ul></div><div class="heading-wrapper"><h3 data-heading="模块堆叠方法(U-Net,FCN, FFN)" dir="auto" class="heading" id="模块堆叠方法(U-Net,FCN,_FFN)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>模块堆叠方法(U-Net,FCN, FFN)</h3><div class="heading-children"><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> U-Net </span></li>
<li data-line="1" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> FCN </span></li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> FCN </span></li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="图像分类" dir="auto" class="heading" id="图像分类"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>图像分类</h3><div class="heading-children"><div><ul>
<li data-line="0" dir="auto">图像分类算是 CV 中最简单的，所有大部分网络都比较原始，探索了很多的机制和技巧，为更加复杂的任务提供坚实基础。这个任务最著名的数据集就是 Le Feifei 领头制作的 ImageNet 数据集。</li>
<li data-line="1" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> LeNet </span></li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> AlexNet </span></li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="物体检测分割" dir="auto" class="heading" id="物体检测分割"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>物体检测分割</h3><div class="heading-children"><div><img src="https://manipulation.csail.mit.edu/data/coco_instance_segmentation.jpeg" referrerpolicy="no-referrer"></div><div><ul>
<li data-line="0" dir="auto">R-CNN </li>
<li data-line="1" dir="auto">Fast R-CNN </li>
<li data-line="2" dir="auto">Faster R-CNN</li>
<li data-line="3" dir="auto">SSD</li>
<li data-line="4" dir="auto">SegmentAnyThing</li>
<li data-line="5" dir="auto">YOLOv1</li>
<li data-line="6" dir="auto">YOLOv2 </li>
<li data-line="7" dir="auto">YOLOv3</li>
<li data-line="8" dir="auto">YOLOv4</li>
<li data-line="9" dir="auto">YOLOv5</li>
<li data-line="10" dir="auto">YOLOv6</li>
<li data-line="11" dir="auto">YOLOv7 </li>
<li data-line="12" dir="auto">YOLOv8</li>
<li data-line="13" dir="auto">YOLOv9</li>
</ul></div><div class="admonition-parent admonition-faq-parent"><div class="callout admonition admonition-faq admonition-plugin " style="--callout-color: 100, 221, 23;" data-callout="faq" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="question-circle" class="svg-inline--fa fa-question-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zM262.655 90c-54.497 0-89.255 22.957-116.549 63.758-3.536 5.286-2.353 12.415 2.715 16.258l34.699 26.31c5.205 3.947 12.621 3.008 16.665-2.122 17.864-22.658 30.113-35.797 57.303-35.797 20.429 0 45.698 13.148 45.698 32.958 0 14.976-12.363 22.667-32.534 33.976C247.128 238.528 216 254.941 216 296v4c0 6.627 5.373 12 12 12h56c6.627 0 12-5.373 12-12v-1.333c0-28.462 83.186-29.647 83.186-106.667 0-58.002-60.165-102-116.531-102zM256 338c-25.365 0-46 20.635-46 46 0 25.364 20.635 46 46 46s46-20.636 46-46c0-25.365-20.635-46-46-46z"></path></svg></div><div class="callout-title-inner admonition-title-content">Faq</div></div><div class="callout-content admonition-content"><p dir="auto">CNN&nbsp;试图通过卷积来表达不同位置数值之间的关系，学习卷积值也就是学习矩阵里的数值之间的特征，所以适合用在图像里面。因为图像就是一个个的像素点形成的矩阵。</p>
<p dir="auto">CNN 普遍有什么特性? </p>
<ol>
<li dir="auto">
<p><strong>局部感受机制</strong> : CNN通过 convolution kernel 捕捉数据的局部特征. 每个kernel只专注于提取某个特定特征(e.g.边缘角点纹理), 有效降低参数量, 提高模型泛化能力.</p>
</li>
<li dir="auto">
<p><strong>权重共享</strong>：卷积层的神经元使用相同的权重，这意味着在进行前向传播和反向传播时，这些权重会被同时更新。这种权重共享机制大大减少了模型的参数数量，从而减轻了过拟合的风险。</p>
</li>
<li dir="auto">
<p><strong>池化</strong>：池化层可以对输入的特征图进行下采样，降低特征的维度，同时保留重要的特征信息。</p>
</li>
<li dir="auto">
<p><strong>平移不变性</strong>：由于权重共享和池化操作，CNN具有平移不变性，即无论目标在图像中的位置如何移动，CNN都可以识别出来。</p>
</li>
<li dir="auto">
<p><strong>多层结构</strong>：CNN通常由多个卷积层和池化层交替堆叠而成，可以提取出图像的高层次特征。</p>
</li>
<li dir="auto">
<p><strong>端到端学习</strong>：CNN可以直接从原始像素数据中学习到有用的特征，无需人工设计特征，大大简化了机器学习的流程。</p>
</li>
</ol>
<ul>
<li dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> CNN有没有尺度不变性和旋转不变性</span></li>
<li dir="auto">CNN（卷积神经网络）本身并不具有尺度不变性和旋转不变性。</li>
<li dir="auto"><strong>尺度不变性</strong>：CNN对于输入图片的尺寸是敏感的，如果输入图片的尺度（大小）发生变化，那么CNN可能无法正确地识别出图片中的对象。这是因为CNN在训练过程中，是对固定尺寸的图片进行学习的，如果输入图片的尺寸改变了，那么CNN的卷积核可能无法正确地匹配图片中的特征。</li>
<li dir="auto"><strong>旋转不变性</strong>：同样，CNN对于输入图片的旋转也是敏感的。如果输入图片被旋转了一定的角度，那么CNN可能无法正确地识别出图片中的对象。这是因为CNN在训练过程中，是假设所有的输入图片都是正向（未旋转）的，如果输入图片被旋转了，那么CNN的卷积核也可能无法正确地匹配图片中的特征。</li>
</ul>
<p dir="auto">然而，虽然CNN本身不具有尺度和旋转不变性，但是我们可以通过一些方法来提高CNN对尺度和旋转的鲁棒性，例如数据增强（对训练图片进行缩放和旋转），或者在CNN中使用一些特殊的卷积层（例如空间变换网络层）来学习图片的尺度和旋转变换。</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=YkXSKZIQ5oo" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=YkXSKZIQ5oo" target="_blank">如何用深度学习提取图片特征？这样做有什么好处？ - YouTube</a></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/15/n5zxyVfGKPI7FLC.png" referrerpolicy="no-referrer"></p>
<p dir="auto"><strong>介绍如何使用预训练深度学习进行特征提取</strong>:</p>
<ul>
<li dir="auto">上图是VGG16. 预训练的话需要在7*515*512层截获, 进行特征输出</li>
<li dir="auto"></li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="LeNet(鼻祖) - 1998" dir="auto" class="heading" id="LeNet(鼻祖)_-_1998"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>LeNet(鼻祖) - 1998</h3><div class="heading-children"><div class="admonition-parent admonition-warning-parent"><div class="callout admonition admonition-warning admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="warning" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Warning</div></div><div class="callout-content admonition-content heading-wrapper"><h3 data-heading="LeNet 是什么? 对深度学习的发展有什么贡献?" dir="auto" class="heading" id="LeNet_是什么?_对深度学习的发展有什么贡献?"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>LeNet 是什么? 对深度学习的发展有什么贡献?</h3>
<p dir="auto"> LeNet(5层)是一种早期的卷积神经网络，由Yann LeCun于1998年提出，主要用于手写数字识别和机器阅读等任务。LeNet的出现使得神经网络能够处理更复杂的图像识别任务，是深度学习领域的一个重要里程碑。</p>
<p dir="auto">LeNet对深度学习的发展贡献主要体现在以下几个方面：</p>
<ol>
<li dir="auto">
<p>网络结构设计：LeNet提出了一种新的网络结构，引入了卷积层、池化层、全连接层的概念，使得神经网络可以更好地处理图像等高维数据。</p>
</li>
<li dir="auto">
<p>参数共享：LeNet提出了参数共享的概念，大大减少了网络的参数数量，使得神经网络可以在有限的计算资源下处理更大规模的数据。</p>
</li>
<li dir="auto">
<p>激活函数：LeNet提出了使用S型激活函数，使得神经网络可以学习和表示更复杂的函数关系。</p>
</li>
<li dir="auto">
<p>反向传播：LeNet第一次使用了反向传播算法来训练神经网络，为后来的深度学习算法提供了重要的参考。</p>
</li>
<li dir="auto">
<p>实践应用：LeNet在手写数字识别等任务上取得了显著的效果，证明了深度学习在实际应用中的有效性。</p>
</li>
</ol>
<p dir="auto">总的来说，LeNet的出现对深度学习的发展起到了推动作用，为后来的深度学习算法提供了重要的理论基础和实践经验。</p><div class="heading-children"></div></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="AlexNet(重启深度学习) - 2012" dir="auto" class="heading" id="AlexNet(重启深度学习)_-_2012"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>AlexNet(重启深度学习) - 2012</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener" class="external-link" href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" target="_blank">ImageNet Classification with Deep Convolutional Neural Networks" by Alex Krizhevsky, Ilya Sutskever, and Geoffrey H. Hinton (2012)</a></p></div><div class="admonition-parent admonition-warning-parent"><div class="callout admonition admonition-warning admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="warning" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Warning</div></div><div class="callout-content admonition-content heading-wrapper"><p dir="auto"><img src="https://cdn.sa.net/2024/04/04/WUoK8NSL6svmPwj.png" referrerpolicy="no-referrer"></p>
<h3 data-heading="AlexNet 是什么? 对深度学习的发展有什么贡献?" dir="auto" class="heading" id="AlexNet_是什么?_对深度学习的发展有什么贡献?">AlexNet 是什么? 对深度学习的发展有什么贡献?</h3>
<p dir="auto"> AlexNet是一个在深度学习和计算机视觉领域非常知名的卷积神经网络模型，由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton在2012年提出，并在当年的ImageNet竞赛中取得了突破性的成果。</p>
<p dir="auto">AlexNet的主要贡献在于：</p>
<ol>
<li dir="auto">
<p>首次证明了深度学习在<strong>大规模数据集</strong>上的有效性：AlexNet在ImageNet数据集上获得了当时最好的结果，这个数据集包含了1000个类别，120万张训练图像，验证和测试各有5万张图像。</p>
</li>
<li dir="auto">
<p>推动了深度学习在计算机视觉领域的应用：AlexNet的成功引起了工业界和学术界的广泛关注，从而推动了深度学习在计算机视觉领域的广泛应用。</p>
</li>
<li dir="auto">
<p>网络结构创新：AlexNet引入了<strong>ReLU（Rectified Linear Unit）激活函数</strong>，相比于传统的sigmoid等激活函数，ReLU在深度网络中有更好的性能和更快的训练速度。此外，AlexNet还首次引入了<strong>Dropout</strong>技术，有效防止了模型过拟合。</p>
</li>
<li dir="auto">
<p><strong>利用GPU加速训练</strong>：AlexNet是首个大规模利用GPU加速训练的深度网络模型，这对于后续深度学习模型的训练有重要影响。</p>
</li>
<li dir="auto">
<p>证明了<strong>数据增强</strong>的有效性.</p>
</li>
<li dir="auto">
<p>比LeNet的网络宽</p>
</li>
<li dir="auto">
<p>有一点分组卷积的意思, 但是这是由于直接卷积当时的显卡放不下. 可视化实验发现, 上下两组卷积学习到了不一样的东西.</p>
</li>
</ol></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=Z8tNqCrHRyM" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=Z8tNqCrHRyM" target="_blank">【如何读论文？】2012年让深度学习起死回生的开山之作：AlexNet - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1311.2901" rel="noopener" class="external-link" href="https://arxiv.org/abs/1311.2901" target="_blank">[1311.2901] Visualizing and Understanding Convolutional Networks - 2013</a></p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="ZfNet(第一次可视化隐藏层) - 2013" dir="auto" class="heading" id="ZfNet(第一次可视化隐藏层)_-_2013"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>ZfNet(第一次可视化隐藏层) - 2013</h3><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p dir="auto">ZFNet（Zeiler and Fergus Network）是一种卷积神经网络（Convolutional Neural Network，简称CNN），由Matthew Zeiler和Rob Fergus在2013年提出。ZFNet在2013年的ImageNet竞赛中取得了第一名，证明了其在图像分类任务上的有效性。</p>
<p dir="auto">ZFNet的主要贡献包括：</p>
<ol>
<li dir="auto">
<p><strong>提出了反向可视化的方法</strong>：ZFNet的作者提出了一种名为反向可视化的方法，能够帮助我们理解并可视化CNN中间层的特征。这种方法通过将每一层的特征映射回像素空间，可以让我们看到网络在每一层中学习到的内容。这种方法对于理解深度学习模型的内部工作机制非常有帮助，特别是对于理解模型的错误和不足。</p>
</li>
<li dir="auto">
<p><strong>对AlexNet进行了改进</strong>：ZFNet实际上是对2012年ImageNet竞赛冠军AlexNet的一个改进。ZFNet的作者通过他们的反向可视化方法发现，AlexNet的第一层使用的11x11的卷积核可能过大，导致了一些信息的丢失。因此，他们将第一层的卷积核大小改为7x7，并对其他一些参数进行了调整，从而改进了模型的性能。</p>
</li>
</ol>
<p dir="auto">在深度学习发展史上，ZFNet有着重要的地位。首先，ZFNet的成功进一步证明了深度学习，特别是CNN在图像分类任务上的有效性。其次，ZFNet的<strong>反向可视化方法</strong>对于深度学习的理解和发展有着重要的影响。这种方法不仅帮助我们理解了深度学习模型的内部工作机制，也为后续的模型改进和设计提供了有价值的指导。</p>
<p dir="auto">至于对模型可视化的贡献，ZFNet的反向可视化方法开创了深度学习模型可视化的新范式。这种方法能够将抽象的特征映射回像素空间，让我们可以直观地看到网络在每一层中学习到的内容。这对于理解深度学习模型的内部工作机制，特别是理解模型的错误和不足，有着极大的帮助。此外，这种方法也为后续的模型可视化研究提供了一个重要的基础。</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=ihs2JbTnhc4&amp;t=6s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=ihs2JbTnhc4&amp;t=6s" target="_blank">模型可视化开山之作ZFNet，一篇文章开创了多少新领域？！ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1311.2901" rel="noopener" class="external-link" href="https://arxiv.org/abs/1311.2901" target="_blank">Visualizing and Understanding Convolutional Networks</a></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/14/QSarIYEXANu87J2.png" referrerpolicy="no-referrer"><br>
<img src="https://cdn.sa.net/2024/04/14/jNFTnZa9Oeo8SpM.png" referrerpolicy="no-referrer"></p>
<p dir="auto">文章灵感来源AlexNet. </p>
<p dir="auto">贡献: </p>
<ul>
<li dir="auto">第一次发现 小 conv kernel 效果会好.</li>
<li dir="auto">为了解释其效果, 就用反向可视化用CNN中间层. 为之后的可视化方法比如GradCAM提供启发. </li>
<li dir="auto">展示了CNN的效果类似于高通滤波器的近似“边缘检测”</li>
<li dir="auto">发现训练不足的话, 对深层的影响比浅层要大. </li>
<li dir="auto">第一次使用对输入图像Mask, 进行消融实验.</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="物体检测分割网络(R-CNN, SSD, YOLO..)" dir="auto" class="heading" id="物体检测分割网络(R-CNN,_SSD,_YOLO..)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>物体检测分割网络(R-CNN, SSD, YOLO..)</h3><div class="heading-children"><div><p align="center" dir="auto"></p></div><div><img alt="alt text" src="https://cdn.sa.net/2024/04/11/r2YpvkcWhUu94E5.png" referrerpolicy="no-referrer" style="width: 800px; max-width: 100%;"></div><div><br></div><div><p dir="auto"><i>Fig. 分割网络发展一览</i></p></div><div></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> R-CNN </span></li>
<li data-line="1" dir="auto">Region-based Convolution Neural Networks(R-CNN) </li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper"><span class="lc-list-marker">%</span> RetinaNet  经典人脸识别</span></li>
<li data-line="3" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> SSD</span></li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> YOLO </span></li>
<li data-line="5" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> Segment Anything</span></li>
<li data-line="6" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper"><span class="lc-list-marker">%</span> SegmentAnyThing Mobile </span></li>
</ul></div><div class="admonition-parent admonition-attention-parent"><div class="callout admonition admonition-attention admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="attention" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Attention</div></div><div class="callout-content admonition-content heading-wrapper"><h3 data-heading="历史上的分割网路" dir="auto" class="heading" id="历史上的分割网路"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>历史上的分割网路</h3>
<h3 data-heading="R-CNN 是什么? 对深度学习的发展有什么贡献?" dir="auto" class="heading" id="R-CNN_是什么?_对深度学习的发展有什么贡献?">R-CNN 是什么? 对深度学习的发展有什么贡献?</h3>
<p dir="auto">R-CNN，全称是 Region-based Convolutional Neural Networks，是一种区域卷积神经网络，用于目标检测任务。它是一种两阶段的目标检测器，首先对图像进行区域提议，然后对这些提议进行分类和位置修正。</p>
<p dir="auto">R-CNN的主要贡献在于，它首次将深度学习用于目标检测任务，大幅度提高了目标检测的准确性。在此之前，目标检测主要依赖于手工设计的特征和简单的分类器，而R-CNN通过学习得到的特征和复杂的分类器，能够更好地处理各种各样的图像和目标。</p>
<p dir="auto">此外，R-CNN也为后续的深度学习目标检测算法，如Fast R-CNN，Faster R-CNN等提供了基础和灵感。这些算法进一步优化了R-CNN的性能和速度，使得深度学习在目标检测任务上的应用更加广泛。</p>
<p dir="auto">成为 目标检测(打框) 和 异常检测的主干网络.</p><div class="heading-children"></div></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=sHZVPHVQg9E" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=sHZVPHVQg9E" target="_blank">为RCNN奠基的工作，正式开启深度学习的大门！ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" rel="noopener" class="external-link" href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" target="_blank">huppelen.nl/publications/selectiveSearchDraft.pdf</a></p>
<p dir="auto"><img alt="aeZ5jmFU7goEnfq.png" src="https://cdn.sa.net/2024/04/14/aeZ5jmFU7goEnfq.png" referrerpolicy="no-referrer"></p>
<ul>
<li dir="auto">监督学习. 训练样本首先(<del>随机</del>某种方式)生成了很多 anchor </li>
<li dir="auto">使用 SVM 结合 HOG features 进行训练</li>
<li dir="auto">然后重复迭代消除随机生成的 anchor 和 label 接近</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=ikq128JdQOI" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=ikq128JdQOI" target="_blank">开山之作RCNN做了什么？什么是图像分割、语义分割？</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1311.2524" rel="noopener" class="external-link" href="https://arxiv.org/abs/1311.2524" target="_blank">[1311.2524] Rich feature hierarchies for accurate object detection and semantic segmentation</a></p>
<p dir="auto"><img alt="lc5TbFJE6IYoyHk.png" src="https://cdn.sa.net/2024/04/14/lc5TbFJE6IYoyHk.png" referrerpolicy="no-referrer"><br>
<img src="https://cdn.sa.net/2024/04/14/bz4R8svkHMarj6G.png" referrerpolicy="no-referrer"><br>
<img alt="kpO5gQi7EGdTzUY.png" src="https://cdn.sa.net/2024/04/14/kpO5gQi7EGdTzUY.png" referrerpolicy="no-referrer"></p>
<p dir="auto">R- CNN基本上和上面的流程一样. 不过使用CNN替换了SVM.<br>
由于CNN的特征提取能力更强, 所以效果也好很多. </p>
<ol>
<li dir="auto">用一定方式在原图上进行生成anchor </li>
<li dir="auto">将anchor里的对象用CNN进行label的预测(class prediction)</li>
<li dir="auto">判断anchor是否合适(bindingbox prediction)</li>
</ol></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=4VN5ZEXXByk" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=4VN5ZEXXByk" target="_blank">RCNN最强优化：Faster RCNN究竟做了什么？ - YouTube</a></p>
<p dir="auto"><img alt="Jyw1XW2cTqMktxA.png" src="https://cdn.sa.net/2024/04/21/Jyw1XW2cTqMktxA.png" referrerpolicy="no-referrer"></p>
<ul>
<li dir="auto">Fast R-CNN  是对R-CNN的性能优化</li>
<li dir="auto">主要优化方式是使用一个名为 ROI Pooling的方式在卷积后图像上生成achor而不是在原图上</li>
<li dir="auto">由于ROI Pooling对小物体的检测能力欠缺, 就发明了faster R-CNN 使用了更加高级的ROI Pooling方法</li>
</ul>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/21/FkSiVCK5UatNeLv.png" referrerpolicy="no-referrer"></p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=_DtHS5wzPp8&amp;t=30s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=_DtHS5wzPp8&amp;t=30s" target="_blank">神操作！计算快了2000倍！空间金字塔池化做了什么？ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1406.4729" rel="noopener" class="external-link" href="https://arxiv.org/abs/1406.4729" target="_blank">[1406.4729] Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition -2014</a></p>
<p dir="auto"><img alt="ZMFVJjKtlxNy8oa.png" src="https://cdn.sa.net/2024/04/18/ZMFVJjKtlxNy8oa.png" referrerpolicy="no-referrer"></p>
<ul>
<li dir="auto">Kaiming He 在 2014 的论文</li>
<li dir="auto">提出了一个名为<strong>SPP-net</strong>的网络, 提出了SPP-net 算法</li>
<li dir="auto">网络中使用了一个空间金字塔池可以使其物体变形, <strong>图像的大小/比例</strong> 有很强的鲁棒性</li>
<li dir="auto">因为它在池化层中引入了多个尺度的空间金字塔池化，从而可以处理不同尺度的输入<br>
图像。</li>
<li dir="auto">整幅图像中只计算一次特征映射避免了重复参数, 允许网络处理输入图像的任意形状和大小</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=lJgwXCyjCQc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=lJgwXCyjCQc" target="_blank">传奇大神何凯明完美作品Mask RCNN，如何达到图片标注分割算法的顶级结果？ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1703.06870" rel="noopener" class="external-link" href="https://arxiv.org/abs/1703.06870" target="_blank">[1703.06870] Mask R-CNN</a></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/21/FbuPlgcD4vqmehI.png" referrerpolicy="no-referrer"></p>
<ul>
<li dir="auto">Kaiming He 在2017的作品</li>
<li dir="auto">Mask R-CNN 借鉴 Fast R-CNN 和 Faster R-CNN的优点, 并增加FCN的特点. </li>
<li dir="auto">使用双线性插值减缓了Fast R-CNN 上, 暴力切割物体边缘的去缺点(RoI Align)</li>
<li dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 到现在还在用</span></li>
</ul></div></div></div><div class="admonition-parent admonition-note-parent"><div class="callout admonition admonition-note admonition-plugin " style="--callout-color: 68, 138, 255;" data-callout="note" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="pencil-alt" class="svg-inline--fa fa-pencil-alt fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Note</div></div><div class="callout-content admonition-content heading-wrapper"><p dir="auto">SSD和YOLO是两种流行的目标检测神经网络，它们在速度和精度上都表现出色。以下是这两种网络的基本概述：</p>
<h3 data-heading="SSD (Single Shot Multibox Detector)" dir="auto" class="heading" id="SSD_(Single_Shot_Multibox_Detector)">SSD (Single Shot Multibox Detector)</h3>
<p dir="auto">SSD是一种单次检测算法，它在单个网络中同时预测边界框和类别概率。这是SSD的基本结构：</p>
<ul>
<li dir="auto"><strong>基础网络</strong>：SSD以一个预训练的基础网络开始，通常是VGG16或ResNet，它负责提取图像特征。</li>
<li dir="auto"><strong>多尺度特征图</strong>：SSD的一个关键特性是使用不同尺寸的特征图进行检测。在接下来的层中，特征图的大小逐渐减小，允许网络同时在多个尺度上检测对象。</li>
<li dir="auto"><strong>卷积预测器</strong>：每个特征图层都有一组卷积滤波器，这些滤波器用于预测边界框的位置和形状以及目标类别的分数。</li>
<li dir="auto"><strong>锚点框</strong>：SSD使用一组预定义的锚点框（或称为默认框），与每个特征图的每个位置相关联。预测则是这些锚点框的调整。</li>
<li dir="auto"><strong>非极大值抑制</strong>（NMS）：在检测结束后使用NMS来清除重复和低置信度的检测框。</li>
<li dir="auto"><strong>损失函数</strong>：SSD的损失函数同时考虑了定位误差（通常使用Smooth L1 损失）和分类误差（通常使用交叉熵损失）。</li>
</ul>
<h3 data-heading="YOLO (You Only Look Once)" dir="auto" class="heading" id="YOLO_(You_Only_Look_Once)">YOLO (You Only Look Once)</h3>
<p dir="auto">YOLO是另一种流行的端到端目标检测系统，其核心观念是将目标检测任务作为一个回归问题来解决，它能够一次性预测图像中的对象类别和位置。下面是YOLO的一般结构：</p>
<ul>
<li dir="auto"><strong>单一网络</strong>：YOLO使用单一的卷积网络直接对图像进行特征提取和预测。</li>
<li dir="auto"><strong>分割网格</strong>：YOLO首先将输入图像划分为一个SxS的网格，每个格子负责预测那些其中心点落在该格子内的物体。</li>
<li dir="auto"><strong>边界框预测</strong>：每个格子会预测B个边界框，包括边界框的位置、大小和置信度，置信度表示框中是否存在对象以及预测准确度。</li>
<li dir="auto"><strong>类别预测</strong>：每个格子还会预测C个类别的概率，表示格子包含的对象属于这些类别的概率。</li>
<li dir="auto"><strong>损失函数</strong>：YOLO同样使用损失函数来同时训练模型的分类和定位准确性。损失由坐标预测的均方误差、对象置信度误差和类别预测误差组成。</li>
<li dir="auto"><strong>版本演进</strong>：YOLO有多个版本（从YOLOv1到YOLOv5等），每个版本都在速度和精度上做了不同程度的优化。</li>
</ul>
<p dir="auto">总结一下，SSD和YOLO都采用了将目标检测问题转化为回归问题的方式。SSD在多个特征图尺度上运行，提高了检测小对象的能力；而YOLO则以其快速的检测速度闻名，并且随着版本的更新不断改进。两者在实际应用中都非常有效，选择使用哪一个通常取决于具体任务和性能需求。</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="VGG(纯CNN最深网络) - 2014" dir="auto" class="heading" id="VGG(纯CNN最深网络)_-_2014"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>VGG(纯CNN最深网络) - 2014</h3><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=5zDFIe4k1Gw" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=5zDFIe4k1Gw" target="_blank">【博士Vlog】VGG是什么？为什么是卷积神经网络的巅峰之作？ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1409.1556" rel="noopener" class="external-link" href="https://arxiv.org/abs/1409.1556" target="_blank">[1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition</a></p>
<p dir="auto"><img alt="dTwEcHBs56pGiMx.png" src="https://cdn.sa.net/2024/04/16/dTwEcHBs56pGiMx.png" referrerpolicy="no-referrer"><br>
VGG-16</p>
<ul>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>非常深
<ul>
<li dir="auto">共19层</li>
<li dir="auto">PS: 工程上使得网络变宽比变深更难</li>
</ul>
</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>非常简单
<ul>
<li dir="auto">只有  <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">卷</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">积</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">核</mjx-utext></mjx-mi><mjx-mi class="mjx-i"><mjx-utext variant="italic" style="font-family: MJXZERO, serif; font-style: italic;">，</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">步</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">长</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">为</mjx-utext></mjx-mi><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-utext variant="italic" style="font-family: MJXZERO, serif; font-style: italic;">，</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">填</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">充</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">为</mjx-utext></mjx-mi><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-utext variant="italic" style="font-family: MJXZERO, serif; font-style: italic;">，</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">所</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">有</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">的</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">池</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">化</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">层</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">都</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">是</mjx-utext></mjx-mi><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-math></mjx-container></span></li>
<li dir="auto">然后简单模块重复</li>
<li dir="auto">发现用小卷积核效果很好, 可能是防止 图 不要快速变得太小</li>
</ul>
</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>效果好
<ul>
<li dir="auto">网络越深越好</li>
</ul>
</li>
<li dir="auto">但是参数数量较大，计算量大</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>用了stage-wise training
<ul>
<li dir="auto">VGG-11, 13, <strong>16</strong>, <strong>19</strong> (现在只有16和19常用)</li>
</ul>
</li>
<li dir="auto">适合迁移学习</li>
</ul></div></div></div><div class="admonition-parent admonition-hint-parent"><div class="callout admonition admonition-hint admonition-plugin " style="--callout-color: 0, 191, 165;" data-callout="hint" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="fire" class="svg-inline--fa fa-fire fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M216 23.86c0-23.8-30.65-32.77-44.15-13.04C48 191.85 224 200 224 288c0 35.63-29.11 64.46-64.85 63.99-35.17-.45-63.15-29.77-63.15-64.94v-85.51c0-21.7-26.47-32.23-41.43-16.5C27.8 213.16 0 261.33 0 320c0 105.87 86.13 192 192 192s192-86.13 192-192c0-170.29-168-193-168-296.14z"></path></svg></div><div class="callout-title-inner admonition-title-content">Hint</div></div><div class="callout-content admonition-content"><p dir="auto">深度神经网络在训练过程中可能会遇到梯度爆炸的问题，这通常发生在梯度的值变得非常大，以至于更新的权重值导致网络不稳定。以下是一些常用的解决方法：</p>
<ol>
<li dir="auto"><strong>梯度裁剪（Gradient Clipping）</strong>：这是一种直接的方法，可以防止梯度值过大。基本思想是设置一个阈值，当梯度的值超过这个阈值时，就将其裁剪到这个阈值。这可以防止梯度爆炸，但不能解决梯度消失的问题。</li>
<li dir="auto"><strong>权重初始化（Weight Initialization）</strong>：合适的权重初始化可以在一定程度上防止梯度爆炸或梯度消失。例如，Xavier初始化和He初始化是两种针对不同类型的激活函数（如sigmoid和ReLU）设计的初始化方法。</li>
<li dir="auto"><strong>批量归一化（Batch Normalization）</strong>：批量归一化可以使网络中各层的输入保持相同的分布，这样可以稳定训练过程，防止梯度爆炸。</li>
<li dir="auto"><strong>使用适当的优化器（Optimizers）</strong>：某些优化器，如Adam、RMSprop等，可以自适应地调整学习率，从而避免梯度过大或过小的问题。</li>
<li dir="auto"><strong>残差连接（Residual Connections）</strong>：在深度神经网络中使用残差连接可以防止梯度消失和爆炸的问题，因为残差连接可以直接将梯度传递到较浅的层。</li>
<li dir="auto"><strong>正则化（Regularization）</strong>：L1和L2正则化可以防止权重变得过大，从而在一定程度上防止梯度爆炸。</li>
</ol>
<p dir="auto">这些方法可以单独使用，也可以结合使用，具体取决于具体的任务和模型。</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="GoogLeNet / Inception(探索各种trick) - 2014" dir="auto" class="heading" id="GoogLeNet_/_Inception(探索各种trick)_-_2014"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>GoogLeNet / Inception(探索各种trick) - 2014</h3><div class="heading-children"><div><p dir="auto">Going Deeper with Convolutions" by Szegedy et al. (2014)<br>
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20191202190625/modified.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto">GoogLeNet(非正式名称)也被称为Inception v1，是一种深度卷积神经网络, 名字来源于网络中的一种特殊结构——Inception模块，它在2014年的ImageNet大规模视觉识别挑战赛（ILSVRC）上取得了很好的成绩。GoogLeNet的主要创新点包括：</p></div><div><ol>
<li data-line="0" dir="auto">
<p><strong>Inception模块</strong>：GoogLeNet引入了Inception模块，这是一种并行连接多个不同类型的卷积核和池化操作的结构。Inception模块的设计理念是让<strong>网络自己决定使用哪种卷积核大小</strong>，或者<strong>是否使用全连接层。通过这种方式，网络可以在不同的尺度上学习特征，从而提高模型的表现力</strong>。</p>
<p>动机：在实际的图像中，有些重要的特征可能是局部的，有些可能是全局的，有些可能在中间尺度。通过使用不同大小的卷积核，我们可以让网络在不同的尺度上学习特征，从而更好地处理这种多尺度的问题。</p>
</li>
<li data-line="4" dir="auto">
<p><strong>深度和宽度的平衡</strong>：GoogLeNet通过增加网络的深度和宽度，提高了模型的表现力。但是，与此同时，GoogLeNet还使用了1x1的卷积核（也被称为点卷积）来减少参数的数量，从而降低了过拟合的风险，并提高了计算效率。</p>
<p>动机：深度神经网络有更强的表现力，但是也更容易过拟合，而且计算效率也较低。通过使用1x1的卷积核，我们可以在保持网络深度的同时，降低参数的数量，从而降低过拟合的风险，并提高计算效率。</p>
</li>
<li data-line="8" dir="auto">
<p><strong>辅助分类器</strong>：GoogLeNet在网络的中间层也添加了分类器，用于进行辅助的分类任务。这些辅助分类器的输出会在训练过程中与主分类器的输出一起被考虑，但是在测试过程中会被忽略。</p>
<p>动机：深度神经网络的一个常见问题是梯度消失，这会导致网络的前面几层难以训练。通过在网络的中间层添加辅助分类器，我们可以让这些层也接收到直接的反馈信号，从而缓解梯度消失的问题。</p>
</li>
</ol></div><div><p dir="auto">这些创新点使GoogLeNet在当时的图像分类任务上表现优秀，并对后续的深度学习研究产生了深远影响。</p></div><div><p dir="auto">随着研究的深入，Google后续还提出了Inception v2、Inception v3等更先进的版本。这些新版本在原有的Inception网络基础上进行了一些改进，例如引入了<strong>批量归一化（Batch Normalization）</strong>、<strong>分解卷积（Factorization into smaller convolutions）</strong>, Label Smoothing 等技术，以提高网络的性能和效率。<br>
Inception v4 添加了residual learning.</p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> <strong>Inception V1（GoogLeNet）</strong>
</span><ul>
<li data-line="1" dir="auto">这是Inception系列的第一个版本，它首次提出了Inception模块，这是一种并行结构，包含了不同尺寸的卷积和池化操作。Inception模块的设计目标是在保持网络深度和宽度的同时，提高网络的计算效率。此外，GoogLeNet还引入了辅助分类器（auxiliary classifiers）来解决梯度消失问题，以及全局平均池化（global average pooling）来替代全连接层，减少了模型的参数数量。</li>
</ul>
</li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> <strong>Inception V2和V3</strong>
</span><ul>
<li data-line="3" dir="auto">这两个版本的主要贡献是引入了两种新的技术：Batch Normalization（BN）和Factorized Convolutions。BN可以加速模型的训练，同时还可以起到一定的正则化效果。Factorized Convolutions则是一种将大的卷积核分解为多个小的卷积核的方法，这可以减少模型的计算复杂性，同时保持模型的性能。</li>
</ul>
</li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> <strong>Inception V4和Inception-ResNet</strong>：
</span><ul>
<li data-line="5" dir="auto">这两个版本的主要贡献是将ResNet的残差连接引入到Inception模块中。这种设计可以进一步提高模型的深度，同时避免梯度消失问题。此外，Inception V4还引入了一种新的Inception模块，这种模块包含了更复杂的并行结构，可以进一步提高模型的性能。</li>
</ul>
</li>
</ul></div><div><p dir="auto">总的来说，Inception系列的主要贡献在于提出了一种新的网络架构（Inception模块），这种架构可以在保持网络深度和宽度的同时，提高网络的计算效率。此外，Inception系列还引入了许多新的技术，如BN、Factorized Convolutions和残差连接，这些技术都对深度学习的发展产生了深远影响。</p></div><div class="admonition-parent admonition-note-parent"><div class="callout admonition admonition-note admonition-plugin " style="--callout-color: 68, 138, 255;" data-callout="note" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="pencil-alt" class="svg-inline--fa fa-pencil-alt fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Note</div></div><div class="callout-content admonition-content"><p dir="auto">正则化(normalization) 在深度学习中非常重要的技术.</p>
<p dir="auto">早在LeNet就提到正则化可以帮助训练.</p>
<p dir="auto">主要的作用是防止模型过拟合和帮助训练(初始化&amp;加速训练).</p>
<p dir="auto">正则化技术有很多种，以下是一些常见的类型：</p>
<ol>
<li dir="auto"><strong>权重衰减（L2正则化）</strong>：这是最常见的正则化技术之一。它通过在损失函数中添加一个与模型参数的平方成正比的项，来鼓励模型使用较小的权重。这可以防止模型过于依赖某个特定的特征，从而提高模型的泛化能力。</li>
<li dir="auto"><strong>L1正则化</strong>：L1正则化类似于权重衰减，但是它添加的是与模型参数的绝对值成正比的项。这会导致模型的一些权重变为0，从而实现特征选择的效果。</li>
<li dir="auto"><strong>Dropout</strong>：这是一种在训练过程中随机关闭一部分神经元的技术。通过这种方式，模型需要学习如何在缺失一部分信息的情况下进行预测，这可以增强模型的鲁棒性，并防止过拟合。</li>
<li dir="auto"><strong>批量归一化（Batch Normalization）</strong>：虽然批量归一化主要是用来解决深度网络中的内部协变量偏移问题，但是它也具有一定的正则化效果。</li>
<li dir="auto"><strong>数据增强</strong>：通过对训练数据进行一些随机的变换（例如旋转、缩放、剪裁等），我们可以增加模型的训练数据，从而防止过拟合。</li>
</ol>
<p dir="auto">这些正则化技术可以单独使用，也可以组合使用，以根据具体的任务和数据来调整模型的复杂性</p></div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=WyBP4r-l-kc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=WyBP4r-l-kc" target="_blank">大模型Xception为什么常用？主要用来做什么？ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1610.02357" rel="noopener" class="external-link" href="https://arxiv.org/abs/1610.02357" target="_blank">[1610.02357] Xception: Deep Learning with Depthwise Separable Convolutions</a></p>
<p dir="auto">Inception的极限优化.</p>
<p dir="auto">Xception是一种深度学习模型，由Google的研究员François Cholletz在2016年开发。Xception是"Extreme Inception"的缩写，这个模型的设计灵感来源于Inception模块，但是它对Inception模块进行了一种更极端的解释。</p>
<p dir="auto">Xception的主要贡献是提出了一种新的卷积操作，即深度可分离卷积（Depthwise Separable Convolution）。这种卷积操作包含两个步骤：深度卷积（Depthwise Convolution）和点卷积（Pointwise Convolution）。</p>
<p dir="auto">深度卷积是对每一个输入通道进行单独的卷积操作，而点卷积则是用1x1的卷积核对所有的输入通道进行卷积。深度可分离卷积的设计目标是在减少模型的计算复杂性的同时，保持模型的性能。</p>
<p dir="auto">除了深度可分离卷积，Xception还提出了一种新的网络架构，这种架构完全放弃了Inception模块中的并行结构，而是将所有的操作都串行化。这种设计基于一个假设，即跨通道的特征和空间特征是可以独立学习的。</p>
<p dir="auto">在实验中，Xception在多个视觉识别任务上都表现出了优于Inception V3的性能，同时模型的计算复杂性也有所降低。</p>
<ul>
<li dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 发现有全连接层非常有必要</span></li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="FCN - 2014 (最后一层是上采样的卷积层)" dir="auto" class="heading" id="FCN_-_2014_(最后一层是上采样的卷积层)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>FCN - 2014 (最后一层是上采样的卷积层)</h3><div class="heading-children"><div><blockquote dir="auto">
<p>Fully Convolutional Networks(FCN) 是最早的用于像素级预测的全卷积网络，用于进行图像分割任务. 在最后一层使用卷积层替换全联接层, 进行上采样或反卷积层使其恢复到输入图像的尺寸. 可以接受任意尺寸的输入图像. 适合执行图像分割任务. </p>
</blockquote></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=oGETsIAxQlw" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=oGETsIAxQlw" target="_blank">最牛论文FCN：只更换一个层，却开创了一个时代！ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1411.4038" rel="noopener" class="external-link" href="https://arxiv.org/abs/1411.4038" target="_blank">[1411.4038] Fully Convolutional Networks for Semantic Segmentation</a></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/21/5o1upKdWrexhS6a.png" referrerpolicy="no-referrer"></p>
<ul>
<li dir="auto">一般网路比如AlexNet, 如果是分类任务. 网络最后一定是全连接层. 而在这篇论文中作者在末尾使用一个CNN, 用于使用于semantic segmentation 这样任务的 pre-pixel prediction.</li>
<li dir="auto">这样类似于不平衡的 Autoencode</li>
</ul></div></div></div><div><ol>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>FCN 的变种</strong>:</p>
<ul>
<li data-line="2" dir="auto"><strong>SegNet</strong>: 主要用于场景理解，其在上采样中使用了存储的最大池化索引，进而减少了模型的参数。</li>
<li data-line="3" dir="auto"><strong>DeepLab系列 (DeepLabv3, DeepLabv3+)</strong>: 结合了空洞性卷积和CRF（条件随机场），进一步提高了分割的精度。</li>
<li data-line="4" dir="auto"><strong>RefineNet</strong>: 采用多路径精细化网络的方式，聚焦多尺度特征的整合，以实现更精确的分割。</li>
<li data-line="5" dir="auto">Mask R-CNN是这类网络中较为出名的一个，它利用FPN（Feature Pyramid Network）进行多尺度特征提取，并能够为每个检测到的对象提供一个高质量的分割掩码（Mask）</li>
</ul>
</li>
</ol></div></div></div><div class="heading-wrapper"><h3 data-heading="ResNet(残差网络) - 2015" dir="auto" class="heading" id="ResNet(残差网络)_-_2015"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>ResNet(残差网络) - 2015</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1512.03385" rel="noopener" class="external-link" href="https://arxiv.org/abs/1512.03385" target="_blank">[1512.03385] Deep Residual Learning for Image Recognition</a></p></div><div><p dir="auto"><img src="https://cdn.sa.net/2024/04/14/73EKPz8eahS4NBm.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto"><img src="https://miro.medium.com/v2/resize:fit:1200/1*6hF97Upuqg_LdsqWY6n_wg.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto"><img src="https://upload.wikimedia.org/wikipedia/commons/b/ba/ResBlock.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1tS4y1G7WE/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1tS4y1G7WE/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">深入理解残差网络(一)-设计动机与理解误区_哔哩哔哩_bilibili</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://mp.weixin.qq.com/s/8dEl63KBB_AHx8p1KaGnWA" rel="noopener" class="external-link" href="https://mp.weixin.qq.com/s/8dEl63KBB_AHx8p1KaGnWA" target="_blank"># 字节大模型一面：“为什么现在深度学习都用 ResNet?” - 看图学</a></p></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=4h5OkS2CzuU" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=4h5OkS2CzuU" target="_blank">【博士Vlog】2015年的深度学习冠军ResNet怎么工作的？有多厉害？ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1512.03385" rel="noopener" class="external-link" href="https://arxiv.org/abs/1512.03385" target="_blank">[1512.03385] Deep Residual Learning for Image Recognition</a></p>
<ul>
<li dir="auto">Kaiming He 的一作</li>
<li dir="auto">动机: 网络变深, 训练很难传递到深层网络, 可能是梯度越深梯度误差越大. (深度网络中的梯度消失和网络退化)</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>提出了 residual learning , 每两个卷积层添加一个短路模块.
<ul>
<li dir="auto">使用 1x1 的卷积层用于补足维度不同层级维度的不同.<br>
<img alt="jgmlAHL2WY3KF81.png" src="https://cdn.sa.net/2024/04/16/jgmlAHL2WY3KF81.png" referrerpolicy="no-referrer"></li>
<li dir="auto">这种方法被称为 identity mapping</li>
</ul>
</li>
<li dir="auto">这个工作的效果是有数学证明的</li>
<li dir="auto">发现效果很好<br>
<img alt="MDdFVGnIvjBAxqP.png" src="https://cdn.sa.net/2024/04/16/MDdFVGnIvjBAxqP.png" referrerpolicy="no-referrer"></li>
</ul>
<p dir="auto">ResNet的发明动机源于深度神经网络的训练困难问题。当网络的深度超过一定程度（如20层）时，即使使用了合适的权重初始化和正则化技术，网络的性能也会开始下降，这种现象被称为退化（degradation）。这与过拟合不同，过拟合是指模型在训练集上表现优秀，但在测试集上表现较差。而退化现象是指随着网络深度的增加，模型的训练误差和测试误差都会增大。这意味着，即使网络的容量增大，网络也不能很好地拟合训练数据，这是一个非常反直觉的现象。</p>
<p dir="auto">为了解决这个问题，Kaiming He等人提出了残差学习的概念。他们假设优化残差映射（即输入和输出之间的差值）比优化原始的未经处理的映射更容易。因此，他们设计了一种新的网络结构，称为残差块（Residual Block）。每个残差块包含了几个卷积层和一个跳跃连接（Skip Connection）。跳跃连接的作用是将输入直接传递到输出，形成一个短路机制，这就是所谓的恒等映射（Identity Mapping）。这种设计使得网络可以随着深度的增加而不断地学习新的特征，而不是只依赖于前面的层来学习所有的特征。</p>
<p dir="auto">利用这种残差学习的概念，ResNet成功地训练了一个超过100层的深度神经网络，这在当时是前所未有的。这使得ResNet在各种视觉任务上都取得了非常好的结果，包括图像分类、物体检测和语义分割等任务。这种深度学习的新范式不仅改变了我们对深度学习的理解，也为后续的深度学习研究提供了新的方向。</p></div></div></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin " style="--callout-color: 0, 200, 83;" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p dir="auto">A checklist of training deep nets</p>
<p dir="auto">All about signal propagation</p>
<ul>
<li dir="auto">ReLU(or similar)</li>
<li dir="auto">initialization</li>
<li dir="auto">normalization</li>
<li dir="auto">residual connection</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><img alt="ilQtqRkT9ngXmI4.png" src="https://cdn.sa.net/2024/04/14/ilQtqRkT9ngXmI4.png" referrerpolicy="no-referrer"></p>
<p dir="auto">ResNeXt是一种深度学习模型，它的设计灵感来源于VGG和ResNet两种网络结构。ResNeXt的主要贡献是引入了“cardinality”（群体大小）这个新的维度，通过增加网络的宽度来提高模型的性能。</p>
<p dir="auto">以下是ResNeXt的主要贡献：</p>
<ol>
<li dir="auto">
<p><strong>引入了群体大小（Cardinality）</strong>：在深度学习中，我们通常通过增加网络的深度（更多的层）或宽度（更多的神经元）来提高模型的性能。然而，ResNeXt提出了一个新的维度，即群体大小（Cardinality），也就是在并行的路径中重复相同的变换。ResNeXt的研究发现，增加群体大小的效果在保持其他参数不变的情况下，优于增加深度或宽度。</p>
</li>
<li dir="auto">
<p><strong>引入了分组卷积（Grouped Convolution）</strong>：ResNeXt中的每个模块都包含了多个并行的路径，每个路径都进行相同的操作，但是操作的输入和输出都是分开的。这种设计可以看作是一种分组卷积的特例，分组卷积是一种有效的方式，可以在保持模型复杂性不变的情况下，增加网络的容量。</p>
</li>
<li dir="auto">
<p><strong>提供了一种易于扩展的模型设计</strong>：ResNeXt模型的设计十分灵活，可以通过简单地调整群体大小和每个群体的宽度来扩展模型。这种设计使得ResNeXt可以很容易地适应不同的计算资源和不同的任务需求。</p>
</li>
<li dir="auto">
<p><strong>在多个视觉识别任务上获得了出色的性能</strong>：在ImageNet分类、COCO object detection、COCO segmentation等任务上，ResNeXt都显示出了优于其他模型的性能。</p>
</li>
<li dir="auto">
<p>发明多头学习. </p>
</li>
</ol>
<p dir="auto">总的来说，ResNeXt的贡献在于提出了一种新的、有效的网络设计策略，这种策略通过引入群体大小（Cardinality）这个新的维度，以及分组卷积（Grouped Convolution），在提高模型性能的同时，保持了模型的计算效率。</p></div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1502.01852" rel="noopener" class="external-link" href="https://arxiv.org/abs/1502.01852" target="_blank">[1502.01852] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></p>
<p dir="auto">这篇2015年的论文发明了 Kaiming'He初始化, 用于初始化使用 ReLu 作为激活函数的网络的初始值. </p>
<p dir="auto"><code>torch::nn::init:kaiming_normal_</code></p>
<p dir="auto">具体思路如下: </p>
<p dir="auto">在深度学习中，权重初始化是一个重要的步骤，因为不同的初始化方法可能会影响模型的收敛速度和最终性能。如果权重初始化不当，可能会导致训练过程中的梯度消失或爆炸问题，从而使模型无法学习。</p>
<p dir="auto">Kaiming初始化（或称He初始化）的思路和动机源于对ReLU（Rectified Linear Unit）激活函数特性的理解。ReLU函数在负数部分的输出为0，这意味着大约一半的神经元在初始化时会被“死亡”（即输出为0），这可能导致反向传播过程中的梯度消失。为了解决这个问题，我们需要一个特殊的初始化策略。</p>
<p dir="auto">Kaiming初始化的基本思想是<strong>保持每一层输出的方差与输入的方差一致</strong>。</p>
<ul>
<li dir="auto">一是训练过程中层数之间权重的方差过大或过小容易导致梯度爆炸&amp;消失</li>
<li dir="auto">二是模型的权重方差过大，可能会导致模型过拟合，因为模型可能会过度依赖某些特定的特征</li>
</ul>
<p dir="auto">具体来说，如果一个层的输入神经元的数量为 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>，那么权重的初始值应该从均值为0，标准差 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-sop"><mjx-c class="mjx-c221A TEX-S1"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.107em;"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math></mjx-container></span>​的正态分布中随机取值。这样，即使经过ReLU激活函数，输出的方差也能保持不变。</p>
<p dir="auto">这种初始化方法可以有效地保持每一层的激活值在经过多层传播后仍然保持有用的信息，从而防止梯度消失或爆炸的问题。这是Kaiming初始化被广泛应用于深度学习中的主要原因</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=o6kCNXscgks" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=o6kCNXscgks" target="_blank">分组卷积的开山之作：ResNeXt讲了什么？为什么有好的效果？ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/323424817/answer/1050776046" rel="noopener" class="external-link" href="https://www.zhihu.com/question/323424817/answer/1050776046" target="_blank">ResNeXt的分类效果为什么比Resnet好? - 知乎</a><br>
<img src="https://cdn.sa.net/2024/04/23/fuXQev1dUBPlhx2.png" referrerpolicy="no-referrer"></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/32913695" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/32913695" target="_blank">深度学习——分类之ResNeXt - 知乎</a></p>
<ul>
<li dir="auto">ResNeXt 是 2016年微软发明的CNN结构</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><del>发明</del>了分组卷积(aka. group convolution), 这使得模型在保持准确性的同时，能够有效地减少计算量. 类似Multi-head. 降低了网络过拟合风险. 
<ul>
<li dir="auto">分组卷积将输入和卷积核分成多个组（Group），然后在每个组上分别进行卷积操作</li>
<li dir="auto">可以让模型在laten space的不同subspace上学习, 增加网络的表达能力和学习到特征的多样化.</li>
<li dir="auto">例如，假设我们有一个具256个通道的输入和一个256x256的卷积核，如果直接进行卷积操作，计算量会大但是，如果我们将输入和卷积核都分成4组，每组分别有64个通道，然后在每个组上分别进行卷积，那么每次卷积的计算量就会大大降低。256*256=65, 536; 64*64*4 = 16, 383 </li>
</ul>
</li>
<li dir="auto">采用模块化设计, 简化的超参数的设置, 方便网络设计</li>
<li dir="auto">网络简单, 容易理解和实现</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="DenseNet(密集连接) - 2016" dir="auto" class="heading" id="DenseNet(密集连接)_-_2016"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>DenseNet(密集连接) - 2016</h3><div class="heading-children"><div><ul>
<li data-line="0" dir="auto">DenseNet有什么特点? 对深度学习的发展作出了什么贡献? </li>
</ul></div><div><p dir="auto">DenseNet，全称Densely Connected Convolutional Networks，是一种在卷积神经网络（CNN）中使用的网络架构。它有以下特点：</p></div><div><ol>
<li data-line="0" dir="auto">
<p>连接方式：不同于传统的神经网络是层与层之间的连接，DenseNet的每一层都与前面所有层连接，后面所有层也都接收其输出作为输入，形成了密集连接。</p>
</li>
<li data-line="2" dir="auto">
<p>参数量：由于使用了特征复用，DenseNet的参数量较少，计算效率较高。</p>
</li>
<li data-line="4" dir="auto">
<p>梯度流：由于所有层都直接连接到输出，因此梯度在反向传播过程中可以直接到达所有层，缓解了深度神经网络中常见的梯度消失问题。</p>
</li>
<li data-line="6" dir="auto">
<p>特征复用：由于每一层都接收所有前面层的特征图作为输入，使得网络有更强的特征复用能力。</p>
</li>
</ol></div><div><p dir="auto">对于深度学习的发展，DenseNet作出了如下贡献：</p></div><div><ol>
<li data-line="0" dir="auto">
<p>提高了模型性能：DenseNet在多个公开数据集上刷新了记录，包括ImageNet、CIFAR-10、CIFAR-100等。</p>
</li>
<li data-line="2" dir="auto">
<p>提高了训练效率：由于DenseNet的特性，使得模型训练过程中梯度传播更加顺畅，有助于提高模型训练的效率。</p>
</li>
<li data-line="4" dir="auto">
<p>提出了新的网络连接方式：DenseNet提出了全新的网络连接方式——密集连接，这为深度学习模型的设计提供了新的思路。</p>
</li>
<li data-line="6" dir="auto">
<p>提升了模型的鲁棒性：DenseNet的特性使得模型对于输入的小变化具有较好的鲁棒性，有助于提高模型的泛化能力。</p>
</li>
</ol></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1608.06993" rel="noopener" class="external-link" href="https://arxiv.org/abs/1608.06993" target="_blank">Densely Connected Convolutional Networks - 2016</a></span></li>
<li data-line="1" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> SENet</span></li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> ShuffleNet</span></li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="MobileNet(网络做小) - 2016" dir="auto" class="heading" id="MobileNet(网络做小)_-_2016"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>MobileNet(网络做小) - 2016</h3><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p dir="auto"><strong>什么是MobileNet? MobileNet 对 深度学习领域优势有什么历史贡献?</strong></p>
<p dir="auto"> MobileNet是一种基于深度神经网络的轻量级模型，主要用于移动和嵌入式视觉应用。它由Google的研究人员在2017年提出，其目标是提供一种高效的深度学习模型，可以在资源受限的设备上运行，如智能手机或嵌入式设备。</p>
<p dir="auto">MobileNet的主要特点是使用<strong>深度可分离的卷积（depthwise separable convolution）来替代传统的卷积</strong>。深度可分离的卷积将一个卷积操作分解为两个更轻量级的子操作，从而大大减少了计算量和模型大小，同时仍然保持了良好的性能。</p>
<p dir="auto">MobileNet对深度学习领域的主要贡献如下：</p>
<ol>
<li dir="auto">
<p>提出了一种新的<strong>深度可分离的卷积</strong>方法，这种方法大大减少了模型的计算量和大小，使得深度学习模型可以在资源受限的设备上运行。</p>
</li>
<li dir="auto">
<p>提出了一种新的网络结构，这种结构可以<strong>根据特定的资源约束（如计算资源、能源需求等）进行调整</strong>，从而在各种不同的情况下都能提供良好的性能。</p>
</li>
<li dir="auto">
<p>MobileNet在许多视觉任务上都表现出了<strong>优异的性能</strong>，如图像分类、物体检测、人脸识别等，这进一步证明了深度可分离卷积的有效性和灵活性。</p>
</li>
</ol>
<p dir="auto">总的来说，MobileNet的出现为移动和嵌入式设备上的深度学习应用开启了新的可能性，它的设计思想和技术也对后续的研究工作产生了深远的影响。</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=5AjzVYkhWQQ" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=5AjzVYkhWQQ" target="_blank">MobileNet v1v2v3都讲了什么？论文速读！ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://paperswithcode.com/method/mobilenetv1" rel="noopener" class="external-link" href="https://paperswithcode.com/method/mobilenetv1" target="_blank">MobileNetV1 Explained | Papers With Code</a></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/28/1HO69DjukBZIfXw.png" referrerpolicy="no-referrer"><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/92134485" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/92134485" target="_blank">深度可分离卷积 - 知乎</a></p>
<ol>
<li dir="auto">
<p>MobileNet V1：首次提出了深度可分离卷积的概念，将传统的卷积分解为深度卷积和逐点卷积两部分，大大降低了计算量和参数数量。此外，还引入了两个超参数，用于在模型复杂度和准确率之间进行权衡。</p>
</li>
<li dir="auto">
<p>MobileNet V2：在V1的基础上，提出了线性瓶颈和反向残差结构，进一步提高了模型的性能。线性瓶颈可以减少模型的计算量，而反向残差结构则可以使模型更深，提高模型的表达能力。</p>
</li>
<li dir="auto">
<p>MobileNet V3：这是一个经过精心设计的结构，结合了搜索空间的设计和网络结构搜索（NAS）。V3在V2的基础上加入了SE模块，并引入了h-swish激活函数，进一步提升了模型的性能。此外，V3还根据输入分辨率和模型复杂度，为不同的任务提供了一系列的模型。</p>
</li>
</ol></div></div></div><div><ul>
<li data-line="0" dir="auto"><strong>传统卷积</strong>，前一层和后一层特征一一对应开销大</li>
<li data-line="1" dir="auto"><strong>分组卷积(Group Convolution)</strong>，将输入通道分为若干组后对每一组在进行卷积最后拼接(有点类似于 Transformer的多头注意力机制)，用于减少计算量迫使网络学习到多个子空间的特征由 2016 年的ResNeXt开始真正发扬光大。但是不同通道之间孤立，限制了模型的性能。</li>
<li data-line="2" dir="auto">针对分组卷积的问题，ShuffleNet 使用<strong>通道混洗</strong>确保不同组之间的信息可以互相交换，进一步提高了模型的表达能力。</li>
<li data-line="3" dir="auto">同样的，MobileNet针对分组卷积的缺点，提出了<strong>深度可分离卷积（Depthwise Separable Convolution）</strong> 将单一的卷积操作分解位 深度卷积(DW)和逐点卷积(PC)。DW 先在每个通道上进行卷积再用 PC的 1 x 1 卷积核对 DW 的结果进行卷积以改变通道数。</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="U-Net(骨干)" dir="auto" class="heading" id="U-Net(骨干)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>U-Net(骨干)</h3><div class="heading-children"><div><blockquote dir="auto">
<p>U-Net 通过对称和跳跃链接, 结合了不同层的语义信息和位置信息, 在保留细节方面表现优异. 适合高分辨率分割任务.</p>
</blockquote></div><div><ol>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>U-Net 的变种</strong>:
<ul>
<li data-line="1" dir="auto"><strong>U-Net++ (Nested U-Net)</strong>: 在 U-Net 的基础上增加了嵌套、密集的跳过路径，以捕获更丰富的特征信息。</li>
<li data-line="2" dir="auto"><strong>V-Net</strong>: 主要用于三维图像分割，以适应医疗图像分析的需求。</li>
<li data-line="3" dir="auto"><strong>Attention U-Net</strong>: 引入了注意力机制来聚焦于图像的特定区域，提高分割的精准度。</li>
</ul>
</li>
</ol></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/search?type=content&amp;q=U%20Net%20" rel="noopener" class="external-link" href="https://www.zhihu.com/search?type=content&amp;q=U%20Net%20" target="_blank">U Net - 搜索结果 - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/483261689/answer/2093332362" rel="noopener" class="external-link" href="https://www.zhihu.com/question/483261689/answer/2093332362" target="_blank">为什么U-Net在医学图像上表现优越？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/480371053/answer/3084572728" rel="noopener" class="external-link" href="https://www.zhihu.com/question/480371053/answer/3084572728" target="_blank">医学图像分割，除了魔改unet，还能有哪些创新点？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/548613974/answer/3229770898" rel="noopener" class="external-link" href="https://www.zhihu.com/question/548613974/answer/3229770898" target="_blank">关于U-Net的魔改到了什么程度了？ - 知乎</a></p></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin " style="--callout-color: 0, 200, 83;" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p dir="auto"><img src="https://cdn.sa.net/2024/04/14/xaL5f6JcCRpPHUY.png" referrerpolicy="no-referrer"></p>
<p dir="auto">分割（Segmentation）是计算机视觉中相当流行的一项任务。在分割任务中，我们试图从背景中移除/提取前景。 前景和背景可以有不同的定义。我们也可以说，这是一项像素分类任务，你的工作是给给定图像中的每个像素分配一个类别。事实上，我们正在处理的气胸数据集就是一项分割任务。</p>
<p dir="auto">分割任务的损失函数有:<br>
二元交叉熵、focal损失、dice损失等.</p>
<p dir="auto">用于分割任务的最常用模型是 U-Net。</p>
<p dir="auto">U-Net 包括两个部分：编码器和解码器。编码器与您目前所见过的任何 U-Net 都是一样的。解码器则有些不同。解码器由上卷积层组成。在上卷积（up-convolutions）（<strong>转置卷积</strong>transposed convolutions）中，我们使用滤波器，当应用到一个小图像时，会产生一个大图像。在 PyTorch 中，您可以使用 ConvTranspose2d 来完成这一操作。必须注意的是，上卷积与上采样并不相同。上采样是一个简单的过程，我们在图像上应用一个函数来调整它的大小。在上卷积中，我们要学习滤波器。我们将编码器的某些部分作为某些解码器的输入。这对 上卷积层非常重要。</p>
<p dir="auto"><strong>U-Net = 收缩层 + 瓶颈层 + 扩展层</strong></p>
<ul>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>收缩层</strong>
<ul>
<li dir="auto">逐渐缩小输入图像的大小, 增加通道数量. 通过一系列卷积层和下采样, 提取图像的局部特征, 转化为更加高级别的抽象特征.</li>
</ul>
</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>瓶颈层</strong>
<ul>
<li dir="auto">由多个卷积层构成. 目标是捕获图像的高级特征, 减少特征图的维度, 保留重要的空间信息. 帮助UNet整合全局和局部星系, 以获取图像细节的上下文关系. </li>
</ul>
</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>扩展层</strong>
<ul>
<li dir="auto">对特征图上采样, 通过一系列上采样和卷积层, 恢复到原始图像的尺寸. 使用跳跃连接, 讲收缩层和扩展层进行连接, 帮助保留细粒度的空间信息, 提高分割结果的准确性和稳定性. </li>
</ul>
</li>
</ul>
<p dir="auto">U-Net 中的 skip connection 使得网络可以学习并纳入多个尺度的信息.</p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://ytzfhqs.github.io/AAAMLP-CN/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E5%88%86%E5%89%B2%E6%96%B9%E6%B3%95/" rel="noopener" class="external-link" href="https://ytzfhqs.github.io/AAAMLP-CN/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E5%88%86%E5%89%B2%E6%96%B9%E6%B3%95/" target="_blank">图像分类和分割方法 - AAAMLP 中译版</a></p>
<p dir="auto">⬆️ 里面有一个 原始 U-Net 的 Pytorch 实现</p></div></div></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content heading-wrapper"><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1505.04597" rel="noopener" class="external-link" href="https://arxiv.org/abs/1505.04597" target="_blank">[1505.04597] U-Net: Convolutional Networks for Biomedical Image Segmentation</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=Y-t5TmbY1Jc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=Y-t5TmbY1Jc" target="_blank">经典论文速读：U-Net讲了什么？为什么现在还在用？ - YouTube</a></p>
<h3 data-heading="摘要" dir="auto" class="heading" id="摘要">摘要</h3>
<ul>
<li dir="auto">论文指出，深度网络的成功训练通常需要成千上万的标注样本。</li>
<li dir="auto">作者提出了一种依赖于<strong>数据增强</strong>的网络和训练策略，以更高效地利用有限的标注样本。</li>
<li dir="auto">U-Net架构包含一个收缩路径来捕获上下文信息和一个对称的扩展路径来进行精确定位。</li>
<li dir="auto">论文展示了该网络可以从很少的图像中进行端到端训练，并在ISBI挑战赛中超过了之前最好的方法（滑动窗口卷积网络）。</li>
<li dir="auto">U-Net在传输光显微镜图像（相差和DIC）上的训练也赢得了2015年ISBI细胞跟踪挑战赛的多个类别。</li>
<li dir="auto">网络速度快，对512x512图像的分割少于一秒即可完成。</li>
</ul>
<h3 data-heading="引言" dir="auto" class="heading" id="引言">引言</h3>
<ul>
<li dir="auto">过去两年中，深度卷积网络在许多视觉识别任务中超越了最先进的技术。</li>
<li dir="auto">卷积网络虽然已经存在很长时间，但由于训练集的大小和考虑的网络规模限制了它们的成功。</li>
<li dir="auto">论文基于“全卷积网络”架构进行了改进和扩展，使其能够在只有很少训练图像的情况下工作，并产生更精确的分割结果。</li>
</ul>
<h3 data-heading="网络架构" dir="auto" class="heading" id="网络架构">网络架构</h3>
<ul>
<li dir="auto">U-Net的架构由收缩路径和扩展路径组成，如图1所示。</li>
<li dir="auto">收缩路径遵循典型的卷积网络架构，通过重复应用3x3卷积、ReLU和2x2最大池化操作进行下采样。(<strong>用于获得上下文</strong>)</li>
<li dir="auto">扩展路径的每一步包括特征图的上采样、“上卷积”和与收缩路径中相应裁剪的特征图的拼接。(<strong>用于精准定位</strong>)</li>
<li dir="auto">U-Net中没有全连接层，<strong>通过互连卷积与反卷积过程中的特征，将上下文信息传递到更高层，实现了信息补充</strong>；</li>
<li dir="auto">这种策略允许通过重叠瓦片策略无缝分割任意大的图像。</li>
<li dir="auto">U-Net 在FCN 的基础上<strong>增加了上采样操作的次数和跳跃连接，使用跳跃连接将解码器的输出特征与编码器的语义特征融合，提高了分割精度，改善了 FCN 上采样不足的问题</strong>。</li>
</ul>
<h3 data-heading="训练" dir="auto" class="heading" id="训练">训练</h3>
<ul>
<li dir="auto">使用Caffe实现的随机梯度下降来训练网络。</li>
<li dir="auto">论文介绍了如何计算能量函数和使用权重图来进行训练。</li>
</ul>
<h3 data-heading="数据增强" dir="auto" class="heading" id="数据增强">数据增强</h3>
<ul>
<li dir="auto">数据增强对于训练网络以获得期望的不变性和鲁棒性属性至关重要，尤其是在只有少量训练样本可用的情况下。</li>
<li dir="auto">论文中提到，随机弹性变形是训练具有很少标注图像的分割网络的关键概念。</li>
</ul>
<p dir="auto">实验中用到的方法:</p>
<ol>
<li dir="auto"><strong>弹性变形（Elastic Deformations）</strong>: 这是一种通过对训练样本应用随机位移向量来生成平滑变形的方法。位移是从标准差为10像素的高斯分布中采样的，然后使用双三次插值计算每个像素的位移。这种增强手段对于模拟组织中的变形特别有效，因为生物医学图像中的组织经常会出现此类变形。</li>
<li dir="auto"><strong>旋转和翻转</strong>: 通过对训练图像进行随机旋转和水平/垂直翻转，增加了模型对这些变换的不变性。</li>
<li dir="auto"><strong>强度变化</strong>: 通过对图像的亮度和对比度进行随机调整，模型能够学会在不同光照和成像条件下进行有效的分割。</li>
<li dir="auto"><strong>丢弃层（Drop-out Layers）</strong>: 在网络的收缩路径末端使用丢弃层进行进一步的隐式数据增强。</li>
</ol>
<h3 data-heading="实验" dir="auto" class="heading" id="实验">实验</h3>
<ul>
<li dir="auto">论文展示了U-Net在三个不同的分割任务中的应用。</li>
<li dir="auto">在电子显微镜记录的神经结构分割中，U-Net在ISBI挑战赛中取得了最佳成绩。</li>
<li dir="auto">论文还展示了在ISBI细胞跟踪挑战赛2015中的细胞分割结果，U-Net在两个最具挑战性的2D传输光数据集上取得了显著的胜利。</li>
</ul>
<h3 data-heading="结论" dir="auto" class="heading" id="结论">结论</h3>
<ul>
<li dir="auto">U-Net架构在多种生物医学分割应用中表现出色。</li>
<li dir="auto">通过弹性变形的数据增强，U-Net只需要很少的标注图像，并且训练时间合理。</li>
<li dir="auto">论文提供了完整的基于Caffe的实现和训练好的网络，并相信U-Net架构可以轻松应用于更多任务。</li>
</ul>
<h3 data-heading="后续改进" dir="auto" class="heading" id="后续改进">后续改进</h3>
<p dir="auto">在U-Net的原始模型之后，有许多研究者对其进行了改进，以适应更复杂的任务和更大的数据集。这些改进主要集中在以下几个方面：</p>
<ol>
<li dir="auto">
<p><strong>更深的网络</strong>：例如，V-Net和Res-U-Net。这些网络在U-Net的基础上添加了更多的层，以增加模型的容量和复杂性。</p>
</li>
<li dir="auto">
<p><strong>更复杂的连接方式</strong>：例如，Dense U-Net。这种网络在每一层之间添加了密集连接，使得每一层的输出都成为下一层的输入。</p>
</li>
<li dir="auto">
<p><strong>注意力机制</strong>：例如，Attention U-Net。这种网络在U-Net的基础上添加了注意力机制，使得网络在进行特征融合时，可以更加关注重要的区域。</p>
</li>
<li dir="auto">
<p><strong>多尺度特征融合</strong>：例如，MultiRes U-Net。这种网络在U-Net的基础上添加了多尺度特征融合，使得网络可以同时捕捉到不同尺度的信息。</p>
</li>
<li dir="auto">
<p><strong>3D U-Net</strong>：原始的U-Net是为2D图像设计的，但在许多医学图像任务中，我们需要处理3D图像。因此，有研究者提出了3D U-Net，以处理3D图像。</p>
</li>
<li dir="auto">
<p><strong>更有效的训练策略</strong>：例如，使用更复杂的数据增强技术，或者使用更复杂的损失函数，如Dice损失或Focal损失。</p>
</li>
</ol>
<p dir="auto">以上都是对U-Net的一些改进，但是需要注意的是，哪种改进最有效，很大程度上取决于具体的任务和数据。</p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/548613974/answer/3229770898" rel="noopener" class="external-link" href="https://www.zhihu.com/question/548613974/answer/3229770898" target="_blank">关于U-Net的魔改到了什么程度了？ - 知乎</a></p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=9psg4H_WwuU" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=9psg4H_WwuU" target="_blank">简单的想法，很好的效果：Unet++ 做了什么？ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://paperswithcode.com/method/unet" rel="noopener" class="external-link" href="https://paperswithcode.com/method/unet" target="_blank">UNet++ Explained | Papers With Code</a></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/29/nFXocHpGhT7ly94.png" referrerpolicy="no-referrer"></p>
<p dir="auto">原始 U-Net 中 Skip-Connection 的存在使得 U-Net 可能会发生在特定输入中浅层 U-Net 效果比 深层要好的结果，但残差又必不可少。</p>
<p dir="auto">U-Net ++ 干脆就把多各不同层数 U-Net 使用 dense connection 叠在了一起(类似于DenseNet).</p>
<p dir="auto">坐着比较两个参数量一致的 U-Net 和 U-Net ++ 发现是 Dense Connection 实际发生了作用。</p>
<p dir="auto">模型在 U-Net 在医学图像分割取得了很好的效果。</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2401.04722" rel="noopener" class="external-link" href="https://arxiv.org/abs/2401.04722" target="_blank">[2401.04722] U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation</a></p></div></div></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1603.06937" rel="noopener" class="external-link" href="https://arxiv.org/abs/1603.06937" target="_blank">Stacked Hourglass Networks for Human Pose Estimation</a> </span></li>
<li data-line="1" dir="auto">Hourglass Network 是由 Newell等人于2016年提出的一种人体姿态估计网络，也被用于其他需要精细定位的任务，如面部关键点检测、手势识别</li>
<li data-line="2" dir="auto">网络的结构是 多个 U-Net 叠加，有不同的叠加方式。这允许网络在不同尺度上捕捉和整合空间信息，帮助网络捕捉到不同大小的人体关节点。同时由于网络对称性也有助于保持空间分辨率和细节信息</li>
<li data-line="3" dir="auto">在每个Hourglass模块的输出后都会添加一个监督信号，即损失函数(中间监督)，这有助于训练过程中的梯度流动, 并提高模型的性能。这种设计类似于深度监督学习（deep supervision）的概念。</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=zMwco6NJgjQ" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=zMwco6NJgjQ" target="_blank">2022年至今AI新赛道，精细物体分割，U Net套娃？为什么效果好？ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2212.00968" rel="noopener" class="external-link" href="https://arxiv.org/abs/2212.00968" target="_blank">[2212.00968] UIU-Net: U-Net in U-Net for Infrared Small Object Detection</a></p></div><div><ul>
<li data-line="0" dir="auto">介绍了一个 U-Net 嵌套 U-Net 的一个名为 UIU-Net 的网络。用于检测精细目标的检测和分割任务。</li>
<li data-line="1" dir="auto"><span style="background:#fff88f">这个有点意思</span>。</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=xowiUqUHw4E" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=xowiUqUHw4E" target="_blank">把AI模型套娃会怎样？效果拔群！精细图片分割算法U2Net是什么？怎么快速发好论文？ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/xuebinqin/U-2-Net" rel="noopener" class="external-link" href="https://github.com/xuebinqin/U-2-Net" target="_blank">GitHub - xuebinqin/U-2-Net: The code for our newly accepted paper in Pattern Recognition 2020: "U^2-Net: Going Deeper with Nested U-Structure for Salient Object Detection."</a><br>
<img style="width:500" src="https://cdn.sa.net/2024/07/07/ylazVSsDBwLGQYb.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto">文章level 1. Applicatoins 2. idea 3. 整理 4. 仿写</p></div><div><p dir="auto">这个模型最大的好处就是精度真的很好，比其他的unet强太多，sg模型中算是佼佼者，非常有实用性。特别是对于同一物体的精细分割很有用。但是问题也是在这里，就是误检的可能性很高，很敏感，非常依赖数据集的质量。 而且这个训练和最后的模型都很轻 哦，还有一个问题就是这个模型对于大尺寸的图像比较不擅长，如果有大尺寸的图像，得预处理裁剪</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=3Fd_Q1xk1Qk" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=3Fd_Q1xk1Qk" target="_blank">世界上最简单水论文思路！学不会无法毕业！UNet怎么玩出花？ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/WangLibo1995/GeoSeg" rel="noopener" class="external-link" href="https://github.com/WangLibo1995/GeoSeg" target="_blank">GitHub - WangLibo1995/GeoSeg: UNetFormer: A UNet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery, ISPRS. Also, including other vision transformers and CNNs for satellite, aerial image and UAV image segmentation.</a></p></div><div><img style="width:500" src="https://cdn.sa.net/2024/07/09/hF7wrNOv6cJAYiI.png" referrerpolicy="no-referrer">
<img style="width:500" src="https://cdn.sa.net/2024/07/09/rc4oqWjefMx3JI2.png" referrerpolicy="no-referrer">
- 从 Input 开始 Transformers 和 CNN 独立计算 ，然后算好了之后加起来。又是套娃(高精度+快模型+Global + Local). 
- 水? 水 但是结果还是 OK。又快分割又精准</div></div></div><div class="heading-wrapper"><h3 data-heading="FPN(特征金字塔网络)" dir="auto" class="heading" id="FPN(特征金字塔网络)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>FPN(特征金字塔网络)</h3><div class="heading-children"><div><blockquote dir="auto">
<p><strong>Feature Pyramid Networks</strong>(FPN) 通过构建特征金字塔和自顶向下路径, 有效利用多尺寸特征, 提高对小目标的检测能力, 在处理多尺度物体时非常有效.</p>
</blockquote></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 和其他骨干的对比。对比 U-Net 和 FCN </span></li>
</ul></div><div><p dir="auto"><strong>FPN 的变种</strong></p></div><div><ul>
<li data-line="0" dir="auto"><strong>Panoptic FPN</strong>: 用于全景分割，结合了 FPN 和语义分割，以处理图像中的每一个像素。</li>
<li data-line="1" dir="auto"><strong>Libra R-CNN</strong>&nbsp;: 集成了 FPN, 平衡不同尺度的特征，通过使用平衡损失和大致均匀采样来增强物体检测。</li>
<li data-line="2" dir="auto"><strong>NAS-FPN (Neural Architecture Search Feature Pyramid Network)</strong>: 使用神经架构搜索来优化和强化FPN结构</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="EfficientNet (终结网络scale经验法则) - 2019" dir="auto" class="heading" id="EfficientNet_(终结网络scale经验法则)_-_2019"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>EfficientNet (终结网络scale经验法则) - 2019</h3><div class="heading-children"><div><blockquote dir="auto">
<p>有章法的缩放网络的宽度和深度, 避免可能的网络浪费</p>
</blockquote></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p dir="auto"><img src="https://img-blog.csdnimg.cn/20210306162502756.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70#pic_center" referrerpolicy="no-referrer"></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/16/5pRuksEVtmT69zi.png" referrerpolicy="no-referrer"></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/16/4XnPsyhY6a3vRCd.png" referrerpolicy="no-referrer"></p>
<p dir="auto">这篇文章是谷歌研究团队发表的《EfficientDet::Scalable and Efficient Object Detection》。论文系<br>
统地研究了针对目标检测任务神经网络架构设计的选择，并提出了一系列关键优化以提升效率。文<br>
章的主要贡献包括：</p>
<ol>
<li dir="auto">提出了<strong>加权双向特征金字塔网络(BiFPN)</strong>,简化了多尺度特征融合的过程。</li>
<li dir="auto">提出了一种<strong>复合缩放方法</strong>，均匀地缩放了所有背骨、特征网络以及分类/框预测网络的分辨率、深度和宽度。</li>
<li dir="auto">基于这些优化以及更好的主干网络，开发了一系列称为EfficientDet的新型目标检测器，与先前的模型相比，各种资源限制下都能达到更好的效率。</li>
</ol>
<p dir="auto">特别值得一提的是，使用单一模型和单一尺度的EfficientDet-D7在C0Co测试集上达到了55.1%的最<br>
新AP(平均精度)，并且只有77O0万参数和4100亿FLOPs,比以前的检测器小4-9倍，使用的FLOPs:少13-42倍。此外，EfficientDet在GPU/CPU上的运行速度比以前的检测器快4-11倍。论文还证明了EfficientDet在Pascal VOC2012语义分割上也表现出色，达到了8174%的mlOU精度，同时FLOPs比DeepLabV3+少9.8倍。</p>
<p dir="auto"><strong>什么是EfficientNet? 在深度学习领域有什么历史贡献?</strong></p>
<p dir="auto"> EfficientNet是一种在深度学习领域中使用的卷积神经网络(CNN)架构。它是由Google的研究员在2019年提出的，该网络使用了一种名为“复合缩放”的方法，通过均衡网络的深度、宽度和解析度，使得模型在保持相同计算资源的情况下，可以达到更高的精度。</p>
<p dir="auto">EfficientNet的核心思想是：不同维度之间的缩放可以带来更好的性能。在之前的一些工作中，研究人员通常只关注单一维度的缩放，例如网络的深度（如ResNet）或宽度（如MobileNet）。然而，EfficientNet指出，这些单一维度的缩放往往会导致资源的浪费。因此，EfficientNet提出了一种复合缩放的方法，同时考虑了深度、宽度和解析度的缩放，从而在有限的资源下获得更好的性能。</p>
<p dir="auto">EfficientNet的历史贡献主要体现在两个方面：</p>
<ol>
<li dir="auto">
<p>EfficientNet提出了一种新的网络缩放方法，这种方法可以在有限的计算资源下，获得更好的性能。这对于那些资源有限，但希望获得高性能模型的场景（如移动设备、嵌入式设备等）具有重要的意义。</p>
</li>
<li dir="auto">
<p>EfficientNet在一系列的图像识别任务上都取得了当时最好的性能，包括ImageNet，CIFAR-100等数据集，这进一步证明了其有效性和优越性。</p>
</li>
</ol>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=jsHda3dhM-g" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=jsHda3dhM-g" target="_blank">【博士Vlog】EfficientNet 和 EfficientDet 讲了什么？有计算资源真的可以为所欲为！！ - YouTube</a></p>
<ul>
<li dir="auto">介绍了EfficentNet(图像分类) 和 EfficientDet(物体检测)</li>
<li dir="auto">网络速度快, 参数小, 效果又好</li>
<li dir="auto">平衡了网络 <code>depth</code> <code>channel</code> <code>width</code> 的设计</li>
<li dir="auto">在ImageNet训练, 上面的平衡法则基本上就是试出来的(Neural Architecture Search)</li>
<li dir="auto"></li>
</ul>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://blog.csdn.net/qq_37541097/article/details/114434046" rel="noopener" class="external-link" href="https://blog.csdn.net/qq_37541097/article/details/114434046" target="_blank">EfficientNet网络详解-CSDN博客</a></p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><img src="https://cdn.sa.net/2024/04/16/VdE6cFM3r7aGgLx.png" referrerpolicy="no-referrer"></p>
<p dir="auto">这张图展示了四种不同的特征网络设计用于多尺度特征融合，它们在目标检讽测模型中起着至关重要<br>
的作用。具体来说：</p>
<p dir="auto">(a)FPN(Feature Pyramid Network):FPN引入了一个自顶向下的通道，以融合从第三层到第七<br>
层的多尺度特征。</p>
<p dir="auto">(b)PANet(Path Aggregation Network):在FPN的基础上增加了一个自底向上的路径，以进一步<br>
增强特征的传递。</p>
<p dir="auto">(c)NAS-FPN(Neural Architecture Search Feature Pyramid Network):使用神经架构搜索找到了<br>
一个不规则的特征网络拓扑结构，并且在网络中重复应用相同的块。</p>
<p dir="auto">(d)BiFPN(Bidirectional Feature Pyramid Network):提出了BiFPN,具有更好的精度和效率权<br>
衡。BFPN加强了特征的上下文融合，通过增加额外的自底向上和自顶向下路径，并在多个尺度之间进行重复的特征融合。</p>
<p dir="auto">整体上，这些设计展示了如何通过不同的策略来增强多尺度特征的融合能力，BFPN作为<br>
EfficientDet模型中的一个组件，它的设计旨在通过增加简单有效的双向路径来提升目标检测的性<br>
能。</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="BiT (最大有监督CNN)" dir="auto" class="heading" id="BiT_(最大有监督CNN)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>BiT (最大有监督CNN)</h3><div class="heading-children"><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=VzJ7us4eQcE" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=VzJ7us4eQcE" target="_blank">CNN之王什么样子？谷歌造出了世界上最大的CNN模型！效果惊人！ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1912.11370" rel="noopener" class="external-link" href="https://arxiv.org/abs/1912.11370" target="_blank">Big Transfer (BiT): General Visual Representation Learning - 2019</a></p>
<p dir="auto"> Google发表. 最大CNN直接表明更大规模CNN scale 能力已经受限了. 这里发现的CNN限制为ViT的发明做了铺垫工作. </p>
<p dir="auto">文章内容:</p>
<ol>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>大力出奇迹把resnet做大到极致
<ol>
<li dir="auto">ResNet *4 : channel 64 - 256</li>
</ol>
</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>不是一般学者能玩的起的
<ol>
<li dir="auto">在ImageNet-21K 和 JFT-300M(最大公开数据集)</li>
<li dir="auto">训练了3个月</li>
</ol>
</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>他们发现了一些困难，也就是大力出奇迹效果不好于是就有了ViT
<ol>
<li dir="auto">发现 Batch Normazation 在并行训练效果不好. 于是就有了 Group Normalizaton.</li>
<li dir="auto">发现 MixUP(数据标签线性组合) 效果不好</li>
</ol>
</li>
<li dir="auto">把注意力机制做的灵活，效果会更好</li>
<li dir="auto">大力出奇迹是末期表现，一般大力出奇迹之后就会有新架构出现. e.g.VGG, BigGAN, GPT-4?</li>
</ol>
<p dir="auto">通过制造更高效的模型，更灵活的架构，而不是傻堆料</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="VanillaNet ?" dir="auto" class="heading" id="VanillaNet_?"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>VanillaNet ?</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2305.12972" rel="noopener" class="external-link" href="https://arxiv.org/abs/2305.12972" target="_blank">[2305.12972] VanillaNet: the Power of Minimalism in Deep Learning</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/search?type=content&amp;q=VanillaNet" rel="noopener" class="external-link" href="https://www.zhihu.com/search?type=content&amp;q=VanillaNet" target="_blank">VanillaNet - 搜索结果 - 知乎</a></p></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p dir="auto"><strong>VanillaNet</strong>是一种注重极简主义和效率的神经网络架构。它的设计简单，层数较少，避免了像深度架构和自注意力这样的复杂操作。VanillaNet的关键特性包括<strong>深度训练策略</strong>，最初使用激活函数训练两个卷积层，随后这个激活函数逐渐简化为恒等映射，允许层合并。此外，VanillaNet还使用<strong>并行堆叠的激活函数</strong>来提高非线性，从而提升简单网络的性能。</p>
<p dir="auto">里面的数学很复杂, 看不懂. </p></div></div></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="RNN(循环神经网络)" dir="auto" class="heading" id="RNN(循环神经网络)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>RNN(循环神经网络)</h2><div class="heading-children"><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p dir="auto">RNN&nbsp;试图通过加入反馈机制来理解一串数值前后的关系，所以适用于语言模型，因为这些数值之间有前后关系，像我们的句子里有先后逻辑</p></div></div></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="faq" class="callout drop-shadow"><div class="callout-title" dir="auto"><div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-help-circle"><circle cx="12" cy="12" r="10"></circle><path d="M9.09 9a3 3 0 0 1 5.83 1c0 2-3 3-3 3"></path><path d="M12 17h.01"></path></svg></div><div class="callout-title-inner">相比Transformer, RNN的优势在于</div></div><div class="callout-content">
<ul>
<li data-line="1" dir="auto">串行结构天然更加适合于序列数据</li>
<li data-line="2" dir="auto">推理更高效 复杂度为 O(N), 而Transformer 为 O(N^2)</li>
</ul>
</div></div></div><div><img style="width:800" src="https://cdn.sa.net/2024/06/30/DWfpLsXvMCzExFt.png" referrerpolicy="no-referrer"></div><div><ul>
<li data-line="0" dir="auto">Recurrent Neural Networks (RNNs), 1982, John Hopfield; 1986, David Rumelhart</li>
<li data-line="1" dir="auto">Long Short-Term Memory networks (LSTMs), LSTM是在1997年由Sepp Hochreiter和Jürgen Schmidhuber</li>
<li data-line="2" dir="auto">Gated Recurrent Units (GRUs), 在2014年由Kyunghyun Cho et. al</li>
</ul></div><div><p dir="auto"><img src="https://cdn.sa.net/2024/04/05/rtqmWbpCE9V7Qin.png" referrerpolicy="no-referrer"><br>
RNN 要从1D数据中学习到特征.<br>
CNN其实用一个1D卷积核配合池化层在数据上滑动其实也可以学习.</p></div><div><p dir="auto"><img src="https://cdn.sa.net/2024/04/05/EGtNkwJ3YXsaHQ9.png" referrerpolicy="no-referrer"><br>
循环神经网络（Recurrent Neural Network，RNN）的深度可以从两个方向进行扩展：时间步长和隐藏层的数量。</p></div><div><ol>
<li data-line="0" dir="auto"><strong>时间步长</strong>：在RNN中，每个时间步都可以看作是网络的一层。因此，处理更长的序列会使网络在时间维度上更深。例如，处理一个由100个词构成的句子的RNN可以被视为一个有100层的网络。然而，这种方法存在一个问题，那就是随着时间步长的增加，网络可能会遇到梯度消失或梯度爆炸的问题，这使得训练变得困难。为了解决这个问题，研究者提出了一些更复杂的RNN变体，如长短期记忆网络（Long Short-Term Memory，LSTM）和门控循环单元（Gated Recurrent Unit，GRU），它们通过引入门控机制来控制信息的流动，从而缓解了梯度消失的问题。</li>
<li data-line="1" dir="auto"><strong>隐藏层的数量</strong>：除了在时间维度上增加深度，我们也可以在空间维度上增加深度(如上图)，即增加隐藏层的数量。这可以通过堆叠多个RNN层来实现，每一层RNN的输出都作为下一层的输入。这种方法可以使网络学习更复杂的特征，但同样也会增加训练的难度。同样，可以使用LSTM或GRU等更复杂的RNN变体来缓解这个问题。</li>
</ol></div><div><p dir="auto">总的来说，使RNN变得更深可以帮助网络学习更复杂的模式，但同时也会增加训练的难度。为了解决这个问题，研究者已经提出了许多方法，如使用更复杂的RNN变体、更好的优化算法、正则化技术等。</p></div><div><blockquote dir="auto">
<p>RNN 接受两个输入：State和Token。它一次通过输入序列一个Token，每个Token更新状态。例如，我们可以使用 RNN 将文本处理成单个状态向量。然后，这可用于将文本分类为“正面”或“负面”。或者我们可以使用最终状态来预测下一个Token，这就是 RNN 用于生成文本的方式。</p>
</blockquote></div><div><p dir="auto"><img src="https://cdn.sa.net/2024/04/05/BcrxtwSMmb4ZED8.png" referrerpolicy="no-referrer"><br>
RNN 但是因为下一个参数的训练要等上一个参数算好才可以进行, 所以不适合多GPU训练.<br>
CNN 就没这个问题, 但它学习到的context windows 由取决于kernel size. </p></div><div class="heading-wrapper"><h3 data-heading="LSTM(长短期记忆网络) &amp; GRU(门控循环单元)" dir="auto" class="heading" id="LSTM(长短期记忆网络)_&amp;_GRU(门控循环单元)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>LSTM(长短期记忆网络) &amp; GRU(门控循环单元)</h3><div class="heading-children"><div><p dir="auto"><img alt="ZLubfyJEvpmVaYq.png" src="https://cdn.sa.net/2024/04/11/ZLubfyJEvpmVaYq.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto">Google's Neural Machine Translation System:Bridging the Gap between Human and Machine Translation",Wu,et al.,2016</p></div><div><p dir="auto">Stack LSTM units<br>
going deep with residual connections</p></div><div><ul>
<li data-line="0" dir="auto">enable 16 layers</li>
<li data-line="1" dir="auto">degrade 4 layers if not using residual </li>
<li data-line="2" dir="auto">like observation in CNNs</li>
</ul></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin " style="--callout-color: 0, 200, 83;" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p dir="auto">WaveNet是DeepMind在2016年提出的一种深度学习模型，它是一种生成模型，可以用于生成原始音频波形，特别是用于文本到语音（Text-to-Speech，TTS）的应用。</p>
<p dir="auto">WaveNet的主要创新之处在于它使用了一种叫做“稀疏卷积”的结构，也被称为“扩张卷积”或“膨胀卷积”。这种结构使得每个输出样本可以看到的输入范围（即感受野）随着层级的增加而指数级增长，而不是像传统的卷积神经网络那样线性增长。这使得WaveNet可以处理长范围的依赖关系，这在音频生成等序列生成任务中是非常重要的。</p>
<p dir="auto">WaveNet的另一个重要特性是它是一个完全生成的模型，也就是说，它一次生成一个样本，然后将这个样本馈送回模型，作为下一个样本的一部分输入。这种自回归的特性使得WaveNet可以生成非常自然和连贯的音频。</p>
<p dir="auto">WaveNet的效果非常出色，它在多种语音生成任务上都取得了最好的效果，包括在非常挑战的文本到语音合成任务上。事实上，Google的云语音合成服务就是基于WaveNet的。</p>
<p dir="auto">总的来说，WaveNet是一种强大的音频生成模型，它通过使用稀疏卷积和自回归结构，可以生成非常自然和连贯的音频。</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="RWKV" dir="auto" class="heading" id="RWKV"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>RWKV</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/676892576" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/676892576" target="_blank">Transformer架构的局限已凸显，被取代还有多久？ - 知乎</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://fullstackdeeplearning.com/blog/posts/rwkv-explainer/" rel="noopener" class="external-link" href="https://fullstackdeeplearning.com/blog/posts/rwkv-explainer/" target="_blank">RWKV, Explained - The Full Stack</a><br>
国人开发, 基于RNN</p></div><div><p dir="auto"> <img src="https://pic1.zhimg.com/v2-34943728e7ffdb60829f6a82fc10c610_r.jpg" referrerpolicy="no-referrer"><br>
<img alt="GPT_versus_RWKV.svg" src="https://rwkv-wiki.github.io/img/GPT_versus_RWKV.svg" referrerpolicy="no-referrer"><br>
<a data-tooltip-position="top" aria-label="https://rwkv-wiki.github.io/" rel="noopener" class="external-link" href="https://rwkv-wiki.github.io/" target="_blank">RWKV Wiki - RWKV Wiki</a><br>
<a data-tooltip-position="top" aria-label="https://wiki.rwkv.com/advance/architecture.html#how-does-rwkv-differ-from-classic-rnn" rel="noopener" class="external-link" href="https://wiki.rwkv.com/advance/architecture.html#how-does-rwkv-differ-from-classic-rnn" target="_blank">RWKV Architecture</a><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/514840332" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/514840332" target="_blank">RWKV-v2-RNN 原理：超越 Transformer，实现 O(T) 的语言建模 - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py" rel="noopener" class="external-link" href="https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py" target="_blank">ChatRWKV/RWKV_in_150_lines.py at main · BlinkDL/ChatRWKV · GitHub</a></p></div><div><ul>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>RWKV</strong>&nbsp;是一种拥有RNN, CNN, Transformer三种优点的RNN网络. 在性能媲美Transformer的基础上，具有O(1)推理复杂度，更易收敛训练，模型参数和内存占用
<ul>
<li data-line="1" dir="auto">inspired by AFT（Attention-Free Transformer）</li>
</ul>
</li>
<li data-line="2" dir="auto">背景: 所有Self-Attention 为基础的模型, 都不肯避免的需要Token之间相互计算, 造成了 O(n^2) 的复杂度; 且位置信息外挂</li>
<li data-line="3" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>RWKV针对这两个问题, 使用
<ul>
<li data-line="4" dir="auto">WKV 计算过程直接向 Token 引入了具有平移不变性的位置编码，不需要引入额外的位置编码。</li>
<li data-line="5" dir="auto">使用两个RWKV层替换了Multi-head Attention 和 前馈网络</li>
<li data-line="6" dir="auto">Token 之间无需相互运算，WKV 计算过程只对各 Token 分别变换并累加结果, 成就了 O(n)的复杂度</li>
</ul>
</li>
<li data-line="7" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>RNN 在训练时并行<img alt="image.png" src="https://cdn.sa.net/2024/01/29/jgsuCJzViMv2PEK.png" referrerpolicy="no-referrer">
<ul>
<li data-line="8" dir="auto"></li>
</ul>
</li>
<li data-line="9" dir="auto">RNN 在推理时串行</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="Mamba" dir="auto" class="heading" id="Mamba"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Mamba</h3><div class="heading-children"><div><blockquote dir="auto">
<p>Mamba 和 牢大 没有关系(认真) </p>
</blockquote></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1dH4y1G7jK/?spm_id_from=333.1007.tianma.1-3-3.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1dH4y1G7jK/?spm_id_from=333.1007.tianma.1-3-3.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">说人话解释Mamba技术原理 Transformers 又被超越了_哔哩哔哩_bilibili</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=Fc3zzOfFjsE" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=Fc3zzOfFjsE" target="_blank">Site Unreachable</a></p></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2312.00752" rel="noopener" class="external-link" href="https://arxiv.org/abs/2312.00752" target="_blank">[2023.12] Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a><br>
<a data-tooltip-position="top" aria-label="https://36kr.com/p/2547379574166664" rel="noopener" class="external-link" href="https://36kr.com/p/2547379574166664" target="_blank">颠覆Transformer霸权，CMU普林斯顿推Mamba新架构，解决致命bug推理速度暴增5倍-36氪</a></p>
<ul>
<li dir="auto">Mamba 为了解决Transformer核心注意力层上下文长度无法很好scale计算资源的局限(在大模型基础热门的背景). 为此提出了结构化空间状态模型(SSM, structured state space models). 实验显示Mamba具有参数线性扩展性(可最高百万Token级别), 和五倍的推理速度. 在多项任务中都达到SOTA. 比如Mamba-3B模型表現和两倍参数量的Transformer模型相当. 向我们展示了一个非常有希望的 Transformer 替代. </li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>研究背景
<ul>
<li dir="auto">在深度学习领域，Transformer架构及其核心的注意力机制已经成为处理各种序列数据的强大工具。然而，Transformer在处理长序列数据时存在计算效率低下的问题，这限制了其在某些应用场景中的使用。为了解决这个问题，研究者们提出了多种子二次时间复杂度的架构，如线性注意力、门控卷积和循环模型等，但这些模型在处理语言等重要模态数据时表现并不如注意力机制。</li>
</ul>
</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>过去的方案及问题
<ul>
<li dir="auto">尽管这些子二次时间复杂度的模型在计算效率上有所提升，但它们在处理离散模态数据（如文本）时，往往无法有效地进行基于内容的推理。这些模型的一个关键弱点是它们无法根据当前输入有选择地传播或遗忘信息。此外，尽管这些模型试图通过高效的卷积操作来实现，但这种变化阻止了它们使用高效的并行计算，这在硬件层面上带来了挑战。</li>
</ul>
</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>本文方案及具体步骤： 
<ul>
<li dir="auto">本文提出了一种新的选择性状态空间模型（Selective State Space Models，简称SSMs），它通过以下几个关键步骤来提高模型的性能：</li>
<li dir="auto"><strong>选择机制</strong>：通过将SSM参数设置为输入的函数，模型能够根据当前的输入选择性地传播或遗忘信息，从而在序列长度维度上动态地处理信息。在实现 Transformer 质量的性能，同时线性缩放序列长度</li>
<li dir="auto"><strong>硬件感知算法</strong>：为了解决选择性SSMs在计算上的挑战，研究者设计了一种硬件感知的并行算法，该算法以递归模式运行模型，并通过扫描操作而不是卷积来计算，同时避免了在GPU内存层次结构之间进行不必要的IO访问。</li>
<li dir="auto"><strong>简化的架构设计</strong>：将SSMs与Transformer的MLP块结合，形成了一个简化的、同质的架构（Mamba），这个架构在不使用注意力或MLP块的情况下，通过选择性状态空间实现了快速推理和线性序列长度扩展。</li>
</ul>
</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>贡献
<ul>
<li dir="auto">在多种模态（语言、音频和基因组学）上都取得SOTA性能，成为跨模态通用序列模型主干的有力候选者。</li>
</ul>
</li>
</ul></div></div></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1eC411L7Ku/?spm_id_from=333.1007.tianma.1-3-3.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1eC411L7Ku/?spm_id_from=333.1007.tianma.1-3-3.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">下个风口？Mamba手推公式&amp;代码手搓_哔哩哔哩_bilibili</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> 论文：</p>
</span></li>
<li data-line="1" dir="auto">
<p>总计：Mamba 从自动控制原理搬用了 选择状态机(Selective State Space,SSM) 。</p>
</li>
<li data-line="2" dir="auto">
<p>优势区间：序列预测，比 LSTM 记忆要好；速度快内存占用小</p>
</li>
<li data-line="5" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> 论文： <a data-tooltip-position="top" aria-label="https://github.com/yuweihao/MambaOut" rel="noopener" class="external-link" href="https://github.com/yuweihao/MambaOut" target="_blank">GitHub - yuweihao/MambaOut: MambaOut: Do We Really Need Mamba for Vision?</a></p>
</span></li>
<li data-line="6" dir="auto">
<p>问题：我们真的需要 Mamba 应用在视觉任务中么？</p>
</li>
<li data-line="7" dir="auto">
<p>方法：做了一个 Gated CNN(MambaOut) 和 Mamba 的消融实验，唯一的区别是后者有一个 SSM </p>
</li>
<li data-line="8" dir="auto">
<p>结果：发现 MambaOut 和 VMamba 效果类似</p>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="Test-Time Training" dir="auto" class="heading" id="Test-Time_Training"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Test-Time Training</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=_UWxAIldem4" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=_UWxAIldem4" target="_blank"># 【人工智能】最新大模型架构TTT | Test-Time Training | 测试时间训练层 | 用机器模型代替RNN隐藏层 | 能否超越Transformer和Mamba | 五年想法终成真</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/661229789/answer/3558938939" rel="noopener" class="external-link" href="https://www.zhihu.com/question/661229789/answer/3558938939" target="_blank"># 全新大语言模型架构 TTT 匹敌Transformer和Mamba，12人团队半数为华人，有哪些突破？</a></p></div><div><ul>
<li data-line="0" dir="auto">本质上使用一个小模型的 Parameters 来替代 Transformer 中的 Hidden Layer</li>
<li data-line="1" dir="auto">RNN 本质上把过去所有时间的 Token 信息，用一个低维向量表示，是一种压缩所以能找到语义关系。但是窗口长了把所有历史信息都撞到固定大小的 Hidden feature 就肯定会装不下所有信息。可以理解为横着，沿着时间轴压缩。</li>
<li data-line="2" dir="auto">Transformer 则是竖着，沿着每个 Token 维度压缩， 所有被处理过的token的hidden vectors共同构成了hidden state</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="GAN(对抗生成网络)" dir="auto" class="heading" id="GAN(对抗生成网络)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>GAN(对抗生成网络)</h2><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=WlUxIge5XjE" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=WlUxIge5XjE" target="_blank">2016年最火的模型：对抗生成网络GAN讲了什么？ - YouTube</a></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/18/Lp31Wo7MJ6Z58KV.png" referrerpolicy="no-referrer"><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1406.2661" rel="noopener" class="external-link" href="https://arxiv.org/abs/1406.2661" target="_blank">[1406.2661] Generative Adversarial Networks</a><br>
<img src="https://cdn.sa.net/2024/04/16/spOkPS5qRoUnYa7.png" referrerpolicy="no-referrer"></p>
<ul>
<li dir="auto"><strong>原理</strong>: 基于博弈论** 生成模型G用于捕获数据分布， 判别器D判断样本是不是来自于G,G的训练过程是最大化D出错的概率 两者做一个Min-Max游戏。在训练或生成样本期间不需要任何马尔可夫链或展开的近似推理网络。</li>
<li dir="auto"><strong>优点</strong></li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>缺点</strong>
<ul>
<li dir="auto">高分辩率的图片生成困难</li>
<li dir="auto">难以训练</li>
<li dir="auto">容易被恶意攻击</li>
<li dir="auto">很难和其他 Control 方法结合</li>
</ul>
</li>
</ul>
<p dir="auto">Generator 和 Discriminator. </p></div></div></div><div><p dir="auto">GAN (生成对抗网络)、CGAN、DCGAN、WGAN (Wasserstein GAN)、StyleGAN、CycleGAN</p></div><div><p dir="auto">RealESRGAN<br>
ESRGAN<br>
GFPGAN</p></div></div></div><div class="heading-wrapper"><h2 data-heading="Autoencoder(自编码器)" dir="auto" class="heading" id="Autoencoder(自编码器)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Autoencoder(自编码器)</h2><div class="heading-children"><div><p dir="auto">auto xxx 就是 输入-输出 都是同一类型</p></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=6KH0moTLpF8" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=6KH0moTLpF8" target="_blank">无监督学习开山之作：自编码器AutoEncoder讲了什么？ - YouTube</a></p>
<p dir="auto"><img alt="ZywXCz1gJvmfp86.png" src="https://cdn.sa.net/2024/04/19/ZywXCz1gJvmfp86.png" referrerpolicy="no-referrer"></p>
<ul>
<li dir="auto">
<p><strong>AutoEncoder 有 Encode和Decode两个部分</strong>. Encoder 负责将输入映射到 latent space, decoder 负责将 latent space 的输入尽可能映射回原始输入. 由于latent space 的维度要远远小于输入维度, 所以通常我们可以最终获得一个原始输入的压缩表示. (通过降维度和输入输出结偶进行输入输出的非线性变化, 完成压缩)</p>
</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p>历史</p>
<ul>
<li dir="auto">Reducing the dimensionality of data with neural networks" (2006) by G. E.Hinton and R. R. Salakhutdinov. 这篇论文是深度自编码器的开创性工作, 提出了一个名为“受限玻滋曼机”模型, 用于进行数据降维任务, 这可以被认为是自编码器的前身. </li>
<li dir="auto">Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion(2008). 提出了一种堆叠去噪自编码器, 通过对自编码器训练是添加噪声使其编码器获得更加鲁棒的表示</li>
<li dir="auto">"Contractive Auto-Encoders: Explicit Invariance During Feature Extraction"(2010) 这篇论文提出了收缩自编码器。收缩自编码器是一种自编码器的变体，它在损失函数中添加了一个项，使得编码对输入的小变化不敏感。这使得收缩自编码器能够学习到输入数据的稳健表示。</li>
<li dir="auto">"Auto-Encoding Variational Bayes"：这篇论文提出了变分自编码器。变分自编码器是一种自编码器的变体，它使用了贝叶斯推理的思想，使得编码不仅仅是一个确定的值，而是一个概率分布。这使得变分自编码器能够生成类似于输入数据的新数据。</li>
</ul>
</li>
<li dir="auto">
<p>U-Net 也算是一种Autoencoder的改进</p>
</li>
</ul></div></div></div><div><blockquote dir="auto">
<p>生成模型<br>
<img alt="sXVMmcQZrtYpqn6.png" src="https://cdn.sa.net/2024/04/16/sXVMmcQZrtYpqn6.png" referrerpolicy="no-referrer"></p>
</blockquote></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p dir="auto">Autoencoder + 概率模型</p>
<p dir="auto">概率图 + 变分贝叶斯</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1sq4y1q77t/?spm_id_from=333.337.search-card.all.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1sq4y1q77t/?spm_id_from=333.337.search-card.all.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">MAE 论文逐段精读【论文精读】_哔哩哔哩_bilibili</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2111.06377" rel="noopener" class="external-link" href="https://arxiv.org/abs/2111.06377" target="_blank">Masked Autoencoders Are Scalable Vision Learners</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/search?type=content&amp;q=Maked%20Autoencoder%20" rel="noopener" class="external-link" href="https://www.zhihu.com/search?type=content&amp;q=Maked%20Autoencoder%20" target="_blank">Maked Autoencoder - 搜索结果 - 知乎</a></p>
<p dir="auto"><img alt="OsZkClwKLYWTVJt.png" src="https://cdn.sa.net/2024/04/22/OsZkClwKLYWTVJt.png" referrerpolicy="no-referrer"></p>
<ul>
<li dir="auto">Kaiming He 在2021年的一作, 在知乎上讨论很高</li>
<li dir="auto">介绍了Masked Autoencoder 这篇文章, 客观上加速了Transformer中CV中的应用</li>
<li dir="auto">实现的原理类似于(Noise2Noise, Denoising AE)和Bert. 随机遮蔽输入图像的一部分, 重构这些被遮蔽的像素信息. 使用一个非对称的编码-解码器, 编码器只看到哪些可见的快, 解码器只负责重构被遮蔽的块. 用于减少内存开销</li>
<li dir="auto">发现遮蔽75%, 用剩下的25%去预测Mask的75%, 可以迫使大模型学习跟多特征</li>
<li dir="auto">在ViT-huge上发现只使用ImageNet-1K进行self-supervised 可以媲美有标号的任务</li>
<li dir="auto">发现Transformer 的确可以学习到很好hidden features</li>
<li dir="auto">CNN也可以用Mask, 比如用一个特定的value设定Mask的值. 卷积很难区分正常pixel和mask pixel的边界</li>
<li dir="auto">提到图片的像素是冗余的, 去除了邻居的像素(高比特率), 允许模型学习全局信息.</li>
<li dir="auto">实验证明在Object Detection, Instance Segementation, Semantic Segmentation的效果都很好. 取得了BERT的在NLP任务类似的效果<br>
<img src="https://cdn.sa.net/2024/04/22/TJcpnLUi1P7hGNx.png" referrerpolicy="no-referrer"></li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="Diffusion(扩散模型)" dir="auto" class="heading" id="Diffusion(扩散模型)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Diffusion(扩散模型)</h2><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://course.fast.ai/" rel="noopener" class="external-link" href="https://course.fast.ai/" target="_blank">Practical Deep Learning for Coders - Practical Deep Learning</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/649097976/answer/3483298372" rel="noopener" class="external-link" href="https://www.zhihu.com/question/649097976/answer/3483298372" target="_blank">为什么vae效果不好，但vae+diffusion效果就好了？</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/716200211" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/716200211" target="_blank"># DDPMs for prediction（一）：DDPM的基本原理和实现</a></p></div><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p dir="auto">这是原来的Diffusion model, 原来效果很差. 我们现在加一个CLIP控制模型的生成.然后将这个 加噪去噪的过程重复几次, 效果就发现很不错. 降噪的backbone 是 U-Net. 我们叫这个新的模型是Stable Diffusion.</p>
<p dir="auto">后面有更多的更多控制生成的控制机制, 比如ControlNet.</p></div></div></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/634573765" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/634573765" target="_blank">stable diffusion原理解读通俗易懂，史诗级万字爆肝长文，喂到你嘴里 - 知乎</a><br>
<img src="https://pic1.zhimg.com/v2-34de1acf7e698b2ef5e37f9562ed15dc_r.jpg" referrerpolicy="no-referrer"></p></div><div><p dir="auto"><img src="https://pic1.zhimg.com/v2-9827eeb683a871ca5f08e64df6411294_r.jpg" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0" dir="auto">Stable Diffusion的原理可以抽象看作forward add noise 和 backward denoising 两部分</li>
<li data-line="1" dir="auto">VAE 首先将猫猫图片, 比如 512*512*3, 从像素空间转换到 latent space, 大小可能是4x64x64. 完成对输入图像的压缩. </li>
<li data-line="2" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>forward diffusion
<ul>
<li data-line="3" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>对 latent space中的猫猫照片(<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container></span>)一步一步的添加高斯噪声, 直到噪声布满整个猫猫(<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span>)
<ul>
<li data-line="4" dir="auto">由于添加的噪声是高斯噪声, 所以不用顺序的加噪. 可以直接累加.</li>
</ul>
</li>
</ul>
</li>
<li data-line="5" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>backward denoising (已知 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span>  -&gt; <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container></span>)
<ul>
<li data-line="6" dir="auto">一个使用ResNet作为主干的U-Net 进行 backward 降噪. </li>
<li data-line="7" dir="auto">输入: Text Embedding + 噪声图片</li>
<li data-line="8" dir="auto">输出: 一个去噪声潜特征空间表达</li>
<li data-line="9" dir="auto">因为满足高斯分布, 可以使用贝叶斯公式连立公式反转马尔可夫链.  只有噪声是未知的. </li>
<li data-line="10" dir="auto">网络输入为 当前时刻的分布<strong>Xt</strong>和时刻<strong>t</strong>，还有之前的<strong>文本向量</strong>. 输出预测噪声</li>
</ul>
</li>
</ul></div><div><p dir="auto"><img src="https://pic1.zhimg.com/v2-8f0baddac9b7893b4cc35cef7f606f58_r.jpg" referrerpolicy="no-referrer"></p></div><div><p dir="auto">上面的Algorithm 1是训练过程，</p></div><div><p dir="auto">其中第二步表示取数据，一般来说都是一类猫，狗什么的，或者一类风格的图片，不能乱七八糟什么图片都来，那模型学不了。</p></div><div><p dir="auto">第三步是说每个图片随机赋予一个时刻的噪声（上面说过），</p></div><div><p dir="auto">第四步，噪声符合高斯分布，</p></div><div><p dir="auto">第五步，真实的噪声和预测的噪声算损失（DDPM输入没有文本向量，所有没有写，你就理解为多加了一个输入），更新参数。直到训练的输出的噪声和真实噪声相差很小，Unet模型训练完毕</p></div><div><p dir="auto">下面我们来到Algorithm2采样(马尔可夫)过程</p></div><div><ol>
<li data-line="0" dir="auto">不就是说Xt符合高斯分布嘛</li>
<li data-line="1" dir="auto">执行T次，依次求Xt-1到X0，不是T个时刻嘛(两步之间可以预测)</li>
<li data-line="2" dir="auto">Xt-1不就是我们逆向扩散推出的公式，Xt-1=μ+σZ，均值和方差都是已知的，唯一的未知噪声Z被Unet模型预测出来，<strong>εθ</strong>这个是指已经训练好的UNet</li>
</ol></div><div><blockquote dir="auto">
<p>Prompt -&gt; CLIP -&gt; Text Embedding -&gt; 借由 Cross-Attention 暴露给去噪U- Net</p>
</blockquote></div><div><pre><code>- 其他语义也可以借此串联的进行Control. e.g. 语义图、 图像、 Inpaint
</code><button class="copy-code-button">复制</button></pre></div><div class="heading-wrapper"><h3 data-heading="DiT" dir="auto" class="heading" id="DiT"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>DiT</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://mp.weixin.qq.com/s/hSVLWCX2XfTgkmYeki43Bg" rel="noopener" class="external-link" href="https://mp.weixin.qq.com/s/hSVLWCX2XfTgkmYeki43Bg" target="_blank"># 神经网络算法 - 一文搞懂DiT（Diffusion Transformer）</a></p></div></div></div><div class="heading-wrapper"><h3 data-heading="下游应用" dir="auto" class="heading" id="下游应用"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>下游应用</h3><div class="heading-children"><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=ZV2onyy35uU" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=ZV2onyy35uU" target="_blank">最新大作：StoryDiffusion，生成连续一致视频，目前Diffusion最好成绩！ - YouTube</a></span></li>
</ul></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=CSDgyma9RUo" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=CSDgyma9RUo" target="_blank">【博士Vlog】Diffusion生成式模型新思路，用时间序列来降噪 - YouTube</a></p>
<ul>
<li dir="auto">介绍了使用Diffusion生成时间序列的论文</li>
<li dir="auto">把UNet改成RNN</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=Uav0zrV6A9U" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=Uav0zrV6A9U" target="_blank">【博士详解】Diffusion和GAN是怎么回事？各自有什么优缺点？ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2105.05233" rel="noopener" class="external-link" href="https://arxiv.org/abs/2105.05233" target="_blank">[2105.05233] Diffusion Models Beat GANs on Image Synthesis</a></p>
<ul>
<li dir="auto">介绍了这篇论文, </li>
</ul>
<p dir="auto">Diffusion 和 GAN 各自有什么优缺点?</p>
<p dir="auto"> Diffusion模型和GAN（生成对抗网络）都是深度学习中广泛使用的生成模型，它们各自有着不同的优缺点。</p>
<p dir="auto">Diffusion模型：<br>
优点：</p>
<ol>
<li dir="auto">理论依据强：Diffusion模型基于物理学中的扩散过程，有着坚实的理论依据。</li>
<li dir="auto">可解释性强：相比于GAN，Diffusion模型的生成过程更容易解释和理解。</li>
<li dir="auto">不依赖于对抗训练：Diffusion模型不需要像GAN那样进行对抗训练，训练过程更稳定。</li>
<li dir="auto">Backward denoising 过程可以借由 cross-attention 机制, 增加各种形式的control</li>
</ol>
<p dir="auto">缺点：</p>
<ol>
<li dir="auto">生成速度慢：由于需要进行多步的扩散过程，Diffusion模型生成样本的速度相比于GAN要慢。</li>
<li dir="auto">需要更多的计算资源：Diffusion模型通常需要更多的计算资源和存储空间。</li>
</ol>
<p dir="auto">GAN：<br>
优点：</p>
<ol>
<li dir="auto">生成质量高：GAN通常能够生成高质量、高分辨率的样本。</li>
<li dir="auto">生成速度快：一旦训练完成，GAN生成样本的速度非常快。</li>
</ol>
<p dir="auto">缺点：</p>
<ol>
<li dir="auto">训练不稳定：GAN的训练过程常常不稳定，可能会遇到梯度消失、模式崩溃等问题。</li>
<li dir="auto">对抗训练难以优化：GAN的对抗训练过程是一个最小化最大化问题，这使得优化过程非常困难。(熊猫_img + 线虫_img = 丹顶鹤_img ????) -&gt; 容易被攻击</li>
<li dir="auto">可解释性差：GAN的生成过程是一个黑盒过程，可解释性较差。</li>
<li dir="auto"></li>
</ol></div></div></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="Transformer (2017)" dir="auto" class="heading" id="Transformer_(2017)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Transformer (2017)</h2><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://paperswithcode.com/paper/attention-is-all-you-need" rel="noopener" class="external-link" href="https://paperswithcode.com/paper/attention-is-all-you-need" target="_blank">Attention Is All You Need | Papers With Code</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://nlp.seas.harvard.edu/annotated-transformer/" rel="noopener" class="external-link" href="https://nlp.seas.harvard.edu/annotated-transformer/" target="_blank">The Annotated Transformer</a><br>
<a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=XfpMkf4rD6E" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=XfpMkf4rD6E" target="_blank">Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy - YouTube</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Ja4y1B7zC/?spm_id_from=333.788.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Ja4y1B7zC/?spm_id_from=333.788.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">[重置版]从零实现transfomer模型 || 理解ChatGPT基石 || pytorch_哔哩哔哩_bilibili</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/648127076?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1716622508470669312&amp;utm_source=io.raindrop.raindropio" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/648127076?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1716622508470669312&amp;utm_source=io.raindrop.raindropio" target="_blank">三万字最全解析！从零实现Transformer（小白必会版😃） - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/zvideo/1722621858900451329?utm_psn=1733842036934078467" rel="noopener" class="external-link" href="https://www.zhihu.com/zvideo/1722621858900451329?utm_psn=1733842036934078467" target="_blank">哈工大PHD竟把Transformer讲的如此简单！ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/589738603/answer/3375664602" rel="noopener" class="external-link" href="https://www.zhihu.com/question/589738603/answer/3375664602" target="_blank">为什么只有基于Transformer的大模型，而没有其他的？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/596771388/answer/3454101159?utm_medium=social&amp;utm_psn=1760630215855116288&amp;utm_source=ZHShareTargetIDMore" rel="noopener" class="external-link" href="https://www.zhihu.com/question/596771388/answer/3454101159?utm_medium=social&amp;utm_psn=1760630215855116288&amp;utm_source=ZHShareTargetIDMore" target="_blank">为什么我还是无法理解transformer？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=eMlx5fFNoYc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=eMlx5fFNoYc" target="_blank">可视化注意力，变形金刚的心脏 | 第 6 章，深度学习 - YouTube</a> (强烈推荐)<br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/500807675" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/500807675" target="_blank">一张图看懂BERT - 知乎</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb" rel="noopener" class="external-link" href="https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb" target="_blank">Build your own Transformer from scratch using Pytorch | by Arjun Sarkar | Towards Data Science</a><br>
<a data-tooltip-position="top" aria-label="https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021" rel="noopener" class="external-link" href="https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021" target="_blank">All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 1 | by Arjun Sarkar | Towards Data Science</a><br>
<a data-tooltip-position="top" aria-label="https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada" rel="noopener" class="external-link" href="https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada" target="_blank">All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 2 | by Arjun Sarkar | Towards Data Science</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://jalammar.github.io/illustrated-transformer/" rel="noopener" class="external-link" href="https://jalammar.github.io/illustrated-transformer/" target="_blank">The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/656637736/answer/3505072060" rel="noopener" class="external-link" href="https://www.zhihu.com/question/656637736/answer/3505072060" target="_blank">zhihu.com/question/656637736/answer/3505072060</a><br>
<img src="https://cdn.sa.net/2024/06/07/3YousaJRfdLPckg.png" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">%</span> 提出背景</p>
</span></li>
<li data-line="1" dir="auto">
<p><strong>2013, RCTM</strong>, 神经网络机器翻译(NMT)开篇之作. 使用 CNN 将连续文本表示为向量(Context, Encoder) 然后传入 RNN 去拟合目标句子(end-to-end)。奠定了 Encoder-Decoder。</p>
</li>
<li data-line="2" dir="auto">
<p><strong>RNN Encoder-Decoder(Cho et al., 2014a)</strong>, Encoder 变成了 RNN. 为了解决长文本的梯度消失/爆炸，然后顺便提出了 GRU， 引入更新门和重置门来控制信息的流动。但是 Encoder和 Decoder 是分开训练的。</p>
</li>
<li data-line="3" dir="auto">
<p><strong>Bahdanau Attention(Bahdanau et al., 2014)</strong>, 来自和RNN Encoder-Decoder 的同一个团队。首次提出了 <span style="background:#fff88f">Attention</span> 这个概念。认为翻译过程中语言之间应该可以对齐 e.g "How(多) old(大) are you(你)". 所以设计了能够度量两种语言之间不同词之间的关联分数 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo></mjx-math></mjx-container></span> (加法注意力)Attention Score. 用于<span style="background:#fff88f">动态的计算输入输出的匹配程度</span>。 注意力在计算解码器隐藏状态之前进行也成为 pre-attention. </p>
</li>
<li data-line="4" dir="auto">
<p><strong>Seq2Seq (Sutskever et al., 2014)</strong>, 之前的神经网络模型通常处理固定大小的输入与输出, Seq2Seq 模型通过编码器将任意长度的输入序列转化为固定长度的向量，并通过解码器将该向量转化为任意长度的输出序列，<span style="background:#fff88f">可以自由处理任意长度的输入和输出</span>。一开始使用的 LSTM 后转用注意力机制。还有一些其他优化技巧在当时堆了很大的参数量取得了很好的效果。</p>
</li>
<li data-line="5" dir="auto">
<p><strong>Luong Attention (Luong et al., 2015)</strong>, 提出了几种计算 注意力分数的方法(e.g.dot product, <span style="background:#fff88f">scaled dot product</span>) . 注意力在计算解码器隐藏状态之后进行也成为 post-attention. </p>
</li>
<li data-line="6" dir="auto">
<p><strong><span style="background:#fff88f">Self attention</span></strong>，之前的 Attention 关注的都是不同序列之间的 Attention，比如机器翻译的原始文本和目标文本，文生图 中的文本序列和图像序列，这些现在被称为Cross Attention. </p>
</li>
<li data-line="7" dir="auto">
<p><strong>MultiHead Self Attention (Lin et al., 2017)</strong>, 借鉴一下卷积网络的多个 kernel 的思想(ResNexT)，作者设计了<span style="background:#fff88f">多个 Attention</span>，期望不同的 Attention 学习到不同的注意力，从而更好的提取特征</p>
</li>
<li data-line="8" dir="auto">
<p><strong>Frustratingly Short Attention Spans in Neural Language Modeling, Micha et al., 2017)</strong>, 这篇论文认为隐状态同时承担了计算注意力分数，计算 context value和隐状态这样任务太重量。提出了使用<span style="background:#fff88f">key</span> 用来计算 attention score；<span style="background:#fff88f">value</span> 用来和 Atention score 相乘；predict 用于预测词的分布</p>
</li>
<li data-line="9" dir="auto">
<p><strong>ResNet(Kaiming, 2015)</strong> , 为了解决加了网络层数性能反而下降的问题(网络退化)，提出了残差链接允许前一层的输出作为后面层的输入的一部分，使得层需要学习的就变成了输入输出差值，即<span style="background:#fff88f">残差</span>。使用一个线性层来调整维度。</p>
</li>
<li data-line="10" dir="auto">
<p><span style="background:#fff88f"> “Attention is all you need”, 2017</span> 。 完全使用自注意力机制取代 RNN, 用于解决梯度爆炸/消失和无法并行计算。由Google Brain团队的研究员在2017年撰写的论文，这篇论文首次提出了 Transformer 模型, 使用自注意力机制(Self-attention将序列问题转换为全连接层问题, 使模型能够在不同位置之间建立动态的关系，避免了传统循环神经网络（RNN）和长短时记忆网络（LSTM）的局限(难以捕捉长序列之间关系, 容易梯度爆炸/消失, 序列结构难以并行, 模型参数随序列长度指数增长, 难以批处理).  强力推动了自然语言处理, 计算机视觉等多领域。</p>
</li>
<li data-line="11" dir="auto">
<img src="https://cdn.sa.net/2024/05/07/rYqOKhGyZV7INkt.png" referrerpolicy="no-referrer" style="width: 400px; max-width: 100%;">
</li>
<li data-line="12" dir="auto" class="lc-list-callout" data-callout="~" style="--lc-callout-color: 124, 77, 255;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">~</span> Blocks : </p>
</span></li>
<li data-line="13" dir="auto">
<p><code>Embedding</code>嵌入层 </p>
</li>
<li data-line="14" dir="auto">
<p><code>Position Encoding</code> 位置编码</p>
</li>
<li data-line="15" dir="auto">
<p><code>Multi-Head Attention</code> 多头注意力</p>
</li>
<li data-line="16" dir="auto">
<p><code>Add &amp; Norm</code> 残差链接和层规范化</p>
</li>
<li data-line="17" dir="auto">
<p><code>FeedForward</code>  前向网络</p>
</li>
<li data-line="18" dir="auto">
<p><code>Encoder</code> 和 <code>Decoder</code></p>
</li>
<li data-line="20" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> Transformer 是一种表达能力非常强的网络。除了自注意力带来的全局注意力机制之外，残差的链接方式，softmax, scaled dot product, 层归一化等各种设计都相辅相成，允许优化器尽可能的优化这些权重。并且matrix operation 的方式非常适合 GPU 并行计算。其模块化的设计和残差链接，允许网络学习数据的一种最短表达。</p>
</span></li>
</ul></div><div class="heading-wrapper"><h3 data-heading="`Multi-Head Attention` 多头注意力" dir="auto" class="heading" id="`Multi-Head_Attention`_多头注意力"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><code>Multi-Head Attention</code> 多头注意力</h3><div class="heading-children"><div><img style="width:500" src="https://cdn.sa.net/2024/06/09/M9zhZeJljmaKxLP.png" referrerpolicy="no-referrer"></div><div><p dir="auto">对于输入序列中的每一个元素，自注意力允许模型在计算其表示时考虑序列中的所有其他元素。自注意力的计算可以表述为：</p></div><div><div class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="4"><mjx-c class="mjx-c73"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c66"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c78"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.082em;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></div></div><div><p dir="auto">使用多个注意力头并行地处理信息，然后将结果拼接起来，捕捉不同子空间的信息：</p></div><div><div class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c4D"></mjx-c><mjx-c class="mjx-c75"></mjx-c><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c48"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c64"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="4"><mjx-c class="mjx-c43"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c63"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c74"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c68"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c64"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c68"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c64"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.413em; margin-left: 0.055em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math></mjx-container></div></div><div><p dir="auto">其中每个头部<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c68"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c64"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span>的计算为：</p></div><div><div class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c68"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c64"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="4"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-msubsup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.294em; margin-left: -0.104em;"><mjx-mi class="mjx-i" size="s" style="margin-left: 0.225em;"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-spacer style="margin-top: 0.18em;"></mjx-spacer><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-msubsup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.247em; margin-left: -0.104em;"><mjx-mi class="mjx-i" size="s" style="margin-left: 0.225em;"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-spacer style="margin-top: 0.193em;"></mjx-spacer><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-msubsup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.25em; margin-left: -0.104em;"><mjx-mi class="mjx-i" size="s" style="margin-left: 0.225em;"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-spacer style="margin-top: 0.18em;"></mjx-spacer><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></div></div><div><p dir="auto"><span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msubsup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.294em; margin-left: -0.104em;"><mjx-mi class="mjx-i" size="s" style="margin-left: 0.225em;"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-spacer style="margin-top: 0.18em;"></mjx-spacer><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msubsup space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.284em; margin-left: -0.104em;"><mjx-mi class="mjx-i" size="s" style="margin-left: 0.225em;"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-spacer style="margin-top: 0.18em;"></mjx-spacer><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msubsup space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.294em; margin-left: -0.104em;"><mjx-mi class="mjx-i" size="s" style="margin-left: 0.225em;"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-spacer style="margin-top: 0.18em;"></mjx-spacer><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msubsup></mjx-math></mjx-container></span> 和 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.055em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math></mjx-container></span> 是可学习的权重矩阵。</p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 为什么要这样设计 QKV </p>
</span></li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 注意力分数为什么要除以 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.082em;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math></mjx-container></span> <a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/339723385/answer/3513306407" rel="noopener" class="external-link" href="https://www.zhihu.com/question/339723385/answer/3513306407" target="_blank">知乎</a> <a data-tooltip-position="top" aria-label="https://spaces.ac.cn/archives/9812" rel="noopener" class="external-link" href="https://spaces.ac.cn/archives/9812" target="_blank">从梯度最大化看Attention的Scale操作 - 科学空间|Scientific Spaces</a></p>
</span></li>
<li data-line="5" dir="auto">
<p>Q K 点积(注意力分数)的方差会随着 维度<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span> 变大而累加，造成元素之间的差异变大，使得 Softmax 更加有可能退化成 Argmax。 而 Argmax 本质上离散的，是不可微的，放在雅克比矩阵里就是只有一个 1 其他全是 0. 同时除以 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span> 也让注意力分数的方差为 1，有助于训练。</p>
</li>
<li data-line="7" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 为什么注意力权重要通过 Softmax 输出？为什么不直接使用 Argmax 或者 Sigmoid? </p>
</span></li>
<li data-line="8" dir="auto">
<p>输出分类概率基本两个选择 Sigmoid 和 Softmax. </p>
</li>
<li data-line="9" dir="auto">
<p>首先 Softmax 输入是一个多元变量，输出的多个类别的输出缩放到一个相对概率分布, 适用于多分类问题。自注意力机制因为要考虑到整个输入序列对于当前 Query 的情况(属于多分类问题)，故使用 Softmax。 相比 Sidmoid， Softmax 的输出更加不均匀适合无监督任务。</p>
</li>
<li data-line="12" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 为什么要使用多头(MHA)？为什么要有这个机制？</p>
</span></li>
<li data-line="13" dir="auto">
<p>多头自注意力机制允许模型共同处理来自不同表示子空间的信息。我的理解是一种多头是一种类似集成学习(ensemble)中的 stacking 方法。单独训练多个基模型后用某种方法合并。多头注意力中合并后要经过一个可学习的线性层就是试图动态调整多个头重要性的权重。同时，注意力机制本身和多头由于是独立计算的，也可以进一步提高模型的并行度，方便多卡集群训练.</p>
</li>
<li data-line="16" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 不同的注意力机制 <img style="width:500" src="https://picx.zhimg.com/v2-162b5bc0cff35cb5368e8ac58ee85845_r.jpg?source=1def8aca" referrerpolicy="no-referrer"></p>
</span></li>
<li data-line="21" dir="auto" class="lc-list-callout" data-callout="$" style="--lc-callout-color: 0, 200, 83;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">$</span> Attention vs Convolution as features extraction<br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/643991477?utm_psn=1765147760792506368" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/643991477?utm_psn=1765147760792506368" target="_blank">卷积 vs. 注意力</a></p>
</span></li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="`Position Encoding` 位置编码" dir="auto" class="heading" id="`Position_Encoding`_位置编码"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><code>Position Encoding</code> 位置编码</h3><div class="heading-children"><div><blockquote dir="auto">
<p>因为Transformer没有循环或卷积结构来处理时序信息，位置编码被添加到输入embedding中以包含序列中元素的位置信息</p>
</blockquote></div><div><div class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c50"></mjx-c><mjx-c class="mjx-c45"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.177em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-n" space="4"><mjx-c class="mjx-c73"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mrow space="2"><mjx-mo class="mjx-s3"><mjx-c class="mjx-c28 TEX-S3"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msup><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-script style="vertical-align: 0.393em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c64"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c6C"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub></mjx-texatom></mjx-script></mjx-msup></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-s3"><mjx-c class="mjx-c29 TEX-S3"></mjx-c></mjx-mo></mjx-mrow></mjx-math></mjx-container></div></div><div><div class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c50"></mjx-c><mjx-c class="mjx-c45"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.177em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-n" space="4"><mjx-c class="mjx-c63"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c73"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mrow space="2"><mjx-mo class="mjx-s3"><mjx-c class="mjx-c28 TEX-S3"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msup><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-script style="vertical-align: 0.393em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c64"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c6C"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub></mjx-texatom></mjx-script></mjx-msup></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-s3"><mjx-c class="mjx-c29 TEX-S3"></mjx-c></mjx-mo></mjx-mrow></mjx-math></mjx-container></div></div><div><p dir="auto">   其中<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>是位置，<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>是维度，<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c64"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c6C"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub></mjx-math></mjx-container></span>是模型的维度。这些位置编码有相同的维度<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c64"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c6C"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub></mjx-math></mjx-container></span>，因此可以将它们加到实际的序列embedding上。</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://spaces.ac.cn/archives/8130" rel="noopener" class="external-link" href="https://spaces.ac.cn/archives/8130" target="_blank">让研究人员绞尽脑汁的Transformer位置编码 - 科学空间|Scientific Spaces</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://kexue.fm/archives/8231" rel="noopener" class="external-link" href="https://kexue.fm/archives/8231" target="_blank">Transformer升级之路：1、Sinusoidal位置编码追根溯源 - 科学空间|Scientific Spaces</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 为什么要使用位置编码? 有什么种类的 Position Encoding?</p>
</span></li>
<li data-line="1" dir="auto">
<p>关键在于 Transformer 中自注意力是只用 Non-local 方法， 整个序列在点积中统一的被处理失去前后关系，不像是 CNN 或者 RNN 这类 local 方法天然带位置信息。所以需要一个额外的方法帮助模型感知到序列顺序。Causal Attention 理论上可以不用加 PE 但是加了 PE 效果好。 <a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/640465759/answer/3488522233" rel="noopener" class="external-link" href="https://www.zhihu.com/question/640465759/answer/3488522233" target="_blank"># 基于Decoder的LLM为何需要位置编码？</a></p>
</li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> Postion Encoding 基本上可以分为：</p>
</span><ul>
<li data-line="3" dir="auto"><span style="background:#fff88f">可学习的位置编码</span>(Position Embedding)： 直接把 PE 作为一个参数放在模型中，期待可以在大数据中自动学习序列的相对关系. 代表, BERT. </li>
<li data-line="4" dir="auto"><span style="background:#fff88f">绝对位置编码</span>：对每个序列和 Embedding 位置赋予一个绝对唯一的位置信息。代表, Transformer, GPT中的正弦-余弦位置编码。使用周期函数的交替使用和在特征维度上的频率缩放赋予每个token唯一的位置信息。</li>
<li data-line="5" dir="auto"><span style="background:#fff88f">相对位置编码</span>：微调注意力机制，代表，旋转位置编码(RoPE)，直接在 Q 上哈达玛积一个 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span></li>
</ul>
</li>
<li data-line="8" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> Transformer 中使用 <code>Sinusoidal Position Embedding</code></p>
<p align="center">
</p><pre><code>&lt;img src="https://cdn.sa.net/2024/05/14/5Ma4P6ZSepFo1wK.png" alt="alt text" height="300"&gt;
&lt;img src="https://cdn.sa.net/2024/05/14/BogsfYTXM5CIL21.png" alt="alt text" height="300"&gt;
&lt;br&gt;
&lt;i&gt;Fig. 正弦-余弦编码规则可视化(embed_dim=32, max_seq_len=50)&lt;/i&gt;
</code><button class="copy-code-button">复制</button></pre>
<p></p>
</span></li>
<li data-line="16" dir="auto">
<p>直观理解就是一个时钟系统。时钟有小时、分钟、秒三个颗粒度，而这个编码有 dim_size / 2 个颗粒度。第i个粒度的旋转角度是 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span>.</p>
</li>
<li data-line="17" dir="auto">
<p>这个积化合差公式对绝对位置的编码可以看作相对位置 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> 的一个线性变换 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.026em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.026em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span>, 这个变换就是每个位置旋转的方式。</p>
</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/647150549/answer/3520531981" rel="noopener" class="external-link" href="https://www.zhihu.com/question/647150549/answer/3520531981" target="_blank">Transformers 中，词向量的排列本身就包含了位置信息，为什么还需要位置编码？</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 什么是旋转位置编码(RoPE)? 为什么现在流行使用 RoPE<br>
<a data-tooltip-position="top" aria-label="https://spaces.ac.cn/archives/8265" rel="noopener" class="external-link" href="https://spaces.ac.cn/archives/8265" target="_blank">Transformer升级之路：2、博采众长的旋转式位置编码 - 科学空间|Scientific Spaces</a><br>
<a data-tooltip-position="top" aria-label="https://spaces.ac.cn/archives/10122" rel="noopener" class="external-link" href="https://spaces.ac.cn/archives/10122" target="_blank">Transformer升级之路：18、RoPE的底数设计原则</a></p>
</span></li>
<li data-line="5" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 什么是编码位置外推问题？</p>
</span></li>
<li data-line="6" dir="auto">
<p>编码位置外推是训练、推理的长度不一致问题，所导致推理的时候有训练没见过的位置编码和推理的时候attention span更大，导致墒增。</p>
</li>
<li data-line="7" dir="auto">
<p>第一个问题解决方案：ALIBI、KERPLE、Sandwich、XPOS、PI、NTK-RoPE(目前看起来这个最强，不用finetune)；</p>
</li>
<li data-line="8" dir="auto">
<p>第二个问题解决方案：softmax的时候加一个&nbsp;<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>&nbsp;系数</p>
</li>
<li data-line="12" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> <a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/657761483/answer/3517582623" rel="noopener" class="external-link" href="https://www.zhihu.com/question/657761483/answer/3517582623" target="_blank">如何看待新的位置编码CoPE，其能够有效提升大模型的性能吗？</a></p>
</span></li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="`Add &amp; Norm`  残差 &amp; 规范化" dir="auto" class="heading" id="`Add_&amp;_Norm`__残差_&amp;_规范化"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><code>Add &amp; Norm</code>  残差 &amp; 规范化</h3><div class="heading-children"><div><blockquote dir="auto">
<p>每一个子层（自注意力或FFN）周围会有一个残差连接，然后跟着层归一化</p>
</blockquote></div><div><div class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c75"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c70"></mjx-c><mjx-c class="mjx-c75"></mjx-c><mjx-c class="mjx-c74"></mjx-c></mjx-mtext><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="4"><mjx-c class="mjx-c4C"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c79"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c6D"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="3"><mjx-c class="mjx-c53"></mjx-c><mjx-c class="mjx-c75"></mjx-c><mjx-c class="mjx-c62"></mjx-c><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c79"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c72"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></div></div><div><p dir="auto">   其中<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c53"></mjx-c><mjx-c class="mjx-c75"></mjx-c><mjx-c class="mjx-c62"></mjx-c><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c79"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c72"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span>表示子层自身的操作，例如多头注意力或前馈网络。<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c4C"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c79"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c6D"></mjx-c></mjx-mtext></mjx-math></mjx-container></span>表示层归一化。</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/395811291/answer/1251829041" rel="noopener" class="external-link" href="https://www.zhihu.com/question/395811291/answer/1251829041" target="_blank">transformer 为什么使用 layer normalization，而不是其他的归一化方法？</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/456863215" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/456863215" target="_blank">Transformer学习笔记三：为什么Transformer要用LayerNorm/Batch Normalization &amp; Layer Normalization （批量&amp;层标准化)</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 为什么要添加残差链接？残差链接添加在什么地方？为什么要这样设计？</p>
</span></li>
<li data-line="6" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> <code>Norm</code> 介绍 Layer Normalization, 为什么Transformer 一般使用 LayerNorm<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/395811291/answer/1251829041" rel="noopener" class="external-link" href="https://www.zhihu.com/question/395811291/answer/1251829041" target="_blank">知乎</a></p>
</span></li>
<li data-line="7" dir="auto">
<p>首先，任何 Normalization 在网络中的应用都是为了缓解内部协方差偏移或者平滑损失函数空间帮助训练。规范化的原则是如果你对那个维度的数据结构感兴趣呢，就别在那个维度进行归一化。</p>
</li>
<li data-line="8" dir="auto">
<p>并行化：同时不需要跨样本计算均值和方差，更加便于 Transformer 并行化。</p>
</li>
<li data-line="9" dir="auto"></li>
</ul></div><div><p dir="auto">变长序列：测试集中出现比训练集更长的数据就意味着多出来的数据没有相应的统计量以供使用，每个批次的统计属性有很大波动，计算出的均值和方差代表性变差，带来过度的噪声，feed网络的数据分布不再稳定，消除 ICS 变得失去意义。不依赖序列长度的 LN 就应允而是</p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> 为什么现在大模型 RMSNorm 和 Post-Norm 取代原有的 LayerNorm</span></li>
<li data-line="1" dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/494661681" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/494661681" target="_blank">苏剑林：为什么Pre Norm的效果不如Post Norm？</a></li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="`FeedFowrd` 前向网络" dir="auto" class="heading" id="`FeedFowrd`_前向网络"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><code>FeedFowrd</code> 前向网络</h3><div class="heading-children"><div><blockquote dir="auto">
<p>在自注意力层和多头注意力层之后，每个位置会通过一个前馈网络（FFN）处理</p>
</blockquote></div><div><div class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c46"></mjx-c><mjx-c class="mjx-c46"></mjx-c><mjx-c class="mjx-c4E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c78"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.104em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.104em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container></div></div><div><p dir="auto">   这里的<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.104em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.104em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container></span>是可学习的参数。</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/622085869/answer/3518358912" rel="noopener" class="external-link" href="https://www.zhihu.com/question/622085869/answer/3518358912" target="_blank">如何理解 Transformers 中 FFNs 的作用？</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> <code>FeedForward</code> 有什么作用必须存在吗？ attention 中也有 softmax，也是非线性，那 FFN 还是必须的么？</p>
</span></li>
<li data-line="1" dir="auto">
<p>Attention机制能够处理词语之间的相互关系，但是它无法进行更为复杂的、非线性的数据处理。实际的数据拟合大多还是用 MLP(原文中是 ReLU + 线性层)。占据整个 Transformer 总参数量的大概 2/3 </p>
</li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> MoE 是什么？有什么作用？</p>
</span></li>
<li data-line="3" dir="auto">
<p>这个概念来自于</p>
</li>
<li data-line="6" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">%</span> 还有一种是</p>
</span></li>
<li data-line="8" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 为什么现在大模型都开始用 GeLU 而不是 ReLU 作为激活函数？</p>
</span></li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="优化机制" dir="auto" class="heading" id="优化机制"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>优化机制</h3><div class="heading-children"><div><img style="width:500" src="https://cdn.sa.net/2024/04/12/mRNF9EhWkDz3noZ.png" referrerpolicy="no-referrer"></div><div><blockquote dir="auto">
<p>建议看苏神的 “Transformer 升级之路”</p>
</blockquote></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 加速Transformer的有关尝试, 可以分为两个方向</p>
</span></li>
<li data-line="1" dir="auto">
<p>降低Attention运算的时间复杂度从 O(N^2) 到 O(N), 比如 Linear Transformer 和 Linformer, 一般是用近似技巧, 但是性能会有所下降</p>
</li>
<li data-line="2" dir="auto">
<p>在不改变attention理论时间复杂度的前提下，尽可能加速attention的运算, 比如 Flash Attention. 原理基本上是减少了GPU的SRAM 和HBM 之间的通信开销</p>
</li>
<li data-line="5" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">%</span> <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=MAbGgsWKrg8&amp;t=2s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=MAbGgsWKrg8&amp;t=2s" target="_blank">【生成式AI導論 2024】第16講：可以加速所有語言模型生成速度的神奇外掛 — Speculative Decoding - YouTube</a></p>
</span></li>
<li data-line="6" dir="auto">
<p>有点像是自回归预测中的分支预测。从外部看就像是一次自回归直接输出多个 Token. 要求预言速度快，就算错了也没关系 </p>
</li>
<li data-line="7" dir="auto">
<p>消耗预测下一个 Token的计算开销，增大 Token 输出吞吐量</p>
</li>
<li data-line="8" dir="auto">
<p>这个预测本身是一个 non-autoregression model 或者 一个压缩后的小模型。或者干脆用多个预测模型</p>
</li>
<li data-line="11" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 为什么要使用 KV Cache ? 只有 decoder-only 的架构才能用 KV Cache 么？<br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/596900067/answer/3040011798" rel="noopener" class="external-link" href="https://www.zhihu.com/question/596900067/answer/3040011798" target="_blank">Transformer推理性能优化技术很重要的一个就是K V cache，能否通俗分析，可以结合代码?</a><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/662498827" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/662498827" target="_blank">大模型推理加速：看图学KV Cache</a></p>
</span></li>
<li data-line="13" dir="auto">
<p>Causal Attention 每次自回归输出 <code>t+1</code> 时刻，并不会影响 <code>t</code> 时刻的 KV. 缓存 KV 避免重复计算就很自然。</p>
</li>
<li data-line="14" dir="auto">
<p>因为 GPU 的显存大小是有限的。它一部分要存放模型的参数和前向计算的激活值，一部分要存放模型的KV Cache。减少KV Cache的目的就是要实现在更少的设备上推理更长的Context，或者在相同的Context长度下让推理的batch size更大，从而实现更快的推理速度或者更大的吞吐总量。最终降低成本。</p>
</li>
<li data-line="17" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> MHA 有什么优化？MQA、GQA、MLA 是什么？<br>
<a data-tooltip-position="top" aria-label="https://spaces.ac.cn/archives/10091" rel="noopener" class="external-link" href="https://spaces.ac.cn/archives/10091" target="_blank">缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA - 科学空间|Scientific Spaces</a></p>
</span></li>
<li data-line="19" dir="auto">
<p><strong>Multi-Query Attention</strong> 的核心简单暴力，直接让每个 Head 共享一个 KV，相比 MHA 直接减少 1/head_num.  使用的LLM 有 PaLM, StarCoder, Gemini，看起来性能损失比较有限</p>
</li>
<li data-line="20" dir="auto">
<p><strong>Group-Query Attention</strong> 是 MHA 和 MQA 的过度。基本就是将 Head 分为 g 组，每组共享一对 KV（当 g=1 时就是 MQA）。使用的模型Llama2/3, ChatGLM3, Qwen2, Yi</p>
</li>
<li data-line="21" dir="auto">
<p><strong>Multi-head Latent Attention</strong> 是 DeepSeek-V2 提出，基本思想是对 KV Cache进行LoRA 取代 GQA 的分割和共享；引入了一个恒等变换技巧来可以进一步压缩KV Cache；采用了一种混合方法来兼容RoPE。是一个非常实用的多头优化技巧，预计未来有更多的 LLM 采用。</p>
</li>
<li data-line="22" dir="auto">
<p><span style="background:#fff88f"> LLM的推理分两部分</span>：第一个Token的生成（Prefill）和后续每个Token的生成（Generation），Prefill阶段涉及到对输入所有Token的并行计算，然后把对应的KV Cache存下来，这部分对于计算、带宽和显存都是瓶颈，MLA虽然增大了计算量，但KV Cache的减少也降低了显存和带宽的压力，大家半斤八两；但是Generation阶段由于每步只计算一个Token，实际上它更多的是带宽瓶颈和显存瓶颈，因此MLA的引入理论上能明显提高Generation的速度。</p>
</li>
<li data-line="25" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 能不能告诉我 Flash attention的大致原理？ <a data-tooltip-position="top" aria-label="https://arxiv.org/pdf/2205.14135" rel="noopener" class="external-link" href="https://arxiv.org/pdf/2205.14135" target="_blank">2205.14135</a><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/693843979" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/693843979" target="_blank">细嚼慢咽地学习FlashAttention</a></p>
</span></li>
<li data-line="27" dir="auto">
<p>背景： RAM 都需要持续供电才能维持比特不变。SRAM 是静态的，因为它不需要刷新就可以维持比特，没有电容，每个比特大概使用 6-8 个晶体管。DRAM 每个比特使用 1 个晶体管有电容可能会随时放电，需要大约几毫秒周期性放电才能维持比特不翻转。<a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/difference-between-sram-and-dram/" rel="noopener" class="external-link" href="https://www.geeksforgeeks.org/difference-between-sram-and-dram/" target="_blank">Difference between SRAM and DRAM - GeeksforGeeks</a>。 现在计算机一般使用多级存储的哈佛架构，L1， L2 是 SRAM. 内存是 DRAM. AI 推理现在有趋势是使用内存和数据内存地址一致普林斯顿架构(aka.冯诺依曼架构) , e.g.Google TPU, Groq, Cerebras WSE. 取消多级存储，全部使用 SRAM。<br>
<img style="width:500" src="https://pic3.zhimg.com/80/v2-8d6e56e5f0ead5e3b51a4ff264e6ebce_1440w.webp" referrerpolicy="no-referrer"></p>
</li>
<li data-line="29" dir="auto">
<p>Flash Attention 的核心在于使用 分块(tiling)和重计算(recomputation)两大技术。通过把数据 block-wise 在 SRAM 上计算注意力，大幅减少了在 DRAM 和 SRAM 上搬运数据造成的时间损耗, Tiling. 而在前向传播中, 不存储整块注意力矩阵，而是存储一些统计量。在反向传播时,通过保存的统计量和小块的输入,可以重新计算需要的注意力值,避免从内存中读取整个注意力矩阵。</p>
</li>
<li data-line="30" dir="auto">
<p>具体的数学细节建议直接看原文，但是不科研没必要。</p>
</li>
<li data-line="33" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> Page Attention</p>
<blockquote dir="auto">
<p>推理prefill节点和decode阶段耗时接近，因为prefill阶段能够充分利用算力。<br>
推理是 IO bound，推理吞吐主要依赖推理batch size。<br>
GPU内存消耗正比于模型大小+sequence length。<br>
KV cache的管理成为长序列，大batchsize的关键。</p>
</blockquote>
</span></li>
</ul></div><div><p dir="auto">所以就提出了page attention。</p></div><div><ul>
<li data-line="0" dir="auto">使用paged kvcache+paged attention，细粒度管理kv cache，结合continuous batching，最大化device内存利用率。</li>
<li data-line="1" dir="auto">paged attention将KV cache按块分布，每块包含一定数量的token的KV。</li>
<li data-line="2" dir="auto">paged kvcache：使用logical kvcache可以随时新增或者删除，最大化kvcache空间利用率，简化continuous batching的kvcache管理。</li>
<li data-line="3" dir="auto">paged attention：使用离散physical kvcache进行attention计算</li>
</ul></div><div><p dir="auto"><strong>VLLM</strong></p></div><div><ul>
<li data-line="0" dir="auto">
<p><strong>kvcache复用：</strong>一个推理请求需要进行多个分支推理，从而选取最优时，可以复用相同的kvcache空间。</p>
</li>
<li data-line="1" dir="auto">
<p>kvcache超分：无需对max_new_tokens分配全量kvcache空间，按推理实际进度随时分配，有效增大推理batch size。</p>
</li>
<li data-line="2" dir="auto">
<p>kvcache swap：超分存在内存超用风险，超用时可swap到主机内存。</p>
</li>
<li data-line="6" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> Deepspeed <a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/688873027" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/688873027" target="_blank">从啥也不会到DeepSpeed————一篇大模型分布式训练的学习过程总结</a></p>
</span></li>
<li data-line="7" dir="auto" class="lc-list-callout" data-callout="~" style="--lc-callout-color: 124, 77, 255;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">~</span> 建议先学习一下目录中的“并行化”内容</p>
</span></li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=AHEDFx-meIg" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=AHEDFx-meIg" target="_blank">这个方法让Transformer快了10倍！究竟是如何做到的？ - YouTube</a></p></div><div><p dir="auto">transformer 里的 QKV 研究问题和答案之间的关系。不去找前后，不去找相邻，就是单纯的问题（Query）和答案(Value)，最多加了一个(Key)来辅助. 因为这是谷歌搜索等搜索引擎最开始的结构。任何一个问题(Query)，会有很多的答案（Value），而之所以能找到这些答案，是因为这些答案里面包包含了有关于这个问题的关键信息（Key） <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span></p></div><div><p dir="auto">这是一个万能形式，任何问题的答案都是通过“问题本身+相关的关键信息”找到的，比如你去谷歌搜索“今天天气怎么样”，这个问题本身就是 Q，而你的语言是“中文”，你的位置是“北京”，你的时间是“今天”，这些就都是 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>，那么找到的答案“下雨”就是 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>。<br>
[]<br>
一般来说肯定是通过方法找到 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span> 中的一些系数，就可以找到正确的 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> 了。我们也可以把 V挪到公式右边，并且把他们存在的关系叫成 attention，那么就是：</p></div><div><p dir="auto">这就是整个 transformer 的最基础结构，有了这个万能结构，只需要学习 F里的各个参数，就可以回答你想要的问题。</p></div><div><p dir="auto">那么，又为什么叫 <strong>transformer</strong> 而不是简单的&nbsp;attention&nbsp;呢？</p></div><div><p dir="auto">因为 transformer 它为了提高这个 F的 运算效率，做了一些规定，比如你的 attention 的输入输出维度需要一样，这样矩阵运算就可以加快。而且多个 attention 合在一起来算，也是为了加快运算速度和效率。</p></div><div><p dir="auto">那整个&nbsp;Transformer里的 <strong>encoded</strong> <strong>decoder</strong> 又是干嘛呢？</p></div><div><p dir="auto">这个 QKV 结构是最基础的单元，为了适应不同的目标，把多个 QKV 的 attention 合在一起以后，它分了两个部分，一个用来学怎么 encode，一个用来学怎么 decode。这个就跟 GAN，对抗神经网络很像，基本就是还是为了找到数据高维空间下的一个空间分割平面。而这里为了前后关系，它把上一个运算出来的结果又给进去去计算下一个部分（decoder&nbsp;部分），所以就可以放在语言任务上（因为语言有前后关系）。</p></div><div><p dir="auto">对于像翻译这种任务，encoder 部分输入是第一种语言，而 decoder 部分是第二种语言，这样一来训练出来的模型就可以做翻译。</p></div><div><p dir="auto">此外, 如果我们处理的是序列数据, 希望保留序列信息, 我们可以添加<strong>Position Encoding</strong>. Position Encoding 有很多方式, 最常用的是 cos 添加角度信息.</p></div><div><p dir="auto">head的概念类似卷积中的通道，只不过每个通道的输入都是一样的，类似于把一个通道的数据复制多次。</p></div><div><p dir="auto">多头注意力的计算过程类似深度可分离卷积，把通道分开计算，再融合到一起. 使用多头的好处有:</p></div><div><ol>
<li data-line="0" dir="auto"><strong>捕获多种类型的信息</strong>：在自然语言处理任务中，一个单词的含义可能会受到其上下文中的多个其他单词的影响。通过多头注意力，模型可以学习到如何根据不同的上下文关注不同的单词。例如，对于句子"我喜欢吃苹果，因为它很甜。"，在处理"甜"这个词时，模型可能需要同时关注"吃"和"苹果"这两个词。通过多头注意力，模型可以在一个头中关注"吃"，在另一个头中关注"苹果"。</li>
<li data-line="2" dir="auto"><strong>在不同的表示子空间中学习</strong>：在多头注意力中，每个头都有自己的权重矩阵，这些矩阵会在训练过程中进行学习。这意味着每个头都可以在不同的表示子空间中捕获输入的不同特征。这使得模型能够捕获更丰富的信息，从而提高模型的表达能力。</li>
<li data-line="4" dir="auto"><strong>并行计算</strong>：多头注意力的设计也使得并行计算成为可能。每个头的计算都是独立的，因此可以在硬件允许的情况下并行进行，从而提高计算效率。</li>
</ol></div><div><p dir="auto">因为 QKV 就是简单的“问题—答案”结构，这个可以应用在一切问题上，图片，语音，文字，或者相互关联相互变化，都可以。</p></div><div><p dir="auto">所以现在如果我回过头来看的话，整个&nbsp;transformers里面的重点就是它把基本的单元从原来神经元之间的连接结构变成了问答的(QKV)attention 结构。</p></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="faq" class="callout drop-shadow"><div class="callout-title" dir="auto"><div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-help-circle"><circle cx="12" cy="12" r="10"></circle><path d="M9.09 9a3 3 0 0 1 5.83 1c0 2-3 3-3 3"></path><path d="M12 17h.01"></path></svg></div><div class="callout-title-inner">Transformer vs. RNN</div></div><div class="callout-content">
<p dir="auto"><strong>相比于RNN, Transformer 的优势(非常明显)主要来自于</strong> </p>
<ul>
<li data-line="2" dir="auto">由于注意力机制将序列中所有位置的信息都一视同仁地看待（除了表征位置信息的位置编码以外）, 而带来的 <span style="background:#fff88f">全局建模能力</span></li>
<li data-line="3" dir="auto">由于自注意力机制中的点积运算更加适合 GPU训练和Transformer 上下参数之间没有依赖, 所导致的能够<span style="background:#fff88f">更好提高训练效率</span>和<span style="background:#fff88f">训练参数的scalability </span></li>
<li data-line="4" dir="auto">不存在RNN中的递归结构, 非常不容易出现训练时的梯度爆炸/消失.</li>
</ul>
</div></div></div><div><ul>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Embedding 输入
<ul>
<li data-line="1" dir="auto">经过Tokenizer之后的RAW数据变为Embedding</li>
<li data-line="2" dir="auto">为了让模型批量处理不同长度的音频，我们将同一个批次中的输入填充 (padding) 到同样长度</li>
</ul>
</li>
<li data-line="3" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>位置掩码(Positional Encoding)</strong>
<ul>
<li data-line="4" dir="auto">
<p>由于Transfomer没有像RNN那样隐含顺序信息,  所以位置掩码被添加到Embedding, 允许模型学到每个Embedding的位置和相对位置(无序数据)信息. 在这之前, 一般需要将相对位置和绝对位置直接注入到模型</p>
</li>
<li data-line="5" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p>位置掩码和Embedding的维度相同, 两者直接相加. 具体的掩盖设计有如下两种</p>
<ul>
<li data-line="6" dir="auto">在模型输入位置添加一个可训练的层</li>
<li data-line="7" dir="auto">或者更加常用的是使用三角函数来编码位置信息</li>
</ul>
<p>其中, pos是输入序列最长长度, i 是序列中位置, d_model是Embedding的维度</p>
</li>
<li data-line="10" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p>位置掩码扩容方法</p>
<ul>
<li data-line="11" dir="auto">简单外推(Extrapolation): 提前预留维度并设为0. 但是可能导致外推维度的训练不充分, 导致外推位置启用后模型性能严重下降.  </li>
<li data-line="12" dir="auto">线性内推: 将数和输之间的区间变小(e.g. 1,2,3.... 1.5,2.2.5,3). 模型学习的特征不一样了, 需要微调让模型重新学习拥挤的映射关系.</li>
<li data-line="13" dir="auto">进制转换: 比如将10进制变为16进制, 表示数量范围变大. 数值范围的天花板变大, 模型一般有泛化能力. 或者将更低进制减少变化程度. </li>
</ul>
</li>
</ul>
</li>
</ul></div><div><p dir="auto"> <img alt="image.png" src="https://cdn.sa.net/2024/01/23/MdOrH9oNbQRCKZz.png" referrerpolicy="no-referrer"></p></div><div><pre><code>- @ Embedding 经过 Postion Encoding的修饰之后输入到自注意力机制当中
- @ 输入经过三个不同的全连接层 `nn.Linear` 获得和Embedding维度相同的 `query`, `key`, `value` 三个矩阵. 
	- 使用三个不同全连接层是因为独立层能够使得QKV保持独立, 使得更它们更有效地捕捉不同类型的信息和关系
	- &amp; QK必须相同维度, 因为要点积. V不一定
	- &nbsp;注意力机制本身并没有对&nbsp;QKV 的内容做出任何限制我们可以这样理解. 我们希望用Q 把 V 中的东西找出来, 而 K 是 V 的钥匙. 如果 Q和K的匹配度越高, 那么就可以在 K 这个位置对应的 V 中找出更多Q要在V中查询的信息. 
	- 比如，我们现在希望计算音频和文本之间的注意力，或者说希望从音频特征中提取和文本相关的信息，那么这个时候应该将文本特征作为&nbsp;Q&nbsp;，音频特征作为&nbsp;K&nbsp;和&nbsp;V&nbsp;（**交叉注意力机制**）；又比如，我们希望计算文本和文本自身的注意力，那么就应该将文本特征同时作为&nbsp;，QKV.&nbsp;（**自注意力机制**）
- @ 计算 `query` 和 `key` 之间的相似度(点积、拼接、感知机). 以下以点积为例.  !
	- `query` 先和 `key` 点积用于衡量每个词（查询）与句子中每个其他词（键）的相关性.
	- 然后，应用缩放因子对这些点积进行缩放。通常，这个缩放因子是键向量维度的平方根的倒数
	- 最后, 使用softmax函数将其归一化为概率. 这个softmax之后的概率表示该个Embedding在该位置的重要程度.
	- ? 矩阵点积为什么可以计算相似度? 因为两矩阵方向相同, 点积较大, 方向越垂直, 点积越小. 在几何上等同于计算两个向量的长度乘积和它们之间夹角余弦的乘积. 同时考虑了矩阵的长度和方向. 同时在GPU运算更加高效
	- ! 自注意力机制本质上是求一个离散概率的数学期望. $EX = \sum_{i=1}^{4} X_i p_i$ ; 离散分布P是softmax之后的值, X 是 V. 
- @ 最后, softmax后的值 和 `value` 相乘. 用于增强该位置Embedding重要性或者减少重要性
- 🌰 : “I eat pizza today”
	- 每个Token(I, eat, pizza, today)先转换为 Embedding
	- 对于每个Token我们使用三个不同的全连接层来生成QKV. 例如，“I”会生成一个Q向量、一个K向量和一个V向量。
	- 接下来，我们计算每个词的Q向量与所有词的K向量的点积。这相当于在评估句子中的每个词与其他每个词之间的关系。例如，计算“I”的Q向量与“I”，“eat”，“pizza”，“today”的K向量的点积
	- 之后对点积应用缩放因子，并使用softmax函数进行归一化，得到注意力权重。
	- 每个Token的Softmax值的V向量进行加权求和。这个加权和代表了考虑到了整个句子上下文的当前词的表示。例如，对于“I”，我们将它的注意力权重与所有词的V向量相乘，并加总起来，得到一个新的加权向量，这个向量就是考虑了整个句子上下文的“I”的表示。
- $ 根据具体任务的需要, 我们会添加掩码(Mask). 其作用是通过修改注意力权重, 以避免不不要或不需要的信息传递到模型中. 比如, 在在翻译任务中我们不希望Encoder访问到句子后面的内容, 我们就可以用Mask遮蔽避免泄漏. 或者在处理变长序列中补全长度(padding)
</code><button class="copy-code-button">复制</button></pre></div><div><ul>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>多头注意力机制(Multi-head Attention)</strong><img alt="image.png" src="https://cdn.sa.net/2024/01/22/9teJRY7VQnXATIE.png" referrerpolicy="no-referrer">
<ul>
<li data-line="1" dir="auto">多头注意力机制允许模型在不同子空间中学习到相关信息, 以捕捉数据的不同方面. 提高并行度并简化模型的计算复杂度. </li>
<li data-line="2" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>将 QKV 分别用h个不同投影矩阵投影h次, 然后分别做点积注意力, 最后将每个头计算出的加权和简单拼接回一个完整的向量后通过一个全连接层产生最终的输出
<ul>
<li data-line="3" dir="auto">相当于人从不同的视角观察一个物体</li>
</ul>
</li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 《Are sixteen heads really better than one?》多头自注意力机制不一定比单头好</span></li>
<li data-line="5" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 《Low-Rank Bottleneck in Multi-head Attention Models》指出Multi-Head Attention的表达能力瓶颈，并提出增大key_size来缓解。
</span><ul>
<li data-line="6" dir="auto">我们将 QK维度称为 key_size, V的维度为 head_size</li>
<li data-line="7" dir="auto">h 是 注意力头的数量. 一般实际是将原始d维度的QKV投影到 d/h)维中, 单独计算后输出 d/h 的结果. 然后将 h*d/h 拼接起来获得最终注意力值</li>
<li data-line="8" dir="auto"><span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msqrt size="s"><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.082em;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span> 可以看作一个二元联合分布. 假设序列长度为 n. 因为 QK维度一样所以有这个分布有 n*n = <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container></span> 的值</li>
<li data-line="9" dir="auto">但是将QK投影之后, 各自分布只有 n*(d/h) 的. 总的参数量为 2nd/h &lt;&lt; n^2. 参数量的减少导致表达能力的削弱. 这就是Low-rank bottleneck, 尤其是 h 较多时.这个叫低秩瓶颈. </li>
<li data-line="10" dir="auto">我们可以增加 d 的维度, 但是这会增加模型的复杂性. 或者减少 h, 但是多头本身就可以增加模型的表达能力</li>
<li data-line="11" dir="auto">我们可以通过只增加 key_size 以增加模型的表达能力, 而尽可能不增加模型的复杂性</li>
</ul>
</li>
<li data-line="12" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 《Talking-head attention》, 将多头的低秩分布叠加(在softmax之前叠加, e.g.加权平均)增强模型的表达能力. 原理是多个高斯分布(GMM)叠加的数量够多, 就可以逼近任意概率分布. </span></li>
</ul>
</li>
<li data-line="13" dir="auto" class="lc-list-callout" data-callout="~" style="--lc-callout-color: 124, 77, 255;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">~</span> 爱因斯坦求和约定（Einstein Summation Convention）<code>torch.ensum</code> 是一种表达多重求和（比如矩阵乘法、点积、批量矩阵乘法等）和某些类型的数据重排操作的简介写法. 比如
</span><ul>
<li data-line="14" dir="auto"><strong>矩阵乘法：</strong> 例如，矩阵乘法 <code>AB</code> 可以表示为 <code>torch.einsum('ij,jk-&gt;ik', [A, B])</code>，其中 <code>A</code> 是一个形状为 <code>(i, j)</code> 的矩阵，<code>B</code> 是一个形状为 <code>(j, k)</code> 的矩阵。</li>
<li data-line="15" dir="auto"><strong>批量矩阵乘法：</strong> 对于批量矩阵乘法，比如有批次的两个矩阵 <code>A</code> 和 <code>B</code>，其操作可以表示为 <code>torch.einsum('bij,bjk-&gt;bik', [A, B])</code>。</li>
<li data-line="16" dir="auto"><strong>求和操作：</strong> 如果你想对一个矩阵的行进行求和，可以使用 <code>torch.einsum('ij-&gt;i', A)</code>。</li>
</ul>
</li>
<li data-line="17" dir="auto" class="lc-list-callout" data-callout="$" style="--lc-callout-color: 0, 200, 83;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">$</span> 这里用序列重新梳理一下Attention的公式<img alt="gpt-transformer.png" src="https://cdn.sa.net/2024/01/24/J7fCRGVWFL5EabS.png" referrerpolicy="no-referrer">
</span><ul>
<li data-line="18" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>令 <code>F[t]</code> 为 t时刻的系统状态(高维状态); 令 <code>x[t]</code> 为 t 时刻的外部输入信息状态; 
<ul>
<li data-line="19" dir="auto">预测 <code>F[t+1]</code> 时，需考虑 <code>F[0]</code>,<code>F[1]</code>, .. <code>F[t]</code>。因此，生成长度 T 的序列，需 <code>O(T^2)</code> 复杂度。</li>
</ul>
</li>
<li data-line="20" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>那么Attention可以简化为 <img alt="image.png" src="https://cdn.sa.net/2024/01/25/kvG1ogqlYeRINpH.png" referrerpolicy="no-referrer">
<ul>
<li data-line="21" dir="auto">每个状态 &nbsp;i 对于后续的潜在贡献是&nbsp;<strong>V</strong>F[i] </li>
<li data-line="22" dir="auto">用&nbsp;<strong>Q</strong>x[t] 矢量，与此前的所有&nbsp;<strong>K</strong>F[i] 矢量分别做点乘，再 exp，得到 x[t] 与之前各个 F[i] 状态的匹配度。</li>
<li data-line="23" dir="auto">如果匹配度&nbsp;exp⁡(Q x[t]∗ K F[i])&nbsp;越大，<strong>V</strong>F[i] 的权重越大</li>
<li data-line="24" dir="auto">分母为归一化因子。</li>
<li data-line="25" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 公式中没有显式出现 t 与 i 的距离信息。我们会采用其它方式（例如位置编码）将其注入系统。</span></li>
</ul>
</li>
</ul>
</li>
</ul></div><div><p dir="auto">我们这里实现一个带Mask和多头的 <code>SelfAttention</code> 的 pytorch示例代码:</p></div><div><p dir="auto"><img src="https://pic3.zhimg.com/v2-c7d7c326d453fa1d143c35abc543cb3a_r.jpg" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0" dir="auto"><strong>编码器(Encoder)</strong> 负责特征编码，即从原始的、比较底层的输入序列信号提取出抽象的、具有明显语义特征的特征信息. 由图可知是由  特征编码、位置编码、和若干个 TransformerBlock 组成(注意力+MLP+*归一化+*残差)</li>
<li data-line="1" dir="auto"><strong>解码器(Decoder)</strong> 负责从编码器得到的原始序列的特征信息中"破译"出目标序列的内容（比如从音频序列的特征中"破译"其对应的文本信息). </li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 这种架构提出的原因是因为, 类似RNN和LSTM这样的模型输入-输出是对应的. 但在有的任务中我们不希望输入-输出的长度相同. </span></li>
<li data-line="3" dir="auto"><strong>信息流</strong>：输入序列首先进入编码器，经过一系列变换后，转换为一组高维表示。这些表示然后被传递到解码器，解码器利用这些信息以及自身的前一步输出来逐步生成最终的输出序列。</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="下游任务" dir="auto" class="heading" id="下游任务"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>下游任务</h3><div class="heading-children"><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span>  iTransformer<br>
<img style="width:500" src="https://cdn.sa.net/2024/04/15/6doERMsal4veXC2.png" referrerpolicy="no-referrer"></p>
</span></li>
<li data-line="2" dir="auto">
<p>蚂蚁集团出的一个维度倒置Transformer, 用于时间信息的注意力机制. </p>
</li>
<li data-line="3" dir="auto">
<p>可以帮助扩大模型窗口</p>
</li>
<li data-line="4" dir="auto">
<p>实现方法主要是改变数据处理的方法</p>
</li>
<li data-line="8" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> 下面两篇文章都希望尽可能使用空间和时间信息实现时序信号的预测</p>
</span></li>
<li data-line="9" dir="auto">
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1912.09363" rel="noopener" class="external-link" href="https://arxiv.org/abs/1912.09363" target="_blank">Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting - 2021</a></p>
</li>
<li data-line="10" dir="auto">
<p>谷歌的动机希望这个模型同时可以处理不同类型的序列, 能够有解释性, 不能有太大的误差累计, 能用上统计信息, 能给出预测范围区间(反正是混合尽可能多的信息)</p>
</li>
<li data-line="11" dir="auto">
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/pdf/2001.02908.pdf" rel="noopener" class="external-link" href="https://arxiv.org/pdf/2001.02908.pdf" target="_blank">Spatial-Temporal Transformer Networks for Traffic Flow Forecasting</a> </p>
</li>
<li data-line="12" dir="auto">
<p>利用时空信息预测交通</p>
</li>
<li data-line="13" dir="auto">
<p>模型实现是spital-temporal(spital +tempora ) * k </p>
</li>
<li data-line="14" dir="auto">
<p>spital 中有卷积层添加对空间的理解能力</p>
</li>
<li data-line="15" dir="auto">
<p>这里在还讲了输入数据改变之后, 数据预处理, Loss设计, 连接维度, 归一化等也会发生改变</p>
</li>
<li data-line="20" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2306.03009" rel="noopener" class="external-link" href="https://arxiv.org/abs/2306.03009" target="_blank">[2306.03009] Using Sequences of Life-events to Predict Human Lives</a></p>
</span></li>
<li data-line="21" dir="auto">
<p>来自丹麦的团队使用 Transformer 和 超过 600 万国人数十年人生活的数据用来预测人们的寿命 e.g. 如出生、就医、上学、搬迁、结婚等人生事件</p>
</li>
<li data-line="22" dir="auto">
<p>提出了一个类似于 <code>Life2Vec</code> ，使用编码-加码结构，使用类似BERT和 Word2Vec的 MLM 的方法进行训练(完形填空)， 预测结果包含从早期死亡率到个性细微差别等</p>
</li>
<li data-line="23" dir="auto">
<p>使用降维对向量进行，展示了模型学习到的生活事件概念空间的结构。</p>
</li>
<li data-line="24" dir="auto">
<p>精神疾病、神经系统疾病、癌症等健康事件会显著增加早期死亡的概率预测值。而一些健康问题如内分泌、营养和代谢性疾病,则会降低早期死亡的预测值</p>
</li>
<li data-line="25" dir="auto">
<p>从事管理职位或高收入职业的人,模型会预测其死亡风险较低。而技术工种则风险偏高。</p>
</li>
<li data-line="26" dir="auto">
<p>男性和移民群体的早期死亡预测值要高于女性。</p>
</li>
<li data-line="27" dir="auto">
<p>与医疗系统互动较多、健康记录较完整的人群。模型的预测表现会更好。</p>
</li>
<li data-line="28" dir="auto">
<p>早期死亡预测准确性对年轻群体较高，对较老年人群,准确性会降低。</p>
</li>
<li data-line="31" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2010.11929" rel="noopener" class="external-link" href="https://arxiv.org/abs/2010.11929" target="_blank">[2010.11929] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a><img src="https://cdn.sa.net/2024/06/06/xAycTYwqjzUFrSZ.png" referrerpolicy="no-referrer"></p>
</span></li>
<li data-line="32" dir="auto">
<p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/356155277" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/356155277" target="_blank">zhuanlan.zhihu.com/p/356155277</a></p>
</li>
<li data-line="33" dir="auto">
<p>基本思想：直接把图像分成固定大小的patchs，然后通过线性变换得到patch embedding，这就类比NLP的words和word embedding</p>
</li>
<li data-line="34" dir="auto">
<p>有一个 <strong>Patch Embedding</strong> 和 可训练的 1-D Position Embedding。发现Position Encoding是 1D, 2D 效果都差不多。</p>
</li>
<li data-line="35" dir="auto">
<p>发现当预训练大小超过100M后 ViT 才相对 CNN 展现出优势(Self-Attention的全注意力机制更加容易在GPU上并行计算; 使用固定大小的Patch替代CNN卷积, 复杂度更低; 没有卷积层的ViT参数更少)</p>
</li>
<li data-line="36" dir="auto">
<p>QKV 维度都是 197 * 768, 用CLS Token 添加位置特征</p>
</li>
<li data-line="37" dir="auto">
<p>ViT 提取到了更加抽象的特征相比AlexNet 但是ViT 训练调参敏感</p>
</li>
<li data-line="39" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">%</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2105.01601" rel="noopener" class="external-link" href="https://arxiv.org/abs/2105.01601" target="_blank">[2105.01601] MLP-Mixer: An all-MLP Architecture for Vision - 2021</a></p>
</span></li>
<li data-line="40" dir="auto">
<p>Google做的ViT的后续扩展. 提出了一个 MLP-Mixer 模型，试图使用纯MLP替代Transformer中Attention机制.  </p>
</li>
<li data-line="41" dir="auto">
<p>和传统依赖于卷积和自注意力的模型不同，MLP-Mixr通过两种类型的MLP层来处理图像，一种在图像块(patches)上独立应用（即特征混合），另一种跨图像块应用（即空间信息混合）。通过在</p>
</li>
<li data-line="42" dir="auto">
<p>在大规模数据集上的图像分类基准测试达到 SOTA 一样的水平。同时保持相近的预训练和推理成本。</p>
</li>
<li data-line="43" dir="auto">
<p>MLP-Mixer 的主要优点在于其架构的简洁性和对大规模数据集的良好适应性。相比传统的CNN和基于注意力的模型，MLP-Mixer 不依赖于卷积操作或自注意力机制，而是仅通过基本的矩阵乘法、数据重排（如重塑和转置）、以及非线性激活函数实现。这种设计简化了模型的结构，可能在某些应用场景下可能更易于优化和扩展。</p>
</li>
</ul></div><div><p dir="auto"><img alt="YaMf5AvRg6EWPXi.png" src="https://cdn.sa.net/2024/04/11/YaMf5AvRg6EWPXi.png" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0" dir="auto">在论文中，图1（Figure 1）展示了MLP-Mixer架构的宏观结构。这个结构接受一系列线性投影的图像块（也称为tokens）作为输入，这些图像块被组织成一个“patches × channels”的表格。MLP-Mixer的输入维度保持不变，即输入的图像块数量（S）和通道数（C）。关键组件包括：</li>
</ul></div><div><ol>
<li data-line="0" dir="auto"><strong>Patches</strong>: 输入图像被分割成不重叠的小块，每个小块都被线性投影到一个期望的隐藏维度C。如果原始输入图像的分辨率为(H, W)，每个图像块的分辨率为(P, P)，那么图像块的数量S = HW/P^2。</li>
<li data-line="1" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>Mixer Layer</strong>: 这是MLP-Mixer的核心组件，包含两种类型的MLP层：token-mixing MLP和channel-mixing MLP。这两种MLP层交替出现，以实现输入的两个维度（空间位置和通道）之间的交互。
<ul>
<li data-line="2" dir="auto"><strong>Token-mixing MLP</strong>: 这种类型的MLP作用于输入表格的列（即它应用于转置后的输入表格X^T），并独立地处理每个通道，从而允许不同空间位置（tokens）之间的信息交流。</li>
<li data-line="3" dir="auto"><strong>Channel-mixing MLP</strong>: 这种类型的MLP作用于输入表格的行，允许不同通道之间的信息交流。        </li>
</ul>
</li>
<li data-line="4" dir="auto"><strong>Fully-connected</strong>: 这是MLP层的基本组成部分，每个MLP层包含两个全连接层（fully-connected layers）和一个GELU非线性激活函数。</li>
<li data-line="5" dir="auto"><strong>Layer Norm</strong>: 在每个MLP层之后，使用层归一化（Layer Normalization）来稳定训练过程。</li>
<li data-line="6" dir="auto"><strong>Skip-connections</strong>: 这些是现代深度学习架构中常用的组件，有助于信息在网络中流动，并减少梯度消失的问题。</li>
<li data-line="7" dir="auto"><strong>Global Average Pooling</strong>: 在所有MLP层之后，使用全局平均池化（Global Average Pooling）来聚合空间信息，为分类头（classifier head）做准备。</li>
<li data-line="8" dir="auto"><strong>Per-patch Fully-connected</strong>: 每个图像块都有一个全连接层，用于最终的分类任务。</li>
<li data-line="9" dir="auto"><strong>Class Layer</strong>: 最后，使用一个线性分类器（通常是一个全连接层）来输出最终的分类结果。</li>
<li data-line="10" dir="auto">MLP-Mixer中的跳跃连接（skip-connections）和层归一化（layer norm on the channels），这些都是帮助模型训练和泛化的重要组件。</li>
</ol></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> iGPT </p>
</span></li>
<li data-line="1" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> BEiT</p>
</span></li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2103.14030" rel="noopener" class="external-link" href="https://arxiv.org/abs/2103.14030" target="_blank">[2103.14030] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></p>
</span></li>
<li data-line="5" dir="auto">
<p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=luP3-Fs0QCo&amp;t=18" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=luP3-Fs0QCo&amp;t=18" target="_blank">Swin Transformer论文精读【论文精读】 - YouTube</a><br>
<img style="width:500" src="https://cdn.sa.net/2024/04/19/zZmck9UM53dYlL4.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/04/19/zfbDPUNJs7O4uxq.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/04/19/hTNnFtGsjYefdw1.png" referrerpolicy="no-referrer"></p>
</li>
<li data-line="10" dir="auto">
<p>微软在2021工作,  Swin来自于与 Shift + Windows. 这个论文的任务是希望ViT可以拥有和CNN一样的层级提取特征的能力.</p>
</li>
<li data-line="11" dir="auto">
<p>他们发现直接用 ViT 的 Non-Local 方法可能会丢失 重要的 CNN 中原有的层次信息。并且对于大分辨率的图片，ViT 的输入复杂度可能会特别的高。</p>
</li>
<li data-line="12" dir="auto">
<p>Swin Transformer 拥有一个层移动窗口，试图在保持 Transformer 原有优点(e.g.多模态建模; 全局感受域; 适合并行)的同时为 ViT 引入局部性感受和层次特征，并减少计算的复杂度和内存占用。</p>
</li>
<li data-line="13" dir="auto">
<p>具体来说，Swin Transformer 先像 ViT 一样把图形切为小的非重叠 Patch 大概 <code>4x4</code>. 然后会有一个由1 或多个 Transformer Block 组成的 Stage, 并以一个patch merging layer启动. 相邻的patches被合并成更大的patches. 由此来实现类似 CNN 的层次特征提取。</p>
</li>
<li data-line="14" dir="auto">
<p>Path Merge 可以 Overlapping 以进行全局-局部感受域的权衡。</p>
</li>
<li data-line="15" dir="auto">
<p>在注意力机制内部拥有一个可变大小的窗口 <code>Shift Windows</code>来关注图像的局部区域。这个窗口在每一层中都会根据一定的模式进行移位，以覆盖整个图像。</p>
</li>
<li data-line="16" dir="auto">
<p>Swin Transformer 通过引入局部感受野和移位窗口机制，将计算复杂度降低到线性（O(N)），使其更适合于大规模图像处理。</p>
</li>
<li data-line="17" dir="auto">
<p>能够适应不同分辨率的输入图像。 并能通过设计不同大小的 Stage 满足不同任务。</p>
</li>
<li data-line="18" dir="auto">
<p>这里 Patch 的位置编码是和 BERT 一样的可学习编码。以此来提高相对位置编码和掩码提高效率</p>
</li>
<li data-line="19" dir="auto">
<p><strong>self-supervised</strong> <a data-tooltip-position="top" aria-label="https://github.com/SwinTransformer/Transformer-SSL" rel="noopener" class="external-link" href="https://github.com/SwinTransformer/Transformer-SSL" target="_blank">GitHub - SwinTransformer/Transformer-SSL: This is an official implementation for "Self-Supervised Learning with Swin Transformers".</a></p>
</li>
<li data-line="20" dir="auto">
<p><strong>swim-unet</strong> <a data-tooltip-position="top" aria-label="https://link.springer.com/chapter/10.1007/978-3-031-25066-8_9" rel="noopener" class="external-link" href="https://link.springer.com/chapter/10.1007/978-3-031-25066-8_9" target="_blank">Swin-Unet: Unet-Like Pure Transformer for&nbsp;Medical Image Segmentation | SpringerLink</a></p>
</li>
<li data-line="21" dir="auto">
<p>好像还有一个魔改CNN声称比Swim Transformer的效果还好. <a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/510965760/answer/2306584675" rel="noopener" class="external-link" href="https://www.zhihu.com/question/510965760/answer/2306584675" target="_blank">如何评价FAIR提出的ConvNeXt：CNN匹敌Swin Transformer? - 知乎</a></p>
</li>
<li data-line="24" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=9A8G7K_o-YA" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=9A8G7K_o-YA" target="_blank">多尺度Transformer的应用：点云分类，效果如何？ - YouTube</a></p>
</span></li>
<li data-line="27" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1907.05572" rel="noopener" class="external-link" href="https://arxiv.org/abs/1907.05572" target="_blank">[1907.05572] R-Transformer: Recurrent Neural Network Enhanced Transformer</a></p>
</span></li>
<li data-line="28" dir="auto">
<p>文章试图结合 RNN 和 Transformer 在无需任何精妙的 Embedding 设计的同时，同时有效捕捉序列中的局部结构与全局长期依赖。</p>
</li>
<li data-line="29" dir="auto">
<p>首先将序列按时间或顺序分割成若干子序列；然后使用 RNN 捕捉子序列之间的局部依赖；在局部依赖被RNN捕捉后，R-Transformer应用自注意力机制处理所有子序列的输出，这有助于捕捉子序列间的全局依赖关系；拥有一个类似于Swin Transformer的设计，允许采用层次化的结构来进一步提高捕捉长期依赖关系的能力</p>
</li>
<li data-line="33" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=upzCDVguQ1U" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=upzCDVguQ1U" target="_blank">套娃也疯狂？TNT挑战最新玩法：Transformer in Transformer论文速看！ - YouTube</a></p>
</span></li>
<li data-line="34" dir="auto"></li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="语言大模型(Large Language Model)" dir="auto" class="heading" id="语言大模型(Large_Language_Model)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>语言大模型(Large Language Model)</h2><div class="heading-children"><div><img alt="alt text" src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dc8387a-0a5a-4a87-89f6-f1688fd5ed99_1578x1436.png" referrerpolicy="no-referrer" style="width: 700px; max-width: 100%;">
[GitHub - Mooler0410/LLMsPracticalGuide: A curated list of practical guide resources of LLMs (LLMs Tree, Examples, Papers)](https://github.com/Mooler0410/LLMsPracticalGuide)</div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://bbycroft.net/llm" rel="noopener" class="external-link" href="https://bbycroft.net/llm" target="_blank">LLM Visualization</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://jalammar.github.io/illustrated-gpt2/" rel="noopener" class="external-link" href="https://jalammar.github.io/illustrated-gpt2/" target="_blank">The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/704408423" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/704408423" target="_blank">大模型推理原理&amp;流程详解</a></p></div><div><ul>
<li data-line="0" dir="auto">
<p>讲的蛮好的，整个大模型的原理设计的基本流程的讲到了</p>
</li>
<li data-line="1" dir="auto">
<p>又说到诸如 Top-k, top-p, temperature 这类的后处理</p>
</li>
<li data-line="2" dir="auto">
<p>也说到 Page Attention </p>
</li>
<li data-line="5" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">%</span> Rule-base </p>
</span></li>
<li data-line="6" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">%</span> Statistical-base</p>
</span></li>
<li data-line="8" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 2018 - Word2Vect 时代</p>
</span></li>
<li data-line="10" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 2019 - LSTM 时代</p>
</span></li>
<li data-line="12" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 2020 - Transformer 时代</p>
</span></li>
<li data-line="13" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 预训练方式()</p>
</span></li>
<li data-line="14" dir="auto">
<p>OpenAI </p>
</li>
</ul></div><div><ol>
<li data-line="0" dir="auto">Casual Mask Self-Supervised </li>
<li data-line="1" dir="auto">Supervised Fine-tuning</li>
<li data-line="2" dir="auto">Reinforcement </li>
</ol></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> Mask方法，双向单向注意力</p>
</span></li>
<li data-line="3" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> Tokenizer </p>
</span></li>
<li data-line="4" dir="auto">
<p>BPE</p>
</li>
<li data-line="5" dir="auto">
<p>"end-game" OpenAI 发布</p>
</li>
</ul></div><div class="heading-wrapper"><h3 data-heading="编解码架构" dir="auto" class="heading" id="编解码架构"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>编解码架构</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/588325646" rel="noopener" class="external-link" href="https://www.zhihu.com/question/588325646" target="_blank">为什么现在的LLM都是Decoder only的架构？</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">%</span> Transformer 沿袭 Word2Vec 的传统</p>
</span></li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> BERT 和 GPT 的区别除了 Mask 机制外还有什么？</p>
</span></li>
<li data-line="3" dir="auto">
<p>BERT 用的 encoder-only, GPT 是 decoder-only. </p>
</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/588325646/answer/2929224109" rel="noopener" class="external-link" href="https://www.zhihu.com/question/588325646/answer/2929224109" target="_blank">为什么现在的LLM都是Decoder only的架构？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/608929992/answer/3119081297" rel="noopener" class="external-link" href="https://www.zhihu.com/question/608929992/answer/3119081297" target="_blank">decoder-only和encoder-decoder transformer在应用时最大的区别是？ - 知乎</a> <a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/642923989" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/642923989" target="_blank">LLM的3种架构：Encoder-only、Decoder-only、encode-decode - 知乎</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> Encoder-decoder， T5 </p>
</span></li>
<li data-line="1" dir="auto">
<p><strong>适用任务</strong>: 这种架构非常适合于那些需要理解输入和生成输出的任务，如机器翻译、文本摘要等。在这些任务中，模型需要首先理解输入文本（通过Encoder部分），然后基于这个理解生成新的文本（通过Decoder部分）</p>
</li>
<li data-line="2" dir="auto">
<p><strong>代表性模型</strong>: Transformer, Google T5</p>
</li>
<li data-line="5" dir="auto">
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1910.10683" rel="noopener" class="external-link" href="https://arxiv.org/abs/1910.10683" target="_blank"><strong>T5</strong> - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer".&nbsp;<em>Colin Raffel et al.</em>&nbsp;JMLR 2019</a></p>
</li>
<li data-line="6" dir="auto">
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2210.02414" rel="noopener" class="external-link" href="https://arxiv.org/abs/2210.02414" target="_blank">GLM-130B: An Open Bilingual Pre-trained Model". 2022</a></p>
</li>
<li data-line="9" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> Encoder-only， BERT </p>
</span></li>
<li data-line="10" dir="auto">
<p><strong>适用任务</strong>: 这种架构适用于只需理解输入文本的任务，比如文本分类、情感分析、命名实体识别等。在这些任务中，模型的目标是分析和理解输入，而不需要生成任何新的文本。</p>
</li>
<li data-line="11" dir="auto">
<p><strong>代表性模型</strong>: BERT（Bidirectional Encoder Representations from Transformers）是这一类架构中最著名的例子。通过预训练一个大型的语料库，BERT学会了理解语言的上下文，可以被用于各种不同的自然语言处理任务</p>
</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/500807675" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/500807675" target="_blank">一张图看懂BERT - 知乎</a></p></div><div><p dir="auto"><img src="https://pic2.zhimg.com/v2-24c08636bfa0c75bb9d9af831fa3650d_r.jpg" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0" dir="auto">
<p><a data-tooltip-position="top" aria-label="https://aclanthology.org/N19-1423.pdf" rel="noopener" class="external-link" href="https://aclanthology.org/N19-1423.pdf" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018</a></p>
</li>
<li data-line="1" dir="auto">
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1907.11692" rel="noopener" class="external-link" href="https://arxiv.org/abs/1907.11692" target="_blank">RoBERTa: A Robustly Optimized BERT Pretraining Approach, 2019</a></p>
</li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> Decoder-only, GPT </p>
</span></li>
<li data-line="5" dir="auto">
<p><strong>适用任务</strong>: Decoder-only架构特别适合于文本生成任务，如语言模型训练、文本生成、代码生成等。这种架构通常会被训练来预测接下来的单词或者序列。</p>
</li>
<li data-line="6" dir="auto">
<p><strong>代表性模型</strong>: GPT（Generative Pre-trained Transformer）系列是这一类架构中最知名的例子。GPT通过大量的预训练，能够生成连贯、相关的文本，可以用于各种生成型任务。</p>
</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://github.com/FreedomIntelligence/Overview-of-ChatGPT" rel="noopener" class="external-link" href="https://github.com/FreedomIntelligence/Overview-of-ChatGPT" target="_blank">GitHub - FreedomIntelligence/Overview-of-ChatGPT</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span>  <a data-tooltip-position="top" aria-label="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener" class="external-link" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank">GPT-1__"Improving Language Understanding by Generative Pre-Training" by Alec Radford et al. (2018)</a></p>
</span></li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span>  <a data-tooltip-position="top" aria-label="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener" class="external-link" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">GPT-2 __"Language Models are Unsupervised Multitask Learners" by Alec Radford et al. (2019)</a></p>
</span></li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2005.14165" rel="noopener" class="external-link" href="https://arxiv.org/abs/2005.14165" target="_blank">GPT-3&nbsp;<strong>"Language Models are Few-Shot Learners"</strong>. NeurIPS 2020.</a></p>
</span></li>
</ul></div><div><img style="width:500" src="https://pic2.zhimg.com/80/v2-05ca7c7623e8ec3557b3ada10b935e69_1440w.webp" referrerpolicy="no-referrer"></div><div><img style="width:500" src="https://pic4.zhimg.com/80/v2-7e0fa496b476d862c12340271969350b_1440w.webp" referrerpolicy="no-referrer"></div><div><p dir="auto">首先是上半部分，prefill：输入编码“你是谁？”，针对输入编码，产生kv cache，然后生成首个token“我”。</p></div><div><p dir="auto">获取了首token之后，开始自回归解码，也就是下半部分。输入一段文本后，生成一段长度为N的输出。单次推理只输出一个token，然后将推理输出的token和输入的tokens拼接在一起，作为下一次推理的输入，不断反复直到遇到终止符。</p></div><div><blockquote dir="auto">
<p>你是谁？--&gt; 我<br>
你是谁？我 --&gt; 是<br>
你是谁？我是 --&gt; 工<br>
你是谁？我是工 --&gt; 程<br>
你是谁？我是工程 --&gt; 师<br>
你是谁？我是工程师 --&gt; EOS</p>
</blockquote></div></div></div><div class="heading-wrapper"><h3 data-heading="训练" dir="auto" class="heading" id="训练"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>训练</h3><div class="heading-children"><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 大模型的 Mask 机制有哪几种？各种都有什么样的特点？</span></li>
<li data-line="1" dir="auto"><strong>Masked Language Modeling (MLM)</strong>：随机 Mask 掉一些 token 然后用其他token 预测 Mask 掉的内容。迫使模型学习使用双向上下文信息，即考虑被masked词前后的词。 代表:BERT</li>
<li data-line="2" dir="auto"><strong>Causal Language Modeling (CLM)</strong> ：模型仅根据先前的token来预测下一个token，适合生成文本任务。代表:GPT</li>
<li data-line="3" dir="auto"><strong>Permutation Language Modeling</strong>: 对输入序列的token进行随机排列，然后预测缺失的部分(反正注意力机制是全局的)，从而更加有效的使用双向上下文。代表，XLNet。</li>
<li data-line="4" dir="auto"><strong>Replaced Token Detection (RTD)</strong>: 不是简单 MASK token, 而是将一些token替换为模型认为可能的其他token，然后预训练模型来判断哪些token是被正确替代的，哪些是被错误替代的。以此让模型更加高效的学习到全部输入，而不仅仅预测 Mask 掉的部分。代表：ELECTRA</li>
<li data-line="5" dir="auto"><strong>Span-based Masking</strong>： 随机mask连续的token序列（即一个span) 强迫模型学习更长的上下文。代表：SpanBERT</li>
</ul></div><div><p dir="auto"><span style="background:#fff88f">Timeline of GPT Advancing</span> </p></div><div><ol>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>Generative Pre-trained Transformer(GPT-1)</strong>, 2018 : 使用 基于Transformer的Decoder-only 模型 和 Casual Masking 这种<span style="background:#fff88f">无监督学习的方法进行预训练</span>. 
<ul>
<li data-line="1" dir="auto">参数量：1.24亿个参数。</li>
<li data-line="2" dir="auto">训练数据：预训练阶段使用了BooksCorpus（约7亿单词的数据量）和其他来源的文本，总共大约40GB的文本数据(大量的书籍、网页和其他文本资料)</li>
</ul>
</li>
<li data-line="3" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>GPT-2</strong>, 2019 : 2 月最初发布, 在多项语言生成任务上表现显著，包括翻译、问答和文本摘要等. 但OpenAI由于担心潜在的滥用问题, 直到 11 月才发布最终开源.  GPT-2 在原有的 GPT 基础上扩展了模型规模，并采用了更大的数据集进行训练（称为“WebText”），这个数据集包含了从互联网抓取的800万个网页文档。没有使用特定任务的微调. GPT-2主要采用了<span style="background:#fff88f">迁移学习技术</span>，能在多种任务中高效应用预训练信息，并进一步提高语言理解能力
<ul>
<li data-line="4" dir="auto">参数量：分别为1.17亿、3.48亿、7.81亿和15.54亿参数的不同版本。</li>
<li data-line="5" dir="auto">训练数据：使用WebText数据集，这是通过在网上爬取数据创建的，包含约8百万个文档，总计约40GB的文本数据。</li>
</ul>
</li>
<li data-line="6" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>GPT-3</strong>, 2020 : OpenAI 发布了 GPT-3，这是一个巨大的模型，具有1750亿个参数。GPT-3 显著提高了生成文本的质量和多样性，并且能够在多任务学习领域实现元学习. - GPT-3 大幅扩展了模型规模，参数量达到了1750亿个，是前一代模型的规模的100倍以上。同时，GPT-3 使用了更大和更多样化的数据集进行训练，尽管还是基于无监督学习，但数据集的多样性让模型展现了更强的通用性和适用性。主要注重泛化能力，few-shot（小样本）的泛化.
<ul>
<li data-line="7" dir="auto">参数量：1750亿个参数。</li>
<li data-line="8" dir="auto">训练数据：使用了一个更大和更多样化的数据集，具体大小和细节没有完全公开，但数据量是GPT-2的几倍，横跨多个语言和领域，估计总数据量在数百GB到数TB范围。在若干报告中，OpenAI 提到 GPT-3 预训练阶段使用了大约570GB的文本数据。这些数据集通常涵盖广泛的互联网文本，以及其他多样化的文本资源。而GPT-4的具体参数和训练数据量暂时还没有准确的公开信息，不过其训练数据集预计进一步增大，以确保模型的生成质量和多样性。</li>
</ul>
</li>
<li data-line="9" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>Instruct-GPT</strong>, 2022 : OpenAI 结合了 GPT-3.5（GPT-3的进阶版本）和 Reinforcement Learning from Human Feedback (RLHF)技术，发布了 ChatGPT，这是一个对话式的AI模型，可以更自然地与人类进行对话.<br>
<img style="width:800" src="https://cdn.sa.net/2024/06/17/fdtnBzpSsvYTcZ5.png" referrerpolicy="no-referrer">
<ul>
<li data-line="11" dir="auto">训练数据和参数规模没有明确的公开数据，但它是在GPT-3的基础上进一步使用人类反馈进行了优化，因此参数规模和数据量应该与GPT-3相似。</li>
</ul>
</li>
<li data-line="12" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>ChatGPT</strong>, 2023 : ChatGPT结合了GPT-3.5和RLHF，这使得模型能更好地遵循指令并在对话上表现得更自然。ChatGPT 的训练使用了一个监督微调阶段（让模型预测人类对话代理的回复），和一个后期的RLHF阶段。
<ol>
<li data-line="13" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<ul>
<li data-line="13" dir="auto">在GPT-3.5的基础上调整，参数规模没有明确公示，但预计将与GPT-3相近。</li>
</ul>
</li>
</ol>
</li>
<li data-line="14" dir="auto"><strong>GPT-4</strong>, 2023 : OpenAI 发布了 GPT-4，一个更加先进的模型，拥有大幅提升的性能和多模态能力，可以理解文本和图像输入. GPT-4 在模型规模和数据集上进一步扩张，也包含了处理图像输入的能力。训练方法很可能继续使用了RLHF或者类似的技术来细化和优化模型对指令的响应。具体细节取决于OpenAI公布的资料，但可以预计，随着模型规模的增大，所使用的训练数据、架构改进、以及优化方法必然也变得更加复杂和多样化。</li>
<li data-line="15" dir="auto"><strong>GPT-4o</strong>, 2024 : <a data-tooltip-position="top" aria-label="https://openai.com/index/hello-gpt-4o/" rel="noopener" class="external-link" href="https://openai.com/index/hello-gpt-4o/" target="_blank">Hello GPT-4o | OpenAI</a></li>
</ol></div><div><p dir="auto">需要注意的是，数据的总大小并不是唯一决定模型质量的因素。数据的质量、多样性以及训练过程中使用的技术（如微调、对抗性训练、参与人的反馈等）也对模型的最终性能有重大影响。</p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> GPT 是如何进行预训练的？</span></li>
<li data-line="1" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> <strong>Unsupervised Pre-Train</strong></span></li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> Causal Masking, Mask Modeling Masking</span></li>
<li data-line="3" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> Continue Pre-Training </span></li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/703735798" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/703735798" target="_blank"># 大模型 | PPO 和 DPO 都不是 RLHF 的理想形式</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 原始的 RLHF 流程包含三个阶段</span></li>
<li data-line="1" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> <strong>Supervised Tuning</strong> </span></li>
<li data-line="2" dir="auto">对一个预训练好的语言模型进行指令微调，即给定指令 prompt 和（通常情况下）人类撰写的回复进行监督训练，SFT 阶段训好的模型会用于 Reward model 和 RLHF policy 的初始化。</li>
<li data-line="3" dir="auto"><a data-tooltip-position="top" aria-label="https://www.chatanything.ai/c/666d3180987a3b037fd301d6" rel="noopener" class="external-link" href="https://www.chatanything.ai/c/666d3180987a3b037fd301d6" target="_blank">Instruction GPT</a>, 有害内容输出变少, 1.3B的InstructGPT 比 175B的GPT-3更加能遵循英语指令. 训练InstructGPT 的核心是使用人类反馈强化学习(RLHF) </li>
<li data-line="4" dir="auto"></li>
<li data-line="5" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> <strong>Reinforcement Learning</strong> </span></li>
<li data-line="6" dir="auto"><strong>Reward Model 阶段</strong>：采集一份人类偏好数据集，包含正样本和负样本，对应一个给定的 prompt 下人类喜欢和不喜欢的回复，利用这份标注可以训练一个二分类器</li>
<li data-line="7" dir="auto">Proximal Policy Optimization</li>
<li data-line="8" dir="auto">Direct Preference Optimization</li>
</ul></div><div><ol start="5">
<li data-line="0" dir="auto">无监督预训练：Mask 下文，用上文预测 Mask 掉的下文。 </li>
</ol></div><div><ul>
<li data-line="0" dir="auto"></li>
</ul></div><div><ol start="6">
<li data-line="0" dir="auto">SFT 有监督训练, Instruct, 让大模型学会特定领域</li>
</ol></div><div><ul>
<li data-line="0" dir="auto">
<p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=l9skLRKpn9o" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=l9skLRKpn9o" target="_blank">InstructGPT 论文精读【论文精读】 - YouTube</a> </p>
</li>
<li data-line="1" dir="auto">
<p><a data-tooltip-position="top" aria-label="https://openai.com/index/instruction-following/" rel="noopener" class="external-link" href="https://openai.com/index/instruction-following/" target="_blank">Aligning language models to follow instructions | OpenAI</a></p>
</li>
<li data-line="5" dir="auto">
<p>2022年的内容,</p>
</li>
</ul></div><div><ol start="7">
<li data-line="0" dir="auto">PPO 强化学习, Alignment, 学习人类便好</li>
</ol></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 什么是单向双向注意力机制？</p>
</span></li>
<li data-line="1" dir="auto">
<p>单向双向指的是自注意力机制在时间步上可以访问的信息范围。训练时 Mask 机制的不同导致了模型</p>
</li>
<li data-line="2" dir="auto">
<p>causal masking 机制使得模型在每个时间步只能基于先前的上下文来预测下一个token。</p>
</li>
<li data-line="3" dir="auto">
<p>MLM 则迫使模型学习同时考虑学习前后上下文。</p>
</li>
<li data-line="5" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 为什么 GPT 要使用 causal masking? 为什么GPT 现在为什么比 BERT 效果好</p>
</span></li>
<li data-line="6" dir="auto">
<p>这个问题关乎 文本生成 和 文本理解 有什么关系？两者是不是可以互相取代，还是各有所长。</p>
</li>
<li data-line="7" dir="auto">
<p>我个人认为 文本生成是一定要理解上下文的。而文本理解，理解是理解了，但是精力也被分散因为是注意力是双向的。</p>
</li>
<li data-line="8" dir="auto">
<p>GPT 比 BERT 效果更好的原因。我的理解是 BERT 还不够大，没有用 RHLF, SFT 机制辅助训练。还有就是 GPT 的技术官是非常非常坚信 Scaling law, 技术路线走的比较笃定，同时 scaling law 真的有用 for now.</p>
</li>
<li data-line="12" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> SFT</p>
</span></li>
<li data-line="14" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> RFHF</p>
</span></li>
<li data-line="15" dir="auto">
<p>主要是让模型可以知道什么回答是好的,什么是不好的</p>
</li>
<li data-line="16" dir="auto">
<p>Agent 在环境中获得奖励的强化学习算法  PPO</p>
</li>
<li data-line="17" dir="auto">
<p>直接在</p>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="微调" dir="auto" class="heading" id="微调"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>微调</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/638809556" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/638809556" target="_blank">大模型高效微调综述上：Adapter Tuning、AdaMix、PET、Prefix-Tuning、Prompt Tuning、P-tuning、P-tuning v2</a><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/639068809" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/639068809" target="_blank">大模型高效微调综述下： DiffPruning、BitFit、LoRa、AdaLoRA、MAM Adapters、UniPELT</a></p></div><div><img src="https://cdn.sa.net/2024/06/05/APSwqcyFJD3tkzO.png" referrerpolicy="no-referrer"></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.cool/arxiv/2106.09685" rel="noopener" class="external-link" href="https://papers.cool/arxiv/2106.09685" target="_blank">LoRA: Low-Rank Adaptation of Large Language Models | Cool Papers - Immersive Paper Discovery</a></p></div><div><ul>
<li data-line="0" dir="auto">低秩矩阵分解LORA 是我们有一个复杂的高秩矩阵(即其中的信息多且分散) 分解为两个或多个低秩矩阵的乘积. 类似于反着用的奇异值分解SVD</li>
<li data-line="1" dir="auto">原始参数 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> 使用 一个增广因子(update factor, <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> )和另一个为偏置因子(bias factor, <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>) 进行数学近似。r 一般比 d 小的多（通常小于 100）</li>
<li data-line="2" dir="auto">简单矩阵权重的从 element-wise 的调整  转换为  row/column-wise 调整的数学近似</li>
<li data-line="3" dir="auto">在实际应用中可以直接加载FFN层，从而不对原有权重造成影响</li>
<li data-line="4" dir="auto">训练的时候是两个低秩矩阵是外挂的，推理的时候由于LORA是对原始参数的累加，所以可以只可以对两个矩阵expand直接加到原始参数上。所以说LORA不会对推理有影响</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/687168177" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/687168177" target="_blank">LoRA及其变体概述：LoRA, DoRA, AdaLoRA, Delta-LoRA</a></p></div><div><ul>
<li data-line="0" dir="auto"><strong>LoRA+</strong> 主要是为 AB 矩阵的训练引入了不同学习率。通常偏置因子的学习率要比增广因子要高的多。<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2402.12354" rel="noopener" class="external-link" href="https://arxiv.org/abs/2402.12354" target="_blank">[2402.12354] LoRA+: Efficient Low Rank Adaptation of Large Models</a></li>
<li data-line="1" dir="auto"><strong>VeRA</strong> 没有训练矩阵A和B而是用共享的随机权值初始化这些矩阵,并添加两个新的向量d和B，微调的时候只训练向量d和B. 因为研究发现大型网络只有一小部分权重用于引导行为，随机初始化能够使模型从一开始就倾向于模型行为。<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2310.11454" rel="noopener" class="external-link" href="https://arxiv.org/abs/2310.11454" target="_blank">[2310.11454] VeRA: Vector-based Random Matrix Adaptation</a></li>
<li data-line="2" dir="auto"><strong>LoRA-FA</strong> 是LoRA与Frozen-A的缩写. 矩阵A在初始化后被冻结, B在 零初始化后训练。效果不错。<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2308.03303" rel="noopener" class="external-link" href="https://arxiv.org/abs/2308.03303" target="_blank">[2308.03303] LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning</a></li>
<li data-line="3" dir="auto"><strong>LoRA-drop</strong> 首先对数据子集进行采样，训练几轮 LoRA 并计算其重要性，freeze 不会剧烈改变行为的数据，只训练部分层的 LoRA.<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2402.07721" rel="noopener" class="external-link" href="https://arxiv.org/abs/2402.07721" target="_blank">[2402.07721] LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation</a></li>
<li data-line="4" dir="auto"><strong>AdaLoRA</strong> 可以决定不同的适配器具有不同的秩 <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2303.10512" rel="noopener" class="external-link" href="https://arxiv.org/abs/2303.10512" target="_blank">[2303.10512] AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning</a></li>
<li data-line="5" dir="auto">weight - decomposition Low-Rank Adaption (<strong>DoRA</strong>) 使用更高阶的矩阵来完成原有大小和方向的分解<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2402.09353" rel="noopener" class="external-link" href="https://arxiv.org/abs/2402.09353" target="_blank">[2402.09353] DoRA: Weight-Decomposed Low-Rank Adaptation</a></li>
<li data-line="6" dir="auto"><strong>Delta-LoRA</strong> 提出 AB的梯度来更新矩阵W  <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2309.02411" rel="noopener" class="external-link" href="https://arxiv.org/abs/2309.02411" target="_blank">[2309.02411] Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices</a></li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://spaces.ac.cn/archives/9590" rel="noopener" class="external-link" href="https://spaces.ac.cn/archives/9590" target="_blank">梯度视角下的LoRA：简介、分析、猜测及推广 - 科学空间|Scientific Spaces</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1701.09175" rel="noopener" class="external-link" href="https://arxiv.org/abs/1701.09175" target="_blank">[1701.09175] Skip Connections Eliminate Singularities</a></p></div><div><ul>
<li data-line="0" dir="auto">
<p>LLM 的很多权重是冗余的。解决它我们可以用 LoRA, 知识蒸馏，压缩剪枝。</p>
</li>
<li data-line="5" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> <a data-tooltip-position="top" aria-label="https://huggingface.co/blog/zh/hf-bitsandbytes-integration" rel="noopener" class="external-link" href="https://huggingface.co/blog/zh/hf-bitsandbytes-integration" target="_blank">大规模 Transformer 模型 8 比特矩阵乘简介 - 基于 Hugging Face Transformers、Accelerate 以及 bitsandbytes</a></p>
</span></li>
<li data-line="6" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p>QLoRA 引入了许多创新来在不牺牲性能的情况下节省显存：</p>
<ul>
<li data-line="7" dir="auto">4 位 NormalFloat（NF4），一种对于正态分布权重而言信息理论上最优的新数据类型</li>
<li data-line="8" dir="auto">双重量化，通过量化量化常数来减少平均内存占用</li>
<li data-line="9" dir="auto">分页优化器，用于管理内存峰值</li>
</ul>
</li>
<li data-line="10" dir="auto">
<p>原文实现：单个 48G 的 GPU 显卡上微调 65B 的参数模型，保持 16 字节微调任务的性能</p>
</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/703524663" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/703524663" target="_blank">【LLM】吴恩达『微调大模型』课程完全笔记</a></p></div><div><ul>
<li data-line="0" dir="auto">什么是微调，在哪些情况下微调可能对你的应用有帮助；</li>
<li data-line="1" dir="auto">微调如何融入训练，它与提示工程或者检索增强生成有何不同，以及如何将这些技术与微调结合使用；</li>
<li data-line="2" dir="auto">深入探讨微调的特殊变体，这种方式已经将GPT3打造成了ChatGPT，称为指令微调，它教会了LLM遵循指令；</li>
<li data-line="3" dir="auto">逐步了解如何微调自己的LLM，准备数据、训练模型，并在代码中进行评估</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="开源大模型" dir="auto" class="heading" id="开源大模型"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>开源大模型</h3><div class="heading-children"><div><blockquote dir="auto">
<p>Llama Qwen 大部分的开源模型架构都差不多. 可能就只有ChatGLM 比较特别用 Encoder-Decoder 和 双向注意力架构.</p>
</blockquote></div><div><div class="mermaid"><svg aria-roledescription="flowchart-v2" role="graphics-document document" viewBox="-8 -50.5 317.82421875 393.28125" height="393.28125" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" width="317.82421875" id="m5cae26d4caa48d7a"><style>#m5cae26d4caa48d7a{font-family:var(--font-mermaid);font-size:16px;fill:var(--text-normal);}#m5cae26d4caa48d7a .error-icon{fill:var(--background-primary);}#m5cae26d4caa48d7a .error-text{fill:var(--text-error);stroke:var(--text-error);}#m5cae26d4caa48d7a .edge-thickness-normal{stroke-width:2px;}#m5cae26d4caa48d7a .edge-thickness-thick{stroke-width:3.5px;}#m5cae26d4caa48d7a .edge-pattern-solid{stroke-dasharray:0;}#m5cae26d4caa48d7a .edge-pattern-dashed{stroke-dasharray:3;}#m5cae26d4caa48d7a .edge-pattern-dotted{stroke-dasharray:2;}#m5cae26d4caa48d7a .marker{fill:var(--text-normal);stroke:var(--text-normal);}#m5cae26d4caa48d7a .marker.cross{stroke:var(--text-normal);}#m5cae26d4caa48d7a svg{font-family:var(--font-mermaid);font-size:16px;}#m5cae26d4caa48d7a .label{font-family:var(--font-mermaid);color:var(--text-normal);}#m5cae26d4caa48d7a .cluster-label text{fill:var(--text-normal);}#m5cae26d4caa48d7a .cluster-label span,#m5cae26d4caa48d7a p{color:var(--text-normal);}#m5cae26d4caa48d7a .label text,#m5cae26d4caa48d7a span,#m5cae26d4caa48d7a p{fill:var(--text-normal);color:var(--text-normal);}#m5cae26d4caa48d7a .node rect,#m5cae26d4caa48d7a .node circle,#m5cae26d4caa48d7a .node ellipse,#m5cae26d4caa48d7a .node polygon,#m5cae26d4caa48d7a .node path{fill:var(--background-primary);stroke:var(--text-muted);stroke-width:1px;}#m5cae26d4caa48d7a .flowchart-label text{text-anchor:middle;}#m5cae26d4caa48d7a .node .label{text-align:center;}#m5cae26d4caa48d7a .node.clickable{cursor:pointer;}#m5cae26d4caa48d7a .arrowheadPath{fill:#0b0b0b;}#m5cae26d4caa48d7a .edgePath .path{stroke:var(--text-normal);stroke-width:2.0px;}#m5cae26d4caa48d7a .flowchart-link{stroke:var(--text-normal);fill:none;}#m5cae26d4caa48d7a .edgeLabel{background-color:var(--background-secondary);text-align:center;}#m5cae26d4caa48d7a .edgeLabel rect{opacity:0.5;background-color:var(--background-secondary);fill:var(--background-secondary);}#m5cae26d4caa48d7a .labelBkg{background-color:var(--background-secondary);}#m5cae26d4caa48d7a .cluster rect{fill:hsl(220.5882352941, 100%, 98.3333333333%);stroke:hsl(220.5882352941, 60%, 88.3333333333%);stroke-width:1px;}#m5cae26d4caa48d7a .cluster text{fill:var(--text-normal);}#m5cae26d4caa48d7a .cluster span,#m5cae26d4caa48d7a p{color:var(--text-normal);}#m5cae26d4caa48d7a div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:var(--font-mermaid);font-size:12px;background:var(--background-secondary-alt);border:1px solid hsl(220.5882352941, 60%, 88.3333333333%);border-radius:2px;pointer-events:none;z-index:100;}#m5cae26d4caa48d7a .flowchartTitleText{text-anchor:middle;font-size:18px;fill:var(--text-normal);}#m5cae26d4caa48d7a foreignObject{overflow:visible;}#m5cae26d4caa48d7a #arrowhead,#m5cae26d4caa48d7a #sequencenumber,#m5cae26d4caa48d7a .cluster text,#m5cae26d4caa48d7a .label text,#m5cae26d4caa48d7a text,#m5cae26d4caa48d7a text.actor{fill:var(--text-normal);}#m5cae26d4caa48d7a line{stroke:var(--text-normal);}#m5cae26d4caa48d7a g&gt;g&gt;circle,#m5cae26d4caa48d7a g&gt;g&gt;path{stroke:var(--background-accent);color:var(--text-normal);}#m5cae26d4caa48d7a .label rect{display:none;}#m5cae26d4caa48d7a .cluster rect{stroke-width:1px;}#m5cae26d4caa48d7a .node circle,#m5cae26d4caa48d7a .node ellipse,#m5cae26d4caa48d7a .node path,#m5cae26d4caa48d7a .node polygon,#m5cae26d4caa48d7a .node rect{fill:var(--background-secondary-alt);stroke:var(--background-modifier-border);stroke-width:1px;}#m5cae26d4caa48d7a .node .label{text-align:center;}#m5cae26d4caa48d7a .node.clickable{cursor:pointer;}#m5cae26d4caa48d7a .arrowheadPath{fill:var(--text-muted);}#m5cae26d4caa48d7a .edgePath .path{stroke:var(--text-muted);stroke-width:1.5px;}#m5cae26d4caa48d7a .edgeLabel{background-color:var(--background-primary);text-align:center;}#m5cae26d4caa48d7a .cluster rect{fill:var(--background-primary-alt);stroke:var(--background-modifier-border);}#m5cae26d4caa48d7a div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-size:12px;background:var(--background-secondary);border:1px solid var(--interactive-accent);border-radius:2px;pointer-events:none;z-index:100;}#m5cae26d4caa48d7a .actor{stroke:var(--background-modifier-border);fill:var(--background-secondary-alt);font-family:inherit!important;}#m5cae26d4caa48d7a text.actor{stroke:none;}#m5cae26d4caa48d7a .actor-line{stroke:var(--text-muted);}#m5cae26d4caa48d7a .messageLine0,#m5cae26d4caa48d7a .messageLine1{stroke-width:1.5;stroke-dasharray:'2 2';stroke:var(--text-normal);}#m5cae26d4caa48d7a #crosshead path{fill:var(--text-normal)!important;stroke:var(--text-normal)!important;}#m5cae26d4caa48d7a .messageText{fill:var(--text-normal);stroke:none;font-family:inherit!important;}#m5cae26d4caa48d7a .labelBox{stroke:var(--background-modifier-border);fill:var(--background-secondary-alt);}#m5cae26d4caa48d7a .labelText,#m5cae26d4caa48d7a .loopText{fill:var(--text-normal);stroke:none;}#m5cae26d4caa48d7a .loopLine{stroke-width:2;stroke-dasharray:'2 2';stroke:var(--background-modifier-border);}#m5cae26d4caa48d7a .activation0,#m5cae26d4caa48d7a .activation1,#m5cae26d4caa48d7a .activation2{fill:var(--background-secondary) stroke:var(--text-muted);}#m5cae26d4caa48d7a .section{stroke:none;opacity:.2;}#m5cae26d4caa48d7a .section0,#m5cae26d4caa48d7a .section2{fill:var(--text-accent);}#m5cae26d4caa48d7a .section1,#m5cae26d4caa48d7a .section3{fill:#fff;opacity:.2;}#m5cae26d4caa48d7a .sectionTitle0,#m5cae26d4caa48d7a .sectionTitle1,#m5cae26d4caa48d7a .sectionTitle2,#m5cae26d4caa48d7a .sectionTitle3{fill:var(--text-normal);}#m5cae26d4caa48d7a .sectionTitle{text-anchor:start;font-size:11px;text-height:14px;}#m5cae26d4caa48d7a .grid .tick{stroke:var(--background-primary-alt);opacity:.8;shape-rendering:crispEdges;}#m5cae26d4caa48d7a .grid path{stroke-width:0;}#m5cae26d4caa48d7a .today{fill:none;stroke:#d42;stroke-width:2px;}#m5cae26d4caa48d7a .task{stroke-width:2;}#m5cae26d4caa48d7a .taskText{text-anchor:middle;}#m5cae26d4caa48d7a .taskText:not([font-size]){font-size:11px;}#m5cae26d4caa48d7a .taskTextOutsideRight{fill:var(--text-normal);text-anchor:start;font-size:11px;}#m5cae26d4caa48d7a .taskTextOutsideLeft{fill:var(--text-normal);text-anchor:end;font-size:11px;}#m5cae26d4caa48d7a .task.clickable,#m5cae26d4caa48d7a g.clickable{cursor:pointer;}#m5cae26d4caa48d7a .taskText.clickable,#m5cae26d4caa48d7a .taskTextOutsideLeft.clickable,#m5cae26d4caa48d7a .taskTextOutsideRight.clickable{cursor:pointer;fill:#003163!important;font-weight:700;}#m5cae26d4caa48d7a .taskText0,#m5cae26d4caa48d7a .taskText1,#m5cae26d4caa48d7a .taskText2,#m5cae26d4caa48d7a .taskText3{fill:#fff;}#m5cae26d4caa48d7a .task0,#m5cae26d4caa48d7a .task1,#m5cae26d4caa48d7a .task2,#m5cae26d4caa48d7a .task3{fill:var(--interactive-accent);stroke:var(--interactive-accent);}#m5cae26d4caa48d7a .taskTextOutside0,#m5cae26d4caa48d7a .taskTextOutside1,#m5cae26d4caa48d7a .taskTextOutside2,#m5cae26d4caa48d7a .taskTextOutside3{fill:var(--text-normal);}#m5cae26d4caa48d7a .active0,#m5cae26d4caa48d7a .active1,#m5cae26d4caa48d7a .active2,#m5cae26d4caa48d7a .active3,#m5cae26d4caa48d7a g.classGroup rect,#m5cae26d4caa48d7a g.stateGroup rect{fill:var(--background-primary-alt);stroke:var(--background-modifier-border);}#m5cae26d4caa48d7a g.classGroup rect,#m5cae26d4caa48d7a g.stateGroup rect{stroke:var(--background-modifier-border);}#m5cae26d4caa48d7a .activeText0,#m5cae26d4caa48d7a .activeText1,#m5cae26d4caa48d7a .activeText2,#m5cae26d4caa48d7a .activeText3{fill:var(--text-normal)!important;}#m5cae26d4caa48d7a .done0,#m5cae26d4caa48d7a .done1,#m5cae26d4caa48d7a .done2,#m5cae26d4caa48d7a .done3{stroke:var(--text-muted);fill:#bbb;stroke-width:2;}#m5cae26d4caa48d7a .doneText0,#m5cae26d4caa48d7a .doneText1,#m5cae26d4caa48d7a .doneText2,#m5cae26d4caa48d7a .doneText3{fill:var(--text-normal)!important;}#m5cae26d4caa48d7a .crit0,#m5cae26d4caa48d7a .crit1,#m5cae26d4caa48d7a .crit2,#m5cae26d4caa48d7a .crit3{stroke:#b1361b;fill:#d42;stroke-width:2;}#m5cae26d4caa48d7a .activeCrit0,#m5cae26d4caa48d7a .activeCrit1,#m5cae26d4caa48d7a .activeCrit2,#m5cae26d4caa48d7a .activeCrit3,#m5cae26d4caa48d7a .classLabel .box{stroke:#b1361b;fill:var(--background-secondary-alt);stroke-width:2;}#m5cae26d4caa48d7a .classLabel .box{stroke:none;stroke-width:0;opacity:.5;}#m5cae26d4caa48d7a .doneCrit0,#m5cae26d4caa48d7a .doneCrit1,#m5cae26d4caa48d7a .doneCrit2,#m5cae26d4caa48d7a .doneCrit3{stroke:#b1361b;fill:#bbb;stroke-width:2;cursor:pointer;shape-rendering:crispEdges;}#m5cae26d4caa48d7a .milestone{transform:rotate(45deg) scale(.8,.8);}#m5cae26d4caa48d7a .milestoneText{font-style:italic;}#m5cae26d4caa48d7a .activeCritText0,#m5cae26d4caa48d7a .activeCritText1,#m5cae26d4caa48d7a .activeCritText2,#m5cae26d4caa48d7a .activeCritText3,#m5cae26d4caa48d7a .doneCritText0,#m5cae26d4caa48d7a .doneCritText1,#m5cae26d4caa48d7a .doneCritText2,#m5cae26d4caa48d7a .doneCritText3{fill:var(--text-normal)!important;}#m5cae26d4caa48d7a .titleText{text-anchor:middle;font-size:18px;fill:var(--text-normal);}#m5cae26d4caa48d7a g.classGroup text{fill:var(--text-normal);stroke:none;font-size:11px;}#m5cae26d4caa48d7a g.classGroup text .title{font-weight:bolder;}#m5cae26d4caa48d7a #aggregationEnd,#m5cae26d4caa48d7a #aggregationStart,#m5cae26d4caa48d7a #compositionEnd,#m5cae26d4caa48d7a #compositionStart,#m5cae26d4caa48d7a g.classGroup line,#m5cae26d4caa48d7a g.stateGroup line{stroke:var(--background-modifier-border);stroke-width:1;}#m5cae26d4caa48d7a .classLabel .label{font-size:11px;}#m5cae26d4caa48d7a .relation{fill:none;}#m5cae26d4caa48d7a .dashed-line{stroke-dasharray:3;}#m5cae26d4caa48d7a #compositionEnd,#m5cae26d4caa48d7a #compositionStart{fill:var(--background-modifier-border);}#m5cae26d4caa48d7a #aggregationEnd,#m5cae26d4caa48d7a #aggregationStart{fill:var(--background-secondary-alt);}#m5cae26d4caa48d7a #dependencyEnd,#m5cae26d4caa48d7a #dependencyStart,#m5cae26d4caa48d7a #extensionEnd,#m5cae26d4caa48d7a #extensionStart{fill:var(--background-modifier-border);stroke:var(--background-modifier-border);stroke-width:1;}#m5cae26d4caa48d7a .branch-label,#m5cae26d4caa48d7a .commit-id,#m5cae26d4caa48d7a .commit-msg{fill:#d3d3d3;color:#d3d3d3;}#m5cae26d4caa48d7a .pieTitleText{text-anchor:middle;font-size:25px;fill:var(--text-normal);}#m5cae26d4caa48d7a .state-note text,#m5cae26d4caa48d7a g.stateGroup text{stroke:none;font-size:10px;}#m5cae26d4caa48d7a g.stateGroup .state-title{font-weight:bolder;fill:var(--text-normal);}#m5cae26d4caa48d7a .stateGroup .composit{fill:#fff;border-bottom:1px;}#m5cae26d4caa48d7a .stateGroup .alt-composit{fill:#e0e0e0;border-bottom:1px;}#m5cae26d4caa48d7a .state-note{stroke:#645c10;fill:#f3edb3;}#m5cae26d4caa48d7a .state-note text{fill:#000;}#m5cae26d4caa48d7a .stateLabel .box{stroke:none;stroke-width:0;fill:var(--background-secondary-alt);opacity:.5;}#m5cae26d4caa48d7a .stateLabel text{fill:var(--text-normal);font-size:10px;font-weight:700;}#m5cae26d4caa48d7a .node circle.state-start{fill:var(--text-normal);stroke:var(--text-normal);}#m5cae26d4caa48d7a .node circle.state-end{stroke:var(--background-primary);stroke-width:2;}#m5cae26d4caa48d7a #statediagram-barbEnd,#m5cae26d4caa48d7a g.stateGroup text{fill:var(--background-modifier-border);}#m5cae26d4caa48d7a .statediagram-cluster rect{stroke-width:1px;}#m5cae26d4caa48d7a .statediagram-cluster rect,#m5cae26d4caa48d7a .statediagram-state .divider{stroke:var(--background-modifier-border);}#m5cae26d4caa48d7a .statediagram-cluster rect .inner{fill:var(--background-secondary-alt);}#m5cae26d4caa48d7a .statediagram-cluster.statediagram-cluster-alt .inner{fill:var(--background-secondary);}#m5cae26d4caa48d7a .cluster-label text,#m5cae26d4caa48d7a .node circle.state-end{fill:var(--text-normal);}#m5cae26d4caa48d7a .statediagram-state rect.divider{stroke-dasharray:10,10;fill:var(--background-secondary);}#m5cae26d4caa48d7a .note-edge{stroke-dasharray:5;}#m5cae26d4caa48d7a .statediagram-note rect{fill:#f3edb3;stroke:#645c10;stroke-width:1px;}#m5cae26d4caa48d7a .error-icon{fill:var(--background-modifier-error);}#m5cae26d4caa48d7a .error-text{fill:var(--text-error);stroke:var(--text-error);}#m5cae26d4caa48d7a :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif;}</style><g><marker orient="auto" markerHeight="12" markerWidth="12" markerUnits="userSpaceOnUse" refY="5" refX="6" viewBox="0 0 10 10" class="marker flowchart" id="m5cae26d4caa48d7a-m5cae26d4caa48d7a_flowchart-pointEnd"><path style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 0 0 L 10 5 L 0 10 z"></path></marker><marker orient="auto" markerHeight="12" markerWidth="12" markerUnits="userSpaceOnUse" refY="5" refX="4.5" viewBox="0 0 10 10" class="marker flowchart" id="m5cae26d4caa48d7a-m5cae26d4caa48d7a_flowchart-pointStart"><path style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 0 5 L 10 10 L 10 0 z"></path></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5" refX="11" viewBox="0 0 10 10" class="marker flowchart" id="m5cae26d4caa48d7a-m5cae26d4caa48d7a_flowchart-circleEnd"><circle style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" r="5" cy="5" cx="5"></circle></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5" refX="-1" viewBox="0 0 10 10" class="marker flowchart" id="m5cae26d4caa48d7a-m5cae26d4caa48d7a_flowchart-circleStart"><circle style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" r="5" cy="5" cx="5"></circle></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5.2" refX="12" viewBox="0 0 11 11" class="marker cross flowchart" id="m5cae26d4caa48d7a-m5cae26d4caa48d7a_flowchart-crossEnd"><path style="stroke-width: 2; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 1,1 l 9,9 M 10,1 l -9,9"></path></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5.2" refX="-1" viewBox="0 0 11 11" class="marker cross flowchart" id="m5cae26d4caa48d7a-m5cae26d4caa48d7a_flowchart-crossStart"><path style="stroke-width: 2; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 1,1 l 9,9 M 10,1 l -9,9"></path></marker><g class="root"><g class="clusters"></g><g class="edgePaths"><path marker-end="url(#m5cae26d4caa48d7a-m5cae26d4caa48d7a_flowchart-pointEnd)" style="fill:none;" class="edge-thickness-normal edge-pattern-solid flowchart-link LS-A LE-B" id="L-A-B-0" d="M162.055,35.797L162.055,41.697C162.055,47.596,162.055,59.396,162.055,70.312C162.055,81.228,162.055,91.261,162.055,96.277L162.055,101.294"></path><path marker-end="url(#m5cae26d4caa48d7a-m5cae26d4caa48d7a_flowchart-pointEnd)" style="fill:none;" class="edge-thickness-normal edge-pattern-solid flowchart-link LS-B LE-C" id="L-B-C-0" d="M162.055,142.391L162.055,146.557C162.055,150.724,162.055,159.057,162.055,166.507C162.055,173.957,162.055,180.524,162.055,183.807L162.055,187.091"></path><path marker-end="url(#m5cae26d4caa48d7a-m5cae26d4caa48d7a_flowchart-pointEnd)" style="fill:none;" class="edge-thickness-normal edge-pattern-solid flowchart-link LS-C LE-D" id="L-C-D-0" d="M123.534,228.188L110.837,234.087C98.14,239.987,72.746,251.786,65.75,262.985C58.755,274.183,70.158,284.78,75.859,290.078L81.561,295.376"></path><path marker-end="url(#m5cae26d4caa48d7a-m5cae26d4caa48d7a_flowchart-pointEnd)" style="fill:none;" class="edge-thickness-normal edge-pattern-solid flowchart-link LS-D LE-C" id="L-D-C-0" d="M123.963,298.984L130.312,293.085C136.66,287.185,149.358,275.385,155.706,264.469C162.055,253.553,162.055,243.52,162.055,238.504L162.055,233.488"></path><path marker-end="url(#m5cae26d4caa48d7a-m5cae26d4caa48d7a_flowchart-pointEnd)" style="fill:none;" class="edge-thickness-normal edge-pattern-solid flowchart-link LS-C LE-E" id="L-C-E-0" d="M196.296,228.188L207.582,234.087C218.869,239.987,241.442,251.786,252.729,262.703C264.016,273.619,264.016,283.652,264.016,288.668L264.016,293.684"></path></g><g class="edgeLabels"><g transform="translate(162.0546875, 71.1953125)" class="edgeLabel"><g transform="translate(-33.34375, -10.3984375)" class="label"><foreignObject height="20.796875" width="66.6875"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel">tokenizer</span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g transform="translate(47.3515625, 263.5859375)" class="edgeLabel"><g transform="translate(-47.3515625, -10.3984375)" class="label"><foreignObject height="20.796875" width="94.703125"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel">hidden_state</span></div></foreignObject></g></g><g transform="translate(162.0546875, 263.5859375)" class="edgeLabel"><g transform="translate(-47.3515625, -10.3984375)" class="label"><foreignObject height="20.796875" width="94.703125"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel">hidden_state</span></div></foreignObject></g></g><g transform="translate(264.015625, 263.5859375)" class="edgeLabel"><g transform="translate(-20.00390625, -10.3984375)" class="label"><foreignObject height="20.796875" width="40.0078125"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel">'EOS'</span></div></foreignObject></g></g></g><g class="nodes"><g transform="translate(162.0546875, 17.8984375)" id="flowchart-A-48" class="node default default flowchart-label"><rect height="35.796875" width="65.3984375" y="-17.8984375" x="-32.69921875" ry="0" rx="0" style="" class="basic label-container"></rect><g transform="translate(-25.19921875, -10.3984375)" style="" class="label"><rect></rect><foreignObject height="20.796875" width="50.3984375"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel">'query'</span></div></foreignObject></g></g><g transform="translate(162.0546875, 124.4921875)" id="flowchart-B-49" class="node default default flowchart-label"><rect height="35.796875" width="55.359375" y="-17.8984375" x="-27.6796875" ry="0" rx="0" style="" class="basic label-container"></rect><g transform="translate(-20.1796875, -10.3984375)" style="" class="label"><rect></rect><foreignObject height="20.796875" width="40.359375"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel">token</span></div></foreignObject></g></g><g transform="translate(162.0546875, 210.2890625)" id="flowchart-C-51" class="node default default flowchart-label"><rect height="35.796875" width="97.3046875" y="-17.8984375" x="-48.65234375" ry="0" rx="0" style="" class="basic label-container"></rect><g transform="translate(-41.15234375, -10.3984375)" style="" class="label"><rect></rect><foreignObject height="20.796875" width="82.3046875"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel">Embedding</span></div></foreignObject></g></g><g transform="translate(104.703125, 316.8828125)" id="flowchart-D-53" class="node default default flowchart-label"><rect height="35.796875" width="57.515625" y="-17.8984375" x="-28.7578125" ry="0" rx="0" style="" class="basic label-container"></rect><g transform="translate(-21.2578125, -10.3984375)" style="" class="label"><rect></rect><foreignObject height="20.796875" width="42.515625"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel">layers</span></div></foreignObject></g></g><g transform="translate(264.015625, 316.8828125)" id="flowchart-E-57" class="node default default flowchart-label"><rect height="35.796875" width="75.6171875" y="-17.8984375" x="-37.80859375" ry="0" rx="0" style="" class="basic label-container"></rect><g transform="translate(-30.30859375, -10.3984375)" style="" class="label"><rect></rect><foreignObject height="20.796875" width="60.6171875"><div style="display: inline-block; white-space: nowrap;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel">'answer'</span></div></foreignObject></g></g></g></g></g><text class="flowchartTitleText" y="-25" x="150.912109375">General Pipeline of Llama</text></svg></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=57xSbOJoV_w" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=57xSbOJoV_w" target="_blank">LLM大语言模型的涌现能力是什么？大力出奇迹是否可行？ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2206.07682" rel="noopener" class="external-link" href="https://arxiv.org/abs/2206.07682" target="_blank">[2206.07682] Emergent Abilities of Large Language Models - 2022</a></p>
<ul>
<li dir="auto">Google发表的论文</li>
<li dir="auto">发现GPT-3在模型参数量达到一定程度时, 模型性能出现了非线性爆发的现象</li>
<li dir="auto">发现不仅在GPT-3 在其他LLM时也发生了 “涌现”</li>
</ul></div></div></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://github.com/deepseek-ai/DeepSeek-V2" rel="noopener" class="external-link" href="https://github.com/deepseek-ai/DeepSeek-V2" target="_blank">GitHub - deepseek-ai/DeepSeek-V2: DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</a><br>
<img style="width:500" src="https://cdn.sa.net/2024/06/06/12DabSMdT59vLzp.png" referrerpolicy="no-referrer"><br>
<a data-tooltip-position="top" aria-label="https://spaces.ac.cn/archives/10091" rel="noopener" class="external-link" href="https://spaces.ac.cn/archives/10091" target="_blank">缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA - 科学空间|Scientific Spaces</a></p></div><div><ul>
<li data-line="0" dir="auto">引入了 MLA（<strong>M</strong>ulti-head&nbsp;<strong>L</strong>atent&nbsp;<strong>A</strong>ttention) </li>
<li data-line="1" dir="auto"><strong>M</strong>ulti-<strong>Q</strong>uery&nbsp;<strong>A</strong>ttention(2019) 基本思想是让所有Attention Head共享同一个K、V。大部分任务的损失都比较有限. 使用的代表模型有PaLM、 StarCoder、 Gemini</li>
<li data-line="2" dir="auto">如果你担心MQA对KV Cache的压缩太严重，以至于会影响模型的学习效率以及最终效果。你们 MHA 和 MQA 之间的 <strong>G</strong>rouped-<strong>Q</strong>uery&nbsp;<strong>A</strong>ttention 应运而生。基本思想是将所有Head分为g个组（g可以整除hℎ），每组共享同一对K、V。使用的代表模型有 Llama2/3 ChatGLM </li>
<li data-line="3" dir="auto">DeepSeek-V2的技术报告里是从低秩投影的角度引入MLA,投影矩阵的方式替代了GQA的分割、重复，并引入了一个恒等变换技巧来可以进一步压缩KV Cache, 同时采用了一种混合方法来兼容RoPE</li>
</ul></div><div class="admonition-parent admonition-quote-parent"><div class="callout admonition admonition-quote admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="quote" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Quote</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2403.07183" rel="noopener" class="external-link" href="https://arxiv.org/abs/2403.07183" target="_blank">[2403.07183] Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews</a></p>
<p dir="auto">Standford出品。发现在各大会议的同行评审中由解决15%的内容由LLM生成。</p>
<p dir="auto">判断文本的来源的方法是创建一个最大似然MLE的统计学方法。（判断AI味有多浓）</p>
<p dir="auto">捕捉到了同行评审同质化的倾向(用词倾向多样化降低)。解释了滥用ChatGPT的乱象。</p>
<p dir="auto">在拒稿的论文中，发现不太回应作者反驳的评审中，使用LLM的比例较高。</p>
<p dir="auto">截稿三天之前，AI味明显增加。</p>
<p dir="auto">AI润色和直接写有区别。发现很多review是直接生成的。</p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/03/DVaiTnpkxsAt7BC.png" referrerpolicy="no-referrer"></p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="LLM可解释性" dir="auto" class="heading" id="LLM可解释性"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>LLM可解释性</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2406.04093" rel="noopener" class="external-link" href="https://arxiv.org/abs/2406.04093" target="_blank">[2406.04093] Scaling and evaluating sparse autoencoders</a><br>
<a data-tooltip-position="top" aria-label="https://www.ithome.com/0/773/913.htm" rel="noopener" class="external-link" href="https://www.ithome.com/0/773/913.htm" target="_blank">OpenAI 公开破解 GPT-4 思维的新方法，Ilya 也参与了 - IT之家</a></p></div><div><ul>
<li data-line="0" dir="auto">训练了一个 1600万 稀疏自编码器, 基于 GPT-4 的激活状态处理了 4000 亿个令牌, 可视化LLM的中间层. </li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="多模态(multimodal)" dir="auto" class="heading" id="多模态(multimodal)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>多模态(multimodal)</h2><div class="heading-children"><div><blockquote dir="auto">
<p>之前的single-task specific 的算法都强烈依赖任务数据不具有泛化能力。以 CLIP 和 SegmentAnyThing 为代表的模型具有了很大迁移潜力；Diffusion模型beatGAN也是依赖通用的文生图能力。未来的多模态允许比文字更多的输入，因为人类的文本很多场景不能或者需要很大成本才能达到我们期望的输出，或者模型无法单从或者很难通过文本理解这个世界。</p>
</blockquote></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://mmmu-benchmark.github.io/#leaderboard" rel="noopener" class="external-link" href="https://mmmu-benchmark.github.io/#leaderboard" target="_blank">MMMU 多模态模型榜单</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 基本架构</p>
</span></li>
<li data-line="1" dir="auto">
<p>Encoder : 将多种自然信号(waveform, frame, text)编码成离散的token或者连续的特征(没有cutoff但是更难训练)，映射到相同的空间</p>
</li>
<li data-line="2" dir="auto">
<p>LLM : 用于推理</p>
</li>
<li data-line="3" dir="auto">
<p>Decoder: LLM 只给出语义信息让下游的专有解码器实际执行数据的输出</p>
</li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 发展趋势</p>
</span></li>
<li data-line="5" dir="auto">
<p>外挂：2023年之前的多模态一般是在 SFT 后加入多模态功能。比如 freeze 语言层，单独训练图片和文本的Adapter层，然后打开语言层再做一个Fine-tune. 这种方法外挂的方法很难有助于 LLM 的推理能力也就是说 LLM 不理解图片之中的泛化，不理解原生图像之间细微的差别。</p>
</li>
<li data-line="6" dir="auto">
<p>原生：一个更好的方法是在 Pre-train 阶段就统一训练多模态数据。但是最好不要一起训练，最好每个模态也是模块化的，方便后期调整.</p>
</li>
<li data-line="7" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 短期局限</p>
</span></li>
<li data-line="8" dir="auto">
<p>长文本对话窗口，长多模态对话窗口</p>
</li>
<li data-line="9" dir="auto">
<p>更加可控和连贯的多模态的输出</p>
</li>
<li data-line="10" dir="auto">
<p>如果找到一个通用的多模态 Pre-train 方法</p>
</li>
<li data-line="13" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 主要的技术挑战</p>
</span></li>
<li data-line="14" dir="auto">
<p>如何进行多模态的融合和对齐？</p>
</li>
<li data-line="15" dir="auto">
<p>数据标注和获取</p>
</li>
<li data-line="16" dir="auto">
<p>计算资源和效率</p>
</li>
<li data-line="19" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 多模态特征融合方法</p>
</span></li>
<li data-line="20" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>简单拼接(concatenation)**</strong> 将两个向量直接拼接在一起形成一个更大的向量。假设有文本向量 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> , 图片向量 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> , 融合向量 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>.  那么拼接有 <span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mrow space="4"><mjx-mo class="mjx-s3"><mjx-c class="mjx-c5B TEX-S3"></mjx-c></mjx-mo><mjx-mtable style="min-width: 0.769em;"><mjx-table><mjx-itable><mjx-mtr><mjx-mtd style="padding-bottom: 0.2em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mtr><mjx-mtr><mjx-mtd style="padding-top: 0.2em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mtr></mjx-itable></mjx-table></mjx-mtable><mjx-mo class="mjx-s3"><mjx-c class="mjx-c5D TEX-S3"></mjx-c></mjx-mo></mjx-mrow></mjx-math></mjx-container></span></p>
</span></li>
<li data-line="23" dir="auto">
<p><strong>优点</strong>: 简单且易于实现；维持原始特征的完整性。</p>
</li>
<li data-line="24" dir="auto">
<p><strong>缺点</strong>: 可能导致某一模态的特征在融合特征中占主导地位；没有显式地模型化不同模态之间的交互；可能导致维度过高，需要更多的数据来训练以避免过拟合。</p>
</li>
<li data-line="25" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>加权求和(weight sum)</strong> <span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> 让&nbsp;T&nbsp;和&nbsp;V&nbsp;具有相同的维度后，应用权重并求和。</p>
</span></li>
<li data-line="28" dir="auto">
<p><strong>优点</strong>: 通过权重可以调整每种模态的重要性。</p>
</li>
<li data-line="29" dir="auto">
<p><strong>缺点</strong>: 与简单串联类似，未显式建模特征交互；需要调整和优化权重。</p>
</li>
<li data-line="31" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>矩阵乘法(matrix multiplication)</strong> <span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mtable justify="left" style="min-width: 0.704em;"><mjx-table><mjx-itable><mjx-mtr><mjx-mtd><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mtr></mjx-itable></mjx-table></mjx-mtable><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow><mjx-script style="vertical-align: 0.477em;"><mjx-mi class="mjx-n" size="s"><mjx-c class="mjx-c22A4"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> 如果&nbsp;T&nbsp;是一个向量而&nbsp;V&nbsp;是一个矩阵，或者有一种方式可以将二者转化为可以相乘的形式，那么可以使用矩阵乘法。或者如果&nbsp;T&nbsp;和&nbsp;V&nbsp;都被映射到了另一个空间中，且在那个空间可以进行矩阵乘法</p>
</span></li>
<li data-line="34" dir="auto">
<p><strong>优点</strong>: 可以显式地编码模态间的互动，特别是使用可学习的权重矩阵。</p>
</li>
<li data-line="35" dir="auto">
<p><strong>缺点</strong>: 维数应当匹配，可能需要附加的过程来保证这一点；计算可能比较昂贵；需要有足够的数据来避免过拟合。</p>
</li>
<li data-line="38" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>哈达玛积(Hadamard Product</strong>) <span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2299"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> 如果&nbsp;T&nbsp;和&nbsp;V&nbsp;有相同的维度，可以执行元素对元素的乘法。</p>
</span></li>
<li data-line="41" dir="auto">
<p><strong>优点</strong>: 易于实现；元素对元素的乘法捕捉局部交互，保留空间结构。</p>
</li>
<li data-line="42" dir="auto">
<p><strong>缺点</strong>: 要求两个模态的维度完全相同；不捕捉全局的相互依赖关系</p>
</li>
<li data-line="45" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>张量积 (Tensor Product)</strong>  <span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2297"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>如果希望捕获所有模态间的交互，可以使用张量积。这将显著增加结果的维度。</p>
</span></li>
<li data-line="48" dir="auto">
<p><strong>优点</strong>: 创建一个丰富的表示，包含所有可能的特征对之间的交互。</p>
</li>
<li data-line="49" dir="auto">
<p><strong>缺点</strong>: 结果的维数随着原始特征空间的增长而呈指数级增长；非常容易过拟合且计算和存储代价很大。</p>
</li>
<li data-line="52" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>线性变换(linear transformation)</strong> <span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D70E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.104em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.104em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>可以分别对不同模态进行线性变换后结合起来</p>
</span></li>
<li data-line="53" dir="auto">
<p><strong>优点</strong>: 线性变换可以通过降维来减小特征空间和减少计算量；可以帮助将数据映射到更有利于区分的空间，提升模型表现。</p>
</li>
<li data-line="54" dir="auto">
<p><strong>缺点</strong>: 线性变换可能无法捕捉非线性特征之间的复杂交互；可能需要额外的数据以确保映射后的空间有足够的区分能力。</p>
</li>
<li data-line="56" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>神经网络(neural network )</strong> <span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="4"><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c75"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c4E"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c77"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c6B"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span>如果想进行复杂的非线性变换可以直接用一个神经网络</p>
</span></li>
<li data-line="59" dir="auto">
<p><strong>优点</strong>: 非线性变换能够捕获复杂的模态间交互；可以通过深度学习的技术来自动学习特征的最佳表示。</p>
</li>
<li data-line="60" dir="auto">
<p><strong>缺点</strong>: 需要大规模的数据集来有效训练；模型可能变得难以解释；训练过程可能计算昂贵。</p>
</li>
<li data-line="62" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>注意力机制(Attention)</strong> <span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="4"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span></p>
</span></li>
<li data-line="65" dir="auto">
<p><strong>优点</strong>: 允许模型学习聚焦于最重要的特征部分；能够处理不同长度的序列数据。</p>
</li>
<li data-line="66" dir="auto">
<p><strong>缺点</strong>: 实现复杂；同样需要足够多的数据来训练注意力权重。</p>
</li>
<li data-line="69" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>共注意力(Co-Attention)</strong></p>
</span></li>
<li data-line="70" dir="auto">
<p>一种常用于多模态融合学习的方法。共注意力机制的核心思想是，当模型处理一种模态的数据时，在计算内部模态注意力的同时也考虑另一种模态之间的交互注意力，允许不同模态之间交互式地影响彼此的注意力(aka. 二维注意力机制)</p>
</li>
<li data-line="71" dir="auto">
<p>计算注意力权重<br>
<span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FC TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-n" space="4"><mjx-c class="mjx-c73"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c66"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c78"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-s3"><mjx-c class="mjx-c28 TEX-S3"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D410 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D40A TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.082em;"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-s3"><mjx-c class="mjx-c29 TEX-S3"></mjx-c></mjx-mo></mjx-mrow></mjx-math></mjx-container></span></p>
</li>
<li data-line="73" dir="auto">
<p>计算自注意力表示<br>
<span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D413 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munder space="4"><mjx-row><mjx-base><mjx-texatom texclass="OP"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c2211 TEX-S2"></mjx-c></mjx-mo></mjx-texatom></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.576em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-under></mjx-row></mjx-munder><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FC TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D413 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mstyle><mjx-mspace style="width: 0.278em;"></mjx-mspace></mjx-mstyle><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D415 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munder space="4"><mjx-row><mjx-base><mjx-texatom texclass="OP"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c2211 TEX-S2"></mjx-c></mjx-mo></mjx-texatom></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.576em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-under></mjx-row></mjx-munder><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FC TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D415 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math></mjx-container></span></p>
</li>
<li data-line="76" dir="auto">
<p>计算交互注意力表示<span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D413 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munder space="4"><mjx-row><mjx-base><mjx-texatom texclass="OP"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c2211 TEX-S2"></mjx-c></mjx-mo></mjx-texatom></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.576em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-under></mjx-row></mjx-munder><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FC TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D415 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mstyle><mjx-mspace style="width: 0.278em;"></mjx-mspace></mjx-mstyle><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D415 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munder space="4"><mjx-row><mjx-base><mjx-texatom texclass="OP"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c2211 TEX-S2"></mjx-c></mjx-mo></mjx-texatom></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.576em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-under></mjx-row></mjx-munder><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FC TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D413 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math></mjx-container></span></p>
</li>
<li data-line="77" dir="auto">
<p>融合特征表示</p>
</li>
</ul></div><div><p dir="auto"><span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D405 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D413 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D413 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mstyle><mjx-mspace style="width: 0.278em;"></mjx-mspace></mjx-mstyle><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D405 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D415 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D415 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math></mjx-container></span></p></div><div><p dir="auto">MLLM 论文</p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="~" style="--lc-callout-color: 124, 77, 255;"><span class="lc-li-wrapper"><span class="lc-list-marker">~</span> 双流网络</span></li>
<li data-line="1" dir="auto"><strong>ViLBERT: Pretrained Vision-and-Language BERT for Multimodal Pretraining" by Jiasen Lu, et al. (2019)</strong>  ViLBERT是多模态领域的开创性工作之一，它采用双流结构分别处理视觉和语言输入，通过协同训练学习跨模态表示。</li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="~" style="--lc-callout-color: 124, 77, 255;"><span class="lc-li-wrapper"><span class="lc-list-marker">~</span> 单流网络</span></li>
<li data-line="3" dir="auto" class="lc-list-callout" data-callout="~" style="--lc-callout-color: 124, 77, 255;"><span class="lc-li-wrapper"><span class="lc-list-marker">~</span> 浅对齐</span></li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/626796690/answer/3524920516" rel="noopener" class="external-link" href="https://www.zhihu.com/question/626796690/answer/3524920516" target="_blank">多模态大语言模型（MLLM）为什么最近的工作中用BLIP2中Q-Former结构的变少了？</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 多模态的 connecter 基本上有 BLIP 的 Q-Former 和 LLaVA 的 MLP 两种方式。现在更多用 MLP</span></li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Pb421n7GD/?spm_id_from=333.337.search-card.all.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Pb421n7GD/?spm_id_from=333.337.search-card.all.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">2024智源大会 谢赛宁 Language Models Need Better Visual Grounding For Meaning And Unders_哔哩哔哩_bilibili</a><br>
<a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=tqI7A7tLXb0&amp;t=113s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=tqI7A7tLXb0&amp;t=113s" target="_blank"># 【人工智能】大语言模型需要更好的视觉基础来理解含义 | 谢赛宁 | DiT作者 | 多模态语言模型的视觉缺陷 | 基于视觉搜索引导的多模态大模型 | GPT-4V | CLIP | SEAL</a></p></div><div><ul>
<li data-line="0" dir="auto">从MLM的视觉缺陷 和 基于视觉搜索引导的多模态大模型两个角度分享了谢团队的两篇关于LLM 时代多模态视觉的表征研究。</li>
<li data-line="1" dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2401.06209" rel="noopener" class="external-link" href="https://arxiv.org/abs/2401.06209" target="_blank">[2401.06209] Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</a> 质疑 2021 年 CLIP 现在是否有能力表征 text-image. 通过 CLIP-Blind Pairs(开源视觉数据集中近似的照片对，然后用CLIP 和 自监督方法提取两张图片的图像嵌入，期望 CLIP 提取的两个嵌入相似度比自监督的要高很多，然后人工标记视觉差别)的方法构建了 MMVP 数据集。对世面上多模态模型的测试结果发现，人类识别视觉差异的能力遥遥领先于vLLM。并总结了 9 种 vLLM 的常见错误。之后构建了一个难度更高的测试集，并尝试使用自监督混合的方式增强 vLLM 的 MMVP 性能。Takeaway, 多模态模型需要有比 CLIP 更强的 text-image 提取器，自监督视觉还有很大的发展潜力。</li>
<li data-line="2" dir="auto"><a data-tooltip-position="top" aria-label="https://vstar-seal.github.io/" rel="noopener" class="external-link" href="https://vstar-seal.github.io/" target="_blank">V_*: Guided Visual Search as a Core Mechanism in Multimodal LLMs</a> 主要讲的是现有的vLLM 还是不能 和人类一样有视觉感知的中心和引导。AI 需要有和人类视觉认知对齐的视觉模型。然后提出了 SEAL(Show, sEArch, and telL ) 的一个视觉大模型的元框架。&lt;- 现在工程上是不断对图像进行切割。<span style="background:#fff88f">但是为此需要知道，明确知道现在知道的信息是否足够；能否显式的列出所需要的额外的信息；在完成视觉搜索并理解后融合搜索的结果；为复杂任务分配更多的计算资源。</span></li>
</ul></div><div class="heading-wrapper"><h3 data-heading="BLIP" dir="auto" class="heading" id="BLIP"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>BLIP</h3><div class="heading-children"><div><blockquote dir="auto">
<p>基本思想：兼顾图文理解和生成的多模态模型（Multimodal mixture of Encoder-Decoder），同时在三个视觉语言目标上联合预训练：图像文本对比学习ITC、图像文本匹配ITM、图像条件语言建模LM；同时提出了一种高效利用网络收集的嘈杂图文对的采样+过滤机制。</p>
</blockquote></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2201.12086" rel="noopener" class="external-link" href="https://arxiv.org/abs/2201.12086" target="_blank">[2201.12086] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a><img src="https://cdn.sa.net/2024/06/06/Dv6iFYtVPZhpaIx.png" referrerpolicy="no-referrer"></span></li>
<li data-line="1" dir="auto">这是一个给图片 Caption 的模型</li>
<li data-line="2" dir="auto">训练一个filter 可以将和图片不符合的文字表述过滤。多过滤几轮就可以获得图片正确的表述</li>
<li data-line="3" dir="auto">自己点进去看架构图(Fig.2)。核心是双向注意力的一个对比学习</li>
<li data-line="4" dir="auto">这里的数据使用 bootstrap 增强数据 (Fig.3)。 处理文本-图片对的方法值得参考</li>
<li data-line="5" dir="auto">blip2用clip做视觉backbone，中间加bert做对齐训练（Qformer）。再用一个MLP/linear接到LLM上，只更新MLP/linear浅层对齐。</li>
</ul></div><div><p dir="auto"><strong>模型侧</strong>：</p></div><div><ul>
<li data-line="0" dir="auto">图像块的编码（ViT）：图像打成patches块后进行编码，增加cls token来记录全局的特征（作用类似位置编码，保留patches的空间特征）；</li>
<li data-line="1" dir="auto">文本的编码（BERT）：对句子进行编码，增加cls token记录句子的全局特征；</li>
<li data-line="2" dir="auto">Image-grounded text encoder：在文本embedding中注入了图像特征，通过在self-attention和FFN中间增加一层cross-attention来对齐text-encoder和img-encoder的特征。</li>
<li data-line="3" dir="auto">Image-grounded text decoder：用causal self-attention层（预测下一个token）代替了双向自注意力层（建立当前输入token的表达）【和左边的encoder共享除了self-attention之外的层】。</li>
</ul></div><div><p dir="auto"><strong>训练loss</strong>:</p></div><div><ul>
<li data-line="0" dir="auto">预训练阶段同时优化3个loss项，每个图文对只过1次vision-transormer(算力消耗较大)，过3次text-transormer。</li>
<li data-line="1" dir="auto">Image-Text Contrastive Loss (ITC)理解功能：优化vision-transormer+text-transormer，让匹配的图文对有较高相似度的表达（用了soft labels），多模态中的经典loss-&gt;使其<strong>互信息最大化</strong>；</li>
<li data-line="2" dir="auto">Image-Text Matching Loss (ITM)理解功能 ：优化Image-grounded text encoder，学习图文的<strong>细粒度匹配</strong>的二分类，采用了hard negative mining strategy；</li>
<li data-line="3" dir="auto">Language Modeling Loss (LM)生成功能：优化image-grounded text decoder，学习如何从给定图生成连贯的文本描述，采用交叉熵代价函数以自回归方式最大化对应文本概率。</li>
</ul></div><div><p dir="auto"><strong>Captioning and Filtering</strong>：一种高效的数据集增强方法，从网页噪声图像文本对中学习，采样生成器+噪声过滤器，都是从相同的预训练模型初始化，并在小型人工注释数据集e.g.COCO上单独进行微调。</p></div><div><ol>
<li data-line="0" dir="auto">采样生成器captioner：基于图像的文本解码器（生成），用LM的loss进行微调</li>
<li data-line="1" dir="auto">噪声过滤器filter：基于图像的文本编码器（判别），用ITC+ITM进行微调，判断图文是否匹配。</li>
</ol></div><div><p dir="auto"><strong>效果</strong>：从用人工标注的图文对得到的预训练模型开始初始化，captioner生成合成文本，filter过滤合成文本和网页描述文本，得到过滤后的相对干净的网页文本和合成文本-&gt;用来训练BLIP，往复进行。</p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> BLIP 2 </span></li>
<li data-line="1" dir="auto">只用 100 万参数的 vLLM </li>
<li data-line="2" dir="auto">GPT4-V 发布对比的多模态模型就是后续的改进 BLIP 2 </li>
</ul></div><div><p dir="auto"><strong>模型侧</strong>：</p></div><div><ul>
<li data-line="0" dir="auto">BLIP-2 由预训练的Image Encoder，预训练的Large Language Model，和一个可学习的 Q-Former 组成。</li>
</ul></div><div><p dir="auto"><strong>Image Encoder</strong>：</p></div><div><ul>
<li data-line="0" dir="auto">从输入图片中提取视觉特征，尝试了两种网络结构，CLIP 训练的 ViT-L/14和EVA-CLIP训练的 ViT-g/14（去掉了最后一层）。</li>
</ul></div><div><p dir="auto"><strong>Large Language Model</strong>：</p></div><div><ul>
<li data-line="0" dir="auto">大语言模型进行文本生成，尝试了接入decoder-based LLM 和 encoder-decoder-based LLM两种结构。</li>
</ul></div><div><p dir="auto"><strong>Q-Former</strong>：</p></div><div><ul>
<li data-line="0" dir="auto">弥补视觉和语言两种模态的modality gap，可以理解为固定图像编码器和固定LLM之间的<strong>信息枢纽</strong>，选取最有用的视觉特征给LLM来生成文本</li>
</ul></div><div><p dir="auto"><strong>How Q-Former?</strong>:</p></div><div><ul>
<li data-line="0" dir="auto">Q-Former由Image Transformer和Text Transformer两个子模块构成，它们共享相同自注意力层。</li>
</ul></div><div><p dir="auto"><strong>Image Transformer</strong>：</p></div><div><ul>
<li data-line="0" dir="auto">通过和image encoder交互来提取视觉特征，输入是一系列（文中用的32个<em>768长度）可学习的 Queries，这些Query通过<strong>自注意力层</strong>相互交互，并通过<strong>交叉注意力层</strong>与冻结的图像特征交互，还可以通过<strong>共享的自注意力层</strong>与文本进行交互；输出的query尺寸是32</em>768，远小于冻结的图像特征257*1024(ViT-L/14)。</li>
</ul></div><div><p dir="auto"><strong>Text Transformer</strong>：</p></div><div><ul>
<li data-line="0" dir="auto">
<p>既作为文本编码器也作为文本解码器，它的自注意力层与Image Transformer共享，根据预训练任务，用不同的self-attention masks来控制Query和文本的交互方式。</p>
</li>
<li data-line="5" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2102.03334" rel="noopener" class="external-link" href="https://arxiv.org/abs/2102.03334" target="_blank">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision, 2021</a><br>
<img src="https://cdn.sa.net/2024/06/05/ER8FhtGMi6X2Uul.png" referrerpolicy="no-referrer"></p>
</span></li>
<li data-line="7" dir="auto">
<p>对文字嵌入+0, 对经过图片嵌入+1.  直接往Transformer 里灌</p>
</li>
<li data-line="8" dir="auto">
<p>设计了两个Loss用于输出图片Caption</p>
</li>
<li data-line="11" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2107.07651" rel="noopener" class="external-link" href="https://arxiv.org/abs/2107.07651" target="_blank">[2107.07651] Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</a></p>
</span></li>
<li data-line="12" dir="auto">
<img style="width:500" src="https://cdn.sa.net/2024/06/06/5fmzYxKbe9wn7sr.png" referrerpolicy="no-referrer">
</li>
<li data-line="13" dir="auto">
<p>这个文字还有点意思。Momentum + CLIP like + 编码解码结构 like </p>
</li>
<li data-line="14" dir="auto">
<p>在对比学习里使用Hard Negative 样本</p>
</li>
<li data-line="15" dir="auto">
<p>Momentum 模型作为教师模型(稳定)，让学生模型(不稳定)去学 Hard Negative 样本的差异，然后使用动量模型对学生模型进行蒸馏，让老师学习学生模型到的东西</p>
</li>
<li data-line="16" dir="auto">
<p>总体思想是想让文字和图像的嵌入使用对比学习对其，然后再融合多模态</p>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="LLaVA" dir="auto" class="heading" id="LLaVA"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>LLaVA</h3><div class="heading-children"><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://llava-vl.github.io/" rel="noopener" class="external-link" href="https://llava-vl.github.io/" target="_blank">LLaVA</a><br>
<img src="https://cdn.sa.net/2024/06/05/wWRJMrULYmflSaT.png" referrerpolicy="no-referrer"></p>
</span></li>
<li data-line="3" dir="auto">
<p><strong>第一阶段(feature alignment)</strong> 图片一个占位符：“描述一下这个 &lt;img&gt;  这个的人在做什么？” “图片中的人似乎正在...” <a data-tooltip-position="top" aria-label="https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain" rel="noopener" class="external-link" href="https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain" target="_blank">liuhaotian/LLaVA-Pretrain · Datasets at Hugging Face</a></p>
</li>
<li data-line="4" dir="auto">
<p><strong>第二阶段(end-to-end fine-tuning)</strong> 同时训练图片和文本 <a data-tooltip-position="top" aria-label="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json" rel="noopener" class="external-link" href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json" target="_blank">llava_v1_5_mix665k.json · liuhaotian/LLaVA-Instruct-150K at main</a> 图片数据集用到COCO, GQA, OCR-VQA, TextVQA, VisualGenome</p>
</li>
<li data-line="5" dir="auto">
<p>基本上就是使用ViT + MLP 把图片变为 image_embedding 然后分两个阶段设计一个融合层将两个模态嵌入拼接在一起送入隐藏层</p>
</li>
<li data-line="6" dir="auto">
<p>llava，clip输出的embedding过linear直接拼接到LLM的输入embedding中，两阶段训练，图文预训练和图文指令训练，均只训练浅对齐linear。可以只用指令数据集，不用预训练，前提指令数据集要够。</p>
</li>
<li data-line="10" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://minigpt-4.github.io/" rel="noopener" class="external-link" href="https://minigpt-4.github.io/" target="_blank">Minigpt-4</a><img src="https://minigpt-4.github.io/images/overview.png" referrerpolicy="no-referrer"></p>
</span></li>
<li data-line="11" dir="auto">
<p>可能是最简单训练vLLM。就是Q-Former 和 ViT 作为图片编码器，Freeze Llama的变种作为基座模型。然后训练一个中间的线性层用于将图片嵌入转为有文本嵌入传入的hidden features。比BLIP2 多了一个线性层</p>
</li>
<li data-line="12" dir="auto">
<p>Image Encoder 和 LLM 不训练，穷逼训练不起...hhhh (10hout <em> 4 </em> A100)</p>
</li>
<li data-line="16" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">%</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2302.14045" rel="noopener" class="external-link" href="https://arxiv.org/abs/2302.14045" target="_blank">[2302.14045] Language Is Not All You Need: Aligning Perception with Language Models</a></p>
</span></li>
<li data-line="17" dir="auto">
<p>这篇文章打算做多模态的原生融合，没怎么讲架构，反正是大力出奇迹</p>
</li>
<li data-line="18" dir="auto">
<p>训练数据 COYO-700M 和 LAION-2B</p>
</li>
<li data-line="19" dir="auto">
<p>提出了一个 Kosmos 的多模态LLM，在Github 和 HG 开源</p>
</li>
<li data-line="20" dir="auto">
<p>激活函数用的 GeLU </p>
</li>
<li data-line="24" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2308.12966" rel="noopener" class="external-link" href="https://arxiv.org/abs/2308.12966" target="_blank">[2308.12966] Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</a><img src="https://cdn.sa.net/2024/06/06/NlgmVFxv2d65BJR.png" referrerpolicy="no-referrer"></p>
</span></li>
<li data-line="25" dir="auto">
<p>Qwen-VL  = Qwen-7b + OpenCLIP ViT-bigG + 位置感知视觉语言 Adapter + 1.5B 图文数据 + Chat对齐</p>
</li>
<li data-line="26" dir="auto">
<p>Connecter 是 Q-Former </p>
</li>
<li data-line="30" dir="auto" class="lc-list-callout" data-callout="$" style="--lc-callout-color: 0, 200, 83;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">$</span> <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=P-z2s9EWNnU" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=P-z2s9EWNnU" target="_blank">当AI进入医学，会有多少人失业？LLaVA-Med为何性能这么好？ - YouTube</a></p>
</span></li>
<li data-line="31" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">%</span> <a data-tooltip-position="top" aria-label="https://github.com/microsoft/LLaVA-Med" rel="noopener" class="external-link" href="https://github.com/microsoft/LLaVA-Med" target="_blank">GitHub - microsoft/LLaVA-Med: Large Language-and-Vision Assistant for Biomedicine, built towards multimodal GPT-4 level capabilities.</a><img src="https://github.com/microsoft/LLaVA-Med/raw/main/images/llava_med_pipeline.png" referrerpolicy="no-referrer"></p>
</span></li>
<li data-line="32" dir="auto">
<p>一天之内将 LLaVa 变为 LlaVa-Med </p>
</li>
<li data-line="33" dir="auto">
<p>Stage 1 : 医学对齐(视觉+文字)</p>
</li>
<li data-line="34" dir="auto">
<p>Stage 2 :<br>
Pre-train + 微调 </p>
</li>
<li data-line="36" dir="auto">
<p>证明 LlaVa 微调潜力很大</p>
</li>
</ul></div><div><p dir="auto">这篇论文介绍了一个名为LLaVA-Med的大型语言和视觉助手，专门针对生物医学领域。以下是对论文的主要内容进行的总结：</p></div><div><ol>
<li data-line="0" dir="auto"><strong>背景与动机</strong>：尽管会话生成式AI在赋能生物医学从业者方面展现出巨大潜力，但目前的研究主要集中在单模态文本上。现有的多模态对话AI虽然在公共网络上数十亿的图文对上取得了快速进展，但它们在理解和交流生物医学图像方面仍然不够精细。</li>
<li data-line="1" dir="auto"><strong>LLaVA-Med的提出</strong>：为了解决这一问题，作者提出了一种成本效益高的方法来训练一个能够回答生物医学图像开放性研究问题的视觉-语言对话助手。关键思想是利用从PubMed Central提取的大规模、广泛覆盖的生物医学图示-标题数据集，使用GPT-4自指导开放式指令生成数据，并采用新颖的课程学习方法微调大型通用领域视觉-语言模型。</li>
<li data-line="2" dir="auto"><strong>方法</strong>：<span style="background:#fff88f">LLaVA-Med首先学习使用图示-标题对来对齐生物医学词汇，然后使用GPT-4生成的指令跟随数据来掌握开放式会话语义</span>，广泛模仿普通人如何逐步获得生物医学知识。</li>
<li data-line="3" dir="auto"><strong>训练过程</strong>：通过这种训练方法，LLaVA-Med可以在不到15小时的时间内（使用八个A100 GPU）完成训练。作者还提出了一种新颖的课程学习方法，首先微调模型以对齐生物医学词汇，然后继续训练模型以学习开放式会话语义。</li>
<li data-line="4" dir="auto"><strong>性能评估</strong>：在三个标准的生物医学视觉问答（VQA）数据集上，微调后的LLaVA-Med在某些指标上超越了先前的监督学习最先进水平。</li>
<li data-line="5" dir="auto"><strong>开源贡献</strong>：为了促进生物医学多模态研究，作者将发布他们的指令跟随数据和LLaVA-Med模型。</li>
<li data-line="6" dir="auto"><strong>相关工作</strong>：论文还讨论了生物医学聊天机器人和生物医学视觉问答的相关工作，并对现有方法进行了区分，包括判别性方法和生成性方法。</li>
<li data-line="7" dir="auto"><strong>模型架构</strong>：LLaVA-Med在架构上类似于语言模型的前缀调整，使用新的可训练模块连接冻结的图像编码器和因果语言模型。</li>
<li data-line="8" dir="auto"><strong>数据集创建</strong>：作者创建了首个多模态生物医学数据集，通过机器-人类协作程序，从广泛存在的生物医学图像-文本对中生成。</li>
<li data-line="9" dir="auto"><strong>实验</strong>：作者进行了实验来研究生成的多模态生物医学指令跟随数据的质量和LLaVA-Med的性能。</li>
<li data-line="10" dir="auto"><strong>结论</strong>：尽管LLaVA-Med在构建有用的生物医学视觉助手方面迈出了重要一步，但它仍然受到幻觉和深度推理不足的限制，这是许多大型多模态模型的常见问题。未来的工作将致力于提高质量和可靠性。</li>
</ol></div><div><p dir="auto">能不能展开说说这篇论文中不同 Stage 对齐模型的具体方法</p></div><div><p dir="auto">当然可以。这篇论文中提出的LLaVA-Med模型通过两个阶段的训练过程来适应生物医学领域的特定需求。下面是对这两个阶段的详细说明：</p></div><div><p dir="auto">第一阶段：生物医学概念特征对齐（Biomedical Concept Feature Alignment）</p></div><div><p dir="auto">在这个阶段，模型的目的是将生物医学图像的特征与预训练语言模型中的文本嵌入对齐。具体方法如下：</p></div><div><ul>
<li data-line="0" dir="auto"><strong>数据集筛选</strong>：从PMC-15M数据集中筛选出600K图像-文本对。</li>
<li data-line="1" dir="auto"><strong>指令生成</strong>：为每对样本创建一个简单的指令，即描述图像内容。</li>
<li data-line="2" dir="auto"><strong>模型训练</strong>：在训练过程中，保持视觉编码器和语言模型的权重不变，只更新连接视觉编码器和语言模型的投影矩阵。这样，模型学习将图像特征映射到预训练语言模型的词汇空间。</li>
</ul></div><div><p dir="auto">这个阶段可以看作是将图像-文本对的词汇扩展到生物医学领域，使得模型能够识别和理解生物医学图像中的概念，并将其与相应的文本描述关联起来。</p></div><div><p dir="auto">第二阶段：端到端指令调整（End-to-End Instruction-Tuning）</p></div><div><p dir="auto">在第二阶段，模型继续接受训练，以学习如何遵循多样化的指令并完成会话中的任务。具体方法包括：</p></div><div><ul>
<li data-line="0" dir="auto"><strong>冻结视觉编码器权重</strong>：在这一阶段，只保持视觉编码器的权重不变，同时更新投影层和语言模型的预训练权重。</li>
<li data-line="1" dir="auto"><strong>指令跟随数据</strong>：使用在第一阶段生成的生物医学语言-图像指令跟随数据来微调模型。这包括多轮对话，模型需要根据提供的生物医学图像生成问题和答案。</li>
<li data-line="2" dir="auto"><strong>聊天机器人开发</strong>：通过在生物医学语言-图像指令跟随数据上微调模型，开发出一个能够与用户交互的生物医学聊天机器人。</li>
</ul></div><div><p dir="auto">在第二阶段，LLaVA-Med模型不仅能够作为生物医学视觉助手与用户互动，而且在标准生物医学VQA数据集上评估时，还能展现出良好的零样本任务迁移性能。</p></div><div><p dir="auto">微调到下游数据集（Fine-tuning to Downstream Datasets）</p></div><div><p dir="auto">在两个阶段的训练之后，如果需要针对特定的生物医学场景开发高度准确和特定于数据集的模型，作者还会在三个生物医学VQA数据集上进行微调。这包括：</p></div><div><ul>
<li data-line="0" dir="auto">给定生物医学图像作为上下文，提供多个自然语言问题。</li>
<li data-line="1" dir="auto">助手以自由形式的文本回应封闭集和开放集问题。</li>
<li data-line="2" dir="auto">对于封闭集问题，每个问题都在提示中构建了候选答案列表。</li>
</ul></div><div><p dir="auto">通过这两个阶段的训练，LLaVA-Med能够适应生物医学领域的特定需求，并在相关的任务上展现出强大的性能。</p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://huggingface.co/blog/paligemma" rel="noopener" class="external-link" href="https://huggingface.co/blog/paligemma" target="_blank">PaliGemma – Google's Cutting-Edge Open Vision Language Model</a><br>
<img style="width:500" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/paligemma/paligemma_arch.png" referrerpolicy="no-referrer"> </p>
</span></li>
<li data-line="2" dir="auto">
<p>SigLIP-So400m(Image encoder) + Gemma-2B 这种架构就很平常</p>
</li>
<li data-line="3" dir="auto">
<p>PaliGemma release include: 可针对下游应用微调的PT Checkpoint;  针对多项任务微调的 Mix checkpoint; 一系列针对不同的学术基准 FT checkpoints</p>
</li>
<li data-line="4" dir="auto">
<p>图片编码分辨率有 <code>224x224</code>&nbsp;、&nbsp;<code>448x448</code>&nbsp;、&nbsp;<code>896x896</code></p>
</li>
<li data-line="8" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2312.03700" rel="noopener" class="external-link" href="https://arxiv.org/abs/2312.03700" target="_blank">[2312.03700] OneLLM: One Framework to Align All Modalities with Language</a> <img src="https://cdn.sa.net/2024/06/15/wSOR32bIY4vpKcy.png" referrerpolicy="no-referrer"></p>
</span></li>
<li data-line="9" dir="auto">
<p>OneLLM 视频、音频、图像、医学、雷达、热成像八项全能.</p>
</li>
<li data-line="10" dir="auto">
<p>这个的特点是使用一个 Universal Encoder 把多种模态, 通过统一的多模态编码器和渐进式多模态对齐管道实现了这一点. (究极缝合怪)</p>
</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="知识蒸馏(Knowledge Distilling)" dir="auto" class="heading" id="知识蒸馏(Knowledge_Distilling)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>知识蒸馏(Knowledge Distilling)</h2><div class="heading-children"><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> Label Smoothing </span></li>
</ul></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=Kqr4jgkccD8" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=Kqr4jgkccD8" target="_blank">必读论文：知识蒸馏的奠基性工作，Label Smoothing讲了什么？ - YouTube</a></p>
<p dir="auto">[[1503.02531] Distilling the Knowledge in a Neural Network ] (<a rel="noopener" class="external-link" href="https://arxiv.org/abs/1503.02531" target="_blank">https://arxiv.org/abs/1503.02531</a>)</p>
<p dir="auto"><strong>Geoffrey Hinton 2015年一作, 是知识蒸馏领域的奠基性论文.</strong> </p>
<p dir="auto">这篇文章提出了一个 Label Smoothing 通过改变 损失函数中 class_index 的置信程度(e.g.比如原先是1现在改小一点为0.923), 来提高模型的泛化能力和过拟合风险. 这个改的方法由一个数学公式支持.</p>
<p dir="auto">在知识蒸馏任务中，标签平滑可以有以下的贡献：</p>
<ol>
<li dir="auto"><strong>更好的泛化能力</strong>：由于标签平滑可以防止模型过拟合，因此使用标签平滑的学生模型通常可以在测试数据上获得更好的性能。</li>
<li dir="auto"><strong>更好的知识蒸馏效果</strong>：在知识蒸馏中，我们希望学生模型能够复制教师模型的输出概率分布。由于标签平滑可以使模型对每个样本的类别有一定的不确定性，因此使用标签平滑的学生模型可能可以更好地复制教师模型的输出概率分布。</li>
<li dir="auto"><strong>更稳定的训练过程</strong>：标签平滑可以使得模型的训练过程更稳定，因为它避免了模型对每个样本的类别过于确定，这可能导致模型在训练过程中产生大的梯度，从而导致训练过程不稳定。</li>
</ol>
<p dir="auto">Inception v3 加入了这个模块. </p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/13/ucUvtyClaRb2QsB.png" referrerpolicy="no-referrer"></p>
<p dir="auto">让轻量的学生网络学习教师模型的行为. 这样我们不仅可以获得一个轻量的网络, 还可以选择学习什么样的内容.</p>
<p dir="auto">CLIP2 使用 ResNet 作为教师网络, 让 Transformer 学习 ResNet 的特征.</p>
<p dir="auto">现在有很多人想 “偷” GPT4的模型行为. </p></div></div></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=DqE8tVjzqqY" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=DqE8tVjzqqY" target="_blank">模型压缩的开山之作：谷歌的《知识蒸馏》讲了什么？ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1503.02531" rel="noopener" class="external-link" href="https://arxiv.org/abs/1503.02531" target="_blank">Distilling the Knowledge in a Neural Network - 2015</a></p>
<ul>
<li dir="auto">三个大牛在2015年的论文, 讨论很general的如何进行知识蒸馏(模型压缩)的内容. </li>
<li dir="auto">动机是把多个模型压缩到一个模型中, 进行知识迁移或者压缩</li>
<li dir="auto">用到了 label smoothing, soft target. 就是不要让 student teacher 单纯傻记答案.  </li>
</ul>
<p dir="auto"><img alt="6HCpMuxFT5ZeB28.png" src="https://cdn.sa.net/2024/04/16/6HCpMuxFT5ZeB28.png" referrerpolicy="no-referrer"></p>
<p dir="auto"><strong>Distilling the Knowledge in a Neural Network 这篇论文? 主要探讨了什么问题? 有什么贡献?</strong></p>
<p dir="auto"> 这篇论文的主要研究方向是神经网络的知识蒸馏。知识蒸馏是一种将大型、复杂模型（教师模型）的知识转移到小型、简单模型（学生模型）的技术。这种方法可以减少模型的计算复杂度和存储需求，同时保持较高的性能。</p>
<p dir="auto">论文的主要贡献包括：</p>
<ol>
<li dir="auto">
<p>提出了一种新的训练方法，即通过训练一个简单的模型（学生模型）来模拟复杂模型（教师模型）的行为，从而达到压缩模型和提高效率的目的。</p>
</li>
<li dir="auto">
<p>提出了一种新的软目标训练方法，即不仅使用原始硬目标（类别标签），还使用教师模型的软目标（类别概率分布）进行训练。这种方法可以使学生模型学习到教师模型的一些隐含知识，从而提高其泛化能力。</p>
</li>
<li dir="auto">
<p>通过实验验证了知识蒸馏的有效性。结果表明，使用知识蒸馏训练的小模型可以达到与原始大模型相近的性能，且计算复杂度和存储需求大大降低。</p>
</li>
</ol></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=EoKBh658Dak" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=EoKBh658Dak" target="_blank">灵魂发问：知识蒸馏真的有用吗？ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2106.05945" rel="noopener" class="external-link" href="https://arxiv.org/abs/2106.05945" target="_blank">[2106.05945] Does Knowledge Distillation Really Work?</a></p>
<p dir="auto">Google 2021 年对知识蒸馏有效性进行质疑. 他从两个方面质疑:</p>
<ul>
<li dir="auto">发现老师和学生预测的分布差别比较大, 意为着学生没有从老师很好的学习到特征</li>
<li dir="auto">发现学生对老师的Agreement(和老师一致的程度)和测试的准确性没有很好的正相关性</li>
<li dir="auto">作者任务优化算法无法很好的传递老师到学生的知识, 即使老师已经学习到了知识</li>
<li dir="auto">作者实验发现复杂的数据和特点网络回使得学生更加难从老师学习到知识</li>
<li dir="auto">作者提出使用类似Mixup和GAN生成数据等数据增强可以改善学生的Agreement和泛化能力</li>
<li dir="auto">需要更多时间用于调参</li>
<li dir="auto">知识蒸馏真的有效. 但有限. </li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=WMQLMo0H42Y" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=WMQLMo0H42Y" target="_blank">ViT太慢了，知识蒸馏后得到的DeiT又快又好！ - YouTube</a></p>
<ul>
<li dir="auto">提出了一个DeiT</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=OJxu9J1Xo8U" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=OJxu9J1Xo8U" target="_blank">2024最新人工智能方向：用小语言模型媲美大语言模型！学会知识蒸馏微软脸书谷歌抢着要你！ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://aclanthology.org/2024.eacl-long.109/" rel="noopener" class="external-link" href="https://aclanthology.org/2024.eacl-long.109/" target="_blank">Aligning Large and Small Language Models via Chain-of-Thought Reasoning - ACL Anthology</a></p>
<img style="width:500" src="https://cdn.sa.net/2024/07/02/uDTbAKJ4wGIpi1q.png" referrerpolicy="no-referrer">
<p dir="auto">文章《Aligning Large and Small Language Models via Chain-of-Thought Reasoning》探讨了一种通过思维链（Chain-of-Thought, CoT）推理方法来提升小型语言模型（Small Language Models, SLMs）推理能力的技术。</p>
<p dir="auto">小型语言模型的挑战</p>
<ul>
<li dir="auto"><strong>参数限制</strong>：小型语言模型由于参数较少，通常难以复制大型模型的思维链推理能力。</li>
<li dir="auto"><strong>性能差距</strong>：尽管通过微调或蒸馏技术从大型模型中学习，小型模型仍然无法具备相同的推理能力。</li>
</ul>
<p dir="auto">提出的解决方案：指令微调思维链（Instruction-Tuning-CoT）</p>
<ul>
<li dir="auto"><strong>方法</strong>：文章提出了一种指令微调思维链的方法，即通过使用由大型语言模型生成的思维链示例来指导小型语言模型。</li>
<li dir="auto"><strong>过程</strong>：通过使用更强大的模型生成的输出，训练小型模型以在被思维链机制提示时生成多步骤的推理答案。</li>
</ul>
<p dir="auto">实验结果</p>
<ul>
<li dir="auto"><strong>基准测试</strong>：该方法在问答和数学推理基准测试中进行了验证。</li>
<li dir="auto"><strong>性能</strong>：通过指令微调思维链方法训练的小型模型在域内和域外场景中均表现优于基线模型，显示出与大型模型相当的思维链推理能力。</li>
</ul>
<p dir="auto">影响</p>
<ul>
<li dir="auto"><strong>可访问性</strong>：这种方法降低了用户在小规模模型上操作的门槛，使高级推理能力更加普及。</li>
<li dir="auto"><strong>多样性</strong>：该方法适用于不同类型的模型，增强了小型模型的推理能力，无论其原始能力如何。</li>
</ul>
<p dir="auto">总之，本文提出了一种通过思维链提示和指令微调来弥合大模型和小模型在复杂推理任务上的性能差距的新方法。该方法使小型语言模型能够实现与大型模型相当的推理能力，从而使高级人工智能技术更加普及和多样化。</p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="对比学习(Contrastive Learning)" dir="auto" class="heading" id="对比学习(Contrastive_Learning)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>对比学习(Contrastive Learning)</h2><div class="heading-children"><div><p dir="auto"><img src="https://miro.medium.com/v2/resize:fit:4416/1*bvMhd_xpVxfJYoKXYp5hug.png" referrerpolicy="no-referrer"></p></div><div><blockquote dir="auto">
<p>上面是Y.LeCun 在 X 上 Post 的流传很广的一张图. 用蛋糕上的樱桃、奶油和蛋糕本身来类比强化学习, 有监督和自监督学习能产生信息量的大小. </p>
<p><strong>对比学习</strong>作为自监督学习中重要一部分, 着重于学习同类实例之间的共同特征，区分非同类实例之间的不同之处。对比学习通过区分不同类别的数据特征, 可以让模型在没有标签的情况下, 通过比较数据之间的差异来学习有用的特征. 这样的特性不但可以充分使用没有标签的数据, 而且也方便学习到的特征迁移到其他任务中(比如.CLIP). </p>
</blockquote></div><div><p dir="auto"><span style="background:#fff88f">对比学习有效的一点是其损失函数具备<strong>困难负样本自发现</strong>的特点, 让负样本已经远离的不要继续远离而是更加关注哪些还没有远离的负样本, 从而使表示空间更加均匀(uniformilty)</span></p></div><div><ul>
<li data-line="0" dir="auto"><code>alignment</code> : 正例之间表示保持较近距离。距离越小，alignment的程度越高</li>
<li data-line="1" dir="auto"><code>uniformity</code> :随机样例的表示应分散在超球面上。数据越均匀，保留的信息越多</li>
</ul></div><div><p dir="auto"><span style="background:#fff88f">一个典型的图片对比学习流程包含.</span></p></div><div><ol>
<li data-line="0" dir="auto">选择一个多类别的图像数据集</li>
<li data-line="1" dir="auto">生成正负样本对: 正样本对(挑选两个同一类的图片, 期望模型学习到它们是相似的); 负样本对(挑选两张不同类的图片, 期望模型学习到它们是不同的)</li>
<li data-line="2" dir="auto">使用神经网络提起图像的特征</li>
<li data-line="3" dir="auto">计算正样本和负样本特征向量的相似度/距离. </li>
</ol></div><div><blockquote dir="auto">
<p>⬇️ 下面是对比学习中一个说明“代理任务“的图片示例. 我们希望 Anchor 和 正样本距离最近和负样本距离最远.<br>
<img src="https://cdn.sa.net/2024/05/05/LYGRgxmVT5aEhl4.png" referrerpolicy="no-referrer"></p>
</blockquote></div><div class="heading-wrapper"><h3 data-heading="MemoryBank" dir="auto" class="heading" id="MemoryBank"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>MemoryBank</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=rPR6yxCLgKc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=rPR6yxCLgKc" target="_blank">对比学习的开山之作！Memory Bank做了什么？成为了业界的标杆！ - YouTube</a></p></div></div></div><div class="heading-wrapper"><h3 data-heading="CLIP(2021)" dir="auto" class="heading" id="CLIP(2021)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>CLIP(2021)</h3><div class="heading-children"><div><blockquote dir="auto">
<p>OpenAI 的 CLIP(Contrastive Language-Image Pre-training) 可能是最知名的对比学习应用. 在史无前例大图片分类数据集(400million的image-text pair对)上进行了训练. 打通了文本和图片的边界(文本和图像在特征域进行对齐), 为之后的多模态大模型打下了基础. </p>
</blockquote></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/711950736" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/711950736" target="_blank"># CLIP和BLIP损失函数分析</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2103.00020" rel="noopener" class="external-link" href="https://arxiv.org/abs/2103.00020" target="_blank">[2103.00020] Learning Transferable Visual Models From Natural Language Supervision</a><br>
<a data-tooltip-position="top" aria-label="https://blog.csdn.net/weixin_44031582/article/details/120469669" rel="noopener" class="external-link" href="https://blog.csdn.net/weixin_44031582/article/details/120469669" target="_blank">CLIP论文笔记--《Learning Transferable Visual Models From Natural Language Supervision》_visual n-grams模型-CSDN博客</a></p></div><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=xkSVUjbDI6I" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=xkSVUjbDI6I" target="_blank">【博士Vlog】OpenAI最新模型CLIP，想法极其简单？但为什么咱们做不了？ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://github.com/openai/CLIP" rel="noopener" class="external-link" href="https://github.com/openai/CLIP" target="_blank">GitHub - openai/CLIP: CLIP (Contrastive Language-Image Pretraining), Predict the most relevant text snippet given an image</a></p>
<p dir="auto"><img alt="v4yTcC7odluDjHG.png" src="https://cdn.sa.net/2024/04/10/v4yTcC7odluDjHG.png" referrerpolicy="no-referrer"></p>
<p dir="auto">基本思想：CLIP的基本算法原理是文本和图像在特征域进行对齐。</p>
<p dir="auto">模型侧：为了对image和text建立联系，首先分别对image和text进行特征提取，image特征提取的backbone可以是resnet系列模型也可以是VIT系列模型；text特征提取目前一般采用bert模型。</p>
<p dir="auto">特征提取之后，由于做了normalize，直接相乘来计算余弦距离，同一pair对的结果趋近于1，不同pair对的结果趋近于0，因为就可以采用对比损失loss（<span style="background:#fff88f">info-nce-loss</span>）【这里要比较大的batch size才能有效果，类似于维护一个大的特征相似度矩阵】</p>
<p dir="auto">利用clip进行图像分类有两种方式，一种是直接利用zero-shot 方式进行预测，如下图所示，将text假设为 a photo of [object], 分别对image 和 text进行特征提取以及余弦距离，当object为目标类别时，相似度最高，即为预测结果（效果好）；还有一种方式就是再重新finetune，同样也是对类别设计几种不同的文本，这样效果能够达到sota的水平！</p>
<ul>
<li dir="auto">对角线的值，关联度最高，应当为一，作为Goal, 训练两个encoder使其优化到Goal</li>
<li dir="auto">训练的数据量极大，图片数据大概100T.(ImageNet是1400万)</li>
<li dir="auto">对比了在ResNet-50和ViT上测试了效果，ViT效果好训练时间短</li>
</ul>
<ul>
<li dir="auto">在有名词描述的数据集上效果好，动词描述的数据集效果差</li>
</ul>
<ul>
<li dir="auto">输入 text 输出 77 token embedding(每个token有768维度)</li>
</ul></div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=SImpr4oEezg" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=SImpr4oEezg" target="_blank">深度学习艺术品鉴赏大师？OpenAI的CLIP崭露头角！ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2204.14244" rel="noopener" class="external-link" href="https://arxiv.org/abs/2204.14244" target="_blank">[2204.14244] CLIP-Art: Contrastive Pre-training for Fine-Grained Art Classification</a></p>
<p dir="auto">介绍了CLIP-Art 这篇论文。</p>
<p dir="auto">艺术品没有很好的标签，且艺术品的分类精细复杂，原有的CLIP不够用。</p>
<p dir="auto">作者使用了一个带标签的艺术品数据集对CLIP模型进行微调。<br>
在IMG_ENCODER, TEXT_ENCODER， 通过使用最小化对比预训练过程中的InfoNCE, 进行了联合嵌入。</p>
<p dir="auto"><img alt="9CbtHiDMVGzxU2L.png" src="https://cdn.sa.net/2024/04/10/9CbtHiDMVGzxU2L.png" referrerpolicy="no-referrer"></p>
<p dir="auto">做了一个消融实验发现比 CLIP 要好。且Label的确有帮助。</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto">Alpha-CLIP</p></div></div></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin " style="--callout-color: 0, 200, 83;" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/688152203" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/688152203" target="_blank">CVPR 2024 | 医学异常检测新工作！采用VLM进行医学图像中的通用异常检测 - 知乎</a></p>
<p dir="auto">介绍了</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="MoCo (2019)" dir="auto" class="heading" id="MoCo_(2019)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>MoCo (2019)</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://github.com/facebookresearch/moco" rel="noopener" class="external-link" href="https://github.com/facebookresearch/moco" target="_blank">GitHub - facebookresearch/moco: PyTorch implementation of MoCo: https://arxiv.org/abs/1911.05722</a></p></div><div><blockquote dir="auto">
<p>Kaiming He 在2019年的作品. </p>
</blockquote></div><div><p dir="auto">提出了一个 <code>InfoNCE Loss</code>的损失函数, 鼓励模型让正样本对距离越近, 负样本距离越小<br>
<img src="https://cdn.sa.net/2024/05/05/CbvJApymEkFQ51K.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto">Momentum Contrast for Unsupervised Visual Representation Learning 这篇论文的主要创新点在说明地方?</p></div><div><p dir="auto"> 这篇论文的主要创新点在于提出了一种新的无监督视觉表示学习方法，即动量对比（Momentum Contrast，MoCo）。</p></div><div><ol>
<li data-line="0" dir="auto">动量编码器：在大多数对比学习中，负样本库是固定的，而在MoCo中，负样本库是动态更新的，极大地丰富了负样本的多样性，提高了模型的泛化能力。</li>
<li data-line="1" dir="auto">MoCo提出了一个基于队列和动量更新的对比学习框架，该框架可以在大规模无标签数据上进行有效学习。</li>
</ol></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=pXvMXfPJZ2M" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=pXvMXfPJZ2M" target="_blank">MoCo 论文逐段精读【论文精读】 - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1911.05722" rel="noopener" class="external-link" href="https://arxiv.org/abs/1911.05722" target="_blank">[1911.05722] Momentum Contrast for Unsupervised Visual Representation Learning</a></p>
<ul>
<li dir="auto">Kaiming He 在 2020年的一作, 这篇论文提出了一种新的无监督学习方法，名为MoCo（Momentum Contrast）</li>
<li dir="auto">在对比学习中，模型需要学习区分不同的数据样本。具体来说，模型需要将相似（或相关）的数据样本映射到接近的表示，而将不相似（或无关）的数据样本映射到远离的表示。这种学习方式需要构造一种对比任务，即给模型提供一对数据样本，并让模型判断这两个样本是否相似或相关。</li>
<li dir="auto">MoCo的一个关键创新是引入了动量编码器和记忆库。动量编码器是一个跟随主编码器更新的模型，但更新速度较慢（即有一定的“动量”）。记忆库则用于存储动量编码器的历史输出，这些输出被用作对比样本。通过这种方式，MoCo能够在大规模数据集上进行有效的对比学习。</li>
<li dir="auto">MoCo在多个视觉任务上都取得了很好的效果，包括图像分类、物体检测等。这表明MoCo学习到的表示具有很好的泛化能力，可以用于多种不同的任务。</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="UnCLIP (2022)" dir="auto" class="heading" id="UnCLIP_(2022)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>UnCLIP (2022)</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://cdn.openai.com/papers/dall-e-2.pdf" rel="noopener" class="external-link" href="https://cdn.openai.com/papers/dall-e-2.pdf" target="_blank">Hierarchical Text-Conditional Image Generation with CLIP Latents</a></p></div><div><img style="width:800" src="https://cdn.sa.net/2024/07/01/Ua9Tu5dtcM64pyB.png" referrerpolicy="no-referrer"></div><div><p dir="auto">OpenAI 的 UNCLIP 模型首次发布于 2022 年 4 月 13 日<a data-tooltip-position="top" aria-label="https://syncedreview.com/2022/04/13/openais-unclip-text-to-image-system-leverages-contrastive-and-diffusion-models-to-achieve-sota-performance/" rel="noopener" class="external-link" href="https://syncedreview.com/2022/04/13/openais-unclip-text-to-image-system-leverages-contrastive-and-diffusion-models-to-achieve-sota-performance/" target="_blank"></a>。这是一个结合对比学习模型（CLIP）和扩散模型的图像生成系统，旨在通过反转 CLIP 图像嵌入来生成图像。UNCLIP 是 OpenAI 的 DALL·E 2 系统的核心组件之一，能够生成高质量和多样化的图像。</p></div><div><p dir="auto">UNCLIP 模型采用了两阶段的生成过程：</p></div><div><ol>
<li data-line="0" dir="auto"><strong>Prior 模型</strong>：首先，Prior 模型根据文本描述生成 CLIP 图像嵌入。这个过程可以通过自回归（autoregressive）模型或扩散（diffusion）模型来实现。研究表明，扩散模型在计算效率和生成质量方面表现更好。</li>
<li data-line="1" dir="auto"><strong>Decoder 模型</strong>：然后，Decoder 模型根据生成的 CLIP 图像嵌入生成最终的图像。这个解码器通常是基于扩散模型的，并且可以生成高分辨率的图像。</li>
</ol></div><div><p dir="auto">这种方法的优势在于，通过显式地生成图像表示，可以在保持图像真实感和与文本描述相似度的同时，提高图像的多样性。此外，UNCLIP 模型还可以生成图像的变体，保持其语义和风格，同时改变图像中非必要的细节。</p></div><div><p dir="auto">UNCLIP 的局限性</p></div><div><p dir="auto">尽管 UNCLIP 模型在图像生成方面取得了显著的成果，但它仍存在一些局限性：</p></div><div><ul>
<li data-line="0" dir="auto"><strong>细节处理</strong>：在处理复杂场景的细节时，模型可能表现不佳。</li>
<li data-line="1" dir="auto"><strong>属性绑定</strong>：在将不同属性绑定到不同对象时，模型可能会出现混淆。</li>
<li data-line="2" dir="auto"><strong>文本生成</strong>：生成连贯的文本（如图像中的文字）仍然是一个挑战。</li>
</ul></div><div><p dir="auto">总的来说，UNCLIP 通过结合 CLIP 的图像嵌入和扩散模型，为图像生成带来了革命性的进步，显著提高了生成图像的多样性和质量</p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="强化学习(RF Learning)" dir="auto" class="heading" id="强化学习(RF_Learning)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>强化学习(RF Learning)</h2><div class="heading-children"><div><blockquote dir="auto">
<p>有一个 Agent 在缓解中互动。我们给这个互动添加一点随机性，如果超出我们的预期就调整 Agent的参数。如果我们喜欢这个互动，那以后就多做一点。</p>
</blockquote></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://hrl.boyuai.com/" rel="noopener" class="external-link" href="https://hrl.boyuai.com/" target="_blank">动手学强化学习</a></p></div><div><img style="width:500" src="https://cdn.sa.net/2024/07/22/J4V5S1wKIzftybh.png" referrerpolicy="no-referrer">
<img style="width:500" src="https://cdn.sa.net/2024/07/22/tJu1x6MlckVZqYy.png" referrerpolicy="no-referrer"></div><div><img style="width:500" src="https://cdn.sa.net/2024/07/22/RAxVyLmb497YvoF.png" referrerpolicy="no-referrer">
<img style="width:500" src="https://cdn.sa.net/2024/07/22/PAqQCVMXpI584tJ.png" referrerpolicy="no-referrer">
<img style="width:500" src="https://cdn.sa.net/2024/07/22/twr21EbI4SBZhqA.png" referrerpolicy="no-referrer"></div><div><img style="width:500" src="https://cdn.sa.net/2024/07/22/NvXcV3PjZao4mOM.png" referrerpolicy="no-referrer">
<img style="width:500" src="https://cdn.sa.net/2024/07/22/jy76txXRWLuYCMD.png" referrerpolicy="no-referrer"></div><div><img style="width:500" src="https://cdn.sa.net/2024/07/22/Hyo9uWDPMdqOiaT.png" referrerpolicy="no-referrer"></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://huggingface.co/blog/zh/rlhf" rel="noopener" class="external-link" href="https://huggingface.co/blog/zh/rlhf" target="_blank">ChatGPT 背后的“功臣”——RLHF 技术详解</a></p></div><div class="heading-wrapper"><h3 data-heading="RLHF基本概念" dir="auto" class="heading" id="RLHF基本概念"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>RLHF基本概念</h3><div class="heading-children"><div><p dir="auto"> 强化学习（Reinforcement Learning, RL）主要关注如何让智能体（agent）通过与环境的交互来学习最优策略。在强化学习中，智能体通过观察环境状态、采取行动并接收奖励或惩罚来学习。目标是最大化累积奖励(长期)，这通常涉及到平衡探索（尝试新行为以发现潜在的更好策略）和利用（使用已知的最优策略）。</p></div><div><p dir="auto">强化学习的基本组成部分包括：</p></div><div><ol>
<li data-line="0" dir="auto"><strong>智能体（Agent）</strong>：学习的主体，它通过与环境交互来做出决策。</li>
<li data-line="1" dir="auto"><strong>环境（Environment）</strong>：智能体所处的世界，它会响应智能体的行动并提供反馈。</li>
<li data-line="2" dir="auto"><strong>状态（State）</strong>：环境的当前情况，智能体会根据状态做出决策。</li>
<li data-line="3" dir="auto"><strong>动作（Action）</strong>：智能体在某个状态下可以采取的行为。</li>
<li data-line="4" dir="auto"><strong>奖励（Reward）</strong>：环境对智能体行为的即时反馈，用于指导智能体的学习过程。</li>
<li data-line="5" dir="auto"><strong>策略（Policy）</strong>：智能体在不同状态下选择动作的规则或函数。</li>
<li data-line="6" dir="auto"><strong>价值函数（Value Function）</strong>：评估在某个状态下采取特定策略的长期期望回报。</li>
<li data-line="7" dir="auto"><strong>模型（Model）</strong>：对环境的内部表示，有些强化学习方法会尝试模拟环境的行为。</li>
</ol></div><div><p dir="auto">强化学习的算法可以分为基于模型的方法和无模型方法两大类。基于模型的方法试图构建一个环境模型来进行规划，而无模型方法则直接从经验中学习策略和价值函数。常见的强化学习算法包括Q-learning、SARSA、Deep Q-Network (DQN)、Policy Gradient Methods等。这些算法在游戏、机器人控制、自动驾驶等领域有着广泛的应用。</p></div><div><img style="width:500" src="https://cdn.sa.net/2024/07/28/f3MKnSUDRN6oBhZ.png" referrerpolicy="no-referrer">
<img style="width:500" src="https://cdn.sa.net/2024/07/30/Wn17A4bdCp5IyFE.png" referrerpolicy="no-referrer">
<img style="width:500" src="https://cdn.sa.net/2024/07/30/8wxGmpNAB3PrcZE.png" referrerpolicy="no-referrer">
<img style="width:500" src="https://cdn.sa.net/2024/07/30/WRLHhGi7aNp4xr9.png" referrerpolicy="no-referrer"></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> 强化学习是什么？什么是on-policy? 什么是 off-policy?</span></li>
<li data-line="1" dir="auto">On-policy = “I  can only learn from my own action”; Off-policy "I can learn from anyone trying to achieve any goal"</li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> 什么是 Online? 什么是 Offline?</span></li>
<li data-line="3" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> 什么是 RLHF？ 为什么有 SFT 还要进行 RLHF?</span></li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="RLHF 必要性" dir="auto" class="heading" id="RLHF_必要性"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>RLHF 必要性</h3><div class="heading-children"><div><img style="width:700" src="https://cdn.openai.com/instruction-following/draft-20220126f/methods.svg" referrerpolicy="no-referrer"> 
Instruction GPT
[# InstructGPT 论文精读【论文精读】](https://www.youtube.com/watch?v=zfIGAwD1jOQ)
[openai.com/index/instruction-following/](https://openai.com/index/instruction-following/)</div><div><p dir="auto">图一展示, 人工标记的问答数据集被用于SFT微调GPT-3. 13K Samples -&gt; SFT</p></div><div><ol>
<li data-line="0" dir="auto">epoch 16</li>
<li data-line="1" dir="auto">OpneIA 一个epoch之后模型就过拟合了, 但是他们认为这对训练RM模型有好处</li>
</ol></div><div><p dir="auto">图二展示, 使用人工SFT标记后的GPT-3的输出, 标记一个偏好数据集. 训练一个RM模型. 具体的方法是训练最后一层然后添加一个全联接层.  33K Samples -&gt; RM</p></div><div><ol>
<li data-line="0" dir="auto">然后没有训练175B版本的GPT-3 是因为 Loss 飞了, 而是训了一个机遇 GPT-3 6B 的RM</li>
</ol></div><div><img style="width:500" src="https://cdn.sa.net/2024/08/04/UhvoQSrxqi674mL.png" referrerpolicy="no-referrer">
使用一个 Pairwise ranking loss 来训练 Reward model 
一个Prompt生成 9 个答案. 然后 9 个答案之间寻找差异, 得到 36 对差异
如果一个奖励函数比另外一个高, 就尽可能的放大它们的差距</div><div><p dir="auto">图三展示, 结合 RM 对模型的输出进行打分, 使用并更新 PPO 算法的 Policy. 31K Samples -&gt; PPO </p></div><div><img style="width:600" src="https://cdn.sa.net/2024/08/04/ZLJ657kMK1WOHmd.png" referrerpolicy="no-referrer">
论文涵盖了大量的任务类型的示例. 这里用的是 OpenAI 熟悉的 PPO 算法. 
<img style="width:500" src="https://cdn.sa.net/2024/08/04/5oftyJdBqpTKmEv.png" referrerpolicy="no-referrer"></div><div><p dir="auto">这个公式是InstructGPT中使用的PPO算法目标函数。让我解释一下它的各个部分：</p></div><div><ol>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>目标函数</strong> (objective): </p>
<ul>
<li data-line="1" dir="auto">目标是最大化策略π(参数为φ)的期望回报。目标函数由两个部分组成：第一个部分是基于强化学习的损失，第二个部分是预训练损失。</li>
</ul>
</li>
<li data-line="3" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>强化学习部分</strong>:</p>
<ul>
<li data-line="4" dir="auto"><span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c1D53C TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.177em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c44 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.394em;"><mjx-texatom size="s" texclass="ORD"><mjx-msubsup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.52em; margin-left: 0px;"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-spacer style="margin-top: 0.18em;"></mjx-spacer><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msubsup></mjx-texatom></mjx-script></mjx-msub></mjx-texatom></mjx-script></mjx-msub></mjx-math></mjx-container></span>  表示从策略 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msubsup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.317em; margin-left: 0px;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-spacer style="margin-top: 0.18em;"></mjx-spacer><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msubsup></mjx-math></mjx-container></span> 生成的数据分布 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c44 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.248em;"><mjx-texatom size="s" texclass="ORD"><mjx-msubsup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.317em; margin-left: 0px;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-spacer style="margin-top: 0.18em;"></mjx-spacer><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msubsup></mjx-texatom></mjx-script></mjx-msub></mjx-math></mjx-container></span> 中采样。</li>
<li data-line="5" dir="auto"><span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span>: 是reward model，通常是一个衡量输出y(output) 和输入x(prompt) 的质量的指标。</li>
<li data-line="6" dir="auto"><span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-n" space="2"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mrow space="2"><mjx-mo class="mjx-s3"><mjx-c class="mjx-c28 TEX-S3"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-msubsup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.317em; margin-left: 0px;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-spacer style="margin-top: 0.18em;"></mjx-spacer><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msubsup><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.289em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-s3"><mjx-c class="mjx-c29 TEX-S3"></mjx-c></mjx-mo></mjx-mrow></mjx-math></mjx-container></span> : 是KL散度的缩放项。首先KL散度是一种用来衡量两个概率分布之间差异的指标, 两个差异越大, KL散度越大. 两个相除, 防止策略过度偏离原始策略, 平衡探索与稳定性. 然后使用系数 beta 控制了KL散度项的权重, beta越大则则策略的变化会受到更严格的限.</li>
</ul>
</li>
<li data-line="9" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>预训练部分</strong>:</p>
<ul>
<li data-line="10" dir="auto"><span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FE TEX-I"></mjx-c></mjx-mi><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c1D53C TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c44 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c70"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c5B TEX-S2"></mjx-c></mjx-mo><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msubsup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.317em; margin-left: 0px;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-spacer style="margin-top: 0.18em;"></mjx-spacer><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msubsup><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-lop"><mjx-c class="mjx-c5D TEX-S2"></mjx-c></mjx-mo></mjx-mrow></mjx-math></mjx-container></span>: 这个部分是一个交叉熵损失项，用于惩罚策略与预训练数据分布的差异。γ是一个权重系数。</li>
<li data-line="11" dir="auto">然后如果这里的 gemma 不设置为 0 则被称为 PPO-ptx</li>
</ul>
</li>
</ol></div><div><p dir="auto">总结起来，这个目标函数结合了强化学习和预训练的目标，通过优化这个函数，可以在保持模型预训练表现的基础上，进一步通过强化学习提升模型表现。</p></div><div><p dir="auto">RL 的基本概念差不多是, 模型生成一些数据, 然后需要外界给输出一个评价. 模型根据评价产生偏好, 进行梯度下降. 学习到评价所给出的偏好. </p></div><div><p dir="auto">这个函数中的 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math></mjx-container></span> 起到的作用就是给输出评价. -&gt; Reward Model </p></div><div><p dir="auto">强化学习里这个模型被称为 Policy.<br>
因为一个 Policy 输入一些状态X(prompt), 输出一些action Y(RL的输出) 出来. 然后 Action 影响环境(梯度)</p></div><div><p dir="auto">基本来讲是 RM 标记的是排序. PPO 标记的实际偏好的输出 Y</p></div><div><img style="width:500" src="https://cdn.sa.net/2024/08/04/c6zLs931BPHRtDC.png" referrerpolicy="no-referrer">
这里是大致的实验结果, Y轴是相比 SFT的 GPT-3 175B的赢率. </div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/596230048/answer/3102096962" rel="noopener" class="external-link" href="https://www.zhihu.com/question/596230048/answer/3102096962" target="_blank"># ChatGPT 为什么不用 Reward-Model 的数据直接 fine-tune，而用 RL？</a></p></div><div><p dir="auto">在InstructGPT等类似模型中，首先训练一个Reward model（奖励模型）而不是直接优化偏好输出有以下几个主要原因：</p></div><div><ol>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>偏好反馈数据的稀疏性</strong>:</p>
<ul>
<li data-line="1" dir="auto">用户偏好的数据往往是稀疏且噪声较大的。直接优化这些偏好数据可能导致模型学习到不稳定或不合理的输出。通过训练一个Reward model，可以将稀疏的用户反馈转化为连续的奖励信号，提供更稳定的优化目标。</li>
</ul>
</li>
<li data-line="3" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>Reward model 可以进行更细粒度的评价</strong>:</p>
<ul>
<li data-line="4" dir="auto">Reward model 能够对输出进行细粒度的评价，而不仅仅是简单的“好”或“坏”标签。这样，模型可以从更丰富的反馈信号中学习，提升其表现。</li>
</ul>
</li>
<li data-line="6" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>增强学习的有效性</strong>:</p>
<ul>
<li data-line="7" dir="auto">通过训练Reward model，将偏好输出转化为奖励信号，结合强化学习（如PPO），可以更有效地进行策略优化。强化学习需要明确的奖励信号来进行策略调整，而Reward model能够提供这种信号。</li>
</ul>
</li>
<li data-line="9" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>减少人力成本</strong>:</p>
<ul>
<li data-line="10" dir="auto">训练Reward model可以减少对大量人工标注的依赖。一次标注的数据可以反复利用，通过Reward model在更多的样本上进行扩展和应用，降低人工参与的频率和成本。</li>
</ul>
</li>
</ol></div><div><p dir="auto">总结来说，训练Reward model是为了更有效地利用稀疏的偏好数据，提供稳定的优化信号，并提高模型的整体性能。</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.jiqizhixin.com/articles/2024-08-09-2" rel="noopener" class="external-link" href="https://www.jiqizhixin.com/articles/2024-08-09-2" target="_blank">Karpathy观点惹争议：RLHF不是真正的强化学习，谷歌、Meta下场反对 | 机器之心</a><br>
<a data-tooltip-position="top" aria-label="https://x.com/karpathy/status/1821277264996352246" rel="noopener" class="external-link" href="https://x.com/karpathy/status/1821277264996352246" target="_blank">x.com</a><br>
基本观点是 RLHF 只是单纯的从人类那边学习偏好, 而不是真正自主的向一个目标优化. <strong>没有真正在进行空间中搜索. RLHF原则上是为 RLHF 目标添加熵奖励</strong><br>
RLHF 中的 使用 RM model 无法真正取代人类的价值偏好.<br>
由于人类标注员从几个候选答案中选出最佳答案要比从头写出理想答案容易得多, 使用偏好数据能够大幅帮助很难有黄金标准的答案. 并且可以帮助减少幻觉. 虽然这是不是RL但是RLHF确实有用.<br>
但是也有反对声音认为, 智能体在与人互动时，给出人类喜欢的答案才是真正的目标. </p></div></div></div><div class="heading-wrapper"><h3 data-heading="PPO" dir="auto" class="heading" id="PPO"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>PPO</h3><div class="heading-children"></div></div><div class="heading-wrapper"><h3 data-heading="DPO" dir="auto" class="heading" id="DPO"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>DPO</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/700580300?" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/700580300?" target="_blank"># 大模型偏好对齐-DPO</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/714364995?utm_campaign=shareopn&amp;utm_medium=social&amp;utm_psn=1807220053936054272&amp;utm_source=wechat_session" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/714364995?utm_campaign=shareopn&amp;utm_medium=social&amp;utm_psn=1807220053936054272&amp;utm_source=wechat_session" target="_blank"># RLHF 及其变体 Iterative DPO/RLOO/REINFORCE 分析</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/676371444" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/676371444" target="_blank"># Direct Preference Optimization（DPO）学习笔记</a></p></div></div></div><div class="heading-wrapper"><h3 data-heading="其他变体" dir="auto" class="heading" id="其他变体"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>其他变体</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://mp.weixin.qq.com/s/1Xc8Dj2Rb_4qa7yth3-QUQ" rel="noopener" class="external-link" href="https://mp.weixin.qq.com/s/1Xc8Dj2Rb_4qa7yth3-QUQ" target="_blank"># 一文搞懂SFT、RLHF、DPO、IFT - 深度学习自然语言处理</a></p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="双流网络(Two-stream Network)" dir="auto" class="heading" id="双流网络(Two-stream_Network)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>双流网络(Two-stream Network)</h2><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/search?q=%E5%8F%8C%E6%B5%81%E7%BD%91%E7%BB%9C&amp;search_source=Suggestion&amp;utm_content=search_suggestion&amp;type=content" rel="noopener" class="external-link" href="https://www.zhihu.com/search?q=%E5%8F%8C%E6%B5%81%E7%BD%91%E7%BB%9C&amp;search_source=Suggestion&amp;utm_content=search_suggestion&amp;type=content" target="_blank">双流网络 - 搜索结果 - 知乎</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/516956247/answer/3310533088" rel="noopener" class="external-link" href="https://www.zhihu.com/question/516956247/answer/3310533088" target="_blank">医学图像的深度学习该怎么做? - 知乎</a></p></div><div><blockquote dir="auto">
<p>Two-stream &gt; TSN &gt; I3D &gt; SlowFast &gt; Timesformer</p>
</blockquote></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content heading-wrapper"><h2 data-heading="双流网络 (Two-Stream Network) - 视频理解领域开山之作" dir="auto" class="heading" id="双流网络_(Two-Stream_Network)_-_视频理解领域开山之作"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>双流网络 (Two-Stream Network) - 视频理解领域开山之作</h2>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=vuqwKP2iDe0" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=vuqwKP2iDe0" target="_blank">双流网络论文逐段精读【论文精读】 - YouTube</a></p>
<p dir="auto">双流网络是一种在视频动作识别领域中非常流行的深度学习架构。它通过同时分析视频的空间和时间信息来进行动作识别，从而提高了识别精度。</p>
<p dir="auto">动机: 2014年的Deep Video 使用 CNN 意外的效果比传统手工特征提取要差, 想要知道为什么</p>
<p dir="auto"><strong>双流网络的核心思想是将视频信息分成两个独立的流进行处理：</strong></p>
<ul>
<li dir="auto"><strong>空间流 (Spatial Stream):</strong>&nbsp;该流处理视频帧，提取其中的静态外观信息，例如场景、物体和人物等。通常使用卷积神经网络 (CNN) 来进行特征提取。</li>
<li dir="auto"><strong>时间流 (Temporal Stream):</strong>&nbsp;该流处理视频帧之间的运动信息，例如光流 (Optical Flow)。光流描述了像素在相邻帧之间的运动方向和速度。通常使用 3D 卷积神经网络 (3D CNN) 或循环神经网络 (RNN) 来进行特征提取。</li>
</ul>
<p dir="auto"><strong>双流网络的优势：</strong></p>
<ul>
<li dir="auto"><strong>充分利用视频信息:</strong>&nbsp;通过同时分析空间和时间信息，双流网络可以更全面地理解视频内容，从而提高动作识别的准确性。比如, 2D视频有更多的连续, 动态信息. 还有音频信号. 视频更加符合人眼的习惯.</li>
<li dir="auto"><strong>鲁棒性强:</strong>&nbsp;即使视频中存在遮挡或光照变化等干扰因素，双流网络仍然能够有效地识别动作。</li>
</ul>
<p dir="auto"><strong>双流网络的应用:</strong></p>
<ul>
<li dir="auto"><strong>动作识别:</strong>&nbsp;这是双流网络最主要的应用领域，例如识别视频中的人物动作 (跑步、跳跃、游泳等)。</li>
<li dir="auto"><strong>行为分析:</strong>&nbsp;可以分析视频中人物的行为，例如打架、拥抱、握手等。</li>
<li dir="auto"><strong>视频描述:</strong>&nbsp;可以根据视频内容生成文本描述。</li>
</ul>
<p dir="auto"><strong>双流网络的局限性:</strong></p>
<ul>
<li dir="auto"><strong>计算量大:</strong>&nbsp;由于需要同时处理两个流的信息，双流网络的计算量较大，需要较高的计算资源。</li>
<li dir="auto"><strong>训练难度大:</strong>&nbsp;双流网络的训练过程比较复杂，需要大量的训练数据和调参技巧。</li>
</ul>
<p dir="auto"><strong>双流网络的发展趋势:</strong></p>
<ul>
<li dir="auto"><strong>更强大的特征提取网络:</strong>&nbsp;研究者们正在探索更强大的特征提取网络，例如基于 Transformer 的网络，以提高双流网络的性能。</li>
<li dir="auto"><strong>多模态融合:</strong>&nbsp;将双流网络与其他模态的信息 (例如音频) 进行融合，可以进一步提高动作识别的准确性。</li>
<li dir="auto"><strong>轻量化模型:</strong>&nbsp;研究者们正在研究如何降低双流网络的计算量，使其能够在移动设备上运行。</li>
</ul>
<p dir="auto"><strong>总而言之，双流网络是一种强大的视频动作识别模型，在视频理解领域具有广泛的应用前景。</strong></p>
<p dir="auto">希望以上信息能帮助你了解双流网络。</p>
<p dir="auto"><strong>如果你想了解更多信息，可以参考以下资源:</strong></p>
<ul>
<li dir="auto"><strong>论文:</strong>&nbsp;Two-Stream Convolutional Networks for Action Recognition in Videos</li>
<li dir="auto"><strong>代码:</strong>&nbsp;<a rel="noopener" class="external-link" href="https://github.com/feichtenhofer/twostreamfusion" target="_blank">https://github.com/feichtenhofer/twostreamfusion</a></li>
</ul><div class="heading-children"></div></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=OOWZl4mx2fM" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=OOWZl4mx2fM" target="_blank">【博士Vlog】全网最全！十分钟看完所有双流神经网络，图片视频处理都在这里了！ - YouTube</a></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/15/YWOvjX8w9GKulFb.png" referrerpolicy="no-referrer"><br>
<img src="https://cdn.sa.net/2024/04/15/BLhnCS2eKHw7lFv.png" referrerpolicy="no-referrer"></p>
<ul>
<li dir="auto">提出了一个新的数据集</li>
<li dir="auto">介绍了使用双流神经网路进行视频预测, 将2D ConvNet 膨胀到 双流 3D ConvNet(网络E)</li>
<li dir="auto">“Two Stream Inflated 3D ConvNet(I3D)” (Inception, VGG16, ResNet)</li>
<li dir="auto">现在已经成为Fondation model</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=vuqwKP2iDe0" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=vuqwKP2iDe0" target="_blank">双流网络论文逐段精读【论文精读】 - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1406.2199" rel="noopener" class="external-link" href="https://arxiv.org/abs/1406.2199" target="_blank">[1406.2199] Two-Stream Convolutional Networks for Action Recognition in Videos</a></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/23/A9rn2DugdVT3vLb.png" referrerpolicy="no-referrer"></p>
<ul>
<li dir="auto">作者是VGG提出者</li>
<li dir="auto">介绍了双流网络的开山之作(上面), 用于视频动作识别任务. <span style="background:#fff88f">很有可能是多模态的先驱</span>. </li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>双流网络的提出动机
<ul>
<li dir="auto">充分利用互联网上大量的视频内容</li>
<li dir="auto">视频有很大信息(连续的, 音频, 更加符合人眼)</li>
<li dir="auto">发现2014年Deep Video 在视频分类任务上效果很差</li>
<li dir="auto">视频天生就提供了很好的数据增强</li>
</ul>
</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>之前
<ul>
<li dir="auto">抽取关键帧</li>
<li dir="auto">或者将整个帧叠起来, 使用erarly fusion/latent fusion进行时空分析</li>
</ul>
</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>作者发现
<ul>
<li dir="auto">之前模型效果差, 是因为CNN只对局部特征学习好</li>
<li dir="auto">可以先提取光流信息, 学习动作分类和光流的映射关系</li>
</ul>
</li>
<li dir="auto">模型两个CNN, 一个关注帧的空间信息(appearance information), 一个关注motion-information(光流). 两个网络都输出分类的概率, 然后简单加权平均就可以.</li>
<li dir="auto">光流的使用可以让模型专注于动作本身, 而忽略图像的背景噪声等</li>
<li dir="auto">提出multi-task learning, 在两个数据集上学习骨干网络</li>
<li dir="auto">谈到 a stack of fream 和 individual frame 的效果差不多. aka. 没有学习到连续的context. 文献14. -&gt; temporal information 很重要, 学习到运动特征很重要</li>
<li dir="auto">探讨了不同光流图叠加的方式</li>
<li dir="auto">光流计算速度慢, 而且密集表示存储空间大</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/692173683?utm_campaign=&amp;utm_medium=social&amp;utm_psn=1763139418939244544&amp;utm_source=io.raindrop.raindropio" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/692173683?utm_campaign=&amp;utm_medium=social&amp;utm_psn=1763139418939244544&amp;utm_source=io.raindrop.raindropio" target="_blank">结合创新！ResNet+Transformer，高性能低参数，准确率达99.12％ - 知乎</a></p></div></div></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=u7M58WLn6Sc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=u7M58WLn6Sc" target="_blank">【论文速读#196】数据集中没有的类别怎么办？OOD问题如何解决？学习这篇论文可以快速发表！ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://perso.crans.org/genest/FEG24.pdf" rel="noopener" class="external-link" href="https://perso.crans.org/genest/FEG24.pdf" target="_blank">Uncertainty-Guided Appearance-Motion Association Network for Out-of-Distribution Action Detection</a><br>
本文提出了一种用于分布外动作检测（ODAD）的不确定性引导的外观 - 运动关联网络（UAAN），以处理动态多媒体场景中具有运动信息的分布外检测问题。</p></div><div><ul>
<li data-line="0" dir="auto"><strong>引言</strong>：现有基于DNN的方法在处理测试样本中的异常值时存在局限性，且大多数OOD检测方法仅关注静态图像，无法处理动态视频数据。因此，本文提出ODAD任务，旨在对未修剪视频进行分类、定位和检测分布外动作。</li>
<li data-line="1" dir="auto"><strong>相关工作</strong>：介绍了时间动作检测和分布外检测的相关工作，现有动作检测方法依赖运动特征，无法区分相似动作，且忽略了对象间的交互；而传统分布外检测方法无法处理视频数据集。</li>
<li data-line="2" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>提出的UAAN</strong>：
<ul>
<li data-line="3" dir="auto"><strong>视频表示</strong>：考虑提取外观感知对象特征和运动感知对象特征，通过Faster R - CNN和I3D网络分别获取，并加入位置编码。</li>
<li data-line="4" dir="auto"><strong>时空对象图用于对象间交互</strong>：构建基于外观和运动感知对象特征的两个分支，通过图卷积网络推理对象关系，通过外观 - 运动关联注意力模块融合特征。</li>
<li data-line="5" dir="auto"><strong>不确定性引导的OOD动作检测</strong>：包括动作 - 背景分割、动作分类和动作定位，通过引入帧感知的动作性得分、基于Beta分布的证据神经网络和边界回归损失来实现。</li>
</ul>
</li>
<li data-line="6" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>实验</strong>：
<ul>
<li data-line="7" dir="auto"><strong>数据集和评估指标</strong>：使用THUMOS14和ActivityNet1.3数据集，采用mAP、AUROC、AUPR、OSDR、FAR@95等评估指标。</li>
<li data-line="8" dir="auto"><strong>与现有方法比较</strong>：UAAN在所有指标上显著优于其他对比方法，能够更好地理解视频动作和挖掘对象间交互。</li>
<li data-line="9" dir="auto"><strong>消融研究</strong>：对每个组件进行消融研究，结果表明每个组件都有积极贡献，通过调整超参数可获得最佳性能。</li>
</ul>
</li>
<li data-line="10" dir="auto"><strong>结论</strong>：UAAN方法有效，能够探索外观特征和运动上下文来推理时空对象间交互。</li>
</ul></div></div></div><div class="heading-wrapper"><h2 data-heading="图神经网络(GNN)" dir="auto" class="heading" id="图神经网络(GNN)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>图神经网络(GNN)</h2><div class="heading-children"><div><p dir="auto">图神经网络是什么? 详细解释它们的网路构造?</p></div><div><p dir="auto"> 图神经网络（Graph Neural Networks，GNN）是一种专门用于处理图结构数据的神经网络。与常见的卷积神经网络（Convolutional Neural Networks，CNN）或者循环神经网络（Recurrent Neural Networks，RNN）不同，GNN能够有效地处理图结构数据中的节点和边的关系。</p></div><div><p dir="auto">GNN的网络构造主要包括以下几个步骤：</p></div><div><ol>
<li data-line="0" dir="auto">
<p>节点表示：每个节点在GNN中都会有一个向量表示。这个向量可以是节点的特征，也可以是节点的标签，或者其他的节点信息。</p>
</li>
<li data-line="2" dir="auto">
<p>邻居信息聚合：GNN会收集每个节点的邻居节点信息，并使用这些信息更新节点的表示。这个过程通常是通过一个聚合函数来实现的，比如求和、平均、最大���等。这个过程可以看作是在图上进行信息传递。</p>
</li>
<li data-line="4" dir="auto">
<p>节点更新：根据聚合得到的邻居信息，通过一个更新函数来更新每个节点的表示。这个更新函数通常是一个非线性函数，比如ReLU、tanh等。</p>
</li>
<li data-line="6" dir="auto">
<p>重复上述过程：在GNN中，上述的邻居信息聚合和节点更新过程会进行多次，每次都会使得节点的表示更加丰富和准确。</p>
</li>
<li data-line="8" dir="auto">
<p>最终输出：在进行了足够多次的邻居信息聚合和节点更新之后，每个节点的表示就可以作为最终的输出。这个输出可以用于节点分类、节点聚类、链接预测等任务。</p>
</li>
</ol></div><div><p dir="auto">需要注意的是，上述的网络构造只是GNN最基础的形式，实际上还有很多种GNN的变种，比如图卷积网络（Graph Convolutional Networks，GCN）、图注意力网络（Graph Attention Networks，GAT）等，它们在节点表示、邻居信息聚合和节点更新等步骤上有各自的特点和优势。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="模型可解释" dir="auto" class="heading" id="模型可解释"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>模型可解释</h2><div class="heading-children"><div class="admonition-parent admonition-failure-parent"><div class="callout admonition admonition-failure admonition-plugin " style="--callout-color: 255, 82, 82;" data-callout="failure" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times-circle" class="svg-inline--fa fa-times-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3 0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3 0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3 0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3 0 17L312 256l65.6 65.1z"></path></svg></div><div class="callout-title-inner admonition-title-content">Failure</div></div><div class="callout-content admonition-content"><p dir="auto">模型解释性很重要，特别是医疗和金融领域。</p>
<p dir="auto">主要的理由有：</p>
<ol>
<li dir="auto"><strong>可信任性</strong>：如果模型的预测能够被解释和理解，那么人们更可能信任模型，特别是在影响重大的决策场景，如医疗诊断和金融信贷中。</li>
<li dir="auto"><strong>模型调试</strong>：可解释性可以帮助我们理解模型在某些情况下为何预测错误，并可以指导我们如何改进模型。</li>
<li dir="auto"><strong>公平性和偏见检查</strong>：如果我们能理解模型的决策过程，那么我们可以更容易地检测到模型是否有不公平的偏见，例如预测是否受到某些不相关因素（如性别或种族）的影响。</li>
<li dir="auto"><strong>符合监管要求</strong>：在某些行业，如金融和医疗，预测模型可能需要满足监管机构的可解释性要求。</li>
<li dir="auto"><strong>特征工程和选择</strong>：解释性模型能够显示哪些特征对预测结果有重大影响，这有助于我们进行更有效的特征选择和工程</li>
</ol></div></div></div><div dir="ltr" style="overflow-x: auto;"><table>
<thead>
<tr>
<th dir="ltr">方法</th>
<th dir="ltr">可解释性水平(0-100)</th>
<th dir="ltr">原因</th>
</tr>
</thead>
<tbody>
<tr>
<td dir="ltr">线性回归/逻辑回归</td>
<td dir="auto">90</td>
<td dir="ltr">输出可以清晰地用输入特征的线性组合表示，且模型权重可解释为各特征的影响大小和方向</td>
</tr>
<tr>
<td dir="ltr">决策树</td>
<td dir="auto">95</td>
<td dir="ltr">每一个决策都基于清晰的规则，规则基于特征值，轻松理解每步决策</td>
</tr>
<tr>
<td dir="ltr">随机森林</td>
<td dir="auto">80</td>
<td dir="ltr">尽管是多个决策树的集合，使得整体解释性相比单个树略弱，但每棵树都基于明确的规则，且可以通过特征重要性指标理解特征对模型的影响</td>
</tr>
<tr>
<td dir="ltr">支持向量机（线性核）</td>
<td dir="auto">85</td>
<td dir="ltr">找到线性界限将特征空间分割为不同类别，权重向量可理解为特征影响，但分类决策通常只受支持向量影响</td>
</tr>
<tr>
<td dir="ltr">支持向量机（非线性核，如RBF）</td>
<td dir="auto">40</td>
<td dir="ltr">通过核函数映射到高维空间，解释性变得复杂，难以明确理解特征如何影响分类界限</td>
</tr>
<tr>
<td dir="ltr">神经网络</td>
<td dir="auto">20</td>
<td dir="ltr">隐藏层和参数众多，难以理解特征如何直接影响输出，通常被视为"黑箱"，尽管有一些可视化、解释工具尝试提高解释性</td>
</tr>
<tr>
<td dir="ltr">K-近邻</td>
<td dir="auto">75</td>
<td dir="ltr">预测结果由最靠近的K个样本投票决定，直观易懂，但不提供特征权重或影响程度信息</td>
</tr>
<tr>
<td dir="ltr">LIME</td>
<td dir="auto">80</td>
<td dir="ltr">LIME通过拟合一个局部线性模型来解释黑盒模型的预测。它可以对任何模型进行解释，并为每个预测指出了重要的局部特征，但结果可能依赖于随机样本和选择的超参数</td>
</tr>
<tr>
<td dir="ltr">SHAP</td>
<td dir="auto">85</td>
<td dir="ltr">SHAP基于博弈论的概念，计算出每个特征对预测的平均贡献，甚至能够对复杂模型如XGBoost、神经网络等提供公平、准确的特征贡献解释。但它的计算复杂度较高，扩展到大型或深度模型需要特定的优化版本</td>
</tr>
<tr>
<td dir="ltr">GradCAM</td>
<td dir="auto"></td>
<td dir="auto"></td>
</tr>
</tbody>
</table></div><div class="admonition-parent admonition-attention-parent"><div class="callout admonition admonition-attention admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="attention" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Attention</div></div><div class="callout-content admonition-content"><p dir="auto">有多种方法可以可视化神经网络的隐藏层。以下是一些常见的方法：</p>
<ol>
<li dir="auto">
<p><strong>激活映射（Activation Maps）</strong>：对于卷积神经网络（CNN），我们可以直接可视化特定层的激活。这可以帮助我们理解网络在哪些区域和特征上被激活。</p>
</li>
<li dir="auto">
<p><strong>过滤器可视化（Filter Visualization）</strong>：对于CNN的卷积层，我们可以可视化学习到的过滤器。对于第一层，这些过滤器可能会显示出一些简单的特征，如边缘和颜色斑块。对于更深的层，过滤器可能会表示更复杂的特征。</p>
</li>
<li dir="auto">
<p><strong>类激活映射（Class Activation Mapping, CAM）</strong>：CAM是一种可视化技术，可以显示出网络在进行特定类别预测时关注的图像区域。</p>
</li>
<li dir="auto">
<p><strong>特征反演（Feature Inversion）</strong>：这种方法试图找到一个输入，其通过网络传播后在特定层产生的激活最接近给定的激活。这可以用于理解特定层的激活表示什么。</p>
</li>
<li dir="auto">
<p><strong>深度梦境（DeepDream）</strong>：DeepDream是一种使用神经网络自身来生成图像的技术。它通过选择一个层，然后尽可能地激活那一层来修改输入图像。</p>
</li>
<li dir="auto">
<p><strong>神经网络去卷积（Deconvolutional Networks）</strong>：这种方法试图将特定层的激活映射回输入空间，以理解网络学习的特征。</p>
</li>
<li dir="auto">
<p><strong>梯度上升（Gradient Ascent）</strong>：这种方法通过优化输入图像以最大化特定层的激活来理解该层的功能。</p>
</li>
<li dir="auto">
<p><strong>t-SNE 和 PCA</strong>：这些降维技术可以用于可视化高维特征空间。</p>
</li>
</ol>
<p dir="auto">每种方法都有其优点和缺点，适用于解决特定的问题。所以，选择哪种方法取决于你的具体需求和你想要解答的问题。</p>
<hr>
<p dir="auto"><strong>Grad-CAM</strong>，全称Gradient-weighted Class Activation Mapping，是一种可视化卷积神经网络决策的技术。它是类激活映射（Class Activation Mapping, CAM）的一种扩展，可以用于任何CNN架构，而不仅仅是那些有全局平均池化层的网络。</p>
<p dir="auto">Grad-CAM的主要思想是使用目标类别相对于最后一个卷积层的梯度来生成一个粗糙的定位图（也称为热图）。这个热图突出显示了对分类决策最重要的图像区域。</p>
<p dir="auto">下面是Grad-CAM的基本步骤：</p>
<ol>
<li dir="auto">选择网络的某个卷积层。</li>
<li dir="auto">对给定的输入和类别，运行前向传播。</li>
<li dir="auto">使用类别的得分计算相对于选定卷积层的梯度。</li>
<li dir="auto">对该层的每个特征映射的梯度进行全局平均池化，得到权重系数。</li>
<li dir="auto">将这些权重应用于相应的特征映射，然后对结果进行求和，得到定位图。</li>
<li dir="auto">对定位图应用ReLU激活函数以保留仅对目标类别有用的特征。</li>
<li dir="auto">将定位图上采样（比如，使用双线性插值）到输入图像的大小，以产生热图。</li>
</ol>
<p dir="auto">通过这种方式，Grad-CAM提供了一种可视化方法，可以清楚地看到模型在做出决策时关注的图像区域。这对于模型的解释性和透明度非常有帮助。</p>
<hr>
<p dir="auto"><strong>激活映射（Activation Maps）</strong>是一种可视化技术，用于显示卷积神经网络（CNN）中特定层的激活。这种方法通常用于理解网络在处理特定输入时，各层是如何响应的。以下是激活映射的主要原理、优势和劣势。</p>
<p dir="auto"><strong>原理</strong>：</p>
<p dir="auto">在CNN中，每个卷积层都会生成一组特征映射（或激活映射），这些映射表示网络在特定层级捕获的特征。例如，第一层可能会捕获简单的边缘和纹理，而更深的层可能会捕获更复杂的特征，如对象的部分或整体。通过可视化这些激活映射，我们可以直观地理解网络在处理输入时的行为。</p>
<p dir="auto"><strong>优势</strong>：</p>
<ol>
<li dir="auto">
<p><strong>直观理解</strong>：激活映射可以直观地揭示网络在处理特定输入时的内部行为，有助于理解网络如何识别和抽取特征。</p>
</li>
<li dir="auto">
<p><strong>故障诊断</strong>：如果网络的性能不佳，检查激活映射可以帮助发现问题。例如，如果某些特征映射始终未激活，或者激活的模式看起来不正确，那可能就是网络配置或训练过程中存在问题。</p>
</li>
</ol>
<p dir="auto"><strong>劣势</strong>：</p>
<ol>
<li dir="auto">
<p><strong>高维复杂性</strong>：对于具有大量特征映射的深度网络，激活映射可能会非常复杂，难以一次性全部理解。</p>
</li>
<li dir="auto">
<p><strong>缺乏高级解释</strong>：虽然激活映射可以显示网络的内部行为，但它们并不能直接解释网络如何做出特定的预测。例如，它们不能告诉我们网络为什么将某个图像分类为猫而不是狗。</p>
</li>
<li dir="auto">
<p><strong>难以解释抽象特征</strong>：在深层网络中，激活映射可能会表示非常抽象的概念，这些概念可能难以理解或解释。</p>
</li>
</ol>
<hr>
<p dir="auto">t-SNE（t-Distributed Stochastic Neighbor Embedding）和PCA（Principal Component Analysis）是两种常用的降维技术，可以用于可视化神经网络的隐藏层。</p>
<ol>
<li dir="auto">
<p><strong>PCA（主成分分析）</strong>：PCA是一种线性降维技术，它通过找到数据中的主要变化方向（即主成分）来减少数据的维数。在神经网络的隐藏层可视化中，PCA可以用来将高维的激活映射投影到2D或3D空间，以便我们可以在图形中看到它们。PCA的主要优点是计算效率高，但由于它是线性的，所以可能无法捕捉到数据中的非线性结构。</p>
</li>
<li dir="auto">
<p><strong>t-SNE（t-分布随机邻域嵌入）</strong>：t-SNE是一种非线性降维技术，它通过尝试保持原始高维空间中的邻域关系来将高维数据映射到低维空间。在神经网络的隐藏层可视化中，t-SNE可以用来揭示高维激活映射中的结构和模式。t-SNE的优点是能够捕捉到数据中的非线性结构，但计算效率较低，且超参数调整可能会影响结果。</p>
</li>
</ol>
<p dir="auto">这两种方法都可以用来理解和解释神经网络的行为。例如，你可以使用它们来查看网络在处理不同类型的输入时，隐藏层的激活是否有明显的区别。这可以帮助你理解网络是否成功地学习到了有用的特征，或者是否存在可能的问题（例如，所有的输入都映射到同一区域，可能表明网络没有成功地区分不同的输入）。</p></div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=5XiFThIeo2U" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=5XiFThIeo2U" target="_blank">【博士Vlog】模型解释哪家强？一篇文章节省你三个月时间！</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2103.10689" rel="noopener" class="external-link" href="https://arxiv.org/abs/2103.10689" target="_blank">[2103.10689] Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond</a></p>
<p dir="auto">介绍了一个模型可解释性的Survey。<br>
对比了 Human label, LIME, GradCAM, SmoothGrad 对图片label的标记水平。</p>
<p dir="auto"><img alt="sgeX82wc4EobN5F.png" src="https://cdn.sa.net/2024/04/08/sgeX82wc4EobN5F.png" referrerpolicy="no-referrer"></p>
<ol>
<li dir="auto">
<p><strong>闭式(Closed-Form)</strong>：线性回归就是一个闭式的例子。线性回归模型的每个特征都有一个系<br>
数，这些系数直接表明了特征对于模型预测的贡献度。由于模型结构简单且系数直接对应于输入<br>
特征的重要性，所以可以直接将这些系数视为模型的解释。</p>
</li>
<li dir="auto">
<p><strong>组合(Composition)</strong>:决策树模型是组合关系的一个例子。决策树在做出预测时，可以直接展示<br>
其决策路径，这个路径可以视为模型的解释。例如，在银行贷款批准的决策树中，每个决策节点<br>
(如信用评分、收入等)和它们的阈值就构成了预测的解释。</p>
</li>
<li dir="auto">
<p><strong>依赖(Dependence)</strong>:深度学习模型中的特征重要性图（如Grad-CAM)就是依赖关系的一个例<br>
子。例如，在一个用于图像识别的卷积神经网络中，Grad-CAM使用模型的梯度信息来突出显示对<br>
于预测类别最重要的图像区域，这些区域的突出显示依赖于模型内部的特定层次。</p>
</li>
<li dir="auto">
<p><strong>代理(Proxy)</strong>：LME(局部可解释模型不透明度解释)算法是代理关系的一个例子。LIME通过<br>
在原始复杂模型（如随机森林或深度神经网络）的预测周围采样，并用这些样本来训练一个简单<br>
的线性模型作为代理。这个简单的线性模型（如线性回归）然后用来解释原始模型在局部区域的<br>
行为。</p>
</li>
</ol></div></div></div><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=hdDE676jJU4" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=hdDE676jJU4" target="_blank">【博士Vlog】如何解释机器学习深度学习？LIME和SHAP方法介绍 - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1602.04938" rel="noopener" class="external-link" href="https://arxiv.org/abs/1602.04938" target="_blank">[1602.04938] "Why Should I Trust You?": Explaining the Predictions of Any Classifier</a></p>
<p dir="auto">介绍了一篇论文关于<strong>LIME</strong>（Local Interpretable Model-Agnostic Explanations）模型解释技术的论文，用于任何分类器的预测。</p>
<p dir="auto">在随机森林和神经网络证明解释性是可行的。</p>
<p dir="auto"><img alt="tU1hkHMvgICK3oZ.png" src="https://cdn.sa.net/2024/04/08/tU1hkHMvgICK3oZ.png" referrerpolicy="no-referrer"></p>
<p dir="auto"><img alt="joRClJi5gWaKL7p.png" src="https://cdn.sa.net/2024/04/08/joRClJi5gWaKL7p.png" referrerpolicy="no-referrer"></p>
<p dir="auto">用一个可解释的模型(e.g.SVM,Random Forest) 来拟合模型局部在一个特定实例下的行为。</p>
<p dir="auto">这个图是LIME一个简化的示例，用来解释LME(局部可解释模型-不透明度解释)的直观概念。在图<br>
中，有一个复杂的决策边界（由蓝色和粉色区域表示），它代表一个黑盒模型的决策函数：，LME<br>
并不了解这个函数。这个决策边界不是线性的，也就是说，它不能被一个简单的线性模型很好地近<br>
似。<br>
在这个决策空间中，有两类实例，用加号和圆点标记。红色粗十字代表需要解释的实例，也就是<br>
说，我们想要了解模型为何将这个特定的实例分类为当前的类别。<br>
LME通过在决策边界附近采样新的实例，并使用原模型来对这些新实例做出预测。这些预测然后<br>
根据它们与需要解释的实例的近似程度被加权（在图中以大小表示）。最后，LME会学习一个简<br>
单的模型（在图中用虚线表示），这个模型仅在局部是准确的，也就是说它在被解释实例的周围是<br>
可靠的，尽管在整个决策空间中不一定可靠。这样，即使原始的黑盒模型很复杂和不可解释，</p>
<p dir="auto"><strong>LME也能提供对<font color="#ffff00">单个</font>预测决策的直观理解。</strong></p>
<p dir="auto"><span style="background:#fff88f">LIME 库已停止维护</span></p></div></div></div><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=JT_9zozHCDM" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=JT_9zozHCDM" target="_blank">Lime和SHAP哪个更好？这个文章做了个对比！ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://www.researchgate.net/publication/378171944_E-XAI_Evaluating_Black-Box_Explainable_AI_Frameworks_for_Network_Intrusion_Detection" rel="noopener" class="external-link" href="https://www.researchgate.net/publication/378171944_E-XAI_Evaluating_Black-Box_Explainable_AI_Frameworks_for_Network_Intrusion_Detection" target="_blank">(PDF) E-XAI: Evaluating Black-Box Explainable AI Frameworks for Network Intrusion Detection</a></p>
<p dir="auto">这篇论文介绍了网络安全领域AI模型的可解释性。</p>
<p dir="auto">有一个E-XAI的一个黑盒用于检测黑客是否入侵，需要提供这个AI的可解释性。</p>
<p dir="auto">作者提出了一个XAI的框架用于评估，使用了LIME和SHAP进行模型可视化和六个指标的评估(准确性稀疏性鲁棒性稳定性效率完整性)。</p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/08/EO4ughnR3jH7IMz.png" referrerpolicy="no-referrer"></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/08/mVYvHPWbdZCFhz2.png" referrerpolicy="no-referrer"></p>
<p dir="auto">对比了LIME和SHAP的可视化结果，发现LIME和SHAP可解释性差不多。文章说在他们的数据集，SHAP效果比较好。</p>
<p dir="auto">这里有医疗AI模型的对比<br>
<a data-tooltip-position="top" aria-label="https://www.mdpi.com/2075-4418/12/2/237" rel="noopener" class="external-link" href="https://www.mdpi.com/2075-4418/12/2/237" target="_blank">Diagnostics | Free Full-Text | Applications of Explainable Artificial Intelligence in Diagnosis and Surgery</a></p>
<p dir="auto"><img alt="eUdzFsShQgtyrCK.png" src="https://cdn.sa.net/2024/04/08/eUdzFsShQgtyrCK.png" referrerpolicy="no-referrer"></p></div></div></div><div class="admonition-parent admonition-info-parent"><div class="callout admonition admonition-info admonition-plugin " style="--callout-color: 0, 184, 212;" data-callout="info" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="info-circle" class="svg-inline--fa fa-info-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"></path></svg></div><div class="callout-title-inner admonition-title-content">Info</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://shap.readthedocs.io/en/latest/" rel="noopener" class="external-link" href="https://shap.readthedocs.io/en/latest/" target="_blank">Welcome to the SHAP documentation — SHAP latest documentation</a></p>
<p dir="auto"><img alt="ByNfczr57LWdCb2.png" src="https://cdn.sa.net/2024/04/08/ByNfczr57LWdCb2.png" referrerpolicy="no-referrer"></p>
<p dir="auto"><strong>SHAP（SHapley Additive exPlanations）</strong>是一种用于解释机器学习模型的算法。它基于博弈论中的Shapley值，这是一种用于分配合作游戏收益的公平方式。在机器学习模型解释的上下文中，SHAP用于分配每个特征对预测结果的贡献。</p>
<p dir="auto">SHAP值的计算过程涉及到所有可能的特征组合，这使得它在理论上是最公平的分配方式。这也意味着SHAP值满足了一些重要的属性，例如效率（所有特征的SHAP值之和等于预测结果）和对称性（如果两个特征对预测结果的贡献相同，那么他们的SHAP值应该相同）。</p>
<p dir="auto">SHAP的主要优点是它提供了一种全局和局部的模型解释方式。全局解释是通过平均所有实例的SHAP值来得到的，它可以给出每个特征对模型预测结果的平均贡献。局部解释是通过计算单个实例的SHAP值来得到的，它可以给出在该实例中每个特征对预测结果的贡献。</p>
<p dir="auto">然而，SHAP的计算过程可能会非常复杂和计算密集，尤其是在有大量特征的情况下。为了解决这个问题，有一些近似的方法被提出，例如KernelSHAP和TreeSHAP，它们在保持解释性的同时减少了计算复杂性。</p>
<p dir="auto"><strong>比如</strong>，一个简单的二元分类问题为例，假设我们有一个机器学习模型，它使用年龄，性别，和体重三个特征来预测一个人是否会患有某种疾病。给定一个具体的个体，例如一个40岁的男性，体重为75公斤，我们的模型预测他有70%的概率会患病。</p>
<p dir="auto">我们可以使用SHAP来解释这个预测结果。首先，我们需要计算每个特征的SHAP值。这涉及到计算所有可能的特征组合，例如只有年龄，只有性别，只有体重，年龄和性别，年龄和体重，性别和体重，以及所有三个特征。对于每一种组合，我们都需要计算出该特征组合对预测结果的贡献，然后按照Shapley值的公式分配给每个特征。</p>
<p dir="auto">假设我们得到的SHAP值是：年龄为0.3，性别为0.1，体重为0.3。这意味着年龄和体重对预测结果的贡献最大，每个都贡献了30%的可能性，性别贡献了10%的可能性。这些SHAP值的总和（0.3 + 0.1 + 0.3 = 0.7）等于模型的预测结果（70%的概率会患病），这满足了SHAP的效率属性。</p>
<p dir="auto">这样，我们就可以更好地理解模型的预测结果了。例如，我们可以看到年龄和体重是最重要的因素，而性别的影响相对较小。这可以帮助我们理解模型的工作原理，以及如何改进模型的性能。</p></div></div></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/696407462" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/696407462" target="_blank">再见Matplotlib！强大的 Scikit-learn 可视化让模型说话 - 知乎</a></p></div></div></div><div class="heading-wrapper"><h2 data-heading="AI4Science" dir="auto" class="heading" id="AI4Science"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>AI4Science</h2><div class="heading-children"><div class="heading-wrapper"><h3 data-heading="AlphaFold" dir="auto" class="heading" id="AlphaFold"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>AlphaFold</h3><div class="heading-children"><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=LEmNu0UJaXc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=LEmNu0UJaXc" target="_blank">Nature上最重要的论文AlphaFold讲了什么？作为一个打工人，咱们需要会些什么才能进公司？ - YouTube</a></p>
<p dir="auto"><a data-tooltip-position="top" aria-label="https://pubmed.ncbi.nlm.nih.gov/34265844/" rel="noopener" class="external-link" href="https://pubmed.ncbi.nlm.nih.gov/34265844/" target="_blank">Highly accurate protein structure prediction with AlphaFold(2021) - PubMed</a></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/11/rGUN9nmDtq1alKe.png" referrerpolicy="no-referrer"></p>
<p dir="auto"><img src="https://cdn.sa.net/2024/04/11/AiKFTORmXEzcL7n.png" referrerpolicy="no-referrer"></p>
<p dir="auto"><strong>AlphaFold v2</strong> 是一种由DeepMind开发的人工智能算法，它的主要目标是解决蛋白质折叠预测的问题。蛋白质折叠预测是一个非常重要的生物学问题，因为一个蛋白质的形状（即它如何折叠）决定了它在生物体内的功能。如果我们能够准确预测蛋白质的结构，那么我们就能更好地理解疾病的发生机制，甚至设计出新的药物。</p>
<p dir="auto">AlphaFold的输入是蛋白质的氨基酸序列，这就像是蛋白质的"配方"。蛋白质是由20种不同的氨基酸以特定的顺序链接起来形成的。AlphaFold从这个序列中预测出蛋白质在三维空间中的结构。</p>
<p dir="auto">AlphaFold的输出是每个氨基酸在三维空间中的位置，以及氨基酸之间的相对距离和角度。这就构成了蛋白质的三维结构模型。</p>
<p dir="auto">AlphaFold2的模型主要由两个部分组成：卷积神经网络（CNN）和Transformer。首先，卷积神经网络用于处理输入的一维氨基酸序列和二维的配对势能图（包含了序列中两个氨基酸之间的相互作用信息），并将这些信息转换为一种更高级的内部表示。然后，这个内部表示被输入到一个Transformer模型中。</p>
<p dir="auto">Transformer模型的关键特性是其自注意力机制，这使得模型能够考虑到输入序列中的所有位置之间的相互关系。在AlphaFold2中，这意味着模型可以考虑到蛋白质中的所有氨基酸对的相互作用，这对于预测蛋白质的三维结构是非常重要的。</p>
<p dir="auto">AlphaFold的工作过程可以分为两个步骤。首先，它生成一个称为"距离图"的东西，这是一个二维图，这些图描述了蛋白质中的氨基酸对之间的相对位置和取向。然后，AlphaFold2使用一个叫做分子动力学的过程，从这些预测的距离和角度图生成蛋白质的三维结构。</p>
<p dir="auto">AlphaFold的性能非常出色，它在2020年的蛋白质结构预测竞赛（CASP）中大放异彩，被誉为解决了蛋白质折叠问题。然而，虽然AlphaFold的预测非常准确，但它仍然有一些局限性，例如它对于某些类型的蛋白质结构（如膜蛋白）的预测能力有限。</p>
<p dir="auto">用 V100 32G <em>128, (EPYC </em>2, 512G RAM) 8 * 16 Nodes,  训练了三个月.</p>
<p dir="auto">额外(主要)介绍了: </p>
<ol>
<li dir="auto">打工人需要对各种算法和技术原理需要有一定的理解和涉猎("面试造火箭"); 最好不要做模型人少大厂. </li>
<li dir="auto">BPS和SPAM的HPC部署脚本</li>
<li dir="auto">简单介绍了分布式式训练, 分布式方法, 节点离线处理方法.</li>
</ol></div></div></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1zC411474Z/?spm_id_from=333.1007.top_right_bar_window_history.content.click" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1zC411474Z/?spm_id_from=333.1007.top_right_bar_window_history.content.click" target="_blank">【吹爆！】强强联手！科大讯飞和中科院终于把【多模态大模型】给讲通透了！CLIP、blip、blip2三种模型原理一次性学透！全程干货分享无废话！_哔哩哔哩_bilibili</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/661854155" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/661854155" target="_blank">视觉大模型 - 知乎</a></p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="邦联学习(Federated Learning)" dir="auto" class="heading" id="邦联学习(Federated_Learning)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>邦联学习(Federated Learning)</h2><div class="heading-children"><div><blockquote dir="auto">
<p>邦联学习就是使多个设备（如手机或服务器）能够共享学习模型的更新，而不是共享原始数据. 用于<strong>保护数据的安全和隐私性并节省带宽</strong>. 在学习中每个计算单元都会有一个模型的副本, 在本地使用本地数据更新模型之后, 再将模型权重发回给中央服务器. 中央服务器再将所有模型聚合起来, 创建一个全局模型后发回给计算单元. 邦联学习在许多领域都有应用，包括医疗、金融和通信等。如，Google使用邦联学习来改进其手机键盘的预测能力，而不需要看到用户的实际输入。</p>
</blockquote></div><div><p dir="auto"><span style="background:#fff88f">联邦学习 = 分布式 + 加密算法</span></p></div><div><ul>
<li data-line="0" dir="auto">隐私数据保存在本地由加密算法保证其安全性</li>
<li data-line="1" dir="auto">梯度是不可能还原成数据的</li>
</ul></div><div><p dir="auto"><span style="background:#fff88f">联邦学习在实际部署中有什么挑战?</span></p></div><div><ol>
<li data-line="0" dir="auto"><strong>隐私保护</strong>：尽管联邦学习的目的是提高数据隐私保护，但它仍然可能面临信息泄露的风险。例如，即使不直接共享数据，通过共享模型更新（如梯度或权重更新）仍然可能暴露敏感信息。研究人员已经展示了如何从共享的模型更新中重构训练数据的攻击方法。</li>
<li data-line="1" dir="auto"><strong>通信开销</strong>: 联邦学习通常需要大量的网络通信，因为参与的设备或服务器需要频繁地发送模型更新到中心服务器进行聚合。这种频繁的通信可能导致显著的网络延迟和高带宽消耗。</li>
<li data-line="2" dir="auto"><strong>系统异质性</strong>：参与联邦学习的设备通常在硬件性能、网络连接速度和数据存储能力上存在差异。这种异质性可能导致训练过程中的不均衡，影响模型的效率和最终性能。</li>
<li data-line="3" dir="auto"><strong>非独立同分布(Non-ID)数据</strong>：在联邦学习场景中，各个节点持有的数据可能具有不同的分布（即非独立同分布）。这可能导致模型在某些节点上表现很好，在其他节点上表现不佳，从而降低了模型的整体性能和泛化能力。⬅️ Shuffle 数据可以缓解</li>
<li data-line="4" dir="auto"><strong>规模管理</strong>：随着参与计算的设备数量增加，协调和管理这些设备，确保稳定和有效的训练过程变得更加困难。</li>
<li data-line="5" dir="auto"><strong>模型毒化和数据毒化攻击</strong>：在开放的联邦学习环境中，恶意参与者可能通过向模型发送错误的更新来影响整体学习过程，即所谓的模型毒化攻击。此外，通过修改自己的本地数据来影响全局模型的学习过程也是可能的。</li>
<li data-line="6" dir="auto"><strong>法律和政策问题</strong>：联邦学习涉及多方数据和模型的共享，可能会触及数据保护法、跨境数据传输规定等复杂的法律和政策问题。</li>
</ol></div><div><p dir="auto"><code>FedAvg</code> <code>FedSgd</code> 是一个基于梯度下降的算法, 专门用于处理邦联学习中数据通信效率低和数据不平衡的问题. </p></div></div></div><div class="heading-wrapper"><h2 data-heading="并行化(Parallel)" dir="auto" class="heading" id="并行化(Parallel)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>并行化(Parallel)</h2><div class="heading-children"><div><blockquote dir="auto">
<p>邦联学习依赖并行化. 我们这里先学习一下深度学习并行化的基本概念. </p>
</blockquote></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> All-reduce 和 Map-reduce 是什么? </span></li>
<li data-line="1" dir="auto"><strong>All-reduce</strong> 和 <strong>MapReduce</strong> 是两种常见的并行和分布式计算模式，它们在数据并行、机器学习训练和其他大规模计算任务中都有广泛应用。</li>
</ul></div><div><ol>
<li data-line="0" dir="auto"><strong>All-reduce</strong>：All-reduce 是一种全局同步操作，被用于在所有参与的处理器（或节点）之间共享信息。在一个 All-reduce 操作中，每个处理器首先有一个初始值，然后这些值被某种方式（例如，求和、求最大值、求最小值等）结合在一起，最后每个处理器都会得到这个最终结果。在深度学习中，All-reduce <strong>常常用于数据并行训练中的梯度聚合步骤</strong>，所有处理器计算出的梯度被求和，然后这个和被分发给所有的处理器。</li>
<li data-line="1" dir="auto"><strong>MapReduce</strong>：MapReduce 是 Google 提出的一种用于<strong>大规模数据处理的编程模型</strong>。它包括两个主要步骤：Map（映射）和 Reduce（归约）。在 Map 步骤，输入数据被分割成多个子集，然后每个子集都独立地应用 Map 函数，生成一系列的键值对。在 Reduce 步骤，所有具有相同键的值被聚集在一起，然后应用 Reduce 函数，生成最终结果。MapReduce 能够处理大规模的数据集，它的主要优点是<strong>可以在成千上万的计算节点上并行执行</strong>。</li>
</ol></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> 什么是 Mirror Strategy?  </span></li>
<li data-line="1" dir="auto">Mirror Strategy 是 TensorFlow 的 <code>tf.distribute.Strategy</code> API 中的一种分布式训练策略，正式名称为 <code>tf.distribute.MirroredStrategy</code>。这种策略支持在一台机器上的多个 GPU 之间进行同步训练，是一种数据并行的实现。</li>
<li data-line="2" dir="auto">在 Mirror Strategy 中，模型的所有变量都会在每个设备（例如 GPU）上复制一份。也就是说，每个设备都有一份完整的模型。在训练过程中，每个设备会使用不同的数据批次对其模型副本进行更新，然后通过 All-reduce 操作同步更新后的变量，确保所有设备上的模型副本始终保持一致。这样，每个设备都可以并行地处理数据并更新模型，从而提高训练速度。</li>
<li data-line="3" dir="auto">Mirror Strategy 的主要优点是它可以提供良好的设备间通信性能，并且易于使用。但是，它<span style="background:#fff88f">只适用于一台机器上的多个 GPU</span>，不适用于跨多台机器的分布式训练。对于跨机器的分布式训练，TensorFlow 提供了其他的策略，如<code>tf.distribute.experimental.MultiWorkerMirroredStrategy</code>。</li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> 有什么大数据框架</span></li>
<li data-line="5" dir="auto"><strong>Hadoop</strong>  是一个由Apache基金会开发的开源框架，用于处理和存储大数据。它的主要特性是能够在大量的硬件中进行分布式处理，从而使得大规模数据的处理变得更加快速和可靠。Hadoop的主要组件包括Hadoop Distributed File System（HDFS）用于存储数据，和MapReduce用于处理数据。</li>
<li data-line="6" dir="auto"><strong>Spark</strong>：Spark是一个用于大规模数据处理的快速和通用的计算系统。它提供了一个高级的API，使得开发人员可以更容易地编写分布式代码。与Hadoop相比，Spark能够提供更快的计算速度和更强大的计算能力。</li>
<li data-line="7" dir="auto"><strong>Flink</strong>：Flink是一个用于批量和流数据处理的开源平台。它的主要特点是能够提供精确的计算结果，即使在面对大量数据时也能保持高性能。</li>
<li data-line="8" dir="auto"><strong>Storm</strong>：Storm是一个用于实时数据处理的开源框架。它能够处理大量的数据流，并且能够保证数据的处理过程是可靠的。</li>
<li data-line="9" dir="auto"><strong>Hive</strong>：Hive是一个基于Hadoop的数据仓库工具，能够将结构化的数据文件映射为一张数据库表，并提供SQL查询功能。</li>
<li data-line="10" dir="auto"><strong>HBase</strong>：HBase是一个基于Hadoop的分布式数据库，能够存储大量的非结构化和半结构化的数据。</li>
</ul></div><div><blockquote dir="auto">
<p>并行训练的主要瓶颈是通信带宽. </p>
</blockquote></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>数据并行（Data Parallelism）</strong></span></li>
<li data-line="1" dir="auto">最常见, 基本思想是拆分数据集, 每个计算单元(e.g.一张GPU)独立的在一个模型副本上在小数据集上进行前向和反向传播最后权重聚合起来更新一个全局模型..所有计算单元共享网络结构和模型权重. <strong>适合处理大型数据集</strong>. 但受到到网络通信延迟、梯度聚合和参数更新的计算开销. 基本的流程如下:</li>
</ul></div><div><ol>
<li data-line="0" dir="auto"><strong>准备阶段</strong>：首先，将训练数据分成多个子集（通常称为批次）。然后，在每个处理器（或计算设备，如GPU）上复制模型的初始参数。</li>
<li data-line="1" dir="auto"><strong>并行训练</strong>：每个处理器使用其分配的数据子集独立地进行前向传播和反向传播，计算模型参数的梯度。</li>
<li data-line="2" dir="auto"><strong>梯度合并</strong>：所有处理器的梯度被收集后，各个GPU上计算得到的梯度需要被合并。这通常通过一种叫做 <code>All-reduce</code> 操作的通信过程完成。在 <code>All-reduce</code> 操作中，所有GPU的梯度会被累加（或取平均）.</li>
<li data-line="3" dir="auto"><strong>全局模型同步或异步更新</strong> : 将聚合权重同步或异步的广播到所有GPU。这确保了每个GPU在更新权重时使用的是全局累计的梯度。</li>
<li data-line="4" dir="auto"><strong>迭代</strong>：重复步骤2-4，直到模型收敛，或满足其他停止条件。</li>
</ol></div><div><ul>
<li data-line="0" dir="auto">
<p><strong>实现工具</strong>: 在PyTorch中，可以使用<code>torch.nn.DataParallel</code>和<code>torch.nn.parallel.DistributedDataParallel</code>类来实现数据并行。在TensorFlow中，可以使用<code>tf.distribute</code>策略来实现。</p>
</li>
<li data-line="3" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>模型并行（Model Parallelism）</strong>：在模型并行中，模型被分割成几个部分，并在多个处理器上并行运行。这种方法特别适用于模型太大而无法在单个处理器上运行的情况。</p>
</span></li>
<li data-line="4" dir="auto">
<p>模型并行适用于模态太大而至于单卡无法装下的情况. 模型并行将模型分割成几个部分, 然后在不同的计算单元上并行的训练模型的部分. 模型并行也需要更新模型权重, 所以也收到通信带宽的限制 .模型并行的基本流程如下：</p>
</li>
</ul></div><div><ol>
<li data-line="0" dir="auto">将模型的不同部分分配到不同的设备上。例如，如果我们有一个深度神经网络，我们可以将网络的不同层分配到不同的GPU上。</li>
<li data-line="1" dir="auto">在每个设备上并行地计算模型的前向传播和反向传播。</li>
<li data-line="2" dir="auto">通过通信链接（例如，NVLink或PCIe）在设备之间交换必要的数据。例如，一层的输出可能需要作为下一层的输入。</li>
<li data-line="3" dir="auto">更新模型的参数。这可能需要在设备之间同步参数。</li>
</ol></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>管道并行（Pipeline Parallelism）</strong>：在管道并行中，模型的不同部分会在不同的时间处理不同的数据。这意味着一部分模型可以开始处理下一个数据点，而另一部分模型仍在处理上一个数据点。这种方法可以减少等待时间，提高效率。</span></li>
<li data-line="3" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>混合并行（Hybrid Parallelism）</strong>：混合并行结合了上述几种方法，以适应各种不同的需求和约束。例如，可以同时使用数据并行和模型并行，以处理大型数据集和大型模型。</span></li>
</ul></div></div></div><div class="heading-wrapper"><h2 data-heading="调优指南" dir="auto" class="heading" id="调优指南"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>调优指南</h2><div class="heading-children"><div class="admonition-parent admonition-hint-parent"><div class="callout admonition admonition-hint admonition-plugin " style="--callout-color: 0, 191, 165;" data-callout="hint" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="fire" class="svg-inline--fa fa-fire fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M216 23.86c0-23.8-30.65-32.77-44.15-13.04C48 191.85 224 200 224 288c0 35.63-29.11 64.46-64.85 63.99-35.17-.45-63.15-29.77-63.15-64.94v-85.51c0-21.7-26.47-32.23-41.43-16.5C27.8 213.16 0 261.33 0 320c0 105.87 86.13 192 192 192s192-86.13 192-192c0-170.29-168-193-168-296.14z"></path></svg></div><div class="callout-title-inner admonition-title-content">Hint</div></div><div class="callout-content admonition-content heading-wrapper"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/41631631/answer/2812816993" rel="noopener" class="external-link" href="https://www.zhihu.com/question/41631631/answer/2812816993" target="_blank">你有哪些deep learning（rnn、cnn）调参的经验？ - 知乎</a></p>
<h3 data-heading="Code pipeline" dir="auto" class="heading" id="Code_pipeline">Code pipeline</h3>
<p dir="auto">e.g. computer vision task</p>
<ol>
<li dir="auto">data(dataset/dataloader)<br>
可以加一下lazy初始化<strong>slot</strong>的技巧减缓内存占用</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>model
<ol>
<li dir="auto">模型代码</li>
<li dir="auto">参数初始化</li>
</ol>
</li>
<li dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>optimizer 
<ol>
<li dir="auto">adad/adamW</li>
<li dir="auto">lr (3.5e-4)</li>
<li dir="auto">lr schedule (warm-up + cos退火)</li>
<li dir="auto">epochs </li>
</ol>
</li>
<li dir="auto">loss func</li>
<li dir="auto">train func</li>
<li dir="auto">eval func</li>
</ol></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://github.com/lartpang/PyTorchTricks" rel="noopener" class="external-link" href="https://github.com/lartpang/PyTorchTricks" target="_blank">GitHub - lartpang/PyTorchTricks: Some tricks of pytorch... :star:</a></p>
<p dir="auto">一个主要讲解如何降低 Pytorch 工程中代码运行速度的repo.<br>
包含预处理加速, 训练策略加速, 优化器加速, 代码和网络构建时的 tricks</p></div></div></div><div><p dir="auto">深度学习有什么调优的具体方针?</p></div><div><ol>
<li data-line="0" dir="auto">
<p>调整学习率：学习率是深度学习中的一个重要参数。它决定了模型在学习过程中对错误的反应程度。如果学习率太高，模型可能会在最优解附近震荡而无法收敛；如果学习率太低，模型收敛速度可能会非常慢。因此，合适的学习率是非常重要的。</p>
</li>
<li data-line="2" dir="auto">
<p>批量大小调整：批量大小决定了每次模型更新的样本数量。较大的批量可以提高计算效率，但可能导致模型过拟合；较小的批量可能使模型更容易收敛到全局最优，但计算效率较低。</p>
</li>
<li data-line="4" dir="auto">
<p>选择合适的优化器：有许多不同的优化器可供选择，如SGD、Adam、RMSProp等。不同的优化器有不同的优缺点，需要根据具体的任务和数据选择最合适的优化器。</p>
</li>
<li data-line="6" dir="auto">
<p>正则化：正则化是一种防止模型过拟合的技术，如L1、L2正则化、dropout等。通过在损失函数中添加一个正则化项，可以限制模型复杂度，防止过拟合。</p>
</li>
<li data-line="8" dir="auto">
<p>调整网络结构：网络结构（如层数、每层神经元数量等）对模型性能有很大影响。通常来说，更复杂的网络结构可以学习更复杂的特征，但也更容易过拟合。</p>
</li>
<li data-line="10" dir="auto">
<p>数据增强：通过对训练数据进行一些变换（如旋转、缩放、剪裁等），可以生成更多的训练样本，提高模型的泛化能力。</p>
</li>
<li data-line="12" dir="auto">
<p>早停：当验证集上的性能不再提高时，停止训练可以防止过拟合。</p>
</li>
<li data-line="14" dir="auto">
<p>参数初始化：参数的初始值对模型的学习过程和最终性能有很大影响。一般来说，合适的参数初始化可以加速模型的收敛，并提高最终性能。</p>
</li>
<li data-line="16" dir="auto">
<p>学习率衰减：随着训练的进行，逐步降低学习率可以帮助模型更好地收敛。</p>
</li>
</ol></div><div><ul>
<li data-line="0" dir="auto">项目一开始，尽量使用一个完善有效的模型，尽快先让代码跑起来</li>
<li data-line="1" dir="auto">Quasi-Random-Search 通过在搜索空间均匀的生成样本点来进行搜索，在优化探索阶段很好用</li>
<li data-line="2" dir="auto">初始超参配置: 1) 模型配置 2) 优化器 3) 训练步数</li>
<li data-line="3" dir="auto">先使用常见的优化器(e.g. Adam, NAdam, SGD with momentum)</li>
<li data-line="4" dir="auto">选用硬件可支持的最大Batch Size, 以减少训练时间， 测试更多新想法。e.g.(batch size = 2^n, 1,2....,n)</li>
<li data-line="5" dir="auto">大多超参最优值对batch size 敏感， 特别是优化器(学习率， 动量)和正则化超参</li>
<li data-line="6" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>提高模型性能遵循增量调优策略
<ul>
<li data-line="7" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<ol>
<li data-line="7" dir="auto">设定下一轮实验的目标(小步快跑)</li>
</ol>
</li>
<li data-line="8" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<ol start="2">
<li data-line="8" dir="auto">设计展开实验(识别目标, 冗余, 固定超参)</li>
</ol>
</li>
<li data-line="9" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<ol start="3">
<li data-line="9" dir="auto">从实验中获得经验</li>
</ol>
</li>
<li data-line="10" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<ol start="4">
<li data-line="10" dir="auto">考虑是否使用新配置</li>
</ol>
</li>
</ul>
</li>
<li data-line="11" dir="auto">如果最佳点在搜索空间的边界， 那么该模型很可能并不稳定</li>
<li data-line="12" dir="auto">训练函数的val error在某个时刻增加时， 就会发生over fitting. 缓解手段有e.g. dropout、标签平滑化、权重衰减</li>
<li data-line="13" dir="auto">isolation 图可以帮助检测更改是否有效</li>
<li data-line="14" dir="auto">贝叶斯优化工具可以在完成“好”的搜索空间探索后，继续精准优化超参</li>
<li data-line="15" dir="auto">在训练误差无限改善时，多试几个参数可能更有帮助</li>
<li data-line="16" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>对于多轮测试， 可以在第一轮进行短时间训练获取最佳模型和优化器参数。第二轮，在较佳超参数上
<ul>
<li data-line="17" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>在第一轮获得超参泛化到更长epochs可能是
<ul>
<li data-line="18" dir="auto">warmup 时长，模型参数初始值</li>
<li data-line="19" dir="auto">优化算法/优化器超参数</li>
<li data-line="20" dir="auto">数据增强方法</li>
<li data-line="21" dir="auto">正则化</li>
</ul>
</li>
<li data-line="22" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>不可能转移
<ul>
<li data-line="23" dir="auto">学习率衰减schedule</li>
</ul>
</li>
</ul>
</li>
<li data-line="24" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>pipeline性能受阻的常见原因
<ul>
<li data-line="25" dir="auto">数据未与训练进程存放在同一位置，导致I/O延迟</li>
<li data-line="26" dir="auto">开销很大的在线预处理</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h2 data-heading="硬件" dir="auto" class="heading" id="硬件"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>硬件</h2><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1P44y1V7bu/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1P44y1V7bu/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">上帝视角看GPU（1）：图形流水线基础_哔哩哔哩_bilibili</a></p></div></div></div><div class="heading-wrapper"><h2 data-heading="阅读笔记" dir="auto" class="heading" id="阅读笔记"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>阅读笔记</h2><div class="heading-children"><div><ul>
<li data-line="0" dir="auto">
<p>什么是拓扑生成网络？</p>
</li>
<li data-line="3" dir="auto">
<p>深度学习中不可微性问题 <a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1vT42197fb/?spm_id_from=333.880.my_history.page.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1vT42197fb/?spm_id_from=333.880.my_history.page.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">如何解决深度学习中的不可微问题（一）</a></p>
</li>
<li data-line="4" dir="auto">
<p>众多周知 DL 依赖的梯度下降要求目标函数可微。对于单元函数来说就是在点 C存在极限且连续。对于多元函数来说就是所有方向的偏导数存在且连续。</p>
</li>
<li data-line="5" dir="auto">
<p>目标函数</p>
</li>
<li data-line="9" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 为什么高维空间的两组随机向量几乎正交?</p>
</span></li>
<li data-line="10" dir="auto">
<p>很多数据都可以看作从某种分布中采样的高维向量。</p>
</li>
<li data-line="11" dir="auto">
<p><span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munderover space="4"><mjx-over style="padding-bottom: 0.192em; padding-left: 0.51em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-over><mjx-box><mjx-munder><mjx-row><mjx-base><mjx-mo class="mjx-lop"><mjx-c class="mjx-c2211 TEX-S2"></mjx-c></mjx-mo></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.148em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-under></mjx-row></mjx-munder></mjx-box></mjx-munderover><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span></p>
</li>
<li data-line="12" dir="auto">
<p>假设 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span> <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span> 都是从正太分布中独立同采样的。中心极限定理表明，当n 变大时，ab点积会趋向于正太分布的均值。同时，向量a和 b 的模长平方的期望值近似等于 n. 因为它们是由单位方差的独立分量构成的。就变成下面的样子</p>
</li>
<li data-line="13" dir="auto">
<p><span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2248"></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-munderover limits="false"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2211 TEX-S1"></mjx-c></mjx-mo><mjx-script style="vertical-align: -0.285em; margin-left: 0px;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-spacer style="margin-top: 0.284em;"></mjx-spacer><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-munderover><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container></span></p>
</li>
<li data-line="14" dir="auto">
<p>n 变大, 夹角肯定越趋向于 0</p>
</li>
<li data-line="15" dir="auto">
<p>简单来说，自由度变大导致任何特定的对齐方向都变得不太可能。</p>
</li>
</ul></div><div class="mod-footer"></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="💾-科技工程/4_深度学习.html#4_深度学习"><div class="tree-item-contents heading-link" heading-name="4_深度学习"><span class="tree-item-title">4_深度学习</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#深度学习名人堂"><div class="tree-item-contents heading-link" heading-name="深度学习名人堂"><span class="tree-item-title">深度学习名人堂</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#基本算法"><div class="tree-item-contents heading-link" heading-name="基本算法"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">基本算法</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#梯度下降_&amp;_反向传播"><div class="tree-item-contents heading-link" heading-name="梯度下降 &amp; 反向传播"><span class="tree-item-title">梯度下降 &amp; 反向传播</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#过拟合_&amp;_欠拟合"><div class="tree-item-contents heading-link" heading-name="过拟合 &amp; 欠拟合"><span class="tree-item-title">过拟合 &amp; 欠拟合</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#数值稳定性和参数初始化"><div class="tree-item-contents heading-link" heading-name="数值稳定性和参数初始化"><span class="tree-item-title">数值稳定性和参数初始化</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#优化器"><div class="tree-item-contents heading-link" heading-name="优化器"><span class="tree-item-title">优化器</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#线性层_+_激活函数"><div class="tree-item-contents heading-link" heading-name="线性层 + 激活函数"><span class="tree-item-title">线性层 + 激活函数</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#正则化"><div class="tree-item-contents heading-link" heading-name="正则化"><span class="tree-item-title">正则化</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#规范化(BN,_LN,_IN,_GN,_RMSNorm)"><div class="tree-item-contents heading-link" heading-name="规范化(BN, LN, IN, GN, RMSNorm)"><span class="tree-item-title">规范化(BN, LN, IN, GN, RMSNorm)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#数据增强"><div class="tree-item-contents heading-link" heading-name="数据增强"><span class="tree-item-title">数据增强</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#多层感知机器(MLP)"><div class="tree-item-contents heading-link" heading-name="多层感知机器(MLP)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">多层感知机器(MLP)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#KAN_(alternative)"><div class="tree-item-contents heading-link" heading-name="KAN (alternative)"><span class="tree-item-title">KAN (alternative)</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#CNN(卷积神经网络)"><div class="tree-item-contents heading-link" heading-name="CNN(卷积神经网络)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">CNN(卷积神经网络)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#模块堆叠方法(U-Net,FCN,_FFN)"><div class="tree-item-contents heading-link" heading-name="模块堆叠方法(U-Net,FCN, FFN)"><span class="tree-item-title">模块堆叠方法(U-Net,FCN, FFN)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#图像分类"><div class="tree-item-contents heading-link" heading-name="图像分类"><span class="tree-item-title">图像分类</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#物体检测分割"><div class="tree-item-contents heading-link" heading-name="物体检测分割"><span class="tree-item-title">物体检测分割</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#LeNet(鼻祖)_-_1998"><div class="tree-item-contents heading-link" heading-name="LeNet(鼻祖) - 1998"><span class="tree-item-title">LeNet(鼻祖) - 1998</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#LeNet_是什么?_对深度学习的发展有什么贡献?"><div class="tree-item-contents heading-link" heading-name="LeNet 是什么? 对深度学习的发展有什么贡献?"><span class="tree-item-title">LeNet 是什么? 对深度学习的发展有什么贡献?</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#AlexNet(重启深度学习)_-_2012"><div class="tree-item-contents heading-link" heading-name="AlexNet(重启深度学习) - 2012"><span class="tree-item-title">AlexNet(重启深度学习) - 2012</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#AlexNet_是什么?_对深度学习的发展有什么贡献?"><div class="tree-item-contents heading-link" heading-name="AlexNet 是什么? 对深度学习的发展有什么贡献?"><span class="tree-item-title">AlexNet 是什么? 对深度学习的发展有什么贡献?</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#ZfNet(第一次可视化隐藏层)_-_2013"><div class="tree-item-contents heading-link" heading-name="ZfNet(第一次可视化隐藏层) - 2013"><span class="tree-item-title">ZfNet(第一次可视化隐藏层) - 2013</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#物体检测分割网络(R-CNN,_SSD,_YOLO..)"><div class="tree-item-contents heading-link" heading-name="物体检测分割网络(R-CNN, SSD, YOLO..)"><span class="tree-item-title">物体检测分割网络(R-CNN, SSD, YOLO..)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#历史上的分割网路"><div class="tree-item-contents heading-link" heading-name="历史上的分割网路"><span class="tree-item-title">历史上的分割网路</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#R-CNN_是什么?_对深度学习的发展有什么贡献?"><div class="tree-item-contents heading-link" heading-name="R-CNN 是什么? 对深度学习的发展有什么贡献?"><span class="tree-item-title">R-CNN 是什么? 对深度学习的发展有什么贡献?</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#SSD_(Single_Shot_Multibox_Detector)"><div class="tree-item-contents heading-link" heading-name="SSD (Single Shot Multibox Detector)"><span class="tree-item-title">SSD (Single Shot Multibox Detector)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#YOLO_(You_Only_Look_Once)"><div class="tree-item-contents heading-link" heading-name="YOLO (You Only Look Once)"><span class="tree-item-title">YOLO (You Only Look Once)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#VGG(纯CNN最深网络)_-_2014"><div class="tree-item-contents heading-link" heading-name="VGG(纯CNN最深网络) - 2014"><span class="tree-item-title">VGG(纯CNN最深网络) - 2014</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#GoogLeNet_/_Inception(探索各种trick)_-_2014"><div class="tree-item-contents heading-link" heading-name="GoogLeNet / Inception(探索各种trick) - 2014"><span class="tree-item-title">GoogLeNet / Inception(探索各种trick) - 2014</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#FCN_-_2014_(最后一层是上采样的卷积层)"><div class="tree-item-contents heading-link" heading-name="FCN - 2014 (最后一层是上采样的卷积层)"><span class="tree-item-title">FCN - 2014 (最后一层是上采样的卷积层)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#ResNet(残差网络)_-_2015"><div class="tree-item-contents heading-link" heading-name="ResNet(残差网络) - 2015"><span class="tree-item-title">ResNet(残差网络) - 2015</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#DenseNet(密集连接)_-_2016"><div class="tree-item-contents heading-link" heading-name="DenseNet(密集连接) - 2016"><span class="tree-item-title">DenseNet(密集连接) - 2016</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#MobileNet(网络做小)_-_2016"><div class="tree-item-contents heading-link" heading-name="MobileNet(网络做小) - 2016"><span class="tree-item-title">MobileNet(网络做小) - 2016</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#U-Net(骨干)"><div class="tree-item-contents heading-link" heading-name="U-Net(骨干)"><span class="tree-item-title">U-Net(骨干)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#摘要"><div class="tree-item-contents heading-link" heading-name="摘要"><span class="tree-item-title">摘要</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#引言"><div class="tree-item-contents heading-link" heading-name="引言"><span class="tree-item-title">引言</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#网络架构"><div class="tree-item-contents heading-link" heading-name="网络架构"><span class="tree-item-title">网络架构</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#训练"><div class="tree-item-contents heading-link" heading-name="训练"><span class="tree-item-title">训练</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#数据增强"><div class="tree-item-contents heading-link" heading-name="数据增强"><span class="tree-item-title">数据增强</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#实验"><div class="tree-item-contents heading-link" heading-name="实验"><span class="tree-item-title">实验</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#结论"><div class="tree-item-contents heading-link" heading-name="结论"><span class="tree-item-title">结论</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#后续改进"><div class="tree-item-contents heading-link" heading-name="后续改进"><span class="tree-item-title">后续改进</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#FPN(特征金字塔网络)"><div class="tree-item-contents heading-link" heading-name="FPN(特征金字塔网络)"><span class="tree-item-title">FPN(特征金字塔网络)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#EfficientNet_(终结网络scale经验法则)_-_2019"><div class="tree-item-contents heading-link" heading-name="EfficientNet (终结网络scale经验法则) - 2019"><span class="tree-item-title">EfficientNet (终结网络scale经验法则) - 2019</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#BiT_(最大有监督CNN)"><div class="tree-item-contents heading-link" heading-name="BiT (最大有监督CNN)"><span class="tree-item-title">BiT (最大有监督CNN)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#VanillaNet_?"><div class="tree-item-contents heading-link" heading-name="VanillaNet ?"><span class="tree-item-title">VanillaNet ?</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#RNN(循环神经网络)"><div class="tree-item-contents heading-link" heading-name="RNN(循环神经网络)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">RNN(循环神经网络)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#LSTM(长短期记忆网络)_&amp;_GRU(门控循环单元)"><div class="tree-item-contents heading-link" heading-name="LSTM(长短期记忆网络) &amp; GRU(门控循环单元)"><span class="tree-item-title">LSTM(长短期记忆网络) &amp; GRU(门控循环单元)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#RWKV"><div class="tree-item-contents heading-link" heading-name="RWKV"><span class="tree-item-title">RWKV</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#Mamba"><div class="tree-item-contents heading-link" heading-name="Mamba"><span class="tree-item-title">Mamba</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#Test-Time_Training"><div class="tree-item-contents heading-link" heading-name="Test-Time Training"><span class="tree-item-title">Test-Time Training</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#GAN(对抗生成网络)"><div class="tree-item-contents heading-link" heading-name="GAN(对抗生成网络)"><span class="tree-item-title">GAN(对抗生成网络)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#Autoencoder(自编码器)"><div class="tree-item-contents heading-link" heading-name="Autoencoder(自编码器)"><span class="tree-item-title">Autoencoder(自编码器)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#Diffusion(扩散模型)"><div class="tree-item-contents heading-link" heading-name="Diffusion(扩散模型)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">Diffusion(扩散模型)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#DiT"><div class="tree-item-contents heading-link" heading-name="DiT"><span class="tree-item-title">DiT</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#下游应用"><div class="tree-item-contents heading-link" heading-name="下游应用"><span class="tree-item-title">下游应用</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#Transformer_(2017)"><div class="tree-item-contents heading-link" heading-name="Transformer (2017)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">Transformer (2017)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#`Multi-Head_Attention`_多头注意力"><div class="tree-item-contents heading-link" heading-name="`Multi-Head Attention` 多头注意力"><span class="tree-item-title"><code>Multi-Head Attention</code> 多头注意力</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#`Position_Encoding`_位置编码"><div class="tree-item-contents heading-link" heading-name="`Position Encoding` 位置编码"><span class="tree-item-title"><code>Position Encoding</code> 位置编码</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#`Add_&amp;_Norm`__残差_&amp;_规范化"><div class="tree-item-contents heading-link" heading-name="`Add &amp; Norm`  残差 &amp; 规范化"><span class="tree-item-title"><code>Add &amp; Norm</code>  残差 &amp; 规范化</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#`FeedFowrd`_前向网络"><div class="tree-item-contents heading-link" heading-name="`FeedFowrd` 前向网络"><span class="tree-item-title"><code>FeedFowrd</code> 前向网络</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#优化机制"><div class="tree-item-contents heading-link" heading-name="优化机制"><span class="tree-item-title">优化机制</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#下游任务"><div class="tree-item-contents heading-link" heading-name="下游任务"><span class="tree-item-title">下游任务</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#语言大模型(Large_Language_Model)"><div class="tree-item-contents heading-link" heading-name="语言大模型(Large Language Model)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">语言大模型(Large Language Model)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#编解码架构"><div class="tree-item-contents heading-link" heading-name="编解码架构"><span class="tree-item-title">编解码架构</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#训练"><div class="tree-item-contents heading-link" heading-name="训练"><span class="tree-item-title">训练</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#微调"><div class="tree-item-contents heading-link" heading-name="微调"><span class="tree-item-title">微调</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#开源大模型"><div class="tree-item-contents heading-link" heading-name="开源大模型"><span class="tree-item-title">开源大模型</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#LLM可解释性"><div class="tree-item-contents heading-link" heading-name="LLM可解释性"><span class="tree-item-title">LLM可解释性</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#多模态(multimodal)"><div class="tree-item-contents heading-link" heading-name="多模态(multimodal)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">多模态(multimodal)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#BLIP"><div class="tree-item-contents heading-link" heading-name="BLIP"><span class="tree-item-title">BLIP</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#LLaVA"><div class="tree-item-contents heading-link" heading-name="LLaVA"><span class="tree-item-title">LLaVA</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#知识蒸馏(Knowledge_Distilling)"><div class="tree-item-contents heading-link" heading-name="知识蒸馏(Knowledge Distilling)"><span class="tree-item-title">知识蒸馏(Knowledge Distilling)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#对比学习(Contrastive_Learning)"><div class="tree-item-contents heading-link" heading-name="对比学习(Contrastive Learning)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">对比学习(Contrastive Learning)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#MemoryBank"><div class="tree-item-contents heading-link" heading-name="MemoryBank"><span class="tree-item-title">MemoryBank</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#CLIP(2021)"><div class="tree-item-contents heading-link" heading-name="CLIP(2021)"><span class="tree-item-title">CLIP(2021)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#MoCo_(2019)"><div class="tree-item-contents heading-link" heading-name="MoCo (2019)"><span class="tree-item-title">MoCo (2019)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#UnCLIP_(2022)"><div class="tree-item-contents heading-link" heading-name="UnCLIP (2022)"><span class="tree-item-title">UnCLIP (2022)</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#强化学习(RF_Learning)"><div class="tree-item-contents heading-link" heading-name="强化学习(RF Learning)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">强化学习(RF Learning)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#RLHF基本概念"><div class="tree-item-contents heading-link" heading-name="RLHF基本概念"><span class="tree-item-title">RLHF基本概念</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#RLHF_必要性"><div class="tree-item-contents heading-link" heading-name="RLHF 必要性"><span class="tree-item-title">RLHF 必要性</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#PPO"><div class="tree-item-contents heading-link" heading-name="PPO"><span class="tree-item-title">PPO</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#DPO"><div class="tree-item-contents heading-link" heading-name="DPO"><span class="tree-item-title">DPO</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#其他变体"><div class="tree-item-contents heading-link" heading-name="其他变体"><span class="tree-item-title">其他变体</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#双流网络(Two-stream_Network)"><div class="tree-item-contents heading-link" heading-name="双流网络(Two-stream Network)"><span class="tree-item-title">双流网络(Two-stream Network)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#双流网络_(Two-Stream_Network)_-_视频理解领域开山之作"><div class="tree-item-contents heading-link" heading-name="双流网络 (Two-Stream Network) - 视频理解领域开山之作"><span class="tree-item-title">双流网络 (Two-Stream Network) - 视频理解领域开山之作</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#图神经网络(GNN)"><div class="tree-item-contents heading-link" heading-name="图神经网络(GNN)"><span class="tree-item-title">图神经网络(GNN)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#模型可解释"><div class="tree-item-contents heading-link" heading-name="模型可解释"><span class="tree-item-title">模型可解释</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#AI4Science"><div class="tree-item-contents heading-link" heading-name="AI4Science"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">AI4Science</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#AlphaFold"><div class="tree-item-contents heading-link" heading-name="AlphaFold"><span class="tree-item-title">AlphaFold</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#邦联学习(Federated_Learning)"><div class="tree-item-contents heading-link" heading-name="邦联学习(Federated Learning)"><span class="tree-item-title">邦联学习(Federated Learning)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#并行化(Parallel)"><div class="tree-item-contents heading-link" heading-name="并行化(Parallel)"><span class="tree-item-title">并行化(Parallel)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#调优指南"><div class="tree-item-contents heading-link" heading-name="调优指南"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">调优指南</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#Code_pipeline"><div class="tree-item-contents heading-link" heading-name="Code pipeline"><span class="tree-item-title">Code pipeline</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#硬件"><div class="tree-item-contents heading-link" heading-name="硬件"><span class="tree-item-title">硬件</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#阅读笔记"><div class="tree-item-contents heading-link" heading-name="阅读笔记"><span class="tree-item-title">阅读笔记</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>