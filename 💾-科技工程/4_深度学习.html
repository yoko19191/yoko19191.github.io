<!DOCTYPE html> <html><head>
		<title>4_深度学习</title>
		<base href="../">
		<meta id="root-path" root-path="../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="🌱 Digital-Garden - 4_深度学习">
		<meta property="og:title" content="4_深度学习">
		<meta property="og:description" content="🌱 Digital-Garden - 4_深度学习">
		<meta property="og:type" content="website">
		<meta property="og:url" content="💾-科技工程/4_深度学习.html">
		<meta property="og:image" content="https://zh.d2l.ai/_images/capacity-vs-error.svg">
		<meta property="og:site_name" content="🌱 Digital-Garden">
		<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.4.0/pixi.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="https://cdn.jsdelivr.net/npm/minisearch@6.3.0/dist/umd/index.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="preload" href="lib/styles/other-plugins.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/other-plugins.css"></noscript><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript></head><body class="publish css-settings-manager native-scrollbars theme-light show-inline-title"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles">mjx-c.mjx-c221A.TEX-S1::before { padding: 0.85em 1.02em 0.35em 0px; content: "√"; }
mjx-c.mjx-c34::before { padding: 0.677em 0.5em 0px 0px; content: "4"; }
mjx-c.mjx-c3A::before { padding: 0.43em 0.278em 0px 0px; content: ":"; }
mjx-c.mjx-c3A6::before { padding: 0.683em 0.722em 0px 0px; content: "Φ"; }
mjx-c.mjx-c28.TEX-S4::before { padding: 1.75em 0.792em 1.249em 0px; content: "("; }
mjx-c.mjx-c29.TEX-S4::before { padding: 1.75em 0.792em 1.249em 0px; content: ")"; }
mjx-c.mjx-c1D713.TEX-I::before { padding: 0.694em 0.651em 0.205em 0px; content: "ψ"; }
mjx-c.mjx-c2248::before { padding: 0.483em 0.778em 0px 0px; content: "≈"; }
mjx-munderover { display: inline-block; text-align: left; }
mjx-munderover:not([limits="false"]) { padding-top: 0.1em; }
mjx-munderover:not([limits="false"]) > * { display: block; }
mjx-munder { display: inline-block; text-align: left; }
mjx-over { text-align: left; }
mjx-munder:not([limits="false"]) { display: inline-table; }
mjx-munder > mjx-row { text-align: left; }
mjx-under { padding-bottom: 0.1em; }
mjx-stretchy-v.mjx-c221A mjx-beg mjx-c::before { content: ""; padding: 0.605em 1.056em 0.014em 0px; }
mjx-stretchy-v.mjx-c221A mjx-ext mjx-c::before { content: ""; width: 1.056em; }
mjx-stretchy-v.mjx-c221A mjx-end mjx-c::before { content: "⎷"; padding: 0.935em 1.056em 0.885em 0px; }
mjx-stretchy-v.mjx-c221A > mjx-end { margin-top: -1.82em; }
mjx-stretchy-v.mjx-c221A > mjx-ext { border-top-width: 0.589em; border-bottom-width: 1.79em; }
mjx-stretchy-v.mjx-c28 mjx-beg mjx-c::before { content: "⎛"; padding: 1.154em 0.875em 0.655em 0px; }
mjx-stretchy-v.mjx-c28 mjx-ext mjx-c::before { content: "⎜"; width: 0.875em; }
mjx-stretchy-v.mjx-c28 mjx-end mjx-c::before { content: "⎝"; padding: 1.165em 0.875em 0.644em 0px; }
mjx-stretchy-v.mjx-c28 > mjx-end { margin-top: -1.809em; }
mjx-stretchy-v.mjx-c28 > mjx-ext { border-top-width: 1.779em; border-bottom-width: 1.779em; }
mjx-stretchy-v.mjx-c29 mjx-beg mjx-c::before { content: "⎞"; padding: 1.154em 0.875em 0.655em 0px; }
mjx-stretchy-v.mjx-c29 mjx-ext mjx-c::before { content: "⎟"; width: 0.875em; }
mjx-stretchy-v.mjx-c29 mjx-end mjx-c::before { content: "⎠"; padding: 1.165em 0.875em 0.644em 0px; }
mjx-stretchy-v.mjx-c29 > mjx-end { margin-top: -1.809em; }
mjx-stretchy-v.mjx-c29 > mjx-ext { border-top-width: 1.779em; border-bottom-width: 1.779em; }
mjx-stretchy-v.mjx-c7B mjx-beg mjx-c::before { content: "⎧"; padding: 0.899em 0.889em 0.01em 0px; }
mjx-stretchy-v.mjx-c7B mjx-ext mjx-c::before { content: "⎪"; width: 0.889em; }
mjx-stretchy-v.mjx-c7B mjx-end mjx-c::before { content: "⎩"; padding: 0.01em 0.889em 0.899em 0px; }
mjx-stretchy-v.mjx-c7B mjx-mid mjx-c::before { content: "⎨"; padding: 1.16em 0.889em 0.66em 0px; }
mjx-stretchy-v.mjx-c7B > mjx-mid { margin-top: -0.91em; margin-bottom: -0.91em; }
mjx-stretchy-v.mjx-c7B > mjx-end { margin-top: -0.909em; }
mjx-stretchy-v.mjx-c7B > mjx-ext { height: 50%; border-top-width: 0.879em; border-bottom-width: 0.879em; }
mjx-stretchy-v.mjx-c7D mjx-beg mjx-c::before { content: "⎫"; padding: 0.899em 0.889em 0.01em 0px; }
mjx-stretchy-v.mjx-c7D mjx-ext mjx-c::before { content: "⎪"; width: 0.889em; }
mjx-stretchy-v.mjx-c7D mjx-end mjx-c::before { content: "⎭"; padding: 0.01em 0.889em 0.899em 0px; }
mjx-stretchy-v.mjx-c7D mjx-mid mjx-c::before { content: "⎬"; padding: 1.16em 0.889em 0.66em 0px; }
mjx-stretchy-v.mjx-c7D > mjx-mid { margin-top: -0.91em; margin-bottom: -0.91em; }
mjx-stretchy-v.mjx-c7D > mjx-end { margin-top: -0.909em; }
mjx-stretchy-v.mjx-c7D > mjx-ext { height: 50%; border-top-width: 0.879em; border-bottom-width: 0.879em; }
mjx-c.mjx-c2211.TEX-S2::before { padding: 0.95em 1.444em 0.45em 0px; content: "∑"; }
mjx-c.mjx-c50::before { padding: 0.683em 0.681em 0px 0px; content: "P"; }
mjx-c.mjx-c1D446.TEX-I::before { padding: 0.705em 0.645em 0.022em 0px; content: "S"; }
mjx-c.mjx-c223C::before { padding: 0.367em 0.778em 0px 0px; content: "∼"; }
mjx-c.mjx-c1D437.TEX-I::before { padding: 0.683em 0.828em 0px 0px; content: "D"; }
mjx-c.mjx-c54::before { padding: 0.677em 0.722em 0px 0px; content: "T"; }
mjx-c.mjx-c2264::before { padding: 0.636em 0.778em 0.138em 0px; content: "≤"; }
mjx-c.mjx-c221A.TEX-S4::before { padding: 1.75em 1.02em 1.25em 0px; content: "√"; }
mjx-c.mjx-c46.TEX-C::before { padding: 0.683em 0.829em 0.032em 0px; content: "F"; }
mjx-c.mjx-c2F::before { padding: 0.75em 0.5em 0.25em 0px; content: "/"; }
mjx-c.mjx-c1D6FF.TEX-I::before { padding: 0.717em 0.444em 0.01em 0px; content: "δ"; }
mjx-c.mjx-cA0::before { padding: 0px 0.25em 0px 0px; content: " "; }
mjx-c.mjx-c3E::before { padding: 0.54em 0.778em 0.04em 0px; content: ">"; }
mjx-c.mjx-c1D429.TEX-B::before { padding: 0.45em 0.639em 0.194em 0px; content: "p"; }
mjx-c.mjx-c1D42A.TEX-B::before { padding: 0.45em 0.607em 0.194em 0px; content: "q"; }
mjx-c.mjx-c1D45D.TEX-I::before { padding: 0.442em 0.503em 0.194em 0px; content: "p"; }
mjx-c.mjx-c1D45E.TEX-I::before { padding: 0.442em 0.46em 0.194em 0px; content: "q"; }
mjx-c.mjx-c5C::before { padding: 0.75em 0.5em 0.25em 0px; content: "\\"; }
mjx-c.mjx-c1D45F.TEX-I::before { padding: 0.442em 0.451em 0.011em 0px; content: "r"; }
mjx-c.mjx-c22A4::before { padding: 0.668em 0.778em 0px 0px; content: "⊤"; }
mjx-c.mjx-c63::before { padding: 0.448em 0.444em 0.011em 0px; content: "c"; }
mjx-c.mjx-c28.TEX-S3::before { padding: 1.45em 0.736em 0.949em 0px; content: "("; }
mjx-c.mjx-c394::before { padding: 0.716em 0.833em 0px 0px; content: "Δ"; }
mjx-c.mjx-c29.TEX-S3::before { padding: 1.45em 0.736em 0.949em 0px; content: ")"; }
mjx-c.mjx-c2260::before { padding: 0.716em 0.778em 0.215em 0px; content: "≠"; }
mjx-c.mjx-c2211.TEX-S1::before { padding: 0.75em 1.056em 0.25em 0px; content: "∑"; }
mjx-c.mjx-c1D43D.TEX-I::before { padding: 0.683em 0.633em 0.022em 0px; content: "J"; }
mjx-c.mjx-c1D435.TEX-I::before { padding: 0.683em 0.759em 0px 0px; content: "B"; }
mjx-c.mjx-c2229::before { padding: 0.598em 0.667em 0.022em 0px; content: "∩"; }
mjx-c.mjx-c222A::before { padding: 0.598em 0.667em 0.022em 0px; content: "∪"; }
mjx-c.mjx-c24::before { padding: 0.75em 0.5em 0.056em 0px; content: "$"; }
mjx-c.mjx-c1D466.TEX-I::before { padding: 0.442em 0.49em 0.205em 0px; content: "y"; }
mjx-c.mjx-c1D45A.TEX-I::before { padding: 0.442em 0.878em 0.011em 0px; content: "m"; }
mjx-c.mjx-c1D44A.TEX-I::before { padding: 0.683em 1.048em 0.022em 0px; content: "W"; }
mjx-c.mjx-c1D457.TEX-I::before { padding: 0.661em 0.412em 0.204em 0px; content: "j"; }
mjx-c.mjx-c2225::before { padding: 0.75em 0.5em 0.25em 0px; content: "∥"; }
mjx-c.mjx-c40::before { padding: 0.705em 0.778em 0.011em 0px; content: "@"; }
mjx-c.mjx-c210E.TEX-I::before { padding: 0.694em 0.576em 0.011em 0px; content: "h"; }
mjx-c.mjx-c1D440.TEX-I::before { padding: 0.683em 1.051em 0px 0px; content: "M"; }
mjx-c.mjx-c2032::before { padding: 0.56em 0.275em 0px 0px; content: "′"; }
mjx-c.mjx-c2026::before { padding: 0.12em 1.172em 0px 0px; content: "…"; }
mjx-c.mjx-c1D454.TEX-I::before { padding: 0.442em 0.477em 0.205em 0px; content: "g"; }
mjx-c.mjx-c2217::before { padding: 0.465em 0.5em 0px 0px; content: "∗"; }
mjx-c.mjx-c1D43B.TEX-I::before { padding: 0.683em 0.888em 0px 0px; content: "H"; }
mjx-c.mjx-c1D459.TEX-I::before { padding: 0.694em 0.298em 0.011em 0px; content: "l"; }
mjx-c.mjx-c1D462.TEX-I::before { padding: 0.442em 0.572em 0.011em 0px; content: "u"; }
mjx-c.mjx-c1D467.TEX-I::before { padding: 0.442em 0.465em 0.011em 0px; content: "z"; }
mjx-c.mjx-c1D448.TEX-I::before { padding: 0.683em 0.767em 0.022em 0px; content: "U"; }
mjx-c.mjx-c1D43C.TEX-I::before { padding: 0.683em 0.504em 0px 0px; content: "I"; }
mjx-c.mjx-c1D44F.TEX-I::before { padding: 0.694em 0.429em 0.011em 0px; content: "b"; }
mjx-mroot { display: inline-block; text-align: left; }
mjx-root { display: inline-block; white-space: nowrap; }
mjx-surd { display: inline-block; vertical-align: top; }
mjx-sqrt { display: inline-block; padding-top: 0.07em; }
mjx-sqrt > mjx-box { border-top: 0.07em solid; }
mjx-sqrt.mjx-tall > mjx-box { padding-left: 0.3em; margin-left: -0.3em; }
mjx-c.mjx-c2061::before { padding: 0px; content: ""; }
mjx-c.mjx-c64::before { padding: 0.694em 0.556em 0.011em 0px; content: "d"; }
mjx-c.mjx-c62::before { padding: 0.694em 0.556em 0.011em 0px; content: "b"; }
mjx-c.mjx-c2E::before { padding: 0.12em 0.278em 0px 0px; content: "."; }
mjx-c.mjx-c221A.TEX-S2::before { padding: 1.15em 1.02em 0.65em 0px; content: "√"; }
mjx-c.mjx-c1D441.TEX-I::before { padding: 0.683em 0.888em 0px 0px; content: "N"; }
mjx-c.mjx-c1D445.TEX-I::before { padding: 0.683em 0.759em 0.021em 0px; content: "R"; }
mjx-c.mjx-c1D70F.TEX-I::before { padding: 0.431em 0.517em 0.013em 0px; content: "τ"; }
mjx-c.mjx-c1D438.TEX-I::before { padding: 0.68em 0.764em 0px 0px; content: "E"; }
mjx-c.mjx-c5B::before { padding: 0.75em 0.278em 0.25em 0px; content: "["; }
mjx-c.mjx-c1D707.TEX-I::before { padding: 0.442em 0.603em 0.216em 0px; content: "μ"; }
mjx-c.mjx-c5D::before { padding: 0.75em 0.278em 0.25em 0px; content: "]"; }
mjx-c.mjx-c1D719.TEX-I::before { padding: 0.694em 0.596em 0.205em 0px; content: "ϕ"; }
mjx-c.mjx-c1D716.TEX-I::before { padding: 0.431em 0.406em 0.011em 0px; content: "ϵ"; }
mjx-c.mjx-c1D703.TEX-I::before { padding: 0.705em 0.469em 0.01em 0px; content: "θ"; }
mjx-c.mjx-c1D714.TEX-I::before { padding: 0.443em 0.622em 0.011em 0px; content: "ω"; }
mjx-c.mjx-c1D452.TEX-I::before { padding: 0.442em 0.466em 0.011em 0px; content: "e"; }
mjx-c.mjx-c1D70B.TEX-I::before { padding: 0.431em 0.57em 0.011em 0px; content: "π"; }
mjx-c.mjx-c1D43F.TEX-I::before { padding: 0.683em 0.681em 0px 0px; content: "L"; }
mjx-c.mjx-c7B::before { padding: 0.75em 0.5em 0.25em 0px; content: "{"; }
mjx-c.mjx-c7D::before { padding: 0.75em 0.5em 0.25em 0px; content: "}"; }
mjx-c.mjx-c1D460.TEX-I::before { padding: 0.442em 0.469em 0.01em 0px; content: "s"; }
mjx-c.mjx-c1D70E.TEX-I::before { padding: 0.431em 0.571em 0.011em 0px; content: "σ"; }
mjx-c.mjx-c1D6FC.TEX-I::before { padding: 0.442em 0.64em 0.011em 0px; content: "α"; }
mjx-c.mjx-c30::before { padding: 0.666em 0.5em 0.022em 0px; content: "0"; }
mjx-c.mjx-c1D464.TEX-I::before { padding: 0.443em 0.716em 0.011em 0px; content: "w"; }
mjx-c.mjx-c1D6F4.TEX-I::before { padding: 0.683em 0.806em 0px 0px; content: "Σ"; }
mjx-c.mjx-c1D456.TEX-I::before { padding: 0.661em 0.345em 0.011em 0px; content: "i"; }
mjx-c.mjx-c3C::before { padding: 0.54em 0.778em 0.04em 0px; content: "<"; }
mjx-c.mjx-c1D44C.TEX-I::before { padding: 0.683em 0.763em 0px 0px; content: "Y"; }
mjx-c.mjx-c7C::before { padding: 0.75em 0.278em 0.249em 0px; content: "|"; }
mjx-c.mjx-c2B::before { padding: 0.583em 0.778em 0.082em 0px; content: "+"; }
mjx-c.mjx-c1D442.TEX-I::before { padding: 0.704em 0.763em 0.022em 0px; content: "O"; }
mjx-c.mjx-c1D450.TEX-I::before { padding: 0.442em 0.433em 0.011em 0px; content: "c"; }
mjx-c.mjx-c1D45C.TEX-I::before { padding: 0.441em 0.485em 0.011em 0px; content: "o"; }
mjx-c.mjx-c1D461.TEX-I::before { padding: 0.626em 0.361em 0.011em 0px; content: "t"; }
mjx-c.mjx-c6C::before { padding: 0.694em 0.278em 0px 0px; content: "l"; }
mjx-c.mjx-c67::before { padding: 0.453em 0.5em 0.206em 0px; content: "g"; }
mjx-c.mjx-c68::before { padding: 0.694em 0.556em 0px 0px; content: "h"; }
mjx-c.mjx-c20::before { padding: 0px 0.25em 0px 0px; content: " "; }
mjx-c.mjx-c72::before { padding: 0.442em 0.392em 0px 0px; content: "r"; }
mjx-c.mjx-c70::before { padding: 0.442em 0.556em 0.194em 0px; content: "p"; }
mjx-c.mjx-c75::before { padding: 0.442em 0.556em 0.011em 0px; content: "u"; }
mjx-c.mjx-c59::before { padding: 0.683em 0.75em 0px 0px; content: "Y"; }
mjx-mtext { display: inline-block; text-align: left; }
mjx-msup { display: inline-block; text-align: left; }
mjx-msqrt { display: inline-block; text-align: left; }
mjx-root { display: inline-block; white-space: nowrap; }
mjx-surd { display: inline-block; vertical-align: top; }
mjx-sqrt { display: inline-block; padding-top: 0.07em; }
mjx-sqrt > mjx-box { border-top: 0.07em solid; }
mjx-sqrt.mjx-tall > mjx-box { padding-left: 0.3em; margin-left: -0.3em; }
mjx-msub { display: inline-block; text-align: left; }
mjx-c.mjx-c1D449.TEX-I::before { padding: 0.683em 0.769em 0.022em 0px; content: "V"; }
mjx-c.mjx-c1D444.TEX-I::before { padding: 0.704em 0.791em 0.194em 0px; content: "Q"; }
mjx-c.mjx-c1D43E.TEX-I::before { padding: 0.683em 0.889em 0px 0px; content: "K"; }
mjx-c.mjx-c41::before { padding: 0.716em 0.75em 0px 0px; content: "A"; }
mjx-c.mjx-c74::before { padding: 0.615em 0.389em 0.01em 0px; content: "t"; }
mjx-c.mjx-c65::before { padding: 0.448em 0.444em 0.011em 0px; content: "e"; }
mjx-c.mjx-c6E::before { padding: 0.442em 0.556em 0px 0px; content: "n"; }
mjx-c.mjx-c69::before { padding: 0.669em 0.278em 0px 0px; content: "i"; }
mjx-c.mjx-c6F::before { padding: 0.448em 0.5em 0.01em 0px; content: "o"; }
mjx-c.mjx-c73::before { padding: 0.448em 0.394em 0.011em 0px; content: "s"; }
mjx-c.mjx-c66::before { padding: 0.705em 0.372em 0px 0px; content: "f"; }
mjx-c.mjx-c6D::before { padding: 0.442em 0.833em 0px 0px; content: "m"; }
mjx-c.mjx-c61::before { padding: 0.448em 0.5em 0.011em 0px; content: "a"; }
mjx-c.mjx-c78::before { padding: 0.431em 0.528em 0px 0px; content: "x"; }
mjx-c.mjx-c22C5::before { padding: 0.31em 0.278em 0px 0px; content: "⋅"; }
mjx-c.mjx-c1D447.TEX-I::before { padding: 0.677em 0.704em 0px 0px; content: "T"; }
mjx-c.mjx-c221A::before { padding: 0.8em 0.853em 0.2em 0px; content: "√"; }
mjx-c.mjx-c2192::before { padding: 0.511em 1em 0.011em 0px; content: "→"; }
mjx-container[jax="CHTML"] { line-height: 0; }
mjx-container [space="1"] { margin-left: 0.111em; }
mjx-container [space="2"] { margin-left: 0.167em; }
mjx-container [space="3"] { margin-left: 0.222em; }
mjx-container [space="4"] { margin-left: 0.278em; }
mjx-container [space="5"] { margin-left: 0.333em; }
mjx-container [rspace="1"] { margin-right: 0.111em; }
mjx-container [rspace="2"] { margin-right: 0.167em; }
mjx-container [rspace="3"] { margin-right: 0.222em; }
mjx-container [rspace="4"] { margin-right: 0.278em; }
mjx-container [rspace="5"] { margin-right: 0.333em; }
mjx-container [size="s"] { font-size: 70.7%; }
mjx-container [size="ss"] { font-size: 50%; }
mjx-container [size="Tn"] { font-size: 60%; }
mjx-container [size="sm"] { font-size: 85%; }
mjx-container [size="lg"] { font-size: 120%; }
mjx-container [size="Lg"] { font-size: 144%; }
mjx-container [size="LG"] { font-size: 173%; }
mjx-container [size="hg"] { font-size: 207%; }
mjx-container [size="HG"] { font-size: 249%; }
mjx-container [width="full"] { width: 100%; }
mjx-box { display: inline-block; }
mjx-block { display: block; }
mjx-itable { display: inline-table; }
mjx-row { display: table-row; }
mjx-row > * { display: table-cell; }
mjx-mtext { display: inline-block; }
mjx-mstyle { display: inline-block; }
mjx-merror { display: inline-block; color: red; background-color: yellow; }
mjx-mphantom { visibility: hidden; }
mjx-assistive-mml { top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); user-select: none; position: absolute !important; padding: 1px 0px 0px !important; border: 0px !important; display: block !important; width: auto !important; overflow: hidden !important; }
mjx-assistive-mml[display="block"] { width: 100% !important; }
mjx-math { display: inline-block; text-align: left; line-height: 0; text-indent: 0px; font-style: normal; font-weight: normal; font-size: 100%; letter-spacing: normal; border-collapse: collapse; overflow-wrap: normal; word-spacing: normal; white-space: nowrap; direction: ltr; padding: 1px 0px; }
mjx-container[jax="CHTML"][display="true"] { display: block; text-align: center; margin: 1em 0px; }
mjx-container[jax="CHTML"][display="true"][width="full"] { display: flex; }
mjx-container[jax="CHTML"][display="true"] mjx-math { padding: 0px; }
mjx-container[jax="CHTML"][justify="left"] { text-align: left; }
mjx-container[jax="CHTML"][justify="right"] { text-align: right; }
mjx-mi { display: inline-block; text-align: left; }
mjx-c { display: inline-block; }
mjx-utext { display: inline-block; padding: 0.75em 0px 0.2em; }
mjx-mo { display: inline-block; text-align: left; }
mjx-stretchy-h { display: inline-table; width: 100%; }
mjx-stretchy-h > * { display: table-cell; width: 0px; }
mjx-stretchy-h > * > mjx-c { display: inline-block; transform: scaleX(1); }
mjx-stretchy-h > * > mjx-c::before { display: inline-block; width: initial; }
mjx-stretchy-h > mjx-ext { overflow: clip visible; width: 100%; }
mjx-stretchy-h > mjx-ext > mjx-c::before { transform: scaleX(500); }
mjx-stretchy-h > mjx-ext > mjx-c { width: 0px; }
mjx-stretchy-h > mjx-beg > mjx-c { margin-right: -0.1em; }
mjx-stretchy-h > mjx-end > mjx-c { margin-left: -0.1em; }
mjx-stretchy-v { display: inline-block; }
mjx-stretchy-v > * { display: block; }
mjx-stretchy-v > mjx-beg { height: 0px; }
mjx-stretchy-v > mjx-end > mjx-c { display: block; }
mjx-stretchy-v > * > mjx-c { transform: scaleY(1); transform-origin: left center; overflow: hidden; }
mjx-stretchy-v > mjx-ext { display: block; height: 100%; box-sizing: border-box; border: 0px solid transparent; overflow: visible clip; }
mjx-stretchy-v > mjx-ext > mjx-c::before { width: initial; box-sizing: border-box; }
mjx-stretchy-v > mjx-ext > mjx-c { transform: scaleY(500) translateY(0.075em); overflow: visible; }
mjx-mark { display: inline-block; height: 0px; }
mjx-mfrac { display: inline-block; text-align: left; }
mjx-frac { display: inline-block; vertical-align: 0.17em; padding: 0px 0.22em; }
mjx-frac[type="d"] { vertical-align: 0.04em; }
mjx-frac[delims] { padding: 0px 0.1em; }
mjx-frac[atop] { padding: 0px 0.12em; }
mjx-frac[atop][delims] { padding: 0px; }
mjx-dtable { display: inline-table; width: 100%; }
mjx-dtable > * { font-size: 2000%; }
mjx-dbox { display: block; font-size: 5%; }
mjx-num { display: block; text-align: center; }
mjx-den { display: block; text-align: center; }
mjx-mfrac[bevelled] > mjx-num { display: inline-block; }
mjx-mfrac[bevelled] > mjx-den { display: inline-block; }
mjx-den[align="right"], mjx-num[align="right"] { text-align: right; }
mjx-den[align="left"], mjx-num[align="left"] { text-align: left; }
mjx-nstrut { display: inline-block; height: 0.054em; width: 0px; vertical-align: -0.054em; }
mjx-nstrut[type="d"] { height: 0.217em; vertical-align: -0.217em; }
mjx-dstrut { display: inline-block; height: 0.505em; width: 0px; }
mjx-dstrut[type="d"] { height: 0.726em; }
mjx-line { display: block; box-sizing: border-box; min-height: 1px; height: 0.06em; border-top: 0.06em solid; margin: 0.06em -0.1em; overflow: hidden; }
mjx-line[type="d"] { margin: 0.18em -0.1em; }
mjx-mrow { display: inline-block; text-align: left; }
mjx-texatom { display: inline-block; text-align: left; }
mjx-mover { display: inline-block; text-align: left; }
mjx-mover:not([limits="false"]) { padding-top: 0.1em; }
mjx-mover:not([limits="false"]) > * { display: block; text-align: left; }
mjx-mn { display: inline-block; text-align: left; }
mjx-mtable { display: inline-block; text-align: center; vertical-align: 0.25em; position: relative; box-sizing: border-box; border-spacing: 0px; border-collapse: collapse; }
mjx-mstyle[size="s"] mjx-mtable { vertical-align: 0.354em; }
mjx-labels { position: absolute; left: 0px; top: 0px; }
mjx-table { display: inline-block; vertical-align: -0.5ex; box-sizing: border-box; }
mjx-table > mjx-itable { vertical-align: middle; text-align: left; box-sizing: border-box; }
mjx-labels > mjx-itable { position: absolute; top: 0px; }
mjx-mtable[justify="left"] { text-align: left; }
mjx-mtable[justify="right"] { text-align: right; }
mjx-mtable[justify="left"][side="left"] { padding-right: 0px !important; }
mjx-mtable[justify="left"][side="right"] { padding-left: 0px !important; }
mjx-mtable[justify="right"][side="left"] { padding-right: 0px !important; }
mjx-mtable[justify="right"][side="right"] { padding-left: 0px !important; }
mjx-mtable[align] { vertical-align: baseline; }
mjx-mtable[align="top"] > mjx-table { vertical-align: top; }
mjx-mtable[align="bottom"] > mjx-table { vertical-align: bottom; }
mjx-mtable[side="right"] mjx-labels { min-width: 100%; }
mjx-mtr { display: table-row; text-align: left; }
mjx-mtr[rowalign="top"] > mjx-mtd { vertical-align: top; }
mjx-mtr[rowalign="center"] > mjx-mtd { vertical-align: middle; }
mjx-mtr[rowalign="bottom"] > mjx-mtd { vertical-align: bottom; }
mjx-mtr[rowalign="baseline"] > mjx-mtd { vertical-align: baseline; }
mjx-mtr[rowalign="axis"] > mjx-mtd { vertical-align: 0.25em; }
mjx-mtd { display: table-cell; text-align: center; padding: 0.215em 0.4em; }
mjx-mtd:first-child { padding-left: 0px; }
mjx-mtd:last-child { padding-right: 0px; }
mjx-mtable > * > mjx-itable > :first-child > mjx-mtd { padding-top: 0px; }
mjx-mtable > * > mjx-itable > :last-child > mjx-mtd { padding-bottom: 0px; }
mjx-tstrut { display: inline-block; height: 1em; vertical-align: -0.25em; }
mjx-labels[align="left"] > mjx-mtr > mjx-mtd { text-align: left; }
mjx-labels[align="right"] > mjx-mtr > mjx-mtd { text-align: right; }
mjx-mtd[extra] { padding: 0px; }
mjx-mtd[rowalign="top"] { vertical-align: top; }
mjx-mtd[rowalign="center"] { vertical-align: middle; }
mjx-mtd[rowalign="bottom"] { vertical-align: bottom; }
mjx-mtd[rowalign="baseline"] { vertical-align: baseline; }
mjx-mtd[rowalign="axis"] { vertical-align: 0.25em; }
mjx-msubsup { display: inline-block; text-align: left; }
mjx-script { display: inline-block; padding-right: 0.05em; padding-left: 0.033em; }
mjx-script > mjx-spacer { display: block; }
mjx-c::before { display: block; width: 0px; }
.MJX-TEX { font-family: MJXZERO, MJXTEX; }
.TEX-B { font-family: MJXZERO, MJXTEX-B; }
.TEX-I { font-family: MJXZERO, MJXTEX-I; }
.TEX-MI { font-family: MJXZERO, MJXTEX-MI; }
.TEX-BI { font-family: MJXZERO, MJXTEX-BI; }
.TEX-S1 { font-family: MJXZERO, MJXTEX-S1; }
.TEX-S2 { font-family: MJXZERO, MJXTEX-S2; }
.TEX-S3 { font-family: MJXZERO, MJXTEX-S3; }
.TEX-S4 { font-family: MJXZERO, MJXTEX-S4; }
.TEX-A { font-family: MJXZERO, MJXTEX-A; }
.TEX-C { font-family: MJXZERO, MJXTEX-C; }
.TEX-CB { font-family: MJXZERO, MJXTEX-CB; }
.TEX-FR { font-family: MJXZERO, MJXTEX-FR; }
.TEX-FRB { font-family: MJXZERO, MJXTEX-FRB; }
.TEX-SS { font-family: MJXZERO, MJXTEX-SS; }
.TEX-SSB { font-family: MJXZERO, MJXTEX-SSB; }
.TEX-SSI { font-family: MJXZERO, MJXTEX-SSI; }
.TEX-SC { font-family: MJXZERO, MJXTEX-SC; }
.TEX-T { font-family: MJXZERO, MJXTEX-T; }
.TEX-V { font-family: MJXZERO, MJXTEX-V; }
.TEX-VB { font-family: MJXZERO, MJXTEX-VB; }
mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c { font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A !important; }
@font-face { font-family: MJXZERO; src: url("lib/fonts/mathjax_zero.woff") format("woff"); }
@font-face { font-family: MJXTEX; src: url("lib/fonts/mathjax_main-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-B; src: url("lib/fonts/mathjax_main-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-I; src: url("lib/fonts/mathjax_math-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-MI; src: url("lib/fonts/mathjax_main-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-BI; src: url("lib/fonts/mathjax_math-bolditalic.woff") format("woff"); }
@font-face { font-family: MJXTEX-S1; src: url("lib/fonts/mathjax_size1-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S2; src: url("lib/fonts/mathjax_size2-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S3; src: url("lib/fonts/mathjax_size3-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S4; src: url("lib/fonts/mathjax_size4-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-A; src: url("lib/fonts/mathjax_ams-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-C; src: url("lib/fonts/mathjax_calligraphic-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-CB; src: url("lib/fonts/mathjax_calligraphic-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-FR; src: url("lib/fonts/mathjax_fraktur-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-FRB; src: url("lib/fonts/mathjax_fraktur-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-SS; src: url("lib/fonts/mathjax_sansserif-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-SSB; src: url("lib/fonts/mathjax_sansserif-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-SSI; src: url("lib/fonts/mathjax_sansserif-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-SC; src: url("lib/fonts/mathjax_script-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-T; src: url("lib/fonts/mathjax_typewriter-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-V; src: url("lib/fonts/mathjax_vector-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-VB; src: url("lib/fonts/mathjax_vector-bold.woff") format("woff"); }
mjx-stretchy-v.mjx-c5B mjx-beg mjx-c::before { content: "⎡"; padding: 1.154em 0.667em 0.645em 0px; }
mjx-stretchy-v.mjx-c5B mjx-ext mjx-c::before { content: "⎢"; width: 0.667em; }
mjx-stretchy-v.mjx-c5B mjx-end mjx-c::before { content: "⎣"; padding: 1.155em 0.667em 0.644em 0px; }
mjx-stretchy-v.mjx-c5B > mjx-end { margin-top: -1.799em; }
mjx-stretchy-v.mjx-c5B > mjx-ext { border-top-width: 1.769em; border-bottom-width: 1.769em; }
mjx-stretchy-v.mjx-c5D mjx-beg mjx-c::before { content: "⎤"; padding: 1.154em 0.667em 0.645em 0px; }
mjx-stretchy-v.mjx-c5D mjx-ext mjx-c::before { content: "⎥"; width: 0.667em; }
mjx-stretchy-v.mjx-c5D mjx-end mjx-c::before { content: "⎦"; padding: 1.155em 0.667em 0.644em 0px; }
mjx-stretchy-v.mjx-c5D > mjx-end { margin-top: -1.799em; }
mjx-stretchy-v.mjx-c5D > mjx-ext { border-top-width: 1.769em; border-bottom-width: 1.769em; }
mjx-c.mjx-c1D436.TEX-I::before { padding: 0.705em 0.76em 0.022em 0px; content: "C"; }
mjx-c.mjx-c28::before { padding: 0.75em 0.389em 0.25em 0px; content: "("; }
mjx-c.mjx-c1D45B.TEX-I::before { padding: 0.442em 0.6em 0.011em 0px; content: "n"; }
mjx-c.mjx-c2C::before { padding: 0.121em 0.278em 0.194em 0px; content: ","; }
mjx-c.mjx-c1D458.TEX-I::before { padding: 0.694em 0.521em 0.011em 0px; content: "k"; }
mjx-c.mjx-c29::before { padding: 0.75em 0.389em 0.25em 0px; content: ")"; }
mjx-c.mjx-c3D::before { padding: 0.583em 0.778em 0.082em 0px; content: "="; }
mjx-c.mjx-c21::before { padding: 0.716em 0.278em 0px 0px; content: "!"; }
mjx-c.mjx-c2212::before { padding: 0.583em 0.778em 0.082em 0px; content: "−"; }
mjx-c.mjx-c1D434.TEX-I::before { padding: 0.716em 0.75em 0px 0px; content: "A"; }
mjx-c.mjx-c1D443.TEX-I::before { padding: 0.683em 0.751em 0px 0px; content: "P"; }
mjx-c.mjx-c1D44E.TEX-I::before { padding: 0.441em 0.529em 0.01em 0px; content: "a"; }
mjx-c.mjx-c2208::before { padding: 0.54em 0.667em 0.04em 0px; content: "∈"; }
mjx-c.mjx-c211D.TEX-A::before { padding: 0.683em 0.722em 0px 0px; content: "R"; }
mjx-c.mjx-c1D706.TEX-I::before { padding: 0.694em 0.583em 0.012em 0px; content: "λ"; }
mjx-c.mjx-c20D7.TEX-V::before { padding: 0.714em 0.5em 0px 0px; content: "→"; }
mjx-c.mjx-c1D463.TEX-I::before { padding: 0.443em 0.485em 0.011em 0px; content: "v"; }
mjx-c.mjx-c33::before { padding: 0.665em 0.5em 0.022em 0px; content: "3"; }
mjx-c.mjx-cD7::before { padding: 0.491em 0.778em 0px 0px; content: "×"; }
mjx-c.mjx-c31::before { padding: 0.666em 0.5em 0px 0px; content: "1"; }
mjx-c.mjx-c32::before { padding: 0.666em 0.5em 0px 0px; content: "2"; }
mjx-c.mjx-c36::before { padding: 0.666em 0.5em 0.022em 0px; content: "6"; }
mjx-c.mjx-c39::before { padding: 0.666em 0.5em 0.022em 0px; content: "9"; }
mjx-c.mjx-c1D44B.TEX-I::before { padding: 0.683em 0.852em 0px 0px; content: "X"; }
mjx-c.mjx-c1D453.TEX-I::before { padding: 0.705em 0.55em 0.205em 0px; content: "f"; }
mjx-c.mjx-c1D465.TEX-I::before { padding: 0.442em 0.572em 0.011em 0px; content: "x"; }
mjx-c.mjx-c1D43A.TEX-I::before { padding: 0.705em 0.786em 0.022em 0px; content: "G"; }
mjx-c.mjx-c222B.TEX-S2::before { padding: 1.36em 0.944em 0.862em 0px; content: "∫"; }
mjx-c.mjx-c221E::before { padding: 0.442em 1em 0.011em 0px; content: "∞"; }
mjx-c.mjx-c1D439.TEX-I::before { padding: 0.68em 0.749em 0px 0px; content: "F"; }
mjx-c.mjx-c1D451.TEX-I::before { padding: 0.694em 0.52em 0.01em 0px; content: "d"; }
</style><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="4_深度学习"><p>4_深度学习</p></h1><div><p><a href="?query=tag:%E8%AE%A1%E7%AE%97%E6%9C%BA" class="tag" target="_blank" rel="noopener">#计算机</a> <a href="?query=tag:%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD" class="tag" target="_blank" rel="noopener">#人工智能</a> <a href="?query=tag:%E5%AE%9E%E7%94%A8" class="tag" target="_blank" rel="noopener">#实用</a> <a href="?query=tag:%E5%B0%8F%E8%AE%B0" class="tag" target="_blank" rel="noopener">#小记</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/638660013/answer/3422970337" rel="noopener" class="external-link" href="https://www.zhihu.com/question/638660013/answer/3422970337" target="_blank">2024年，深度学习，你心目中的top10算法是什么？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://zh.d2l.ai/index.html" rel="noopener" class="external-link" href="https://zh.d2l.ai/index.html" target="_blank">《动手学深度学习》 — 动手学深度学习 2.0.0 documentation</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/mli/paper-reading" rel="noopener" class="external-link" href="https://github.com/mli/paper-reading" target="_blank">GitHub - mli/paper-reading: 深度学习经典、新论文逐段精读</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap" rel="noopener" class="external-link" href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap" target="_blank">GitHub - floodsung/Deep-Learning-Papers-Reading-Roadmap: Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/exacity/deeplearningbook-chinese" rel="noopener" class="external-link" href="https://github.com/exacity/deeplearningbook-chinese" target="_blank">GitHub - exacity/deeplearningbook-chinese: Deep Learning Book Chinese Translation（花书）</a></p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 深度学习的核心是反向传播和梯度下降. SGD yyds Adam yyds </span></li>
<li data-line="1" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 如果把 DL 放在原有 ML 的pipeline 当中，DL 真正革命的是自动化的another word端到端的特征提取和特征筛选。两者结合在一起形成表征学习(representation learning)。 e.g.卷积核，注意力权重</span></li>
<li data-line="2" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> DL 的最重要的特点就是可以提取 hidden features. 通过hidden features 提取然后用 MLP 超强的建模拟合能力完成各种抽象程度超高的任务. 完成一种表征空间映射到另一个空间的建模。 e.g.理解语言图片生成图片视频</span></li>
</ul></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=hvmTCesa30c" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=hvmTCesa30c" target="_blank"># 深度学习简史，从感知机到lenet到vgg到resnet到clip</a></p>
<p><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1SZ421a7v3/?spm_id_from=333.1365.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1SZ421a7v3/?spm_id_from=333.1365.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">何恺明教授在MIT的第一课：卷积神经网络_哔哩哔哩_bilibili</a></p>
<ul>
<li>1957 - 细胞感知机(Perceptron) 只能处理线性问题</li>
<li>1969 - 发现感知机不能处理XOR问题，陷入第一次寒冬</li>
<li>1974 - 第一次发明了反向传播算法，但没有引起注意</li>
<li>1986 - 重新发明了反向传播算法，引发深度学习的热潮</li>
<li>1990 - SVM的发明和深度学习的一些问题(e.g.梯度爆炸)，陷入第二次寒冬</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>1998 - LeNet成功运用在MINST手写数据集, 
<ul>
<li>introduced convolution, pooling, full-connected layer, train by backprop end to end</li>
</ul>
</li>
<li>2009 - ImageNet第一次举办</li>
<li>2012 - AlexNet在ImageNet上大放异彩</li>
<li>2014 - VGG证明网络越深越好</li>
<li>2014 - GoogLeNet 发明Inception 发明多个并行的卷积层和1x1卷积</li>
<li>2015 - ResNet </li>
<li>2016 - DenseNet 每两个Block一个Skip Connection</li>
<li>2017 - Xception改进了Inception将Max Pooling放在网络后部分</li>
<li>2017 - ResNext引入分组卷积(多头机制)</li>
<li>2017 - MobileNets 引入了深度可变卷积</li>
<li>2018 - NasNet 引入了可变多头机制</li>
<li>2019 - EfficentNet 可以灵活改变网络的深度宽度和channel</li>
<li>2021 - CLIP引入对比学习鼓励模型将图像和描述该图像的文本表示靠近</li>
</ul></div></div></div><div class="heading-wrapper"><h2 data-heading="深度学习名人堂" class="heading" id="深度学习名人堂"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>深度学习名人堂</h2><div class="heading-children"><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p>谁是何恺明？他对深度学习做出了什么贡献？</p>
<p> <strong>何恺明（Kaiming He）</strong>是一位在计算机视觉和深度学习领域具有重要影响的科学家。他在深度学习，尤其是在计算机视觉方面的贡献，对推动该领域的发展起到了重要作用。</p>
<p>何恺明最为人熟知的工作之一是他对卷积神经网络（CNNs）结构改进的贡献。他参与开发了几种重要的网络结构和训练方法，包括：</p>
<ol>
<li>
<p><strong>残差网络（Residual Networks, ResNets）</strong>：在2015年的《深度残差网络》论文中，何恺明及其合作者提出了残差网络架构。这一架构通过引入“残差块”来解决深层网络训练难题，使得可以训练出比以往更深的神经网络而不会出现性能下降问题。ResNet极大地推动了图像识别、物体检测等领域的发展，并被广泛应用于商业和研究中。</p>
</li>
<li>
<p><strong>特征金字塔网络（Feature Pyramid Networks, FPNs）</strong>：在目标检测任务中，特征金字塔结构可以有效地处理不同尺寸的物体。FPN通过高效地结合不同层次的特征表示，显著提高了目标检测和分割任务的性能。</p>
</li>
<li>
<p><strong>Mask R-CNN</strong>：这是一种用于目标检测和实例分割任务的框架，基于Faster R-CNN而来，并加入了一个分支以输出物体掩码。Mask R-CNN简单、灵活且性能强大，对实例分割领域产生了深远影响。</p>
</li>
</ol>
<p>除此之外，何恺明还参与了其他多项重要研究工作，并且常常在顶级会议和期刊如CVPR、ICCV、NeurIPS、ECCV等发表论文。他所参与开发的技术已被广泛应用于多个领域，并促进了计算机视觉技术在实际应用中的落地。</p>
<p>何恺明目前是Facebook AI Research（FAIR）团队的一员，在业界享有很高声誉。通过持续地贡献高质量研究成果，何恺明为推动人工智能和机器学习技术向前发展做出了显著贡献。</p></div></div></div><div><p>深度学习领域有什么奠基性的学者? 他们各自都做了什么奠基性的工作?</p></div><div><p> 深度学习领域有很多奠基性的学者，以下是其中的一部分：</p></div><div><ol>
<li data-line="0">
<p>杨立昆（Yann LeCun）：现任Facebook首席人工智能科学家，他在卷积神经网络（Convolutional Neural Networks, CNN）的研发上做出了巨大贡献，这种网络是目前图像识别等领域广泛使用的工具。他的这项工作也为他赢得了2018年的图灵奖。</p>
</li>
<li data-line="2">
<p>吴恩达（Andrew Ng）：吴恩达是斯坦福大学的教授，他的在线课程“机器学习”引导了数百万人进入了这个领域。他还是Google大脑（Google Brain）项目的创始人，该项目对深度学习的商业应用做出了巨大贡献。此外，他还是Coursera的创始人之一。</p>
</li>
<li data-line="4">
<p>杰弗里·辛顿（Geoffrey Hinton）：被誉为“深度学习之父”，他在神经网络和深度学习领域的研究为当前的人工智能热潮打下了基础。他的研究包括反向传播、深度信念网络和长短期记忆网络（LSTM）等。</p>
</li>
<li data-line="6">
<p>Yoshua Bengio：他是加拿大蒙特利尔大学的教授，也是深度学习领域的先驱者之一。他的研究包括反向传播、深度学习和人工神经网络等。</p>
</li>
<li data-line="8">
<p>Ian Goodfellow：他是深度学习领域的重要人物，是生成对抗网络（Generative Adversarial Networks, GANs）的创造者，这是一种能够生成逼真图像的人工智能模型。</p>
</li>
<li data-line="10">
<p>Juergen Schmidhuber：他在循环神经网络（Recurrent Neural Networks, RNNs）和长短期记忆网络（Long Short-Term Memory, LSTM）的研发上做出了重要贡献。</p>
</li>
</ol></div><div><p>以上这些学者的工作都对深度学习领域产生了深远的影响，推动了人工智能的发展。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="基本算法" class="heading" id="基本算法"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>基本算法</h2><div class="heading-children"><div class="heading-wrapper"><h3 data-heading="反向传播 &amp; 梯度优化" class="heading" id="反向传播_&amp;_梯度优化"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>反向传播 &amp; 梯度优化</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;t=322s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;t=322s" target="_blank">The spelled-out intro to neural networks and backpropagation: building micrograd - YouTube</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=i7V1ci9dkxg" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=i7V1ci9dkxg" target="_blank">梯度下降是什么？为什么你不用知道具体原理？ - YouTube</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html" rel="noopener" class="external-link" href="https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html" target="_blank">4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0 documentation</a></p></div><div><ul>
<li data-line="0">前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。</li>
<li data-line="1">反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。</li>
<li data-line="2">在训练深度学习模型时，前向传播和反向传播是相互依赖的。</li>
<li data-line="3">训练比预测需要更多的内存。</li>
<li data-line="4">(现在的)反向传播算法是由保罗·韦贝罗斯在1986年发明的。</li>
<li data-line="5" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 深度学习试图解决的是一个最优化问题，数学里叫<del>凸优化(Convex Optimization)</del>
</span><ul>
<li data-line="6">凸优化是一种特殊类型的优化问题，其中目标函数和约束都是凸的。在数学中，一个函数被称为凸函数，如果其定义域是一个凸集，并且对于定义域中的任何两点，函数在这两点之间的线段上的值总是小于或等于线段端点处的值。这种性质使得凸优化问题相对易于解决，因为它们有一个全局最优解，而且我们可以使用有效的算法找到这个解。</li>
</ul>
</li>
<li data-line="7" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 梯度下降方法是一种常见的用于解决优化问题的迭代方法，特别是在机器学习和深度学习中。在每一步，梯度下降方法都会沿着目标函数的负梯度方向（即函数下降最快的方向）更新参数，直到找到函数的最小值</span></li>
<li data-line="8">对于凸优化问题，梯度下降是一种非常有效的方法，因为我们知道只有一个全局最小值，而且梯度下降总是朝向这个最小值移动。然而，对于非凸函数，梯度下降可能会陷入局部最小值，并不能找到全局最小值。</li>
<li data-line="9" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 反向传播的基本思想是使用链式法则来计算损失函数关于网络参数的梯度。</span></li>
<li data-line="10">训练神经网络的目标通常是通过调整网络的参数（即神经元之间的权重和偏置）来最小化某个损失函数。损失函数衡量的是网络的输出与实际目标值之间的差距。为了找到使损失函数最小化的参数，我们需要知道每个参数的变化如何影响损失函数的值，这就需要计算损失函数关于每个参数的梯度。</li>
<li data-line="11"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>反向传播算法的步骤大致如下：
<ol>
<li data-line="12"><strong>前向传播</strong>：输入样本传递通过网络，生成预测输出，并计算出损失函数的值。</li>
<li data-line="13"><strong>反向传播</strong>：从输出层开始，计算损失函数关于每一层的梯度，并将这些梯度传递到前一层。这就是“反向传播”名字的来源。</li>
<li data-line="14"><strong>权重更新</strong>：使用这些梯度来更新网络的权重和偏置，通常使用一种优化算法，如梯度下降。<br>
这个过程在每个训练样本或者每个训练批次上重复进行，直到网络的性能达到满意的水平。</li>
</ol>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="过拟合 &amp; 欠拟合" class="heading" id="过拟合_&amp;_欠拟合"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>过拟合 &amp; 欠拟合</h3><div class="heading-children"><div><p><img alt="Fetching Title#1050" src="https://zh.d2l.ai/_images/capacity-vs-error.svg" referrerpolicy="no-referrer"></p></div></div></div><div class="heading-wrapper"><h3 data-heading="数值稳定性和参数初始化" class="heading" id="数值稳定性和参数初始化"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>数值稳定性和参数初始化</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html" rel="noopener" class="external-link" href="https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html" target="_blank">4.8. 数值稳定性和模型初始化 — 动手学深度学习 2.0.0 documentation</a></p></div><div><ul>
<li data-line="0">梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。</li>
<li data-line="1">需要用启发式的初始化方法来确保初始梯度既不太大也不太小。</li>
<li data-line="2">ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</li>
<li data-line="3">随机初始化是保证在进行优化前打破对称性的关键。</li>
<li data-line="4">Xavier初始化表明，对于每一层，输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="优化器" class="heading" id="优化器"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>优化器</h3><div class="heading-children"><div><blockquote>
<p>直接 Adma + Cos LR Schedule</p>
</blockquote></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 梯度下降</p>
</span></li>
<li data-line="1">
<p>简单的梯度下降无法处理很多常见，比如损失函数的鞍部</p>
</li>
<li data-line="4" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> 额外技巧</p>
</span></li>
</ul></div><div style="overflow-x: auto;"><table>
<thead>
<tr>
<th>优化器</th>
<th>简单原理</th>
<th>参数</th>
<th>额外技巧</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>SGD</td>
<td>随机挑一部分数据(现在一般是随机挑一个 batch所以也称为mini-batch方法)，按梯度反方向更新参数，固定学习率.</td>
<td>学习率 <code>η</code></td>
<td>bootstrap</td>
<td>小规模数据集，稀疏数据</td>
</tr>
<tr>
<td>Momentum</td>
<td>在SGD基础上考虑历史梯度，加速收敛</td>
<td>学习率 <code>η</code>，动量 <code>γ</code></td>
<td>动量技术，累积历史梯度</td>
<td>大规模深度学习，适用于大多数场景</td>
</tr>
<tr>
<td>Nesterov</td>
<td>在动量基础上提前计算梯度，预知下一步位置</td>
<td>学习率 <code>η</code>，动量 <code>γ</code></td>
<td>Nesterov加速梯度</td>
<td>神经网络，需要快速收敛</td>
</tr>
<tr>
<td>Adagrad</td>
<td>自适应调整各参数的学习率，便于稀疏数据</td>
<td>学习率 <code>η</code>，累加梯度平方</td>
<td>自适应学习率</td>
<td>处理稀疏数据的问题</td>
</tr>
<tr>
<td>Adadelta</td>
<td>RMSprop的一个扩展，它在计算学习率的时候不仅考虑了过去所有梯度的平方的平均值，还对参数更新的变化量也进行平均，从而减少学习率的急剧下降。</td>
<td></td>
<td>自适应学习率</td>
<td></td>
</tr>
<tr>
<td>RMSprop</td>
<td>改进Adagrad，滑动平均梯度平方，防止学习率急剧下降</td>
<td>学习率 <code>η</code>，衰减率 <code>β</code></td>
<td>移动平均梯度平方</td>
<td>神经网络，非稳定梯度的问题</td>
</tr>
<tr>
<td>Adam</td>
<td>结合Momentum和RMSprop，做一阶和二阶矩估计的自适应学习率</td>
<td>学习率 <code>η</code>，动量 <code>β1</code>，衰减率 <code>β2</code></td>
<td>动量和自适应学习率</td>
<td>深度学习中的多数场景</td>
</tr>
<tr>
<td>BFGS</td>
<td>使用梯度信息和近似海森矩阵，更新参数</td>
<td>近似海森矩阵</td>
<td>拟牛顿方法</td>
<td>函数优化，小规模数据</td>
</tr>
<tr>
<td>Newton's Method</td>
<td>使用二阶导数信息（海森矩阵），实现快速收敛</td>
<td>海森矩阵</td>
<td>二阶方法</td>
<td>函数优化，凸问题</td>
</tr>
<tr>
<td>Genetic Algorithm（GA）</td>
<td>启用自然选择原理，通过交叉突变等实现全局搜索</td>
<td>种群大小，交叉率，变异率</td>
<td>演化算法</td>
<td>复杂优化问题，全局搜索</td>
</tr>
<tr>
<td>Particle Swarm Optimization（PSO）</td>
<td>粒子群模拟鸟群狩猎行为，搜索最优解</td>
<td>粒子数，个体和社会学习因子</td>
<td>群体智能方法</td>
<td>多峰值问题，全局最优</td>
</tr>
</tbody>
</table></div><div><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/323747423/answer/2576604040" rel="noopener" class="external-link" href="https://www.zhihu.com/question/323747423/answer/2576604040" target="_blank">如何理解Adam算法(Adaptive Moment Estimation)？ - 知乎</a></p></div><div><p><img alt="lr.png" src="https://cdn.sa.net/2024/04/12/jUk6e3tn9Tq5uLA.png" referrerpolicy="no-referrer"></p></div><div><p><img alt="local_minima.png" src="https://cdn.sa.net/2024/04/12/gGlHIW1atNzVdb3.png" referrerpolicy="no-referrer"></p></div><div><p><img alt="gradient.png" src="https://cdn.sa.net/2024/04/12/47KZPnfkexdgSVJ.png" referrerpolicy="no-referrer"></p></div><div><p><img alt="batch.png" src="https://cdn.sa.net/2024/04/12/xVZaYLTvUhuPDXJ.png" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0">梯度下降</li>
<li data-line="1">SGD</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="线性层 + 激活函数" class="heading" id="线性层_+_激活函数"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>线性层 + 激活函数</h3><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><img src="https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2424009416f21db643e21_Group%20807.jpg" referrerpolicy="no-referrer"></p>
<p>神经网络中为什么要有激活函数?</p>
<p> 神经网络中的激活函数有两个主要作用：</p>
<ol>
<li>
<p><strong>引入非线性因素</strong>：由于大多数真实世界的数据都是非线性的，所以我们需要激活函数来引入非线性因素，使得神经网络可以适应更复杂的数据，提高模型的预测能力。如果没有激活函数，无论神经网络有多少层，其输出都是输入的线性组合，这样的神经网络模型无法处理复杂的数据模式。</p>
</li>
<li>
<p><strong>模拟生物神经元的阈值触发机制</strong>：神经元在接收到的信号强度超过某个阈值时才会被激活，这种机制通过激活函数得以在神经网络中实现。</p>
</li>
</ol>
<p>总的来说，激活函数的存在使得神经网络更具有通用性和适应性，能够处理更复杂的问题。</p></div></div></div><div><p><img src="https://media.licdn.com/dms/image/D4D12AQH2F3GJ9wen_Q/article-cover_image-shrink_720_1280/0/1688885174323?e=2147483647&amp;v=beta&amp;t=gFWxErTLLWBc6iRWDxCBRxkdJ7ob24cmjWZAOuKN9o4" referrerpolicy="no-referrer"></p></div><div><blockquote>
<p>下面所有的激活函数都是为了解决非线性问题，因为如果网络中所有层都是线性的，那么无论网络有多少层，最后都可以被简化为一个线性函数。</p>
</blockquote></div><div><p><a data-tooltip-position="top" aria-label="https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html" rel="noopener" class="external-link" href="https://zh.d2l.ai/chapter_multilayer-perceptrons/mlp.html" target="_blank">4.1. 多层感知机 — 动手学深度学习 2.0.0 documentation</a></p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> ReLU（Rectified Linear Unit) <span style="background:#fff88f">最常用</span></p>
</span><ul>
<li data-line="1"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>优势
<ul>
<li data-line="2">计算简单：ReLU函数只需要判断输入是否大于0，计算复杂度非常低。</li>
<li data-line="3">解决梯度消失问题：在反向传播过程中，ReLU函数的梯度要么为0，要么为1，不会出现梯度消失的问题，有助于深度神经网络的训练。</li>
<li data-line="4">稀疏激活性：ReLU会使部分神经元的输出为0，这样的结果使得神经网络的激活具有稀疏性，并且可以减少神经元之间的依赖。</li>
</ul>
</li>
<li data-line="5"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>劣势
<ul>
<li data-line="6">"死亡"ReLU问题：在训练过程中，有可能部分神经元始终不被激活，导致对应的参数无法更新(传递到激活函数的数值非常不符合正态分布)。一旦出现这种情况，这些神经元将对任何数据的预测都没有贡献，即出现了"死亡"ReLU问题。</li>
<li data-line="7">输出不以0为中心：ReLU函数的输出是非负的，这可能会影响模型的训练效果。</li>
</ul>
</li>
<li data-line="8"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>动机
<ul>
<li data-line="9">ReLU激活函数是为了解决神经网络训练过程中的一些问题而提出的。在ReLU之前，常用的激活函数如sigmoid或tanh，它们在输入值较大或较小时的梯度接近于0，导致梯度下降训练非常缓慢，这就是所谓的梯度消失问题。ReLU激活函数的提出，有效地解决了这个问题。</li>
</ul>
</li>
</ul>
</li>
<li data-line="10" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> Leaky ReLU</p>
</span><ul>
<li data-line="11">原理：Leaky ReLU是对ReLU的改进，解决了神经元“死亡”的问题，当x&lt;0时，有一个小的梯度，而不是完全的0。</li>
<li data-line="12">优点：解决了ReLU的神经元“死亡”问题；相比于ReLU函数，Leaky ReLU全程都有梯度，训练时的收敛速度会比较快。</li>
<li data-line="13">缺点：Leaky ReLU的参数通常都是经验设置，可能需要通过交叉验证来进行选择。</li>
</ul>
</li>
<li data-line="15" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> Parameterized ReLU (pReLU)</p>
</span><ul>
<li data-line="16"><a data-tooltip-position="top" aria-label="https://zh.d2l.ai/chapter_references/zreferences.html#id59" rel="noopener" class="external-link" title="He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). Delving deep into rectifiers: surpassing human-level performance on imagenet classification. Proceedings of the IEEE international conference on computer vision (pp. 1026–1034)." href="https://zh.d2l.ai/chapter_references/zreferences.html#id59" target="_blank">He&nbsp;<em>et al.</em>, 2015</a></li>
<li data-line="17">在负数部分添加了一个线性项</li>
</ul>
</li>
<li data-line="19" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> GELU（Gaussian Error Linear Units）</p>
</span><ul>
<li data-line="20"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>优势
<ul>
<li data-line="21">GELU激活函数在负数部分的处理上，相比ReLU等其他激活函数，提供了更为平滑的梯度，可以缓解梯度消失问题。</li>
<li data-line="22">GELU相比于其他激活函数，如sigmoid和tanh, 在正数部分提供近似线性的映射，这样可以减少模型的复杂度，也使得模型的训练更加稳定。</li>
<li data-line="23">在一些神经网络模型中，<span style="background:#fff88f">如Transformer等，GELU表现出了更好的性能</span>。</li>
</ul>
</li>
<li data-line="24"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>劣势
<ul>
<li data-line="25">GELU的计算复杂度相比ReLU等激活函数要高，因为它包含了tanh和指数运算。</li>
<li data-line="26">虽然GELU在一些模型上有更好的性能，但并非在所有情况下都是最优的。</li>
</ul>
</li>
<li data-line="27"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>动机
<ul>
<li data-line="28">GELU是为了解决ReLU等激活函数在处理负数输入时出现的梯度消失问题，同时保持在正数部分的近似线性映射，以此来提高模型的训练稳定性和性能。</li>
</ul>
</li>
</ul>
</li>
</ul></div><div><blockquote>
<p>Sigmoid和Tanh函数都是早期神经网络中最常用的激活函数，它们的主要目的是为了解决线性函数无法处理的复杂问题，如异或问题（XOR）。同时，由于它们都能将输入值压缩到一定范围内，因此在处理一些需要输出在特定范围内的问题时非常有用。</p>
</blockquote></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">%</span> Sigmoid函数：
</span><ul>
<li data-line="1">原理：Sigmoid函数可以将任何实数映射到（0，1）之间，其输出可以看作概率，用于二分类问题。</li>
<li data-line="2">优点：输出的结果在0和1之间，适合用于输出层，值域范围有限，优化稳定。</li>
<li data-line="3">缺点：存在梯度消失问题，对于深层网络训练困难；计算复杂，训练时间长；输出不以0为中心。</li>
</ul>
</li>
<li data-line="4" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">%</span> Tanh函数
</span><ul>
<li data-line="5">原理：Tanh函数是Sigmoid函数的变体，将实数映射到（-1，1）之间。</li>
<li data-line="6">优点：输出以0为中心，解决了Sigmoid函数的这个问题。</li>
<li data-line="7">缺点：同样存在梯度消失问题，对于深度神经网络存在困难。</li>
</ul>
</li>
<li data-line="8" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">%</span> Softmax函数
</span><ul>
<li data-line="9">原理：Softmax函数可以看做是多分类的Sigmoid，将任意实数映射成为一个概率分布。</li>
<li data-line="10">优点：适合多分类问题。</li>
<li data-line="11">缺点：由于Softmax的输出是一个概率分布，因此其输出节点之间存在依赖关系，这使得模型的解释性变差。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="正则化" class="heading" id="正则化"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>正则化</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6" rel="noopener" class="external-link" href="https://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6" target="_blank">Different Normalization Layers in Deep Learning | by Nilesh Vijayrania | Towards Data Science</a></p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>数据增强(Data Augmentation)</strong>
</span><ul>
<li data-line="1"><strong>原理</strong>：通过对训练数据进行一系列的变换，如旋转、缩放、剪切等，生成新的训练样本，以增加训练数据的多样性，并防止过拟合。</li>
<li data-line="2"><strong>优劣势</strong>：数据增强可以有效地防止过拟合，提高模型的泛化能力，但是数据增强需要花费额外的计算资源。</li>
<li data-line="3"><strong>适用场景</strong>：主要用于图像、音频等领域，尤其是当训练数据较少时。</li>
<li data-line="4"><strong>提出动机</strong>：为了增加训练数据的多样性，防止过拟合。</li>
<li data-line="5"><strong>实际效果</strong>：在实际应用中，数据增强可以有效地防止过拟合，提高模型的泛化能力。</li>
</ul>
</li>
</ul></div><div><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/680237923?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1735297419670257664&amp;utm_source=io.raindrop.raindropio" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/680237923?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1735297419670257664&amp;utm_source=io.raindrop.raindropio" target="_blank"># L2 正则化比大多数人想象的更加神奇</a><br>
<a data-tooltip-position="top" aria-label="https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html" rel="noopener" class="external-link" href="https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html" target="_blank">4.5. 权重衰减 — 动手学深度学习 2.0.0 documentation</a></p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">@</span>  <strong>L1/L2正则化</strong>
</span><ul>
<li data-line="1"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>原理</strong>
<ul>
<li data-line="2">L1和L2正则化是一种用于防止模型过拟合的技术。它们通过在损失函数中添加一个惩罚项来实现。这个惩罚项会随着模型复杂度的增加而增加，从而鼓励模型选择更简单的解释数据的方式。</li>
<li data-line="3">L1正则化: L1正则化的惩罚项是模型系数的绝对值之和，即<code>||w||_1 = Σ|wi|</code>。L1正则化会产生稀疏的权重，即许多权重为0，这使得模型更加简单，易于解释，也有助于特征选择。</li>
<li data-line="4">L2正则化: L2正则化的惩罚项是模型系数的平方和，即 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D6F4 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container></span>。L2正则化会产生较小的权重，但不会使它们为0，这使得模型更加稳定，对于噪声的抗性更强。</li>
</ul>
</li>
<li data-line="5"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>优劣势</strong>
<ul>
<li data-line="6">L1可以产生<strong>稀疏解</strong>，有特征选择作用，但是L1正则化的优化问题非光滑，难以求解。L2可以防止过拟合，但是不能产生稀疏解。</li>
<li data-line="7">L1正则化的优点是可以产生稀疏的模型，有助于特征选择。但是L1正则化的优化问题非光滑，难以求解, 导致可能会忽略一些有用的特征。</li>
<li data-line="8">L2正则化的优点是可以保留所有的特征，对于噪声的抗性更强。缺点是可能会让模型变得过于复杂，不易于解释。</li>
</ul>
</li>
<li data-line="9"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>适用场景</strong>：
<ul>
<li data-line="10">当特征数量很大，且有很多无关的特征时，可以使用L1正则化进行特征选择。<br>
-当特征都有一定的相关性，且希望模型对噪声有较强的抗性时，可以使用L2正则化。</li>
</ul>
</li>
<li data-line="12"><strong>提出动机</strong>：L1和L2正则化的提出是为了防止模型过拟合，通过添加一个惩罚项来限制模型的复杂度，使得模型更加稳健，对于噪声的抗性更强。</li>
<li data-line="13"><strong>实际效果</strong>：在实际应用中，L1/L2正则化可以有效地防止过拟合，提高模型的泛化能力。在某些任务中，L1正则化可以有效地进行特征选择，提高模型的解释性. 在其他任务中，L2正则化可以提高模型的稳定性和抗噪声能力</li>
</ul>
</li>
</ul></div><div><p><a data-tooltip-position="top" aria-label="https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html" rel="noopener" class="external-link" href="https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html" target="_blank">4.6. 暂退法（Dropout） — 动手学深度学习 2.0.0 documentation</a></p></div><div><ul>
<li data-line="0">
<p>暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。</p>
</li>
<li data-line="1">
<p>暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。</p>
</li>
<li data-line="2">
<p>暂退法将活性值ℎ替换为具有期望值ℎ的随机变量。</p>
</li>
<li data-line="3">
<p>暂退法仅在训练期间使用。</p>
</li>
<li data-line="4" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>Dropout</strong></p>
</span><ul>
<li data-line="5"><strong>原理</strong>：在每次迭代训练过程中，随机地将部分神经元的输出设置为0。</li>
<li data-line="6"><strong>优劣势</strong>：Dropout可以有效地防止过拟合，提高模型的泛化能力，但是Dropout也会使得模型的收敛速度变慢。</li>
<li data-line="7"><strong>适用场景</strong>：用于防止神经网络的过拟合。</li>
<li data-line="8"><strong>提出动机</strong>：为了防止神经网络的过拟合，提高模型的泛化能力。</li>
<li data-line="9"><strong>实际效果</strong>：在实际应用中，Dropout可以有效地防止过拟合，提高模型的泛化能力。</li>
</ul>
</li>
<li data-line="12" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>池化层(Pooling layer)</strong></p>
</span><ul>
<li data-line="13">Max Pooling 最大池化, 可检测边缘</li>
<li data-line="14">Average Pooling, 平均池化可平滑图像</li>
<li data-line="15"><strong>原理</strong>：池化层是一种降采样操作，常见的有最大池化和平均池化。最大池化是取滑动窗口内的最大值作为输出，平均池化是取滑动窗口内的平均值作为输出。</li>
<li data-line="16"><strong>优劣势</strong>：池化层可以有效地降低数据的维度，防止过拟合，提高模型的泛化能力，但是也会丢失一部分信息。</li>
<li data-line="17"><strong>适用场景</strong>：用于卷积神经网络中，降低数据的维度，防止过拟合。</li>
<li data-line="18"><strong>提出动机</strong>：为了降低数据的维度，防止过拟合，提高模型的泛化能力。</li>
<li data-line="19"><strong>实际效果</strong>：在实际应用中，池化层可以有效地降低数据的维度，防止过拟合，提高模型的泛化能力。</li>
</ul>
</li>
<li data-line="21" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>Early stopping</strong></p>
</span><ul>
<li data-line="22"><strong>原理</strong>：在训练过程中，当验证集的损失函数值不再下降时，停止训练。</li>
<li data-line="23"><strong>优劣势</strong>：Early stopping可以有效地防止过拟合，提高模型的泛化能力，但是Early stopping的提前停止点的选择有一定的难度。</li>
<li data-line="24"><strong>适用场景</strong>：用于防止神经网络的过拟合。</li>
<li data-line="25"><strong>提出动机</strong>：为了防止神经网络的过拟合，提高模型的泛化能力。</li>
<li data-line="26"><strong>实际效果</strong>：在实际应用中，Early stopping可以有效地防止过拟合，提高模型的泛化能力。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="规范化(BN, LN, IN, GN, RMSNorm)" class="heading" id="规范化(BN,_LN,_IN,_GN,_RMSNorm)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>规范化(BN, LN, IN, GN, RMSNorm)</h3><div class="heading-children"><div>  <img src="https://cdn.sa.net/2024/05/21/x4bsKMrdv1PW8Lm.png" referrerpolicy="no-referrer" style="width: 800px; max-width: 100%;">
  <br>
  <i>Fig. Wu and He, "Group Normalization", ECCV 2018
<br></i></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>批归一化(Batch normalization)</strong></p>
</span><ul>
<li data-line="1">Batch Normalization是由Sergey loffe和Christian Szegedy发明的。这项技术首次出现在2015年的一篇名为"Batch Normalization:Accelerating Deep Network Training by Reducing InternalCovariate Shift"的论文中。这篇论文彻底改变了深度学习训川练过程，特别是在训练非常深的神经网络时。</li>
<li data-line="2"><strong>原理</strong>：在每一层的输出上进行标准化操作，使得输出的均值为0，方差为1。减少所谓的内部协变量偏移(internal covariate shift)</li>
<li data-line="3"><strong>优劣势</strong>：Batch normalization可以加速神经网络的训练，同时也可以防止过拟合，但是Batch normalization对于小批量的样本效果不好。</li>
<li data-line="4"><strong>适用场景</strong>：用于加速神经网络的训练，防止过拟合。</li>
<li data-line="5"><strong>提出动机</strong>：为了加速神经网络的训练，防止过拟合。</li>
<li data-line="6"><strong>实际效果</strong>：在实际应用中，Batch normalization可以有效地加速神经网络的训练，防止过拟合。</li>
<li data-line="7" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 对不同序列中同一位置的数据进行归一化操作, 所有网络都常用</span></li>
</ul>
</li>
<li data-line="10" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>层归一化(Layer Normalization)</strong></p>
</span><ul>
<li data-line="11"><strong>原理</strong>：与批归一化类似，层归一化是对每一层的输出进行归一化，使得输出的均值为0，方差为1，但不同的是，层归一化是在每个样本内部进行，而不是在批次之间。</li>
<li data-line="12"><strong>优劣势</strong>：层归一化可以加速神经网络的训练，同时也可以防止过拟合，而且对于小批量的样本效果也好。</li>
<li data-line="13"><strong>适用场景</strong>：用于加速神经网络的训练，防止过拟合，尤其适用于批量较小的情况。</li>
<li data-line="14"><strong>提出动机</strong>：为了解决批归一化在小批量样本上的问题，提出了层归一化。</li>
<li data-line="15"><strong>实际效果</strong>：在实际应用中，层归一化可以有效地加速神经网络的训练，防止过拟合。</li>
<li data-line="16" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 对相同序列中不同位置的数据进行归一化操作, Transforme常用(便于处理不定长)</span></li>
</ul>
</li>
<li data-line="22" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>添加噪声(Noise Regularization)</strong></p>
</span><ul>
<li data-line="23">
<p><strong>原理</strong>：在训练过程中，将噪声添加到输入数据或网络权重中，以增强模型的鲁棒性，并防止过拟合。</p>
</li>
<li data-line="24">
<p><strong>优劣势</strong>：添加噪声可以有效地防止过拟合，提高模型的泛化能力，但是如果噪声添加过多，可能会影响模型的性能。</p>
</li>
<li data-line="25">
<p><strong>适用场景</strong>：用于防止神经网络的过拟合，尤其适用于在训练数据中存在噪声的情况。</p>
</li>
<li data-line="26">
<p><strong>提出动机</strong>：为了增强模型的鲁棒性，防止过拟合。</p>
</li>
<li data-line="27">
<p><strong>实际效果</strong>：在实际应用中，添加噪声可以有效地防止过拟合，提高模型的泛化能力。</p>
</li>
<li data-line="29" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>标签平滑（Label Smoothing）</strong> <span style="background:#fff88f">常用于分类任务和知识蒸馏</span></p>
</span><ul>
<li data-line="30"><strong>原理</strong>：标签平滑是一种正则化技术，旨在减少过拟合。在训练深度学习模型时，我们通常使用"硬"标签，例如，如果一个样本属于第i类，那么它的标签就是一个全为0的向量，只有第i位为1。标签平滑通过将这些"硬"标签转换为"软"标签，即在标签中引入一些噪声，使得模型不会过于自信，从而提高其泛化能力。</li>
<li data-line="31"><strong>优劣势</strong>：标签平滑的优点是可以防止模型过拟合，提高模型的泛化能力。此外，它还可以防止模型对于一些不确定或错误标签的过度反应。然而，它的缺点是可能会导致模型的训练速度变慢，因为它使得损失函数的优化变得更加复杂。</li>
<li data-line="32"><strong>适用场景</strong>：标签平滑主要用于深度学习领域，尤其是在处理分类问题时，如图像分类、文本分类等。</li>
<li data-line="33"><strong>提出动机</strong>：为了防止模型过拟合，提高模型的泛化能力。</li>
<li data-line="34"><strong>实际效果</strong>：在实际应用中，标签平滑已被证明可以提高模型的泛化能力，特别是在训练大规模深度学习模型时。</li>
</ul>
</li>
</ul>
</li>
<li data-line="40" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> <strong>集成方法（Ensemble Methods）</strong></p>
</span><ul>
<li data-line="41">训练多个模型，然后将它们的输出进行组合，以提高模型的泛化能力。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="数据增强" class="heading" id="数据增强"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>数据增强</h3><div class="heading-children"><div><p>图像任务<br>
<img src="https://cdn.sa.net/2024/04/21/KqJvrHPoaintzkm.png" referrerpolicy="no-referrer"></p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="CNN(卷积神经网络)" class="heading" id="CNN(卷积神经网络)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>CNN(卷积神经网络)</h2><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1SZ421a7v3/?spm_id_from=333.1365.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1SZ421a7v3/?spm_id_from=333.1365.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">何恺明教授在MIT的第一课：卷积神经网络_哔哩哔哩_bilibili</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/407763732/answer/3452197126" rel="noopener" class="external-link" href="https://www.zhihu.com/question/407763732/answer/3452197126" target="_blank">卷积神经网络中的特征到底是什么？它和传统图像处理中的比如轮廓、颜色等特征有什么异同？ - 知乎</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://ytzfhqs.github.io/AAAMLP-CN/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E5%88%86%E5%89%B2%E6%96%B9%E6%B3%95/" rel="noopener" class="external-link" href="https://ytzfhqs.github.io/AAAMLP-CN/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E5%88%86%E5%89%B2%E6%96%B9%E6%B3%95/" target="_blank">图像分类和分割方法 - AAAMLP 中译版</a><br>
里面有Padding 和 dilation 的说明.</p></div><div class="admonition-parent admonition-faq-parent"><div class="callout admonition admonition-faq admonition-plugin " style="--callout-color: 100, 221, 23;" data-callout="faq" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="question-circle" class="svg-inline--fa fa-question-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zM262.655 90c-54.497 0-89.255 22.957-116.549 63.758-3.536 5.286-2.353 12.415 2.715 16.258l34.699 26.31c5.205 3.947 12.621 3.008 16.665-2.122 17.864-22.658 30.113-35.797 57.303-35.797 20.429 0 45.698 13.148 45.698 32.958 0 14.976-12.363 22.667-32.534 33.976C247.128 238.528 216 254.941 216 296v4c0 6.627 5.373 12 12 12h56c6.627 0 12-5.373 12-12v-1.333c0-28.462 83.186-29.647 83.186-106.667 0-58.002-60.165-102-116.531-102zM256 338c-25.365 0-46 20.635-46 46 0 25.364 20.635 46 46 46s46-20.636 46-46c0-25.365-20.635-46-46-46z"></path></svg></div><div class="callout-title-inner admonition-title-content">Faq</div></div><div class="callout-content admonition-content"><p>CNN&nbsp;试图通过卷积来表达不同位置数值之间的关系，学习卷积值也就是学习矩阵里的数值之间的特征，所以适合用在图像里面。因为图像就是一个个的像素点形成的矩阵。</p>
<p>CNN 普遍有什么特性? </p>
<ol>
<li>
<p><strong>局部感受机制</strong> : CNN通过 convolution kernel 捕捉数据的局部特征. 每个kernel只专注于提取某个特定特征(e.g.边缘角点纹理), 有效降低参数量, 提高模型泛化能力.</p>
</li>
<li>
<p><strong>权重共享</strong>：卷积层的神经元使用相同的权重，这意味着在进行前向传播和反向传播时，这些权重会被同时更新。这种权重共享机制大大减少了模型的参数数量，从而减轻了过拟合的风险。</p>
</li>
<li>
<p><strong>池化</strong>：池化层可以对输入的特征图进行下采样，降低特征的维度，同时保留重要的特征信息。</p>
</li>
<li>
<p><strong>平移不变性</strong>：由于权重共享和池化操作，CNN具有平移不变性，即无论目标在图像中的位置如何移动，CNN都可以识别出来。</p>
</li>
<li>
<p><strong>多层结构</strong>：CNN通常由多个卷积层和池化层交替堆叠而成，可以提取出图像的高层次特征。</p>
</li>
<li>
<p><strong>端到端学习</strong>：CNN可以直接从原始像素数据中学习到有用的特征，无需人工设计特征，大大简化了机器学习的流程。</p>
</li>
</ol>
<ul>
<li class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> CNN有没有尺度不变性和旋转不变性</span></li>
<li>CNN（卷积神经网络）本身并不具有尺度不变性和旋转不变性。</li>
<li><strong>尺度不变性</strong>：CNN对于输入图片的尺寸是敏感的，如果输入图片的尺度（大小）发生变化，那么CNN可能无法正确地识别出图片中的对象。这是因为CNN在训练过程中，是对固定尺寸的图片进行学习的，如果输入图片的尺寸改变了，那么CNN的卷积核可能无法正确地匹配图片中的特征。</li>
<li><strong>旋转不变性</strong>：同样，CNN对于输入图片的旋转也是敏感的。如果输入图片被旋转了一定的角度，那么CNN可能无法正确地识别出图片中的对象。这是因为CNN在训练过程中，是假设所有的输入图片都是正向（未旋转）的，如果输入图片被旋转了，那么CNN的卷积核也可能无法正确地匹配图片中的特征。</li>
</ul>
<p>然而，虽然CNN本身不具有尺度和旋转不变性，但是我们可以通过一些方法来提高CNN对尺度和旋转的鲁棒性，例如数据增强（对训练图片进行缩放和旋转），或者在CNN中使用一些特殊的卷积层（例如空间变换网络层）来学习图片的尺度和旋转变换。</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=YkXSKZIQ5oo" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=YkXSKZIQ5oo" target="_blank">如何用深度学习提取图片特征？这样做有什么好处？ - YouTube</a></p>
<p><img src="https://cdn.sa.net/2024/04/15/n5zxyVfGKPI7FLC.png" referrerpolicy="no-referrer"></p>
<p><strong>介绍如何使用预训练深度学习进行特征提取</strong>:</p>
<ul>
<li>上图是VGG16. 预训练的话需要在7*515*512层截获, 进行特征输出</li>
<li></li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="LeNet(鼻祖) - 1998" class="heading" id="LeNet(鼻祖)_-_1998"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>LeNet(鼻祖) - 1998</h3><div class="heading-children"><div class="admonition-parent admonition-warning-parent"><div class="callout admonition admonition-warning admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="warning" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Warning</div></div><div class="callout-content admonition-content heading-wrapper"><h3 data-heading="LeNet 是什么? 对深度学习的发展有什么贡献?" class="heading" id="LeNet_是什么?_对深度学习的发展有什么贡献?"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>LeNet 是什么? 对深度学习的发展有什么贡献?</h3>
<p> LeNet(5层)是一种早期的卷积神经网络，由Yann LeCun于1998年提出，主要用于手写数字识别和机器阅读等任务。LeNet的出现使得神经网络能够处理更复杂的图像识别任务，是深度学习领域的一个重要里程碑。</p>
<p>LeNet对深度学习的发展贡献主要体现在以下几个方面：</p>
<ol>
<li>
<p>网络结构设计：LeNet提出了一种新的网络结构，引入了卷积层、池化层、全连接层的概念，使得神经网络可以更好地处理图像等高维数据。</p>
</li>
<li>
<p>参数共享：LeNet提出了参数共享的概念，大大减少了网络的参数数量，使得神经网络可以在有限的计算资源下处理更大规模的数据。</p>
</li>
<li>
<p>激活函数：LeNet提出了使用S型激活函数，使得神经网络可以学习和表示更复杂的函数关系。</p>
</li>
<li>
<p>反向传播：LeNet第一次使用了反向传播算法来训练神经网络，为后来的深度学习算法提供了重要的参考。</p>
</li>
<li>
<p>实践应用：LeNet在手写数字识别等任务上取得了显著的效果，证明了深度学习在实际应用中的有效性。</p>
</li>
</ol>
<p>总的来说，LeNet的出现对深度学习的发展起到了推动作用，为后来的深度学习算法提供了重要的理论基础和实践经验。</p><div class="heading-children"></div></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="AlexNet(重启深度学习) - 2012" class="heading" id="AlexNet(重启深度学习)_-_2012"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>AlexNet(重启深度学习) - 2012</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener" class="external-link" href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" target="_blank">ImageNet Classification with Deep Convolutional Neural Networks" by Alex Krizhevsky, Ilya Sutskever, and Geoffrey H. Hinton (2012)</a></p></div><div class="admonition-parent admonition-warning-parent"><div class="callout admonition admonition-warning admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="warning" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Warning</div></div><div class="callout-content admonition-content heading-wrapper"><p><img src="https://cdn.sa.net/2024/04/04/WUoK8NSL6svmPwj.png" referrerpolicy="no-referrer"></p>
<h3 data-heading="AlexNet 是什么? 对深度学习的发展有什么贡献?" class="heading" id="AlexNet_是什么?_对深度学习的发展有什么贡献?">AlexNet 是什么? 对深度学习的发展有什么贡献?</h3>
<p> AlexNet是一个在深度学习和计算机视觉领域非常知名的卷积神经网络模型，由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton在2012年提出，并在当年的ImageNet竞赛中取得了突破性的成果。</p>
<p>AlexNet的主要贡献在于：</p>
<ol>
<li>
<p>首次证明了深度学习在<strong>大规模数据集</strong>上的有效性：AlexNet在ImageNet数据集上获得了当时最好的结果，这个数据集包含了1000个类别，120万张训练图像，验证和测试各有5万张图像。</p>
</li>
<li>
<p>推动了深度学习在计算机视觉领域的应用：AlexNet的成功引起了工业界和学术界的广泛关注，从而推动了深度学习在计算机视觉领域的广泛应用。</p>
</li>
<li>
<p>网络结构创新：AlexNet引入了<strong>ReLU（Rectified Linear Unit）激活函数</strong>，相比于传统的sigmoid等激活函数，ReLU在深度网络中有更好的性能和更快的训练速度。此外，AlexNet还首次引入了<strong>Dropout</strong>技术，有效防止了模型过拟合。</p>
</li>
<li>
<p><strong>利用GPU加速训练</strong>：AlexNet是首个大规模利用GPU加速训练的深度网络模型，这对于后续深度学习模型的训练有重要影响。</p>
</li>
<li>
<p>证明了<strong>数据增强</strong>的有效性.</p>
</li>
<li>
<p>比LeNet的网络宽</p>
</li>
<li>
<p>有一点分组卷积的意思, 但是这是由于直接卷积当时的显卡放不下. 可视化实验发现, 上下两组卷积学习到了不一样的东西.</p>
</li>
</ol></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=Z8tNqCrHRyM" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=Z8tNqCrHRyM" target="_blank">【如何读论文？】2012年让深度学习起死回生的开山之作：AlexNet - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1311.2901" rel="noopener" class="external-link" href="https://arxiv.org/abs/1311.2901" target="_blank">[1311.2901] Visualizing and Understanding Convolutional Networks - 2013</a></p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="ZfNet(第一次可视化隐藏层) - 2013" class="heading" id="ZfNet(第一次可视化隐藏层)_-_2013"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>ZfNet(第一次可视化隐藏层) - 2013</h3><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p>ZFNet（Zeiler and Fergus Network）是一种卷积神经网络（Convolutional Neural Network，简称CNN），由Matthew Zeiler和Rob Fergus在2013年提出。ZFNet在2013年的ImageNet竞赛中取得了第一名，证明了其在图像分类任务上的有效性。</p>
<p>ZFNet的主要贡献包括：</p>
<ol>
<li>
<p><strong>提出了反向可视化的方法</strong>：ZFNet的作者提出了一种名为反向可视化的方法，能够帮助我们理解并可视化CNN中间层的特征。这种方法通过将每一层的特征映射回像素空间，可以让我们看到网络在每一层中学习到的内容。这种方法对于理解深度学习模型的内部工作机制非常有帮助，特别是对于理解模型的错误和不足。</p>
</li>
<li>
<p><strong>对AlexNet进行了改进</strong>：ZFNet实际上是对2012年ImageNet竞赛冠军AlexNet的一个改进。ZFNet的作者通过他们的反向可视化方法发现，AlexNet的第一层使用的11x11的卷积核可能过大，导致了一些信息的丢失。因此，他们将第一层的卷积核大小改为7x7，并对其他一些参数进行了调整，从而改进了模型的性能。</p>
</li>
</ol>
<p>在深度学习发展史上，ZFNet有着重要的地位。首先，ZFNet的成功进一步证明了深度学习，特别是CNN在图像分类任务上的有效性。其次，ZFNet的<strong>反向可视化方法</strong>对于深度学习的理解和发展有着重要的影响。这种方法不仅帮助我们理解了深度学习模型的内部工作机制，也为后续的模型改进和设计提供了有价值的指导。</p>
<p>至于对模型可视化的贡献，ZFNet的反向可视化方法开创了深度学习模型可视化的新范式。这种方法能够将抽象的特征映射回像素空间，让我们可以直观地看到网络在每一层中学习到的内容。这对于理解深度学习模型的内部工作机制，特别是理解模型的错误和不足，有着极大的帮助。此外，这种方法也为后续的模型可视化研究提供了一个重要的基础。</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=ihs2JbTnhc4&amp;t=6s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=ihs2JbTnhc4&amp;t=6s" target="_blank">模型可视化开山之作ZFNet，一篇文章开创了多少新领域？！ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1311.2901" rel="noopener" class="external-link" href="https://arxiv.org/abs/1311.2901" target="_blank">Visualizing and Understanding Convolutional Networks</a></p>
<p><img src="https://cdn.sa.net/2024/04/14/QSarIYEXANu87J2.png" referrerpolicy="no-referrer"><br>
<img src="https://cdn.sa.net/2024/04/14/jNFTnZa9Oeo8SpM.png" referrerpolicy="no-referrer"></p>
<p>文章灵感来源AlexNet. </p>
<p>贡献: </p>
<ul>
<li>第一次发现 小 conv kernel 效果会好.</li>
<li>为了解释其效果, 就用反向可视化用CNN中间层. 为之后的可视化方法比如GradCAM提供启发. </li>
<li>展示了CNN的效果类似于高通滤波器的近似“边缘检测”</li>
<li>发现训练不足的话, 对深层的影响比浅层要大. </li>
<li>第一次使用对输入图像Mask, 进行消融实验.</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="分割网络(R-CNN, SSD, YOLO..)" class="heading" id="分割网络(R-CNN,_SSD,_YOLO..)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>分割网络(R-CNN, SSD, YOLO..)</h3><div class="heading-children"><div><p align="center"></p></div><div><img alt="alt text" src="https://cdn.sa.net/2024/04/11/r2YpvkcWhUu94E5.png" referrerpolicy="no-referrer" style="width: 800px; max-width: 100%;"></div><div><br></div><div><p><i>Fig. 分割网络发展一览</i></p></div><div></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> R-CNN </span></li>
<li data-line="1">Region-based Convolution Neural Networks(R-CNN) </li>
<li data-line="2" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> SSD</span></li>
<li data-line="3" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> YOLO </span></li>
<li data-line="4" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> Segment Anything</span></li>
<li data-line="5" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> RetinaNet  经典人脸识别</span></li>
</ul></div><div class="admonition-parent admonition-attention-parent"><div class="callout admonition admonition-attention admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="attention" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Attention</div></div><div class="callout-content admonition-content heading-wrapper"><h3 data-heading="历史上的分割网路" class="heading" id="历史上的分割网路"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>历史上的分割网路</h3>
<h3 data-heading="R-CNN 是什么? 对深度学习的发展有什么贡献?" class="heading" id="R-CNN_是什么?_对深度学习的发展有什么贡献?">R-CNN 是什么? 对深度学习的发展有什么贡献?</h3>
<p>R-CNN，全称是 Region-based Convolutional Neural Networks，是一种区域卷积神经网络，用于目标检测任务。它是一种两阶段的目标检测器，首先对图像进行区域提议，然后对这些提议进行分类和位置修正。</p>
<p>R-CNN的主要贡献在于，它首次将深度学习用于目标检测任务，大幅度提高了目标检测的准确性。在此之前，目标检测主要依赖于手工设计的特征和简单的分类器，而R-CNN通过学习得到的特征和复杂的分类器，能够更好地处理各种各样的图像和目标。</p>
<p>此外，R-CNN也为后续的深度学习目标检测算法，如Fast R-CNN，Faster R-CNN等提供了基础和灵感。这些算法进一步优化了R-CNN的性能和速度，使得深度学习在目标检测任务上的应用更加广泛。</p>
<p>成为 目标检测(打框) 和 异常检测的主干网络.</p><div class="heading-children"></div></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=sHZVPHVQg9E" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=sHZVPHVQg9E" target="_blank">为RCNN奠基的工作，正式开启深度学习的大门！ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" rel="noopener" class="external-link" href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" target="_blank">huppelen.nl/publications/selectiveSearchDraft.pdf</a></p>
<p><img alt="aeZ5jmFU7goEnfq.png" src="https://cdn.sa.net/2024/04/14/aeZ5jmFU7goEnfq.png" referrerpolicy="no-referrer"></p>
<ul>
<li>监督学习. 训练样本首先(<del>随机</del>某种方式)生成了很多 anchor </li>
<li>使用 SVM 结合 HOG features 进行训练</li>
<li>然后重复迭代消除随机生成的 anchor 和 label 接近</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=ikq128JdQOI" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=ikq128JdQOI" target="_blank">开山之作RCNN做了什么？什么是图像分割、语义分割？</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1311.2524" rel="noopener" class="external-link" href="https://arxiv.org/abs/1311.2524" target="_blank">[1311.2524] Rich feature hierarchies for accurate object detection and semantic segmentation</a></p>
<p><img alt="lc5TbFJE6IYoyHk.png" src="https://cdn.sa.net/2024/04/14/lc5TbFJE6IYoyHk.png" referrerpolicy="no-referrer"><br>
<img src="https://cdn.sa.net/2024/04/14/bz4R8svkHMarj6G.png" referrerpolicy="no-referrer"><br>
<img alt="kpO5gQi7EGdTzUY.png" src="https://cdn.sa.net/2024/04/14/kpO5gQi7EGdTzUY.png" referrerpolicy="no-referrer"></p>
<p>R- CNN基本上和上面的流程一样. 不过使用CNN替换了SVM.<br>
由于CNN的特征提取能力更强, 所以效果也好很多. </p>
<ol>
<li>用一定方式在原图上进行生成anchor </li>
<li>将anchor里的对象用CNN进行label的预测(class prediction)</li>
<li>判断anchor是否合适(bindingbox prediction)</li>
</ol></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=4VN5ZEXXByk" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=4VN5ZEXXByk" target="_blank">RCNN最强优化：Faster RCNN究竟做了什么？ - YouTube</a></p>
<p><img alt="Jyw1XW2cTqMktxA.png" src="https://cdn.sa.net/2024/04/21/Jyw1XW2cTqMktxA.png" referrerpolicy="no-referrer"></p>
<ul>
<li>Fast R-CNN  是对R-CNN的性能优化</li>
<li>主要优化方式是使用一个名为 ROI Pooling的方式在卷积后图像上生成achor而不是在原图上</li>
<li>由于ROI Pooling对小物体的检测能力欠缺, 就发明了faster R-CNN 使用了更加高级的ROI Pooling方法</li>
</ul>
<p><img src="https://cdn.sa.net/2024/04/21/FkSiVCK5UatNeLv.png" referrerpolicy="no-referrer"></p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=_DtHS5wzPp8&amp;t=30s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=_DtHS5wzPp8&amp;t=30s" target="_blank">神操作！计算快了2000倍！空间金字塔池化做了什么？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1406.4729" rel="noopener" class="external-link" href="https://arxiv.org/abs/1406.4729" target="_blank">[1406.4729] Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition -2014</a></p>
<p><img alt="ZMFVJjKtlxNy8oa.png" src="https://cdn.sa.net/2024/04/18/ZMFVJjKtlxNy8oa.png" referrerpolicy="no-referrer"></p>
<ul>
<li>Kaiming He 在 2014 的论文</li>
<li>提出了一个名为<strong>SPP-net</strong>的网络, 提出了SPP-net 算法</li>
<li>网络中使用了一个空间金字塔池可以使其物体变形, <strong>图像的大小/比例</strong> 有很强的鲁棒性</li>
<li>因为它在池化层中引入了多个尺度的空间金字塔池化，从而可以处理不同尺度的输入<br>
图像。</li>
<li>整幅图像中只计算一次特征映射避免了重复参数, 允许网络处理输入图像的任意形状和大小</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=lJgwXCyjCQc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=lJgwXCyjCQc" target="_blank">传奇大神何凯明完美作品Mask RCNN，如何达到图片标注分割算法的顶级结果？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1703.06870" rel="noopener" class="external-link" href="https://arxiv.org/abs/1703.06870" target="_blank">[1703.06870] Mask R-CNN</a></p>
<p><img src="https://cdn.sa.net/2024/04/21/FbuPlgcD4vqmehI.png" referrerpolicy="no-referrer"></p>
<ul>
<li>Kaiming He 在2017的作品</li>
<li>Mask R-CNN 借鉴 Fast R-CNN 和 Faster R-CNN的优点, 并增加FCN的特点. </li>
<li>使用双线性插值减缓了Fast R-CNN 上, 暴力切割物体边缘的去缺点(RoI Align)</li>
<li class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 到现在还在用</span></li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="VGG(纯CNN最深网络) - 2014" class="heading" id="VGG(纯CNN最深网络)_-_2014"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>VGG(纯CNN最深网络) - 2014</h3><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=5zDFIe4k1Gw" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=5zDFIe4k1Gw" target="_blank">【博士Vlog】VGG是什么？为什么是卷积神经网络的巅峰之作？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1409.1556" rel="noopener" class="external-link" href="https://arxiv.org/abs/1409.1556" target="_blank">[1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition</a></p>
<p><img alt="dTwEcHBs56pGiMx.png" src="https://cdn.sa.net/2024/04/16/dTwEcHBs56pGiMx.png" referrerpolicy="no-referrer"><br>
VGG-16</p>
<ul>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>非常深
<ul>
<li>共19层</li>
<li>PS: 工程上使得网络变宽比变深更难</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>非常简单
<ul>
<li>只有  <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">卷</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">积</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">核</mjx-utext></mjx-mi><mjx-mi class="mjx-i"><mjx-utext variant="italic" style="font-family: MJXZERO, serif; font-style: italic;">，</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">步</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">长</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">为</mjx-utext></mjx-mi><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-utext variant="italic" style="font-family: MJXZERO, serif; font-style: italic;">，</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">填</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">充</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">为</mjx-utext></mjx-mi><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-utext variant="italic" style="font-family: MJXZERO, serif; font-style: italic;">，</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">所</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">有</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">的</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">池</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">化</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">层</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">都</mjx-utext></mjx-mi><mjx-mi class="mjx-n"><mjx-utext variant="normal" style="font-family: MJXZERO, serif;">是</mjx-utext></mjx-mi><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-math></mjx-container></span></li>
<li>然后简单模块重复</li>
<li>发现用小卷积核效果很好, 可能是防止 图 不要快速变得太小</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>效果好
<ul>
<li>网络越深越好</li>
</ul>
</li>
<li>但是参数数量较大，计算量大</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>用了stage-wise training
<ul>
<li>VGG-11, 13, <strong>16</strong>, <strong>19</strong> (现在只有16和19常用)</li>
</ul>
</li>
<li>适合迁移学习</li>
</ul></div></div></div><div class="admonition-parent admonition-hint-parent"><div class="callout admonition admonition-hint admonition-plugin " style="--callout-color: 0, 191, 165;" data-callout="hint" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="fire" class="svg-inline--fa fa-fire fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M216 23.86c0-23.8-30.65-32.77-44.15-13.04C48 191.85 224 200 224 288c0 35.63-29.11 64.46-64.85 63.99-35.17-.45-63.15-29.77-63.15-64.94v-85.51c0-21.7-26.47-32.23-41.43-16.5C27.8 213.16 0 261.33 0 320c0 105.87 86.13 192 192 192s192-86.13 192-192c0-170.29-168-193-168-296.14z"></path></svg></div><div class="callout-title-inner admonition-title-content">Hint</div></div><div class="callout-content admonition-content"><p>深度神经网络在训练过程中可能会遇到梯度爆炸的问题，这通常发生在梯度的值变得非常大，以至于更新的权重值导致网络不稳定。以下是一些常用的解决方法：</p>
<ol>
<li><strong>梯度裁剪（Gradient Clipping）</strong>：这是一种直接的方法，可以防止梯度值过大。基本思想是设置一个阈值，当梯度的值超过这个阈值时，就将其裁剪到这个阈值。这可以防止梯度爆炸，但不能解决梯度消失的问题。</li>
<li><strong>权重初始化（Weight Initialization）</strong>：合适的权重初始化可以在一定程度上防止梯度爆炸或梯度消失。例如，Xavier初始化和He初始化是两种针对不同类型的激活函数（如sigmoid和ReLU）设计的初始化方法。</li>
<li><strong>批量归一化（Batch Normalization）</strong>：批量归一化可以使网络中各层的输入保持相同的分布，这样可以稳定训练过程，防止梯度爆炸。</li>
<li><strong>使用适当的优化器（Optimizers）</strong>：某些优化器，如Adam、RMSprop等，可以自适应地调整学习率，从而避免梯度过大或过小的问题。</li>
<li><strong>残差连接（Residual Connections）</strong>：在深度神经网络中使用残差连接可以防止梯度消失和爆炸的问题，因为残差连接可以直接将梯度传递到较浅的层。</li>
<li><strong>正则化（Regularization）</strong>：L1和L2正则化可以防止权重变得过大，从而在一定程度上防止梯度爆炸。</li>
</ol>
<p>这些方法可以单独使用，也可以结合使用，具体取决于具体的任务和模型。</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="GoogLeNet / Inception(探索各种trick) - 2014" class="heading" id="GoogLeNet_/_Inception(探索各种trick)_-_2014"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>GoogLeNet / Inception(探索各种trick) - 2014</h3><div class="heading-children"><div><p>Going Deeper with Convolutions" by Szegedy et al. (2014)<br>
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20191202190625/modified.png" referrerpolicy="no-referrer"></p></div><div><p>GoogLeNet(非正式名称)也被称为Inception v1，是一种深度卷积神经网络, 名字来源于网络中的一种特殊结构——Inception模块，它在2014年的ImageNet大规模视觉识别挑战赛（ILSVRC）上取得了很好的成绩。GoogLeNet的主要创新点包括：</p></div><div><ol>
<li data-line="0">
<p><strong>Inception模块</strong>：GoogLeNet引入了Inception模块，这是一种并行连接多个不同类型的卷积核和池化操作的结构。Inception模块的设计理念是让<strong>网络自己决定使用哪种卷积核大小</strong>，或者<strong>是否使用全连接层。通过这种方式，网络可以在不同的尺度上学习特征，从而提高模型的表现力</strong>。</p>
<p>动机：在实际的图像中，有些重要的特征可能是局部的，有些可能是全局的，有些可能在中间尺度。通过使用不同大小的卷积核，我们可以让网络在不同的尺度上学习特征，从而更好地处理这种多尺度的问题。</p>
</li>
<li data-line="4">
<p><strong>深度和宽度的平衡</strong>：GoogLeNet通过增加网络的深度和宽度，提高了模型的表现力。但是，与此同时，GoogLeNet还使用了1x1的卷积核（也被称为点卷积）来减少参数的数量，从而降低了过拟合的风险，并提高了计算效率。</p>
<p>动机：深度神经网络有更强的表现力，但是也更容易过拟合，而且计算效率也较低。通过使用1x1的卷积核，我们可以在保持网络深度的同时，降低参数的数量，从而降低过拟合的风险，并提高计算效率。</p>
</li>
<li data-line="8">
<p><strong>辅助分类器</strong>：GoogLeNet在网络的中间层也添加了分类器，用于进行辅助的分类任务。这些辅助分类器的输出会在训练过程中与主分类器的输出一起被考虑，但是在测试过程中会被忽略。</p>
<p>动机：深度神经网络的一个常见问题是梯度消失，这会导致网络的前面几层难以训练。通过在网络的中间层添加辅助分类器，我们可以让这些层也接收到直接的反馈信号，从而缓解梯度消失的问题。</p>
</li>
</ol></div><div><p>这些创新点使GoogLeNet在当时的图像分类任务上表现优秀，并对后续的深度学习研究产生了深远影响。</p></div><div><p>随着研究的深入，Google后续还提出了Inception v2、Inception v3等更先进的版本。这些新版本在原有的Inception网络基础上进行了一些改进，例如引入了<strong>批量归一化（Batch Normalization）</strong>、<strong>分解卷积（Factorization into smaller convolutions）</strong>, Label Smoothing 等技术，以提高网络的性能和效率。<br>
Inception v4 添加了residual learning.</p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> <strong>Inception V1（GoogLeNet）</strong>
</span><ul>
<li data-line="1">这是Inception系列的第一个版本，它首次提出了Inception模块，这是一种并行结构，包含了不同尺寸的卷积和池化操作。Inception模块的设计目标是在保持网络深度和宽度的同时，提高网络的计算效率。此外，GoogLeNet还引入了辅助分类器（auxiliary classifiers）来解决梯度消失问题，以及全局平均池化（global average pooling）来替代全连接层，减少了模型的参数数量。</li>
</ul>
</li>
<li data-line="2" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> <strong>Inception V2和V3</strong>
</span><ul>
<li data-line="3">这两个版本的主要贡献是引入了两种新的技术：Batch Normalization（BN）和Factorized Convolutions。BN可以加速模型的训练，同时还可以起到一定的正则化效果。Factorized Convolutions则是一种将大的卷积核分解为多个小的卷积核的方法，这可以减少模型的计算复杂性，同时保持模型的性能。</li>
</ul>
</li>
<li data-line="4" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> <strong>Inception V4和Inception-ResNet</strong>：
</span><ul>
<li data-line="5">这两个版本的主要贡献是将ResNet的残差连接引入到Inception模块中。这种设计可以进一步提高模型的深度，同时避免梯度消失问题。此外，Inception V4还引入了一种新的Inception模块，这种模块包含了更复杂的并行结构，可以进一步提高模型的性能。</li>
</ul>
</li>
</ul></div><div><p>总的来说，Inception系列的主要贡献在于提出了一种新的网络架构（Inception模块），这种架构可以在保持网络深度和宽度的同时，提高网络的计算效率。此外，Inception系列还引入了许多新的技术，如BN、Factorized Convolutions和残差连接，这些技术都对深度学习的发展产生了深远影响。</p></div><div class="admonition-parent admonition-note-parent"><div class="callout admonition admonition-note admonition-plugin " style="--callout-color: 68, 138, 255;" data-callout="note" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="pencil-alt" class="svg-inline--fa fa-pencil-alt fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Note</div></div><div class="callout-content admonition-content"><p>正则化(normalization) 在深度学习中非常重要的技术.</p>
<p>早在LeNet就提到正则化可以帮助训练.</p>
<p>主要的作用是防止模型过拟合和帮助训练(初始化&amp;加速训练).</p>
<p>正则化技术有很多种，以下是一些常见的类型：</p>
<ol>
<li><strong>权重衰减（L2正则化）</strong>：这是最常见的正则化技术之一。它通过在损失函数中添加一个与模型参数的平方成正比的项，来鼓励模型使用较小的权重。这可以防止模型过于依赖某个特定的特征，从而提高模型的泛化能力。</li>
<li><strong>L1正则化</strong>：L1正则化类似于权重衰减，但是它添加的是与模型参数的绝对值成正比的项。这会导致模型的一些权重变为0，从而实现特征选择的效果。</li>
<li><strong>Dropout</strong>：这是一种在训练过程中随机关闭一部分神经元的技术。通过这种方式，模型需要学习如何在缺失一部分信息的情况下进行预测，这可以增强模型的鲁棒性，并防止过拟合。</li>
<li><strong>批量归一化（Batch Normalization）</strong>：虽然批量归一化主要是用来解决深度网络中的内部协变量偏移问题，但是它也具有一定的正则化效果。</li>
<li><strong>数据增强</strong>：通过对训练数据进行一些随机的变换（例如旋转、缩放、剪裁等），我们可以增加模型的训练数据，从而防止过拟合。</li>
</ol>
<p>这些正则化技术可以单独使用，也可以组合使用，以根据具体的任务和数据来调整模型的复杂性</p></div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=WyBP4r-l-kc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=WyBP4r-l-kc" target="_blank">大模型Xception为什么常用？主要用来做什么？ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1610.02357" rel="noopener" class="external-link" href="https://arxiv.org/abs/1610.02357" target="_blank">[1610.02357] Xception: Deep Learning with Depthwise Separable Convolutions</a></p>
<p>Inception的极限优化.</p>
<p>Xception是一种深度学习模型，由Google的研究员François Cholletz在2016年开发。Xception是"Extreme Inception"的缩写，这个模型的设计灵感来源于Inception模块，但是它对Inception模块进行了一种更极端的解释。</p>
<p>Xception的主要贡献是提出了一种新的卷积操作，即深度可分离卷积（Depthwise Separable Convolution）。这种卷积操作包含两个步骤：深度卷积（Depthwise Convolution）和点卷积（Pointwise Convolution）。</p>
<p>深度卷积是对每一个输入通道进行单独的卷积操作，而点卷积则是用1x1的卷积核对所有的输入通道进行卷积。深度可分离卷积的设计目标是在减少模型的计算复杂性的同时，保持模型的性能。</p>
<p>除了深度可分离卷积，Xception还提出了一种新的网络架构，这种架构完全放弃了Inception模块中的并行结构，而是将所有的操作都串行化。这种设计基于一个假设，即跨通道的特征和空间特征是可以独立学习的。</p>
<p>在实验中，Xception在多个视觉识别任务上都表现出了优于Inception V3的性能，同时模型的计算复杂性也有所降低。</p>
<ul>
<li class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 发现有全连接层非常有必要</span></li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="FCN - 2014 (骨干, 不平衡的U-Net)" class="heading" id="FCN_-_2014_(骨干,_不平衡的U-Net)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>FCN - 2014 (骨干, 不平衡的U-Net)</h3><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=oGETsIAxQlw" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=oGETsIAxQlw" target="_blank">最牛论文FCN：只更换一个层，却开创了一个时代！ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1411.4038" rel="noopener" class="external-link" href="https://arxiv.org/abs/1411.4038" target="_blank">[1411.4038] Fully Convolutional Networks for Semantic Segmentation</a></p>
<p><img src="https://cdn.sa.net/2024/04/21/5o1upKdWrexhS6a.png" referrerpolicy="no-referrer"></p>
<ul>
<li>一般网路比如AlexNet, 如果是分类任务. 网络最后一定是全连接层. 而在这篇论文中作者在末尾使用一个CNN, 用于使用于semantic segmentation 这样任务的 pre-pixel prediction.</li>
<li>这样类似于不平衡的 Autoencode</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="ResNet(残差网络) - 2015" class="heading" id="ResNet(残差网络)_-_2015"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>ResNet(残差网络) - 2015</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1512.03385" rel="noopener" class="external-link" href="https://arxiv.org/abs/1512.03385" target="_blank">[1512.03385] Deep Residual Learning for Image Recognition</a></p></div><div><p><img src="https://cdn.sa.net/2024/04/14/73EKPz8eahS4NBm.png" referrerpolicy="no-referrer"></p></div><div><p><img src="https://miro.medium.com/v2/resize:fit:1200/1*6hF97Upuqg_LdsqWY6n_wg.png" referrerpolicy="no-referrer"></p></div><div><p><img src="https://upload.wikimedia.org/wikipedia/commons/b/ba/ResBlock.png" referrerpolicy="no-referrer"></p></div><div><p><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1tS4y1G7WE/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1tS4y1G7WE/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">深入理解残差网络(一)-设计动机与理解误区_哔哩哔哩_bilibili</a></p></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=4h5OkS2CzuU" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=4h5OkS2CzuU" target="_blank">【博士Vlog】2015年的深度学习冠军ResNet怎么工作的？有多厉害？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1512.03385" rel="noopener" class="external-link" href="https://arxiv.org/abs/1512.03385" target="_blank">[1512.03385] Deep Residual Learning for Image Recognition</a></p>
<ul>
<li>Kaiming He 的一作</li>
<li>动机: 网络变深, 训练很难传递到深层网络, 可能是梯度越深梯度误差越大. (深度网络中的梯度消失和网络退化)</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>提出了 residual learning , 每两个卷积层添加一个短路模块.
<ul>
<li>使用 1x1 的卷积层用于补足维度不同层级维度的不同.<br>
<img alt="jgmlAHL2WY3KF81.png" src="https://cdn.sa.net/2024/04/16/jgmlAHL2WY3KF81.png" referrerpolicy="no-referrer"></li>
<li>这种方法被称为 identity mapping</li>
</ul>
</li>
<li>这个工作的效果是有数学证明的</li>
<li>发现效果很好<br>
<img alt="MDdFVGnIvjBAxqP.png" src="https://cdn.sa.net/2024/04/16/MDdFVGnIvjBAxqP.png" referrerpolicy="no-referrer"></li>
</ul>
<p>ResNet的发明动机源于深度神经网络的训练困难问题。当网络的深度超过一定程度（如20层）时，即使使用了合适的权重初始化和正则化技术，网络的性能也会开始下降，这种现象被称为退化（degradation）。这与过拟合不同，过拟合是指模型在训练集上表现优秀，但在测试集上表现较差。而退化现象是指随着网络深度的增加，模型的训练误差和测试误差都会增大。这意味着，即使网络的容量增大，网络也不能很好地拟合训练数据，这是一个非常反直觉的现象。</p>
<p>为了解决这个问题，Kaiming He等人提出了残差学习的概念。他们假设优化残差映射（即输入和输出之间的差值）比优化原始的未经处理的映射更容易。因此，他们设计了一种新的网络结构，称为残差块（Residual Block）。每个残差块包含了几个卷积层和一个跳跃连接（Skip Connection）。跳跃连接的作用是将输入直接传递到输出，形成一个短路机制，这就是所谓的恒等映射（Identity Mapping）。这种设计使得网络可以随着深度的增加而不断地学习新的特征，而不是只依赖于前面的层来学习所有的特征。</p>
<p>利用这种残差学习的概念，ResNet成功地训练了一个超过100层的深度神经网络，这在当时是前所未有的。这使得ResNet在各种视觉任务上都取得了非常好的结果，包括图像分类、物体检测和语义分割等任务。这种深度学习的新范式不仅改变了我们对深度学习的理解，也为后续的深度学习研究提供了新的方向。</p></div></div></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin " style="--callout-color: 0, 200, 83;" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p>A checklist of training deep nets</p>
<p>All about signal propagation</p>
<ul>
<li>ReLU(or similar)</li>
<li>initialization</li>
<li>normalization</li>
<li>residual connection</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><img alt="ilQtqRkT9ngXmI4.png" src="https://cdn.sa.net/2024/04/14/ilQtqRkT9ngXmI4.png" referrerpolicy="no-referrer"></p>
<p>ResNeXt是一种深度学习模型，它的设计灵感来源于VGG和ResNet两种网络结构。ResNeXt的主要贡献是引入了“cardinality”（群体大小）这个新的维度，通过增加网络的宽度来提高模型的性能。</p>
<p>以下是ResNeXt的主要贡献：</p>
<ol>
<li>
<p><strong>引入了群体大小（Cardinality）</strong>：在深度学习中，我们通常通过增加网络的深度（更多的层）或宽度（更多的神经元）来提高模型的性能。然而，ResNeXt提出了一个新的维度，即群体大小（Cardinality），也就是在并行的路径中重复相同的变换。ResNeXt的研究发现，增加群体大小的效果在保持其他参数不变的情况下，优于增加深度或宽度。</p>
</li>
<li>
<p><strong>引入了分组卷积（Grouped Convolution）</strong>：ResNeXt中的每个模块都包含了多个并行的路径，每个路径都进行相同的操作，但是操作的输入和输出都是分开的。这种设计可以看作是一种分组卷积的特例，分组卷积是一种有效的方式，可以在保持模型复杂性不变的情况下，增加网络的容量。</p>
</li>
<li>
<p><strong>提供了一种易于扩展的模型设计</strong>：ResNeXt模型的设计十分灵活，可以通过简单地调整群体大小和每个群体的宽度来扩展模型。这种设计使得ResNeXt可以很容易地适应不同的计算资源和不同的任务需求。</p>
</li>
<li>
<p><strong>在多个视觉识别任务上获得了出色的性能</strong>：在ImageNet分类、COCO object detection、COCO segmentation等任务上，ResNeXt都显示出了优于其他模型的性能。</p>
</li>
<li>
<p>发明多头学习. </p>
</li>
</ol>
<p>总的来说，ResNeXt的贡献在于提出了一种新的、有效的网络设计策略，这种策略通过引入群体大小（Cardinality）这个新的维度，以及分组卷积（Grouped Convolution），在提高模型性能的同时，保持了模型的计算效率。</p></div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1502.01852" rel="noopener" class="external-link" href="https://arxiv.org/abs/1502.01852" target="_blank">[1502.01852] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></p>
<p>这篇2015年的论文发明了 Kaiming'He初始化, 用于初始化使用 ReLu 作为激活函数的网络的初始值. </p>
<p><code>torch::nn::init:kaiming_normal_</code></p>
<p>具体思路如下: </p>
<p>在深度学习中，权重初始化是一个重要的步骤，因为不同的初始化方法可能会影响模型的收敛速度和最终性能。如果权重初始化不当，可能会导致训练过程中的梯度消失或爆炸问题，从而使模型无法学习。</p>
<p>Kaiming初始化（或称He初始化）的思路和动机源于对ReLU（Rectified Linear Unit）激活函数特性的理解。ReLU函数在负数部分的输出为0，这意味着大约一半的神经元在初始化时会被“死亡”（即输出为0），这可能导致反向传播过程中的梯度消失。为了解决这个问题，我们需要一个特殊的初始化策略。</p>
<p>Kaiming初始化的基本思想是<strong>保持每一层输出的方差与输入的方差一致</strong>。</p>
<ul>
<li>一是训练过程中层数之间权重的方差过大或过小容易导致梯度爆炸&amp;消失</li>
<li>二是模型的权重方差过大，可能会导致模型过拟合，因为模型可能会过度依赖某些特定的特征</li>
</ul>
<p>具体来说，如果一个层的输入神经元的数量为 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>，那么权重的初始值应该从均值为0，标准差 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-sop"><mjx-c class="mjx-c221A TEX-S1"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.107em;"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math></mjx-container></span>​的正态分布中随机取值。这样，即使经过ReLU激活函数，输出的方差也能保持不变。</p>
<p>这种初始化方法可以有效地保持每一层的激活值在经过多层传播后仍然保持有用的信息，从而防止梯度消失或爆炸的问题。这是Kaiming初始化被广泛应用于深度学习中的主要原因</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=o6kCNXscgks" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=o6kCNXscgks" target="_blank">分组卷积的开山之作：ResNeXt讲了什么？为什么有好的效果？ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/323424817/answer/1050776046" rel="noopener" class="external-link" href="https://www.zhihu.com/question/323424817/answer/1050776046" target="_blank">ResNeXt的分类效果为什么比Resnet好? - 知乎</a><br>
<img src="https://cdn.sa.net/2024/04/23/fuXQev1dUBPlhx2.png" referrerpolicy="no-referrer"></p>
<p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/32913695" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/32913695" target="_blank">深度学习——分类之ResNeXt - 知乎</a></p>
<ul>
<li>ResNeXt 是 2016年微软发明的CNN结构</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><del>发明</del>了分组卷积(aka. group convolution), 这使得模型在保持准确性的同时，能够有效地减少计算量. 类似Multi-head. 降低了网络过拟合风险. 
<ul>
<li>分组卷积将输入和卷积核分成多个组（Group），然后在每个组上分别进行卷积操作</li>
<li>可以让模型在laten space的不同subspace上学习, 增加网络的表达能力和学习到特征的多样化.</li>
<li>例如，假设我们有一个具256个通道的输入和一个256x256的卷积核，如果直接进行卷积操作，计算量会大但是，如果我们将输入和卷积核都分成4组，每组分别有64个通道，然后在每个组上分别进行卷积，那么每次卷积的计算量就会大大降低。256*256=65, 536; 64*64*4 = 16, 383 </li>
</ul>
</li>
<li>采用模块化设计, 简化的超参数的设置, 方便网络设计</li>
<li>网络简单, 容易理解和实现</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="DenseNet(密集连接) - 2016" class="heading" id="DenseNet(密集连接)_-_2016"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>DenseNet(密集连接) - 2016</h3><div class="heading-children"><div><ul>
<li data-line="0">DenseNet有什么特点? 对深度学习的发展作出了什么贡献? </li>
</ul></div><div><p>DenseNet，全称Densely Connected Convolutional Networks，是一种在卷积神经网络（CNN）中使用的网络架构。它有以下特点：</p></div><div><ol>
<li data-line="0">
<p>连接方式：不同于传统的神经网络是层与层之间的连接，DenseNet的每一层都与前面所有层连接，后面所有层也都接收其输出作为输入，形成了密集连接。</p>
</li>
<li data-line="2">
<p>参数量：由于使用了特征复用，DenseNet的参数量较少，计算效率较高。</p>
</li>
<li data-line="4">
<p>梯度流：由于所有层都直接连接到输出，因此梯度在反向传播过程中可以直接到达所有层，缓解了深度神经网络中常见的梯度消失问题。</p>
</li>
<li data-line="6">
<p>特征复用：由于每一层都接收所有前面层的特征图作为输入，使得网络有更强的特征复用能力。</p>
</li>
</ol></div><div><p>对于深度学习的发展，DenseNet作出了如下贡献：</p></div><div><ol>
<li data-line="0">
<p>提高了模型性能：DenseNet在多个公开数据集上刷新了记录，包括ImageNet、CIFAR-10、CIFAR-100等。</p>
</li>
<li data-line="2">
<p>提高了训练效率：由于DenseNet的特性，使得模型训练过程中梯度传播更加顺畅，有助于提高模型训练的效率。</p>
</li>
<li data-line="4">
<p>提出了新的网络连接方式：DenseNet提出了全新的网络连接方式——密集连接，这为深度学习模型的设计提供了新的思路。</p>
</li>
<li data-line="6">
<p>提升了模型的鲁棒性：DenseNet的特性使得模型对于输入的小变化具有较好的鲁棒性，有助于提高模型的泛化能力。</p>
</li>
</ol></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1608.06993" rel="noopener" class="external-link" href="https://arxiv.org/abs/1608.06993" target="_blank">Densely Connected Convolutional Networks - 2016</a></span></li>
<li data-line="1" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> SENet</span></li>
<li data-line="2" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> ShuffleNet</span></li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="MobileNet(网络做小) - 2016" class="heading" id="MobileNet(网络做小)_-_2016"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>MobileNet(网络做小) - 2016</h3><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><strong>什么是MobileNet? MobileNet 对 深度学习领域优势有什么历史贡献?</strong></p>
<p> MobileNet是一种基于深度神经网络的轻量级模型，主要用于移动和嵌入式视觉应用。它由Google的研究人员在2017年提出，其目标是提供一种高效的深度学习模型，可以在资源受限的设备上运行，如智能手机或嵌入式设备。</p>
<p>MobileNet的主要特点是使用<strong>深度可分离的卷积（depthwise separable convolution）来替代传统的卷积</strong>。深度可分离的卷积将一个卷积操作分解为两个更轻量级的子操作，从而大大减少了计算量和模型大小，同时仍然保持了良好的性能。</p>
<p>MobileNet对深度学习领域的主要贡献如下：</p>
<ol>
<li>
<p>提出了一种新的<strong>深度可分离的卷积</strong>方法，这种方法大大减少了模型的计算量和大小，使得深度学习模型可以在资源受限的设备上运行。</p>
</li>
<li>
<p>提出了一种新的网络结构，这种结构可以<strong>根据特定的资源约束（如计算资源、能源需求等）进行调整</strong>，从而在各种不同的情况下都能提供良好的性能。</p>
</li>
<li>
<p>MobileNet在许多视觉任务上都表现出了<strong>优异的性能</strong>，如图像分类、物体检测、人脸识别等，这进一步证明了深度可分离卷积的有效性和灵活性。</p>
</li>
</ol>
<p>总的来说，MobileNet的出现为移动和嵌入式设备上的深度学习应用开启了新的可能性，它的设计思想和技术也对后续的研究工作产生了深远的影响。</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=5AjzVYkhWQQ" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=5AjzVYkhWQQ" target="_blank">MobileNet v1v2v3都讲了什么？论文速读！ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://paperswithcode.com/method/mobilenetv1" rel="noopener" class="external-link" href="https://paperswithcode.com/method/mobilenetv1" target="_blank">MobileNetV1 Explained | Papers With Code</a></p>
<p><img src="https://cdn.sa.net/2024/04/28/1HO69DjukBZIfXw.png" referrerpolicy="no-referrer"><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/92134485" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/92134485" target="_blank">深度可分离卷积 - 知乎</a></p>
<ol>
<li>
<p>MobileNet V1：首次提出了深度可分离卷积的概念，将传统的卷积分解为深度卷积和逐点卷积两部分，大大降低了计算量和参数数量。此外，还引入了两个超参数，用于在模型复杂度和准确率之间进行权衡。</p>
</li>
<li>
<p>MobileNet V2：在V1的基础上，提出了线性瓶颈和反向残差结构，进一步提高了模型的性能。线性瓶颈可以减少模型的计算量，而反向残差结构则可以使模型更深，提高模型的表达能力。</p>
</li>
<li>
<p>MobileNet V3：这是一个经过精心设计的结构，结合了搜索空间的设计和网络结构搜索（NAS）。V3在V2的基础上加入了SE模块，并引入了h-swish激活函数，进一步提升了模型的性能。此外，V3还根据输入分辨率和模型复杂度，为不同的任务提供了一系列的模型。</p>
</li>
</ol></div></div></div><div><ul>
<li data-line="0"><strong>传统卷积</strong>，前一层和后一层特征一一对应开销大</li>
<li data-line="1"><strong>分组卷积(Group Convolution)</strong>，将输入通道分为若干组后对每一组在进行卷积最后拼接(有点类似于 Transformer的多头注意力机制)，用于减少计算量迫使网络学习到多个子空间的特征由 2016 年的ResNeXt开始真正发扬光大。但是不同通道之间孤立，限制了模型的性能。</li>
<li data-line="2">针对分组卷积的问题，ShuffleNet 使用<strong>通道混洗</strong>确保不同组之间的信息可以互相交换，进一步提高了模型的表达能力。</li>
<li data-line="3">同样的，MobileNet针对分组卷积的缺点，提出了<strong>深度可分离卷积（Depthwise Separable Convolution）</strong> 将单一的卷积操作分解位 深度卷积(DW)和逐点卷积(PC)。DW 先在每个通道上进行卷积再用 PC的 1 x 1 卷积核对 DW 的结果进行卷积以改变通道数。</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="U-Net(骨干)" class="heading" id="U-Net(骨干)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>U-Net(骨干)</h3><div class="heading-children"><div><blockquote>
<p>U-Net </p>
</blockquote></div><div><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/search?type=content&amp;q=U%20Net%20" rel="noopener" class="external-link" href="https://www.zhihu.com/search?type=content&amp;q=U%20Net%20" target="_blank">U Net - 搜索结果 - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/483261689/answer/2093332362" rel="noopener" class="external-link" href="https://www.zhihu.com/question/483261689/answer/2093332362" target="_blank">为什么U-Net在医学图像上表现优越？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/480371053/answer/3084572728" rel="noopener" class="external-link" href="https://www.zhihu.com/question/480371053/answer/3084572728" target="_blank">医学图像分割，除了魔改unet，还能有哪些创新点？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/548613974/answer/3229770898" rel="noopener" class="external-link" href="https://www.zhihu.com/question/548613974/answer/3229770898" target="_blank">关于U-Net的魔改到了什么程度了？ - 知乎</a></p></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin " style="--callout-color: 0, 200, 83;" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p><img src="https://cdn.sa.net/2024/04/14/xaL5f6JcCRpPHUY.png" referrerpolicy="no-referrer"></p>
<p>分割（Segmentation）是计算机视觉中相当流行的一项任务。在分割任务中，我们试图从背景中移除/提取前景。 前景和背景可以有不同的定义。我们也可以说，这是一项像素分类任务，你的工作是给给定图像中的每个像素分配一个类别。事实上，我们正在处理的气胸数据集就是一项分割任务。</p>
<p>分割任务的损失函数有:<br>
二元交叉熵、focal损失、dice损失等.</p>
<p>用于分割任务的最常用模型是 U-Net。</p>
<p>U-Net 包括两个部分：编码器和解码器。编码器与您目前所见过的任何 U-Net 都是一样的。解码器则有些不同。解码器由上卷积层组成。在上卷积（up-convolutions）（<strong>转置卷积</strong>transposed convolutions）中，我们使用滤波器，当应用到一个小图像时，会产生一个大图像。在 PyTorch 中，您可以使用 ConvTranspose2d 来完成这一操作。必须注意的是，上卷积与上采样并不相同。上采样是一个简单的过程，我们在图像上应用一个函数来调整它的大小。在上卷积中，我们要学习滤波器。我们将编码器的某些部分作为某些解码器的输入。这对 上卷积层非常重要。</p>
<p><strong>U-Net = 收缩层 + 瓶颈层 + 扩展层</strong></p>
<ul>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>收缩层</strong>
<ul>
<li>逐渐缩小输入图像的大小, 增加通道数量. 通过一系列卷积层和下采样, 提取图像的局部特征, 转化为更加高级别的抽象特征.</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>瓶颈层</strong>
<ul>
<li>由多个卷积层构成. 目标是捕获图像的高级特征, 减少特征图的维度, 保留重要的空间信息. 帮助UNet整合全局和局部星系, 以获取图像细节的上下文关系. </li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>扩展层</strong>
<ul>
<li>对特征图上采样, 通过一系列上采样和卷积层, 恢复到原始图像的尺寸. 使用跳跃连接, 讲收缩层和扩展层进行连接, 帮助保留细粒度的空间信息, 提高分割结果的准确性和稳定性. </li>
</ul>
</li>
</ul>
<p>U-Net 中的 skip connection 使得网络可以学习并纳入多个尺度的信息.</p>
<p><a data-tooltip-position="top" aria-label="https://ytzfhqs.github.io/AAAMLP-CN/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E5%88%86%E5%89%B2%E6%96%B9%E6%B3%95/" rel="noopener" class="external-link" href="https://ytzfhqs.github.io/AAAMLP-CN/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E5%88%86%E5%89%B2%E6%96%B9%E6%B3%95/" target="_blank">图像分类和分割方法 - AAAMLP 中译版</a></p>
<p>⬆️ 里面有一个 原始 U-Net 的 Pytorch 实现</p></div></div></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content heading-wrapper"><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1505.04597" rel="noopener" class="external-link" href="https://arxiv.org/abs/1505.04597" target="_blank">[1505.04597] U-Net: Convolutional Networks for Biomedical Image Segmentation</a></p>
<p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=Y-t5TmbY1Jc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=Y-t5TmbY1Jc" target="_blank">经典论文速读：U-Net讲了什么？为什么现在还在用？ - YouTube</a></p>
<h3 data-heading="摘要" class="heading" id="摘要">摘要</h3>
<ul>
<li>论文指出，深度网络的成功训练通常需要成千上万的标注样本。</li>
<li>作者提出了一种依赖于<strong>数据增强</strong>的网络和训练策略，以更高效地利用有限的标注样本。</li>
<li>U-Net架构包含一个收缩路径来捕获上下文信息和一个对称的扩展路径来进行精确定位。</li>
<li>论文展示了该网络可以从很少的图像中进行端到端训练，并在ISBI挑战赛中超过了之前最好的方法（滑动窗口卷积网络）。</li>
<li>U-Net在传输光显微镜图像（相差和DIC）上的训练也赢得了2015年ISBI细胞跟踪挑战赛的多个类别。</li>
<li>网络速度快，对512x512图像的分割少于一秒即可完成。</li>
</ul>
<h3 data-heading="引言" class="heading" id="引言">引言</h3>
<ul>
<li>过去两年中，深度卷积网络在许多视觉识别任务中超越了最先进的技术。</li>
<li>卷积网络虽然已经存在很长时间，但由于训练集的大小和考虑的网络规模限制了它们的成功。</li>
<li>论文基于“全卷积网络”架构进行了改进和扩展，使其能够在只有很少训练图像的情况下工作，并产生更精确的分割结果。</li>
</ul>
<h3 data-heading="网络架构" class="heading" id="网络架构">网络架构</h3>
<ul>
<li>U-Net的架构由收缩路径和扩展路径组成，如图1所示。</li>
<li>收缩路径遵循典型的卷积网络架构，通过重复应用3x3卷积、ReLU和2x2最大池化操作进行下采样。(<strong>用于获得上下文</strong>)</li>
<li>扩展路径的每一步包括特征图的上采样、“上卷积”和与收缩路径中相应裁剪的特征图的拼接。(<strong>用于精准定位</strong>)</li>
<li>U-Net中没有全连接层，<strong>通过互连卷积与反卷积过程中的特征，将上下文信息传递到更高层，实现了信息补充</strong>；</li>
<li>这种策略允许通过重叠瓦片策略无缝分割任意大的图像。</li>
<li>U-Net 在FCN 的基础上<strong>增加了上采样操作的次数和跳跃连接，使用跳跃连接将解码器的输出特征与编码器的语义特征融合，提高了分割精度，改善了 FCN 上采样不足的问题</strong>。</li>
</ul>
<h3 data-heading="训练" class="heading" id="训练">训练</h3>
<ul>
<li>使用Caffe实现的随机梯度下降来训练网络。</li>
<li>论文介绍了如何计算能量函数和使用权重图来进行训练。</li>
</ul>
<h3 data-heading="数据增强" class="heading" id="数据增强">数据增强</h3>
<ul>
<li>数据增强对于训练网络以获得期望的不变性和鲁棒性属性至关重要，尤其是在只有少量训练样本可用的情况下。</li>
<li>论文中提到，随机弹性变形是训练具有很少标注图像的分割网络的关键概念。</li>
</ul>
<p>实验中用到的方法:</p>
<ol>
<li><strong>弹性变形（Elastic Deformations）</strong>: 这是一种通过对训练样本应用随机位移向量来生成平滑变形的方法。位移是从标准差为10像素的高斯分布中采样的，然后使用双三次插值计算每个像素的位移。这种增强手段对于模拟组织中的变形特别有效，因为生物医学图像中的组织经常会出现此类变形。</li>
<li><strong>旋转和翻转</strong>: 通过对训练图像进行随机旋转和水平/垂直翻转，增加了模型对这些变换的不变性。</li>
<li><strong>强度变化</strong>: 通过对图像的亮度和对比度进行随机调整，模型能够学会在不同光照和成像条件下进行有效的分割。</li>
<li><strong>丢弃层（Drop-out Layers）</strong>: 在网络的收缩路径末端使用丢弃层进行进一步的隐式数据增强。</li>
</ol>
<h3 data-heading="实验" class="heading" id="实验">实验</h3>
<ul>
<li>论文展示了U-Net在三个不同的分割任务中的应用。</li>
<li>在电子显微镜记录的神经结构分割中，U-Net在ISBI挑战赛中取得了最佳成绩。</li>
<li>论文还展示了在ISBI细胞跟踪挑战赛2015中的细胞分割结果，U-Net在两个最具挑战性的2D传输光数据集上取得了显著的胜利。</li>
</ul>
<h3 data-heading="结论" class="heading" id="结论">结论</h3>
<ul>
<li>U-Net架构在多种生物医学分割应用中表现出色。</li>
<li>通过弹性变形的数据增强，U-Net只需要很少的标注图像，并且训练时间合理。</li>
<li>论文提供了完整的基于Caffe的实现和训练好的网络，并相信U-Net架构可以轻松应用于更多任务。</li>
</ul>
<h3 data-heading="后续改进" class="heading" id="后续改进">后续改进</h3>
<p>在U-Net的原始模型之后，有许多研究者对其进行了改进，以适应更复杂的任务和更大的数据集。这些改进主要集中在以下几个方面：</p>
<ol>
<li>
<p><strong>更深的网络</strong>：例如，V-Net和Res-U-Net。这些网络在U-Net的基础上添加了更多的层，以增加模型的容量和复杂性。</p>
</li>
<li>
<p><strong>更复杂的连接方式</strong>：例如，Dense U-Net。这种网络在每一层之间添加了密集连接，使得每一层的输出都成为下一层的输入。</p>
</li>
<li>
<p><strong>注意力机制</strong>：例如，Attention U-Net。这种网络在U-Net的基础上添加了注意力机制，使得网络在进行特征融合时，可以更加关注重要的区域。</p>
</li>
<li>
<p><strong>多尺度特征融合</strong>：例如，MultiRes U-Net。这种网络在U-Net的基础上添加了多尺度特征融合，使得网络可以同时捕捉到不同尺度的信息。</p>
</li>
<li>
<p><strong>3D U-Net</strong>：原始的U-Net是为2D图像设计的，但在许多医学图像任务中，我们需要处理3D图像。因此，有研究者提出了3D U-Net，以处理3D图像。</p>
</li>
<li>
<p><strong>更有效的训练策略</strong>：例如，使用更复杂的数据增强技术，或者使用更复杂的损失函数，如Dice损失或Focal损失。</p>
</li>
</ol>
<p>以上都是对U-Net的一些改进，但是需要注意的是，哪种改进最有效，很大程度上取决于具体的任务和数据。</p>
<p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/548613974/answer/3229770898" rel="noopener" class="external-link" href="https://www.zhihu.com/question/548613974/answer/3229770898" target="_blank">关于U-Net的魔改到了什么程度了？ - 知乎</a></p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=9psg4H_WwuU" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=9psg4H_WwuU" target="_blank">简单的想法，很好的效果：Unet++ 做了什么？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://paperswithcode.com/method/unet" rel="noopener" class="external-link" href="https://paperswithcode.com/method/unet" target="_blank">UNet++ Explained | Papers With Code</a></p>
<p><img src="https://cdn.sa.net/2024/04/29/nFXocHpGhT7ly94.png" referrerpolicy="no-referrer"></p>
<p>原始 U-Net 中 Skip-Connection 的存在使得 U-Net 可能会发生在特定输入中浅层 U-Net 效果比 深层要好的结果，但残差又必不可少。</p>
<p>U-Net ++ 干脆就把多各不同层数 U-Net 使用 dense connection 叠在了一起(类似于DenseNet).</p>
<p>坐着比较两个参数量一致的 U-Net 和 U-Net ++ 发现是 Dense Connection 实际发生了作用。</p>
<p>模型在 U-Net 在医学图像分割取得了很好的效果。</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2401.04722" rel="noopener" class="external-link" href="https://arxiv.org/abs/2401.04722" target="_blank">[2401.04722] U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation</a></p></div></div></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1603.06937" rel="noopener" class="external-link" href="https://arxiv.org/abs/1603.06937" target="_blank">Stacked Hourglass Networks for Human Pose Estimation</a> </span></li>
<li data-line="1">Hourglass Network 是由 Newell等人于2016年提出的一种人体姿态估计网络，也被用于其他需要精细定位的任务，如面部关键点检测、手势识别</li>
<li data-line="2">网络的结构是 多个 U-Net 叠加，有不同的叠加方式。这允许网络在不同尺度上捕捉和整合空间信息，帮助网络捕捉到不同大小的人体关节点。同时由于网络对称性也有助于保持空间分辨率和细节信息</li>
<li data-line="3">在每个Hourglass模块的输出后都会添加一个监督信号，即损失函数(中间监督)，这有助于训练过程中的梯度流动, 并提高模型的性能。这种设计类似于深度监督学习（deep supervision）的概念。</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="FPN(骨干)" class="heading" id="FPN(骨干)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>FPN(骨干)</h3><div class="heading-children"><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 和其他骨干的对比。对比 U-Net 和 FCN </span></li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="EfficientNet (终结网络scale经验法则) - 2019" class="heading" id="EfficientNet_(终结网络scale经验法则)_-_2019"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>EfficientNet (终结网络scale经验法则) - 2019</h3><div class="heading-children"><div><blockquote>
<p>有章法的缩放网络的宽度和深度, 避免可能的网络浪费</p>
</blockquote></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><img src="https://img-blog.csdnimg.cn/20210306162502756.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70#pic_center" referrerpolicy="no-referrer"></p>
<p><img src="https://cdn.sa.net/2024/04/16/5pRuksEVtmT69zi.png" referrerpolicy="no-referrer"></p>
<p><img src="https://cdn.sa.net/2024/04/16/4XnPsyhY6a3vRCd.png" referrerpolicy="no-referrer"></p>
<p>这篇文章是谷歌研究团队发表的《EfficientDet::Scalable and Efficient Object Detection》。论文系<br>
统地研究了针对目标检测任务神经网络架构设计的选择，并提出了一系列关键优化以提升效率。文<br>
章的主要贡献包括：</p>
<ol>
<li>提出了<strong>加权双向特征金字塔网络(BiFPN)</strong>,简化了多尺度特征融合的过程。</li>
<li>提出了一种<strong>复合缩放方法</strong>，均匀地缩放了所有背骨、特征网络以及分类/框预测网络的分辨率、深度和宽度。</li>
<li>基于这些优化以及更好的主干网络，开发了一系列称为EfficientDet的新型目标检测器，与先前的模型相比，各种资源限制下都能达到更好的效率。</li>
</ol>
<p>特别值得一提的是，使用单一模型和单一尺度的EfficientDet-D7在C0Co测试集上达到了55.1%的最<br>
新AP(平均精度)，并且只有77O0万参数和4100亿FLOPs,比以前的检测器小4-9倍，使用的FLOPs:少13-42倍。此外，EfficientDet在GPU/CPU上的运行速度比以前的检测器快4-11倍。论文还证明了EfficientDet在Pascal VOC2012语义分割上也表现出色，达到了8174%的mlOU精度，同时FLOPs比DeepLabV3+少9.8倍。</p>
<p><strong>什么是EfficientNet? 在深度学习领域有什么历史贡献?</strong></p>
<p> EfficientNet是一种在深度学习领域中使用的卷积神经网络(CNN)架构。它是由Google的研究员在2019年提出的，该网络使用了一种名为“复合缩放”的方法，通过均衡网络的深度、宽度和解析度，使得模型在保持相同计算资源的情况下，可以达到更高的精度。</p>
<p>EfficientNet的核心思想是：不同维度之间的缩放可以带来更好的性能。在之前的一些工作中，研究人员通常只关注单一维度的缩放，例如网络的深度（如ResNet）或宽度（如MobileNet）。然而，EfficientNet指出，这些单一维度的缩放往往会导致资源的浪费。因此，EfficientNet提出了一种复合缩放的方法，同时考虑了深度、宽度和解析度的缩放，从而在有限的资源下获得更好的性能。</p>
<p>EfficientNet的历史贡献主要体现在两个方面：</p>
<ol>
<li>
<p>EfficientNet提出了一种新的网络缩放方法，这种方法可以在有限的计算资源下，获得更好的性能。这对于那些资源有限，但希望获得高性能模型的场景（如移动设备、嵌入式设备等）具有重要的意义。</p>
</li>
<li>
<p>EfficientNet在一系列的图像识别任务上都取得了当时最好的性能，包括ImageNet，CIFAR-100等数据集，这进一步证明了其有效性和优越性。</p>
</li>
</ol>
<p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=jsHda3dhM-g" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=jsHda3dhM-g" target="_blank">【博士Vlog】EfficientNet 和 EfficientDet 讲了什么？有计算资源真的可以为所欲为！！ - YouTube</a></p>
<ul>
<li>介绍了EfficentNet(图像分类) 和 EfficientDet(物体检测)</li>
<li>网络速度快, 参数小, 效果又好</li>
<li>平衡了网络 <code>depth</code> <code>channel</code> <code>width</code> 的设计</li>
<li>在ImageNet训练, 上面的平衡法则基本上就是试出来的(Neural Architecture Search)</li>
<li></li>
</ul>
<p><a data-tooltip-position="top" aria-label="https://blog.csdn.net/qq_37541097/article/details/114434046" rel="noopener" class="external-link" href="https://blog.csdn.net/qq_37541097/article/details/114434046" target="_blank">EfficientNet网络详解-CSDN博客</a></p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><img src="https://cdn.sa.net/2024/04/16/VdE6cFM3r7aGgLx.png" referrerpolicy="no-referrer"></p>
<p>这张图展示了四种不同的特征网络设计用于多尺度特征融合，它们在目标检讽测模型中起着至关重要<br>
的作用。具体来说：</p>
<p>(a)FPN(Feature Pyramid Network):FPN引入了一个自顶向下的通道，以融合从第三层到第七<br>
层的多尺度特征。</p>
<p>(b)PANet(Path Aggregation Network):在FPN的基础上增加了一个自底向上的路径，以进一步<br>
增强特征的传递。</p>
<p>(c)NAS-FPN(Neural Architecture Search Feature Pyramid Network):使用神经架构搜索找到了<br>
一个不规则的特征网络拓扑结构，并且在网络中重复应用相同的块。</p>
<p>(d)BiFPN(Bidirectional Feature Pyramid Network):提出了BiFPN,具有更好的精度和效率权<br>
衡。BFPN加强了特征的上下文融合，通过增加额外的自底向上和自顶向下路径，并在多个尺度之间进行重复的特征融合。</p>
<p>整体上，这些设计展示了如何通过不同的策略来增强多尺度特征的融合能力，BFPN作为<br>
EfficientDet模型中的一个组件，它的设计旨在通过增加简单有效的双向路径来提升目标检测的性<br>
能。</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="BiT (最大有监督CNN)" class="heading" id="BiT_(最大有监督CNN)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>BiT (最大有监督CNN)</h3><div class="heading-children"><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=VzJ7us4eQcE" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=VzJ7us4eQcE" target="_blank">CNN之王什么样子？谷歌造出了世界上最大的CNN模型！效果惊人！ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1912.11370" rel="noopener" class="external-link" href="https://arxiv.org/abs/1912.11370" target="_blank">Big Transfer (BiT): General Visual Representation Learning - 2019</a></p>
<p> Google发表. 最大CNN直接表明更大规模CNN scale 能力已经受限了. 这里发现的CNN限制为ViT的发明做了铺垫工作. </p>
<p>文章内容:</p>
<ol>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>大力出奇迹把resnet做大到极致
<ol>
<li>ResNet *4 : channel 64 - 256</li>
</ol>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>不是一般学者能玩的起的
<ol>
<li>在ImageNet-21K 和 JFT-300M(最大公开数据集)</li>
<li>训练了3个月</li>
</ol>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>他们发现了一些困难，也就是大力出奇迹效果不好于是就有了ViT
<ol>
<li>发现 Batch Normazation 在并行训练效果不好. 于是就有了 Group Normalizaton.</li>
<li>发现 MixUP(数据标签线性组合) 效果不好</li>
</ol>
</li>
<li>把注意力机制做的灵活，效果会更好</li>
<li>大力出奇迹是末期表现，一般大力出奇迹之后就会有新架构出现. e.g.VGG, BigGAN, GPT-4?</li>
</ol>
<p>通过制造更高效的模型，更灵活的架构，而不是傻堆料</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="VanillaNet ?" class="heading" id="VanillaNet_?"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>VanillaNet ?</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2305.12972" rel="noopener" class="external-link" href="https://arxiv.org/abs/2305.12972" target="_blank">[2305.12972] VanillaNet: the Power of Minimalism in Deep Learning</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/search?type=content&amp;q=VanillaNet" rel="noopener" class="external-link" href="https://www.zhihu.com/search?type=content&amp;q=VanillaNet" target="_blank">VanillaNet - 搜索结果 - 知乎</a></p></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><strong>VanillaNet</strong>是一种注重极简主义和效率的神经网络架构。它的设计简单，层数较少，避免了像深度架构和自注意力这样的复杂操作。VanillaNet的关键特性包括<strong>深度训练策略</strong>，最初使用激活函数训练两个卷积层，随后这个激活函数逐渐简化为恒等映射，允许层合并。此外，VanillaNet还使用<strong>并行堆叠的激活函数</strong>来提高非线性，从而提升简单网络的性能。</p>
<p>里面的数学很复杂, 看不懂. </p></div></div></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="RNN(循环神经网络)" class="heading" id="RNN(循环神经网络)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>RNN(循环神经网络)</h2><div class="heading-children"><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p>RNN&nbsp;试图通过加入反馈机制来理解一串数值前后的关系，所以适用于语言模型，因为这些数值之间有前后关系，像我们的句子里有先后逻辑</p></div></div></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="faq" class="callout drop-shadow"><div class="callout-title"><div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-help-circle"><circle cx="12" cy="12" r="10"></circle><path d="M9.09 9a3 3 0 0 1 5.83 1c0 2-3 3-3 3"></path><path d="M12 17h.01"></path></svg></div><div class="callout-title-inner">相比Transformer, RNN的优势在于</div></div><div class="callout-content">
<ul>
<li data-line="1">串行结构天然更加适合于序列数据</li>
<li data-line="2">推理更高效 复杂度为 O(N), 而Transformer 为 O(N^2)</li>
</ul>
</div></div></div><div><p><img src="https://cdn.sa.net/2024/04/05/rtqmWbpCE9V7Qin.png" referrerpolicy="no-referrer"><br>
RNN 要从1D数据中学习到特征.<br>
CNN其实用一个1D卷积核配合池化层在数据上滑动其实也可以学习.</p></div><div><p><img src="https://cdn.sa.net/2024/04/05/EGtNkwJ3YXsaHQ9.png" referrerpolicy="no-referrer"><br>
循环神经网络（Recurrent Neural Network，RNN）的深度可以从两个方向进行扩展：时间步长和隐藏层的数量。</p></div><div><ol>
<li data-line="0"><strong>时间步长</strong>：在RNN中，每个时间步都可以看作是网络的一层。因此，处理更长的序列会使网络在时间维度上更深。例如，处理一个由100个词构成的句子的RNN可以被视为一个有100层的网络。然而，这种方法存在一个问题，那就是随着时间步长的增加，网络可能会遇到梯度消失或梯度爆炸的问题，这使得训练变得困难。为了解决这个问题，研究者提出了一些更复杂的RNN变体，如长短期记忆网络（Long Short-Term Memory，LSTM）和门控循环单元（Gated Recurrent Unit，GRU），它们通过引入门控机制来控制信息的流动，从而缓解了梯度消失的问题。</li>
<li data-line="1"><strong>隐藏层的数量</strong>：除了在时间维度上增加深度，我们也可以在空间维度上增加深度(如上图)，即增加隐藏层的数量。这可以通过堆叠多个RNN层来实现，每一层RNN的输出都作为下一层的输入。这种方法可以使网络学习更复杂的特征，但同样也会增加训练的难度。同样，可以使用LSTM或GRU等更复杂的RNN变体来缓解这个问题。</li>
</ol></div><div><p>总的来说，使RNN变得更深可以帮助网络学习更复杂的模式，但同时也会增加训练的难度。为了解决这个问题，研究者已经提出了许多方法，如使用更复杂的RNN变体、更好的优化算法、正则化技术等。</p></div><div><blockquote>
<p>RNN 接受两个输入：State和Token。它一次通过输入序列一个Token，每个Token更新状态。例如，我们可以使用 RNN 将文本处理成单个状态向量。然后，这可用于将文本分类为“正面”或“负面”。或者我们可以使用最终状态来预测下一个Token，这就是 RNN 用于生成文本的方式。</p>
</blockquote></div><div><p><img src="https://cdn.sa.net/2024/04/05/BcrxtwSMmb4ZED8.png" referrerpolicy="no-referrer"><br>
RNN 但是因为下一个参数的训练要等上一个参数算好才可以进行, 所以不适合多GPU训练.<br>
CNN 就没这个问题, 但它学习到的context windows 由取决于kernel size. </p></div><div class="heading-wrapper"><h3 data-heading="LSTM(长短期记忆网络) &amp; GRU(门控循环单元)" class="heading" id="LSTM(长短期记忆网络)_&amp;_GRU(门控循环单元)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>LSTM(长短期记忆网络) &amp; GRU(门控循环单元)</h3><div class="heading-children"><div><p><img alt="ZLubfyJEvpmVaYq.png" src="https://cdn.sa.net/2024/04/11/ZLubfyJEvpmVaYq.png" referrerpolicy="no-referrer"></p></div><div><p>Google's Neural Machine Translation System:Bridging the Gap between Human and Machine Translation",Wu,et al.,2016</p></div><div><p>Stack LSTM units<br>
going deep with residual connections</p></div><div><ul>
<li data-line="0">enable 16 layers</li>
<li data-line="1">degrade 4 layers if not using residual </li>
<li data-line="2">like observation in CNNs</li>
</ul></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin " style="--callout-color: 0, 200, 83;" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p>WaveNet是DeepMind在2016年提出的一种深度学习模型，它是一种生成模型，可以用于生成原始音频波形，特别是用于文本到语音（Text-to-Speech，TTS）的应用。</p>
<p>WaveNet的主要创新之处在于它使用了一种叫做“稀疏卷积”的结构，也被称为“扩张卷积”或“膨胀卷积”。这种结构使得每个输出样本可以看到的输入范围（即感受野）随着层级的增加而指数级增长，而不是像传统的卷积神经网络那样线性增长。这使得WaveNet可以处理长范围的依赖关系，这在音频生成等序列生成任务中是非常重要的。</p>
<p>WaveNet的另一个重要特性是它是一个完全生成的模型，也就是说，它一次生成一个样本，然后将这个样本馈送回模型，作为下一个样本的一部分输入。这种自回归的特性使得WaveNet可以生成非常自然和连贯的音频。</p>
<p>WaveNet的效果非常出色，它在多种语音生成任务上都取得了最好的效果，包括在非常挑战的文本到语音合成任务上。事实上，Google的云语音合成服务就是基于WaveNet的。</p>
<p>总的来说，WaveNet是一种强大的音频生成模型，它通过使用稀疏卷积和自回归结构，可以生成非常自然和连贯的音频。</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="TransformerOut? Linear RNN Can help" class="heading" id="TransformerOut?_Linear_RNN_Can_help"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>TransformerOut? Linear RNN Can help</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/676892576" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/676892576" target="_blank">Transformer架构的局限已凸显，被取代还有多久？ - 知乎</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://fullstackdeeplearning.com/blog/posts/rwkv-explainer/" rel="noopener" class="external-link" href="https://fullstackdeeplearning.com/blog/posts/rwkv-explainer/" target="_blank">RWKV, Explained - The Full Stack</a><br>
国人开发, 基于RNN</p></div><div><p> <img src="https://pic1.zhimg.com/v2-34943728e7ffdb60829f6a82fc10c610_r.jpg" referrerpolicy="no-referrer"><br>
<img alt="GPT_versus_RWKV.svg" src="https://rwkv-wiki.github.io/img/GPT_versus_RWKV.svg" referrerpolicy="no-referrer"><br>
<a data-tooltip-position="top" aria-label="https://rwkv-wiki.github.io/" rel="noopener" class="external-link" href="https://rwkv-wiki.github.io/" target="_blank">RWKV Wiki - RWKV Wiki</a><br>
<a data-tooltip-position="top" aria-label="https://wiki.rwkv.com/advance/architecture.html#how-does-rwkv-differ-from-classic-rnn" rel="noopener" class="external-link" href="https://wiki.rwkv.com/advance/architecture.html#how-does-rwkv-differ-from-classic-rnn" target="_blank">RWKV Architecture</a><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/514840332" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/514840332" target="_blank">RWKV-v2-RNN 原理：超越 Transformer，实现 O(T) 的语言建模 - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py" rel="noopener" class="external-link" href="https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py" target="_blank">ChatRWKV/RWKV_in_150_lines.py at main · BlinkDL/ChatRWKV · GitHub</a></p></div><div><ul>
<li data-line="0"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>RWKV</strong>&nbsp;是一种拥有RNN, CNN, Transformer三种优点的RNN网络. 在性能媲美Transformer的基础上，具有O(1)推理复杂度，更易收敛训练，模型参数和内存占用
<ul>
<li data-line="1">inspired by AFT（Attention-Free Transformer）</li>
</ul>
</li>
<li data-line="2">背景: 所有Self-Attention 为基础的模型, 都不肯避免的需要Token之间相互计算, 造成了 O(n^2) 的复杂度; 且位置信息外挂</li>
<li data-line="3"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>RWKV针对这两个问题, 使用
<ul>
<li data-line="4">WKV 计算过程直接向 Token 引入了具有平移不变性的位置编码，不需要引入额外的位置编码。</li>
<li data-line="5">使用两个RWKV层替换了Multi-head Attention 和 前馈网络</li>
<li data-line="6">Token 之间无需相互运算，WKV 计算过程只对各 Token 分别变换并累加结果, 成就了 O(n)的复杂度</li>
</ul>
</li>
<li data-line="7"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>RNN 在训练时并行<img alt="image.png" src="https://cdn.sa.net/2024/01/29/jgsuCJzViMv2PEK.png" referrerpolicy="no-referrer">
<ul>
<li data-line="8"></li>
</ul>
</li>
<li data-line="9">RNN 在推理时串行</li>
</ul></div><div><blockquote>
<p>Mamba 和 牢大 没有关系(认真) </p>
</blockquote></div><div><p><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1dH4y1G7jK/?spm_id_from=333.1007.tianma.1-3-3.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1dH4y1G7jK/?spm_id_from=333.1007.tianma.1-3-3.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">说人话解释Mamba技术原理 Transformers 又被超越了_哔哩哔哩_bilibili</a></p></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2312.00752" rel="noopener" class="external-link" href="https://arxiv.org/abs/2312.00752" target="_blank">[2023.12] Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a><br>
<a data-tooltip-position="top" aria-label="https://36kr.com/p/2547379574166664" rel="noopener" class="external-link" href="https://36kr.com/p/2547379574166664" target="_blank">颠覆Transformer霸权，CMU普林斯顿推Mamba新架构，解决致命bug推理速度暴增5倍-36氪</a></p>
<ul>
<li>Mamba 为了解决Transformer核心注意力层上下文长度无法很好scale计算资源的局限(在大模型基础热门的背景). 为此提出了结构化空间状态模型(SSM, structured state space models). 实验显示Mamba具有参数线性扩展性(可最高百万Token级别), 和五倍的推理速度. 在多项任务中都达到SOTA. 比如Mamba-3B模型表現和两倍参数量的Transformer模型相当. 向我们展示了一个非常有希望的 Transformer 替代. </li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>研究背景
<ul>
<li>在深度学习领域，Transformer架构及其核心的注意力机制已经成为处理各种序列数据的强大工具。然而，Transformer在处理长序列数据时存在计算效率低下的问题，这限制了其在某些应用场景中的使用。为了解决这个问题，研究者们提出了多种子二次时间复杂度的架构，如线性注意力、门控卷积和循环模型等，但这些模型在处理语言等重要模态数据时表现并不如注意力机制。</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>过去的方案及问题
<ul>
<li>尽管这些子二次时间复杂度的模型在计算效率上有所提升，但它们在处理离散模态数据（如文本）时，往往无法有效地进行基于内容的推理。这些模型的一个关键弱点是它们无法根据当前输入有选择地传播或遗忘信息。此外，尽管这些模型试图通过高效的卷积操作来实现，但这种变化阻止了它们使用高效的并行计算，这在硬件层面上带来了挑战。</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>本文方案及具体步骤： 
<ul>
<li>本文提出了一种新的选择性状态空间模型（Selective State Space Models，简称SSMs），它通过以下几个关键步骤来提高模型的性能：</li>
<li><strong>选择机制</strong>：通过将SSM参数设置为输入的函数，模型能够根据当前的输入选择性地传播或遗忘信息，从而在序列长度维度上动态地处理信息。在实现 Transformer 质量的性能，同时线性缩放序列长度</li>
<li><strong>硬件感知算法</strong>：为了解决选择性SSMs在计算上的挑战，研究者设计了一种硬件感知的并行算法，该算法以递归模式运行模型，并通过扫描操作而不是卷积来计算，同时避免了在GPU内存层次结构之间进行不必要的IO访问。</li>
<li><strong>简化的架构设计</strong>：将SSMs与Transformer的MLP块结合，形成了一个简化的、同质的架构（Mamba），这个架构在不使用注意力或MLP块的情况下，通过选择性状态空间实现了快速推理和线性序列长度扩展。</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>贡献
<ul>
<li>在多种模态（语言、音频和基因组学）上都取得SOTA性能，成为跨模态通用序列模型主干的有力候选者。</li>
</ul>
</li>
</ul></div></div></div><div><p><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1eC411L7Ku/?spm_id_from=333.1007.tianma.1-3-3.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1eC411L7Ku/?spm_id_from=333.1007.tianma.1-3-3.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">下个风口？Mamba手推公式&amp;代码手搓_哔哩哔哩_bilibili</a></p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> 论文：</p>
</span></li>
<li data-line="1">
<p>总计：Mamba 从自动控制原理搬用了 选择状态机(Selective State Space,SSM) 。</p>
</li>
<li data-line="2">
<p>优势区间：序列预测，比 LSTM 记忆要好；速度快内存占用小</p>
</li>
<li data-line="5" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> 论文： <a data-tooltip-position="top" aria-label="https://github.com/yuweihao/MambaOut" rel="noopener" class="external-link" href="https://github.com/yuweihao/MambaOut" target="_blank">GitHub - yuweihao/MambaOut: MambaOut: Do We Really Need Mamba for Vision?</a></p>
</span></li>
<li data-line="6">
<p>问题：我们真的需要 Mamba 应用在视觉任务中么？</p>
</li>
<li data-line="7">
<p>方法：做了一个 Gated CNN(MambaOut) 和 Mamba 的消融实验，唯一的区别是后者有一个 SSM </p>
</li>
<li data-line="8">
<p>结果：发现 MambaOut 和 VMamba 效果类似</p>
</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="Transformer (2017)" class="heading" id="Transformer_(2017)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Transformer (2017)</h2><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://paperswithcode.com/paper/attention-is-all-you-need" rel="noopener" class="external-link" href="https://paperswithcode.com/paper/attention-is-all-you-need" target="_blank">Attention Is All You Need | Papers With Code</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://nlp.seas.harvard.edu/annotated-transformer/" rel="noopener" class="external-link" href="https://nlp.seas.harvard.edu/annotated-transformer/" target="_blank">The Annotated Transformer</a><br>
<a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=XfpMkf4rD6E" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=XfpMkf4rD6E" target="_blank">Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy - YouTube</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Ja4y1B7zC/?spm_id_from=333.788.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Ja4y1B7zC/?spm_id_from=333.788.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">[重置版]从零实现transfomer模型 || 理解ChatGPT基石 || pytorch_哔哩哔哩_bilibili</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/648127076?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1716622508470669312&amp;utm_source=io.raindrop.raindropio" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/648127076?utm_campaign=&amp;utm_medium=social&amp;utm_oi=58982500663296&amp;utm_psn=1716622508470669312&amp;utm_source=io.raindrop.raindropio" target="_blank">三万字最全解析！从零实现Transformer（小白必会版😃） - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/zvideo/1722621858900451329?utm_psn=1733842036934078467" rel="noopener" class="external-link" href="https://www.zhihu.com/zvideo/1722621858900451329?utm_psn=1733842036934078467" target="_blank">哈工大PHD竟把Transformer讲的如此简单！ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/589738603/answer/3375664602" rel="noopener" class="external-link" href="https://www.zhihu.com/question/589738603/answer/3375664602" target="_blank">为什么只有基于Transformer的大模型，而没有其他的？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/596771388/answer/3454101159?utm_medium=social&amp;utm_psn=1760630215855116288&amp;utm_source=ZHShareTargetIDMore" rel="noopener" class="external-link" href="https://www.zhihu.com/question/596771388/answer/3454101159?utm_medium=social&amp;utm_psn=1760630215855116288&amp;utm_source=ZHShareTargetIDMore" target="_blank">为什么我还是无法理解transformer？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=eMlx5fFNoYc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=eMlx5fFNoYc" target="_blank">可视化注意力，变形金刚的心脏 | 第 6 章，深度学习 - YouTube</a> (强烈推荐)<br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/500807675" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/500807675" target="_blank">一张图看懂BERT - 知乎</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb" rel="noopener" class="external-link" href="https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb" target="_blank">Build your own Transformer from scratch using Pytorch | by Arjun Sarkar | Towards Data Science</a><br>
<a data-tooltip-position="top" aria-label="https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021" rel="noopener" class="external-link" href="https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021" target="_blank">All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 1 | by Arjun Sarkar | Towards Data Science</a><br>
<a data-tooltip-position="top" aria-label="https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada" rel="noopener" class="external-link" href="https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada" target="_blank">All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 2 | by Arjun Sarkar | Towards Data Science</a></p></div><div class="heading-wrapper"><h3 data-heading="&quot;Attention Is All You Need&quot;" class="heading" id="&quot;Attention_Is_All_You_Need&quot;"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>"Attention Is All You Need"</h3><div class="heading-children"><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">%</span> 提出背景</p>
</span></li>
<li data-line="1">
<p><strong>2013, RCTM</strong>, 神经网络机器翻译(NMT)开篇之作. 使用 CNN 将连续文本表示为向量(Context, Encoder) 然后传入 RNN 去拟合目标句子(end-to-end)。奠定了 Encoder-Decoder。</p>
</li>
<li data-line="2">
<p><strong>RNN Encoder-Decoder(Cho et al., 2014a)</strong>, Encoder 变成了 RNN. 为了解决长文本的梯度消失/爆炸，然后顺便提出了 GRU， 引入更新门和重置门来控制信息的流动。但是 Encoder和 Decoder 是分开训练的。</p>
</li>
<li data-line="3">
<p><strong>Bahdanau Attention(Bahdanau et al., 2014)</strong>, 来自和RNN Encoder-Decoder 的同一个团队。首次提出了 <span style="background:#fff88f">Attention</span> 这个概念。认为翻译过程中语言之间应该可以对齐 e.g "How(多) old(大) are you(你)". 所以设计了能够度量两种语言之间不同词之间的关联分数 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo></mjx-math></mjx-container></span> (加法注意力)Attention Score. 用于<span style="background:#fff88f">动态的计算输入输出的匹配程度</span>。 注意力在计算解码器隐藏状态之前进行也成为 pre-attention. </p>
</li>
<li data-line="4">
<p><strong>Seq2Seq (Sutskever et al., 2014)</strong>, 之前的神经网络模型通常处理固定大小的输入与输出, Seq2Seq 模型通过编码器将任意长度的输入序列转化为固定长度的向量，并通过解码器将该向量转化为任意长度的输出序列，<span style="background:#fff88f">可以自由处理任意长度的输入和输出</span>。一开始使用的 LSTM 后转用注意力机制。还有一些其他优化技巧在当时堆了很大的参数量取得了很好的效果。</p>
</li>
<li data-line="5">
<p><strong>Luong Attention (Luong et al., 2015)</strong>, 提出了几种计算 注意力分数的方法(e.g.dot product, <span style="background:#fff88f">scaled dot product</span>) . 注意力在计算解码器隐藏状态之后进行也成为 post-attention. </p>
</li>
<li data-line="6">
<p><strong><span style="background:#fff88f">Self attention</span></strong>，之前的 Attention 关注的都是不同序列之间的 Attention，比如机器翻译的原始文本和目标文本，文生图 中的文本序列和图像序列，这些现在被称为Cross Attention. </p>
</li>
<li data-line="7">
<p><strong>MultiHead Self Attention (Lin et al., 2017)</strong>, 借鉴一下卷积网络的多个 kernel 的思想(ResNexT)，作者设计了<span style="background:#fff88f">多个 Attention</span>，期望不同的 Attention 学习到不同的注意力，从而更好的提取特征</p>
</li>
<li data-line="8">
<p><strong>Frustratingly Short Attention Spans in Neural Language Modeling, Micha et al., 2017)</strong>, 这篇论文认为隐状态同时承担了计算注意力分数，计算 context value和隐状态这样任务太重量。提出了使用<span style="background:#fff88f">key</span> 用来计算 attention score；<span style="background:#fff88f">value</span> 用来和 Atention score 相乘；predict 用于预测词的分布</p>
</li>
<li data-line="9">
<p><strong>ResNet(Kaiming, 2015)</strong> , 为了解决加了网络层数性能反而下降的问题(网络退化)，提出了残差链接允许前一层的输出作为后面层的输入的一部分，使得层需要学习的就变成了输入输出差值，即<span style="background:#fff88f">残差</span>。使用一个线性层来调整维度。</p>
</li>
<li data-line="10">
<p>“Attention is all you need”, 2017 。 完全使用自注意力机制取代 RNN, 用于解决梯度爆炸/消失和无法并行计算。</p>
</li>
<li data-line="11">
<img src="https://cdn.sa.net/2024/05/07/rYqOKhGyZV7INkt.png" referrerpolicy="no-referrer" style="width: 400px; max-width: 100%;">
</li>
<li data-line="12" class="lc-list-callout" data-callout="~" style="--lc-callout-color: 124, 77, 255;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">~</span> Blocks : </p>
</span></li>
<li data-line="13">
<p><code>Embedding</code> </p>
</li>
<li data-line="14">
<p><code>Position Encoding</code> </p>
</li>
<li data-line="15">
<p><code>Multi-Head Attention</code> </p>
</li>
<li data-line="16">
<p><code>Add &amp; Norm</code> </p>
</li>
<li data-line="17">
<p><code>FeedForward</code>  <a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/622085869/answer/3518358912" rel="noopener" class="external-link" href="https://www.zhihu.com/question/622085869/answer/3518358912" target="_blank">如何理解 Transformers 中 FFNs 的作用？</a></p>
</li>
<li data-line="18" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 自注意力机制</p>
</span></li>
<li data-line="20" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> Transformer 位置编码(Position Encoding) <a data-tooltip-position="top" aria-label="https://spaces.ac.cn/archives/8130" rel="noopener" class="external-link" href="https://spaces.ac.cn/archives/8130" target="_blank">科学空间</a></p>
</span></li>
<li data-line="21"></li>
</ul></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p>block </p>
<ul>
<li>Self-Attention(QKV)</li>
<li>Encoder-decoder </li>
</ul>
<p>transformer 里的 QKV 研究问题和答案之间的关系。不去找前后，不去找相邻，就是单纯的问题（Query）和答案(Value)，最多加了一个(Key)来辅助. 因为这是谷歌搜索等搜索引擎最开始的结构。任何一个问题(Query)，会有很多的答案（Value），而之所以能找到这些答案，是因为这些答案里面包包含了有关于这个问题的关键信息（Key） <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span></p>
<p>这是一个万能形式，任何问题的答案都是通过“问题本身+相关的关键信息”找到的，比如你去谷歌搜索“今天天气怎么样”，这个问题本身就是 Q，而你的语言是“中文”，你的位置是“北京”，你的时间是“今天”，这些就都是 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>，那么找到的答案“下雨”就是 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>。</p>
<p>一般来说肯定是通过方法找到 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span> 中的一些系数，就可以找到正确的 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> 了。我们也可以把 V挪到公式右边，并且把他们存在的关系叫成 attention，那么就是：</p>
<div class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="4"><mjx-c class="mjx-c73"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c66"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c78"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.082em;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></div>
<p>这就是整个 transformer 的最基础结构，有了这个万能结构，只需要学习 F里的各个参数，就可以回答你想要的问题。</p>
<p>那么，又为什么叫 <strong>transformer</strong> 而不是简单的&nbsp;attention&nbsp;呢？</p>
<p>因为 transformer 它为了提高这个 F的 运算效率，做了一些规定，比如你的 attention 的输入输出维度需要一样，这样矩阵运算就可以加快。而且多个 attention 合在一起来算，也是为了加快运算速度和效率。</p>
<p>那整个&nbsp;Transformer里的 <strong>encoded</strong> <strong>decoder</strong> 又是干嘛呢？</p>
<p>这个 QKV 结构是最基础的单元，为了适应不同的目标，把多个 QKV 的 attention 合在一起以后，它分了两个部分，一个用来学怎么 encode，一个用来学怎么 decode。这个就跟 GAN，对抗神经网络很像，基本就是还是为了找到数据高维空间下的一个空间分割平面。而这里为了前后关系，它把上一个运算出来的结果又给进去去计算下一个部分（decoder&nbsp;部分），所以就可以放在语言任务上（因为语言有前后关系）。</p>
<p>对于像翻译这种任务，encoder 部分输入是第一种语言，而 decoder 部分是第二种语言，这样一来训练出来的模型就可以做翻译。</p>
<p>此外, 如果我们处理的是序列数据, 希望保留序列信息, 我们可以添加<strong>Position Encoding</strong>. Position Encoding 有很多方式, 最常用的是 cos 添加角度信息.</p>
<p>GPT&nbsp;的全名是 Generative Pretrained Transformers，它用的是&nbsp;<strong>self-attention</strong>&nbsp;结构，也就是 encoder 部分是前半句（一个字，一个句子），decoder 部分是后半句（或者下一个字，或者下个句子，都行），就是自己学自己，不是两种不同语言，所以就是 self-attention。</p>
<p>当然这只是语言上的应用，像&nbsp;ChatGPT，LLama 这种的。</p>
<p><img alt="bAflYsDOjLtaX7M.png" src="https://cdn.sa.net/2024/04/12/bAflYsDOjLtaX7M.png" referrerpolicy="no-referrer"></p>
<p>head的概念类似卷积中的通道，只不过每个通道的输入都是一样的，类似于把一个通道的数据复制多次。</p>
<p>多头注意力的计算过程类似深度可分离卷积，把通道分开计算，再融合到一起. 使用多头的好处有:</p>
<ol>
<li><strong>捕获多种类型的信息</strong>：在自然语言处理任务中，一个单词的含义可能会受到其上下文中的多个其他单词的影响。通过多头注意力，模型可以学习到如何根据不同的上下文关注不同的单词。例如，对于句子"我喜欢吃苹果，因为它很甜。"，在处理"甜"这个词时，模型可能需要同时关注"吃"和"苹果"这两个词。通过多头注意力，模型可以在一个头中关注"吃"，在另一个头中关注"苹果"。</li>
<li><strong>在不同的表示子空间中学习</strong>：在多头注意力中，每个头都有自己的权重矩阵，这些矩阵会在训练过程中进行学习。这意味着每个头都可以在不同的表示子空间中捕获输入的不同特征。这使得模型能够捕获更丰富的信息，从而提高模型的表达能力。</li>
<li><strong>并行计算</strong>：多头注意力的设计也使得并行计算成为可能。每个头的计算都是独立的，因此可以在硬件允许的情况下并行进行，从而提高计算效率。</li>
</ol>
<p>因为 QKV 就是简单的“问题—答案”结构，这个可以应用在一切问题上，图片，语音，文字，或者相互关联相互变化，都可以。</p>
<p>所以现在如果我回过头来看的话，整个&nbsp;transformers里面的重点就是它把基本的单元从原来神经元之间的连接结构变成了问答的(QKV)attention 结构。</p>
<p>同时Transformer的复杂度不高.<br>
<img alt="mRNF9EhWkDz3noZ.png" src="https://cdn.sa.net/2024/04/12/mRNF9EhWkDz3noZ.png" referrerpolicy="no-referrer"></p></div></div></div><div><p><img src="https://picx.zhimg.com/v2-162b5bc0cff35cb5368e8ac58ee85845_r.jpg?source=1def8aca" referrerpolicy="no-referrer"></p></div><div class="admonition-parent admonition-tip-parent"><div class="callout admonition admonition-tip admonition-plugin " style="--callout-color: 0, 191, 165;" data-callout="tip" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="fire" class="svg-inline--fa fa-fire fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M216 23.86c0-23.8-30.65-32.77-44.15-13.04C48 191.85 224 200 224 288c0 35.63-29.11 64.46-64.85 63.99-35.17-.45-63.15-29.77-63.15-64.94v-85.51c0-21.7-26.47-32.23-41.43-16.5C27.8 213.16 0 261.33 0 320c0 105.87 86.13 192 192 192s192-86.13 192-192c0-170.29-168-193-168-296.14z"></path></svg></div><div class="callout-title-inner admonition-title-content">Tip</div></div><div class="callout-content admonition-content"><p>历史</p>
<ul>
<li>注意力机制最早由Google团队在2014年<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1406.6247" rel="noopener" class="external-link" href="https://arxiv.org/abs/1406.6247" target="_blank"> Recurrent Models of Visual Attention</a>论文中提出, 使用RNN + 注意力机制对图片进行分类.</li>
<li><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1409.0473" rel="noopener" class="external-link" href="https://arxiv.org/abs/1409.0473" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a>  在同年, 将注意力机制引入NLP, 在机器翻译任务上将翻译和对其同时进行. 点燃了注意力机制在基于RNN/CNN的NLP任务的大放异彩. </li>
<li>2017年Google团队发表 Attention is all you need, 首次提出自注意力机制, 允许模型在序列中的不同位置之间建立动态关系</li>
<li>自2017年Transformer提出以后，关于Transformer模型结构的改进层出不穷，比如语音识别的Conformer、Branchformer、E-Branchformer、R-Transformer 等等</li>
<li>目前(2023), Transformer已经成为了自然语言处理领域绝对主流的网络架构</li>
<li></li>
</ul></div></div></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="faq" class="callout drop-shadow"><div class="callout-title"><div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-help-circle"><circle cx="12" cy="12" r="10"></circle><path d="M9.09 9a3 3 0 0 1 5.83 1c0 2-3 3-3 3"></path><path d="M12 17h.01"></path></svg></div><div class="callout-title-inner">Transformer vs. RNN</div></div><div class="callout-content">
<p><strong>相比于RNN, Transformer 的优势(非常明显)主要来自于</strong> </p>
<ul>
<li data-line="2">由于注意力机制将序列中所有位置的信息都一视同仁地看待（除了表征位置信息的位置编码以外）, 而带来的 <span style="background:#fff88f">全局建模能力</span></li>
<li data-line="3">由于自注意力机制中的点积运算更加适合 GPU训练和Transformer 上下参数之间没有依赖, 所导致的能够<span style="background:#fff88f">更好提高训练效率</span>和<span style="background:#fff88f">训练参数的scalability </span></li>
<li data-line="4">不存在RNN中的递归结构, 非常不容易出现训练时的梯度爆炸/消失.</li>
</ul>
</div></div></div><div class="admonition-parent admonition-faq-parent"><div class="callout admonition admonition-faq admonition-plugin " style="--callout-color: 100, 221, 23;" data-callout="faq" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="question-circle" class="svg-inline--fa fa-question-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zM262.655 90c-54.497 0-89.255 22.957-116.549 63.758-3.536 5.286-2.353 12.415 2.715 16.258l34.699 26.31c5.205 3.947 12.621 3.008 16.665-2.122 17.864-22.658 30.113-35.797 57.303-35.797 20.429 0 45.698 13.148 45.698 32.958 0 14.976-12.363 22.667-32.534 33.976C247.128 238.528 216 254.941 216 296v4c0 6.627 5.373 12 12 12h56c6.627 0 12-5.373 12-12v-1.333c0-28.462 83.186-29.647 83.186-106.667 0-58.002-60.165-102-116.531-102zM256 338c-25.365 0-46 20.635-46 46 0 25.364 20.635 46 46 46s46-20.636 46-46c0-25.365-20.635-46-46-46z"></path></svg></div><div class="callout-title-inner admonition-title-content">Faq</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/644491667/answer/3398148139" rel="noopener" class="external-link" href="https://www.zhihu.com/question/644491667/answer/3398148139" target="_blank">为什么transformer在图像的效果比CNN好？ - 知乎</a></p>
<p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/531529633" rel="noopener" class="external-link" href="https://www.zhihu.com/question/531529633" target="_blank">在CV界，传统卷积已经彻底输给Transformer了吗？ - 知乎</a></p>
<p>至少在CV领域, Transformers 的效果不一定有CNN好, 尤其是在数据量较少的时候.</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=oe59jBeOFL4" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=oe59jBeOFL4" target="_blank">iTransformer讲了一个什么改进模型的思路？ - YouTube</a></p>
<p><img src="https://cdn.sa.net/2024/04/15/6doERMsal4veXC2.png" referrerpolicy="no-referrer"><br>
<img src="https://cdn.sa.net/2024/04/15/qlOrbzJUfvus4Dw.png" referrerpolicy="no-referrer"></p>
<p>蚂蚁集团出的一个维度倒置Transformer, 用于时间信息的注意力机制. </p>
<ul>
<li>可以帮助扩大模型窗口</li>
<li>实现方法主要是改变数据处理的方法</li>
</ul></div></div></div><div><p><strong>"Attention is All You Need"</strong> 是一篇由Google Brain团队的研究员在2017年撰写的论文，这篇论文首次提出了 Transformer 模型, 使用自注意力机制(Self-attention将序列问题转换为全连接层问题, 使模型能够在不同位置之间建立动态的关系，避免了传统循环神经网络（RNN）和长短时记忆网络（LSTM）的局限(难以捕捉长序列之间关系, 容易梯度爆炸/消失, 序列结构难以并行, 模型参数随序列长度指数增长, 难以批处理).  强力推动了自然语言处理, 计算机视觉等多</p></div><div><p>这是 Transformer - model architecture, 其中有两个最关键关键组件. 多头注意力机制, 位置掩码.</p></div><div><p><img alt="image.png" src="https://cdn.sa.net/2024/04/14/Dw4yhfKm5lctjg1.png" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p>Embedding 输入</p>
<ul>
<li data-line="1">经过Tokenizer之后的RAW数据变为Embedding</li>
<li data-line="2">为了让模型批量处理不同长度的音频，我们将同一个批次中的输入填充 (padding) 到同样长度</li>
</ul>
</li>
<li data-line="3"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>位置掩码(Positional Encoding)</strong></p>
<ul>
<li data-line="4">由于Transfomer没有像RNN那样隐含顺序信息,  所以位置掩码被添加到Embedding, 允许模型学到每个Embedding的位置和相对位置(无序数据)信息. 在这之前, 一般需要将相对位置和绝对位置直接注入到模型</li>
<li data-line="5"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>位置掩码和Embedding的维度相同, 两者直接相加. 具体的掩盖设计有如下两种
<ul>
<li data-line="6">在模型输入位置添加一个可训练的层</li>
<li data-line="7">或者更加常用的是使用三角函数来编码位置信息<br>
<span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msup><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-script style="vertical-align: 0.53em;"><mjx-mfrac size="s"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msub size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.34em;"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-script></mjx-msup></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span><span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msup><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-script style="vertical-align: 0.53em;"><mjx-mfrac size="s"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msub size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.34em;"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-script></mjx-msup></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span>其中, pos是输入序列最长长度, i 是序列中位置, d_model是Embedding的维度</li>
</ul>
</li>
<li data-line="11"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>位置掩码扩容方法
<ul>
<li data-line="12">简单外推(Extrapolation): 提前预留维度并设为0. 但是可能导致外推维度的训练不充分, 导致外推位置启用后模型性能严重下降.  </li>
<li data-line="13">线性内推: 将数和输之间的区间变小(e.g. 1,2,3.... 1.5,2.2.5,3). 模型学习的特征不一样了, 需要微调让模型重新学习拥挤的映射关系.</li>
<li data-line="14">进制转换: 比如将10进制变为16进制, 表示数量范围变大. 数值范围的天花板变大, 模型一般有泛化能力. 或者将更低进制减少变化程度. </li>
</ul>
</li>
</ul>
</li>
<li data-line="15"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>自注意力机制(Self-attention)</strong> 是整个架构的基础 <span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munderover space="4"><mjx-over style="padding-bottom: 0.192em; padding-left: 0.279em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-over><mjx-box><mjx-munder><mjx-row><mjx-base><mjx-mo class="mjx-lop"><mjx-c class="mjx-c2211 TEX-S2"></mjx-c></mjx-mo></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.148em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-under></mjx-row></mjx-munder></mjx-box></mjx-munderover><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> <img alt="image.png" src="https://cdn.sa.net/2024/01/23/MdOrH9oNbQRCKZz.png" referrerpolicy="no-referrer"></p>
<ul>
<li data-line="17" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> Embedding 经过 Postion Encoding的修饰之后输入到自注意力机制当中</span></li>
<li data-line="18" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 输入经过三个不同的全连接层 <code>nn.Linear</code> 获得和Embedding维度相同的 <code>query</code>, <code>key</code>, <code>value</code> 三个矩阵. 
</span><ul>
<li data-line="19">使用三个不同全连接层是因为独立层能够使得QKV保持独立, 使得更它们更有效地捕捉不同类型的信息和关系</li>
<li data-line="20" class="lc-list-callout" data-callout="&amp;" style="--lc-callout-color: 255, 214, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">&amp;</span> QK必须相同维度, 因为要点积. V不一定</span></li>
<li data-line="21">&nbsp;注意力机制本身并没有对&nbsp;QKV 的内容做出任何限制我们可以这样理解. 我们希望用Q 把 V 中的东西找出来, 而 K 是 V 的钥匙. 如果 Q和K的匹配度越高, 那么就可以在 K 这个位置对应的 V 中找出更多Q要在V中查询的信息. </li>
<li data-line="22">比如，我们现在希望计算音频和文本之间的注意力，或者说希望从音频特征中提取和文本相关的信息，那么这个时候应该将文本特征作为&nbsp;Q&nbsp;，音频特征作为&nbsp;K&nbsp;和&nbsp;V&nbsp;（<strong>交叉注意力机制</strong>）；又比如，我们希望计算文本和文本自身的注意力，那么就应该将文本特征同时作为&nbsp;，QKV.&nbsp;（<strong>自注意力机制</strong>）</li>
</ul>
</li>
<li data-line="23" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 计算 <code>query</code> 和 <code>key</code> 之间的相似度(点积、拼接、感知机). 以下以点积为例.  <img alt="image.png" src="https://cdn.sa.net/2024/01/22/z5lTBPSoeMfd12j.png" referrerpolicy="no-referrer">
</span><ul>
<li data-line="24"><code>query</code> 先和 <code>key</code> 点积用于衡量每个词（查询）与句子中每个其他词（键）的相关性.</li>
<li data-line="25">然后，应用缩放因子对这些点积进行缩放。通常，这个缩放因子是键向量维度的平方根的倒数</li>
<li data-line="26">最后, 使用softmax函数将其归一化为概率. 这个softmax之后的概率表示该个Embedding在该位置的重要程度.</li>
<li data-line="27" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> 矩阵点积为什么可以计算相似度? 因为两矩阵方向相同, 点积较大, 方向越垂直, 点积越小. 在几何上等同于计算两个向量的长度乘积和它们之间夹角余弦的乘积. 同时考虑了矩阵的长度和方向. 同时在GPU运算更加高效</span></li>
<li data-line="28" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 自注意力机制本质上是求一个离散概率的数学期望. <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munderover space="4" limits="false"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2211 TEX-S1"></mjx-c></mjx-mo><mjx-script style="vertical-align: -0.285em; margin-left: 0px;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom><mjx-spacer style="margin-top: 0.291em;"></mjx-spacer><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-munderover><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.024em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span> ; 离散分布P是softmax之后的值, X 是 V. </span></li>
</ul>
</li>
<li data-line="29" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> 最后, softmax后的值 和 <code>value</code> 相乘. 用于增强该位置Embedding重要性或者减少重要性</span></li>
<li data-line="30"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>🌰 : “I eat pizza today”
<ul>
<li data-line="31">每个Token(I, eat, pizza, today)先转换为 Embedding</li>
<li data-line="32">对于每个Token我们使用三个不同的全连接层来生成QKV. 例如，“I”会生成一个Q向量、一个K向量和一个V向量。</li>
<li data-line="33">接下来，我们计算每个词的Q向量与所有词的K向量的点积。这相当于在评估句子中的每个词与其他每个词之间的关系。例如，计算“I”的Q向量与“I”，“eat”，“pizza”，“today”的K向量的点积</li>
<li data-line="34">之后对点积应用缩放因子，并使用softmax函数进行归一化，得到注意力权重。</li>
<li data-line="35">每个Token的Softmax值的V向量进行加权求和。这个加权和代表了考虑到了整个句子上下文的当前词的表示。例如，对于“I”，我们将它的注意力权重与所有词的V向量相乘，并加总起来，得到一个新的加权向量，这个向量就是考虑了整个句子上下文的“I”的表示。</li>
</ul>
</li>
<li data-line="36" class="lc-list-callout" data-callout="$" style="--lc-callout-color: 0, 200, 83;"><span class="lc-li-wrapper"><span class="lc-list-marker">$</span> 根据具体任务的需要, 我们会添加掩码(Mask). 其作用是通过修改注意力权重, 以避免不不要或不需要的信息传递到模型中. 比如, 在在翻译任务中我们不希望Encoder访问到句子后面的内容, 我们就可以用Mask遮蔽避免泄漏. 或者在处理变长序列中补全长度(padding)</span></li>
</ul>
</li>
<li data-line="37"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>多头注意力机制(Multi-head Attention)</strong><img alt="image.png" src="https://cdn.sa.net/2024/01/22/9teJRY7VQnXATIE.png" referrerpolicy="no-referrer"></p>
<ul>
<li data-line="38">多头注意力机制允许模型在不同子空间中学习到相关信息, 以捕捉数据的不同方面. 提高并行度并简化模型的计算复杂度. </li>
<li data-line="39"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>将 QKV 分别用h个不同投影矩阵投影h次, 然后分别做点积注意力, 最后将每个头计算出的加权和简单拼接回一个完整的向量后通过一个全连接层产生最终的输出
<ul>
<li data-line="40">相当于人从不同的视角观察一个物体</li>
</ul>
</li>
<li data-line="41" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 《Are sixteen heads really better than one?》多头自注意力机制不一定比单头好</span></li>
<li data-line="42" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 《Low-Rank Bottleneck in Multi-head Attention Models》指出Multi-Head Attention的表达能力瓶颈，并提出增大key_size来缓解。
</span><ul>
<li data-line="43">我们将 QK维度称为 key_size, V的维度为 head_size</li>
<li data-line="44">h 是 注意力头的数量. 一般实际是将原始d维度的QKV投影到 d/h)维中, 单独计算后输出 d/h 的结果. 然后将 h*d/h 拼接起来获得最终注意力值</li>
<li data-line="45"><span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msqrt size="s"><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.082em;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math></mjx-container></span> 可以看作一个二元联合分布. 假设序列长度为 n. 因为 QK维度一样所以有这个分布有 n*n = <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container></span> 的值</li>
<li data-line="46">但是将QK投影之后, 各自分布只有 n*(d/h) 的. 总的参数量为 2nd/h &lt;&lt; n^2. 参数量的减少导致表达能力的削弱. 这就是Low-rank bottleneck, 尤其是 h 较多时.这个叫低秩瓶颈. </li>
<li data-line="47">我们可以增加 d 的维度, 但是这会增加模型的复杂性. 或者减少 h, 但是多头本身就可以增加模型的表达能力</li>
<li data-line="48">我们可以通过只增加 key_size 以增加模型的表达能力, 而尽可能不增加模型的复杂性</li>
</ul>
</li>
<li data-line="49" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 《Talking-head attention》, 将多头的低秩分布叠加(在softmax之前叠加, e.g.加权平均)增强模型的表达能力. 原理是多个高斯分布(GMM)叠加的数量够多, 就可以逼近任意概率分布. </span></li>
</ul>
</li>
<li data-line="50" class="lc-list-callout" data-callout="~" style="--lc-callout-color: 124, 77, 255;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">~</span> 爱因斯坦求和约定（Einstein Summation Convention）<code>torch.ensum</code> 是一种表达多重求和（比如矩阵乘法、点积、批量矩阵乘法等）和某些类型的数据重排操作的简介写法. 比如</p>
</span><ul>
<li data-line="51"><strong>矩阵乘法：</strong> 例如，矩阵乘法 <code>AB</code> 可以表示为 <code>torch.einsum('ij,jk-&gt;ik', [A, B])</code>，其中 <code>A</code> 是一个形状为 <code>(i, j)</code> 的矩阵，<code>B</code> 是一个形状为 <code>(j, k)</code> 的矩阵。</li>
<li data-line="52"><strong>批量矩阵乘法：</strong> 对于批量矩阵乘法，比如有批次的两个矩阵 <code>A</code> 和 <code>B</code>，其操作可以表示为 <code>torch.einsum('bij,bjk-&gt;bik', [A, B])</code>。</li>
<li data-line="53"><strong>求和操作：</strong> 如果你想对一个矩阵的行进行求和，可以使用 <code>torch.einsum('ij-&gt;i', A)</code>。</li>
</ul>
</li>
<li data-line="54" class="lc-list-callout" data-callout="$" style="--lc-callout-color: 0, 200, 83;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">$</span> 这里用序列重新梳理一下Attention的公式<img alt="gpt-transformer.png" src="https://cdn.sa.net/2024/01/24/J7fCRGVWFL5EabS.png" referrerpolicy="no-referrer"></p>
</span><ul>
<li data-line="55"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>令 <code>F[t]</code> 为 t时刻的系统状态(高维状态); 令 <code>x[t]</code> 为 t 时刻的外部输入信息状态; 
<ul>
<li data-line="56">预测 <code>F[t+1]</code> 时，需考虑 <code>F[0]</code>,<code>F[1]</code>, .. <code>F[t]</code>。因此，生成长度 T 的序列，需 <code>O(T^2)</code> 复杂度。</li>
</ul>
</li>
<li data-line="57"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>那么Attention可以简化为 <img alt="image.png" src="https://cdn.sa.net/2024/01/25/kvG1ogqlYeRINpH.png" referrerpolicy="no-referrer">
<ul>
<li data-line="58">每个状态 &nbsp;i 对于后续的潜在贡献是&nbsp;<strong>V</strong>F[i] </li>
<li data-line="59">用&nbsp;<strong>Q</strong>x[t] 矢量，与此前的所有&nbsp;<strong>K</strong>F[i] 矢量分别做点乘，再 exp，得到 x[t] 与之前各个 F[i] 状态的匹配度。</li>
<li data-line="60">如果匹配度&nbsp;exp⁡(Q x[t]∗ K F[i])&nbsp;越大，<strong>V</strong>F[i] 的权重越大</li>
<li data-line="61">分母为归一化因子。</li>
<li data-line="62" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 公式中没有显式出现 t 与 i 的距离信息。我们会采用其它方式（例如位置编码）将其注入系统。</span></li>
</ul>
</li>
</ul>
</li>
</ul></div><div><p>我们这里实现一个带Mask和多头的 <code>SelfAttention</code> 的 pytorch示例代码:</p></div><div><p>编码器-解码器相关论文起源于 <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1409.3215" rel="noopener" class="external-link" href="https://arxiv.org/abs/1409.3215" target="_blank">Sequence to Sequence Learning with Neural Networks</a><br>
Transformer 完全基于注意力机制, 拥有编码器(Encoder)和解码器(Decoder)两个部分, 两者时常搭配使用, 但用途和模型结构相互独立, 也没有规定网络架构. 你可以编码器用Transformer解码器用LSTM...<br>
<img src="https://pic3.zhimg.com/v2-c7d7c326d453fa1d143c35abc543cb3a_r.jpg" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0"><strong>编码器(Encoder)</strong> 负责特征编码，即从原始的、比较底层的输入序列信号提取出抽象的、具有明显语义特征的特征信息. 由图可知是由  特征编码、位置编码、和若干个 TransformerBlock 组成(注意力+MLP+*归一化+*残差)</li>
<li data-line="1"><strong>解码器(Decoder)</strong> 负责从编码器得到的原始序列的特征信息中"破译"出目标序列的内容（比如从音频序列的特征中"破译"其对应的文本信息). </li>
<li data-line="2" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 这种架构提出的原因是因为, 类似RNN和LSTM这样的模型输入-输出是对应的. 但在有的任务中我们不希望输入-输出的长度相同. </span></li>
<li data-line="3"><strong>信息流</strong>：输入序列首先进入编码器，经过一系列变换后，转换为一组高维表示。这些表示然后被传递到解码器，解码器利用这些信息以及自身的前一步输出来逐步生成最终的输出序列。</li>
</ul></div><div><p>我们在这里结合之前的<code>SelfAttention</code>代码, 构建一个 <code>TransformerBlock</code> , 后搭建Encoder和Decoder. </p></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="faq" class="callout drop-shadow"><div class="callout-title"><div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-help-circle"><circle cx="12" cy="12" r="10"></circle><path d="M9.09 9a3 3 0 0 1 5.83 1c0 2-3 3-3 3"></path><path d="M12 17h.01"></path></svg></div><div class="callout-title-inner">为什么GPT要使用Decoder-only架构 而非 BERT的Encoder-only 架构?</div></div><div class="callout-content">
<p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/588325646/answer/2929224109" rel="noopener" class="external-link" href="https://www.zhihu.com/question/588325646/answer/2929224109" target="_blank">为什么现在的LLM都是Decoder only的架构？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/608929992/answer/3119081297" rel="noopener" class="external-link" href="https://www.zhihu.com/question/608929992/answer/3119081297" target="_blank">decoder-only和encoder-decoder transformer在应用时最大的区别是？ - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/642923989" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/642923989" target="_blank">LLM的3种架构：Encoder-only、Decoder-only、encode-decode - 知乎</a></p>
<ul>
<li data-line="4"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Encoder-Decoder架构 
<ul>
<li data-line="5"><strong>适用任务</strong>: 这种架构非常适合于那些需要理解输入和生成输出的任务，如机器翻译、文本摘要等。在这些任务中，模型需要首先理解输入文本（通过Encoder部分），然后基于这个理解生成新的文本（通过Decoder部分）。</li>
<li data-line="6"><strong>代表性模型</strong>: 最著名的例子是原始的Transformer模型，它首次在论文《Attention is All You Need》中被提出。这个模型在机器翻译任务中取得了显著的成效。</li>
</ul>
</li>
<li data-line="7"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Encoder-only架构
<ul>
<li data-line="8"><strong>适用任务</strong>: 这种架构适用于只需理解输入文本的任务，比如文本分类、情感分析、命名实体识别等。在这些任务中，模型的目标是分析和理解输入，而不需要生成任何新的文本。</li>
<li data-line="9"><strong>代表性模型</strong>: BERT（Bidirectional Encoder Representations from Transformers）是这一类架构中最著名的例子。通过预训练一个大型的语料库，BERT学会了理解语言的上下文，可以被用于各种不同的自然语言处理任务</li>
</ul>
</li>
<li data-line="10"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Decoder-only架构
<ul>
<li data-line="11"><strong>适用任务</strong>: Decoder-only架构特别适合于文本生成任务，如语言模型训练、文本生成、代码生成等。这种架构通常会被训练来预测接下来的单词或者序列。</li>
<li data-line="12"><strong>代表性模型</strong>: GPT（Generative Pre-trained Transformer）系列是这一类架构中最知名的例子。GPT通过大量的预训练，能够生成连贯、相关的文本，可以用于各种生成型任务。</li>
</ul>
</li>
</ul>
</div></div></div><div><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=bQ5BoolX9Ag&amp;t=13s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=bQ5BoolX9Ag&amp;t=13s" target="_blank">StatQuest - Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly Explained!!!</a></p></div><div><div data-callout-metadata="" data-callout-fold="" data-callout="check" class="callout drop-shadow"><div class="callout-title"><div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-check"><path d="M20 6 9 17l-5-5"></path></svg></div><div class="callout-title-inner">加速Transformer的有关尝试, 可以分为两个方向</div></div><div class="callout-content">
<ul>
<li data-line="1">降低Attention运算的时间复杂度从 O(N^2) 到 O(N), 比如 Linear Transformer 和 Linformer, 一般是用近似技巧, 但是性能会有所下降</li>
<li data-line="2">在不改变attention理论时间复杂度的前提下，尽可能加速attention的运算, 比如 Flash Attention. 原理基本上是减少了GPU的SRAM (<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Static_random-access_memory" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Static_random-access_memory" target="_blank">Static Random Access Memory</a>) 和HBM (<a data-tooltip-position="top" aria-label="https://semiwiki.com/wikis/semiconductor-ip-wikis/high-bandwidth-memory-hbm-wiki/#:~:text=High%20Bandwidth%20Memory%20%28HBM%29%20is%20a%20high-performance%20RAM,HBM%20were%20the%20AMD%20Fiji%20GPUs%20in%202015" rel="noopener" class="external-link" href="https://semiwiki.com/wikis/semiconductor-ip-wikis/high-bandwidth-memory-hbm-wiki/#:~:text=High%20Bandwidth%20Memory%20%28HBM%29%20is%20a%20high-performance%20RAM,HBM%20were%20the%20AMD%20Fiji%20GPUs%20in%202015" target="_blank">High Bandwidth Memory</a>)之间的通信开销</li>
</ul>
</div></div></div><div><p>一个很好的大模型可视化项目<br>
<a data-tooltip-position="top" aria-label="https://bbycroft.net/llm" rel="noopener" class="external-link" href="https://bbycroft.net/llm" target="_blank">LLM Visualization</a>#### Vision Transformer(ViT)</p></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin " style="--callout-color: 0, 200, 83;" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=-Qt35Ky6Xh4&amp;t=22s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=-Qt35Ky6Xh4&amp;t=22s" target="_blank">当Transformer遇上空间和时间，就变成全能模型了！ - YouTube</a></p>
<ul>
<li>视频介绍了两篇Transformer的变体</li>
<li><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1912.09363" rel="noopener" class="external-link" href="https://arxiv.org/abs/1912.09363" target="_blank">Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting - 2021</a><br>
<img src="https://cdn.sa.net/2024/04/22/HZxAGs7vymgElbk.png" referrerpolicy="no-referrer"><br>
- 谷歌的动机希望这个模型同时可以处理不同类型的序列, 能够有解释性, 不能有太大的误差累计, 能用上统计信息, 能给出预测范围区间<br>
- 反正是混合尽可能多的信息</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><a data-tooltip-position="top" aria-label="https://arxiv.org/pdf/2001.02908.pdf" rel="noopener" class="external-link" href="https://arxiv.org/pdf/2001.02908.pdf" target="_blank">Spatial-Temporal Transformer Networks for Traffic Flow Forecasting</a> 
<ul>
<li>利用时空信息预测交通</li>
<li>模型实现是spital-temporal(spital +tempora ) * k </li>
<li>spital 中有卷积层添加对空间的理解能力</li>
<li>这里在还讲了输入数据改变之后, 数据预处理, Loss设计, 连接维度, 归一化等也会发生改变</li>
<li></li>
</ul>
</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="Vision Task" class="heading" id="Vision_Task"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Vision Task</h3><div class="heading-children"><div><div data-callout-metadata="" data-callout-fold="" data-callout="check" class="callout drop-shadow"><div class="callout-title"><div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-check"><path d="M20 6 9 17l-5-5"></path></svg></div><div class="callout-title-inner">Check</div></div><div class="callout-content">
<p><img src="https://cdn.sa.net/2024/04/05/25mqLdI7Fth491z.png" referrerpolicy="no-referrer"><br>
<a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV13N4y1J7BD/?spm_id_from=333.1007.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV13N4y1J7BD/?spm_id_from=333.1007.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">论文研读之简化版Transformer：Simplifying Transformer Blocks_哔哩哔哩_bilibili</a><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/677511164" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/677511164" target="_blank">【论文详解】简化版Transformer：Simplifying Transformer Blocks - 知乎</a><br>
<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/451568838" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/451568838" target="_blank"># Vison Transformer学习</a></p>
</div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=yRPVQKJaStw" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=yRPVQKJaStw" target="_blank">视觉Transformer是什么？ViT为什么是现在使用最广泛的模型？以及LLM的前世今生！ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2010.11929" rel="noopener" class="external-link" href="https://arxiv.org/abs/2010.11929" target="_blank">[2010.11929] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p>
<p>介绍了ViT , 一个现今最流行的模型. GPT, CLIP等著名项目都适用的模型Backbone. </p>
<p>ViT 把 CNN模块全部丢弃, 发现在图像分类任务上准确率和效率上都很好. </p>
<p>原因是:</p>
<ol>
<li>Self-Attention的全注意力机制更加容易在GPU上并行计算</li>
<li>使用固定大小的Patch替代CNN卷积, 复杂度更低</li>
<li>没有卷积层的ViT参数更少</li>
</ol>
<p>讲了</p>
<ul>
<li>Transformer 具有局部感受域和CNN的kernel看一个Size不一样</li>
</ul>
<p><img src="https://cdn.sa.net/2024/04/10/64YDWzTCkNELalb.png" referrerpolicy="no-referrer"></p>
<p>ViT 处理图像的方法</p>
<ul>
<li>Position Encoding 是 1D, 2D 效果都差不多</li>
<li>QKV 维度都是 197 * 768</li>
<li>用CLS Token 添加位置特征</li>
</ul>
<p>ViT 提取到了更加抽象的特征相比AlexNet<br>
ViT 训练调参敏感</p></div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=PCKnpM2g4r4" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=PCKnpM2g4r4" target="_blank">用MLP做Transformer？异想天开！其实注意力机制很简单！ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2105.01601" rel="noopener" class="external-link" href="https://arxiv.org/abs/2105.01601" target="_blank">[2105.01601] MLP-Mixer: An all-MLP Architecture for Vision - 2021</a></p>
<p>Google做的ViT的后续扩展. 使用纯MLP替代Transformer中Attention机制. </p>
<p>这篇文章介绍了一个全新的视觉模型架构一一MLP-Mixer,它完全基于多层感知机(MLPs)。与<br>
传统依赖于卷积和自注意力的模型不同，MLP-Mixr通过两种类型的MLP层来处理图像，一种在图<br>
像块(patches)上独立应用（即特征混合），另一种跨图像块应用（即空间信息混合）。通过在<br>
大规模数据集上训练或使用现代正则化方案，MLP-Mⅸr能够在图像分类基准测试中达到与最先进<br>
模型相当的性能，同时保持与它们相近的预训练和推断成本。作者希望这一结果能激发进一步的研<br>
究，超越目前基于卷积神经网络(CNNs)和Transformers的范畴。</p>
<p>MLP-Mixer 的主要优点在于其架构的简洁性和对大规模数据集的良好适应性。相比传统的CNN和基<br>
于注意力的模型，MLP-Mixer 不依赖于卷积操作或自注意力机制，而是仅通过基本的矩阵乘法、数<br>
据重排（如重塑和转置）、以及非线性激活函数实现。这种设计简化了模型的结构，同时在一些情<br>
况下，如在非常大的数据集上训练时，能够达到与最先进模型相似甚至更好的性能。此外，由于其<br>
简单的矩阵运算基础，MLP-Mxr在某些应用场景下可能更易于优化和扩展。</p>
<p><img alt="YaMf5AvRg6EWPXi.png" src="https://cdn.sa.net/2024/04/11/YaMf5AvRg6EWPXi.png" referrerpolicy="no-referrer"></p>
<p>在论文中，图1（Figure 1）展示了MLP-Mixer架构的宏观结构。这个结构接受一系列线性投影的图像块（也称为tokens）作为输入，这些图像块被组织成一个“patches × channels”的表格。MLP-Mixer的输入维度保持不变，即输入的图像块数量（S）和通道数（C）。</p>
<p>图1中的关键组件包括：</p>
<ol>
<li>
<p><strong>Patches</strong>: 输入图像被分割成不重叠的小块，每个小块都被线性投影到一个期望的隐藏维度C。如果原始输入图像的分辨率为(H, W)，每个图像块的分辨率为(P, P)，那么图像块的数量S = HW/P^2。</p>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>Mixer Layer</strong>: 这是MLP-Mixer的核心组件，包含两种类型的MLP层：token-mixing MLP和channel-mixing MLP。这两种MLP层交替出现，以实现输入的两个维度（空间位置和通道）之间的交互。</p>
<ul>
<li><strong>Token-mixing MLP</strong>: 这种类型的MLP作用于输入表格的列（即它应用于转置后的输入表格X^T），并独立地处理每个通道，从而允许不同空间位置（tokens）之间的信息交流。<br>
</li>
<li><strong>Channel-mixing MLP</strong>: 这种类型的MLP作用于输入表格的行，允许不同通道之间的信息交流。<br>
</li>
</ul>
</li>
<li>
<p><strong>Fully-connected</strong>: 这是MLP层的基本组成部分，每个MLP层包含两个全连接层（fully-connected layers）和一个GELU非线性激活函数。</p>
</li>
<li>
<p><strong>Layer Norm</strong>: 在每个MLP层之后，使用层归一化（Layer Normalization）来稳定训练过程。</p>
</li>
<li>
<p><strong>Skip-connections</strong>: 这些是现代深度学习架构中常用的组件，有助于信息在网络中流动，并减少梯度消失的问题。</p>
</li>
<li>
<p><strong>Global Average Pooling</strong>: 在所有MLP层之后，使用全局平均池化（Global Average Pooling）来聚合空间信息，为分类头（classifier head）做准备。</p>
</li>
<li>
<p><strong>Per-patch Fully-connected</strong>: 每个图像块都有一个全连接层，用于最终的分类任务。</p>
</li>
<li>
<p><strong>Class Layer</strong>: 最后，使用一个线性分类器（通常是一个全连接层）来输出最终的分类结果。</p>
</li>
</ol>
<p>图1还展示了MLP-Mixer中的跳跃连接（skip-connections）和层归一化（layer norm on the channels），这些都是帮助模型训练和泛化的重要组件。</p>
<p><strong>添加了token-mixing MLP</strong> </p>
<p><strong>添加了channel-mixing MLP</strong></p>
<p>自己看图, 有 layer features 的可视化</p></div></div></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content heading-wrapper"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=luP3-Fs0QCo&amp;t=18" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=luP3-Fs0QCo&amp;t=18" target="_blank">Swin Transformer论文精读【论文精读】 - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2103.14030" rel="noopener" class="external-link" href="https://arxiv.org/abs/2103.14030" target="_blank">[2103.14030] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></p>
<p><a data-tooltip-position="top" aria-label="https://github.com/microsoft/Swin-Transformer" rel="noopener" class="external-link" href="https://github.com/microsoft/Swin-Transformer" target="_blank">GitHub - microsoft/Swin-Transformer: This is an official implementation for "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows".</a></p>
<p><img src="https://cdn.sa.net/2024/04/19/zfbDPUNJs7O4uxq.png" referrerpolicy="no-referrer"><br>
<img src="https://cdn.sa.net/2024/04/19/hTNnFtGsjYefdw1.png" referrerpolicy="no-referrer"><br>
<img alt="zZmck9UM53dYlL4.png" src="https://cdn.sa.net/2024/04/19/zZmck9UM53dYlL4.png" referrerpolicy="no-referrer"></p>
<ul>
<li>微软在2021工作, 在获得了 ICCV 2021 的 best paper. 成为一个CV任务的baseline和基础模型</li>
<li>Swim来自于与 Shift + Windows. 这个论文的任务是希望ViT可以拥有和CNN一样的层级提取特征的能力</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>挑战</strong>:
<ul>
<li>直接用ViT, 不同特征的尺寸不能使用相同大小的patch进行自注意力容易丢失信息</li>
<li>对于大分辨率的图像, ViT输入的会使得复杂度变得特别高</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>解决方法</strong>
<ul>
<li>使用移动窗口获取特征, 让两个窗口之间有交互(overlapping), 利于建立全局感受域</li>
<li>自注意力发生在窗口之内, 计算开销不是由图像分辨率指数变大, 而是线性增长. </li>
<li>分层结构有更好的全局-局部感受域, 利于下游CV任务 </li>
<li>提出了Patch Merging, 将四个小patch合并成一个(类似于CNN中Pooling)</li>
<li>除此之外使用相对位置编码和掩码提高效率</li>
</ul>
</li>
<li><strong>self-supervised</strong> <a data-tooltip-position="top" aria-label="https://github.com/SwinTransformer/Transformer-SSL" rel="noopener" class="external-link" href="https://github.com/SwinTransformer/Transformer-SSL" target="_blank">GitHub - SwinTransformer/Transformer-SSL: This is an official implementation for "Self-Supervised Learning with Swin Transformers".</a></li>
<li><strong>swim-unet</strong> <a data-tooltip-position="top" aria-label="https://link.springer.com/chapter/10.1007/978-3-031-25066-8_9" rel="noopener" class="external-link" href="https://link.springer.com/chapter/10.1007/978-3-031-25066-8_9" target="_blank">Swin-Unet: Unet-Like Pure Transformer for&nbsp;Medical Image Segmentation | SpringerLink</a></li>
<li class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> 好像还有一个魔改CNN声称比Swim Transformer的效果还好. <a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/510965760/answer/2306584675" rel="noopener" class="external-link" href="https://www.zhihu.com/question/510965760/answer/2306584675" target="_blank">如何评价FAIR提出的ConvNeXt：CNN匹敌Swin Transformer? - 知乎</a></span></li>
</ul>
<h5 data-heading="什么是Swin Transformer. 能不能详细讲解一下它的原理和网络结构." class="heading" id="什么是Swin_Transformer._能不能详细讲解一下它的原理和网络结构.">什么是Swin Transformer. 能不能详细讲解一下它的原理和网络结构.</h5>
<p>Swin Transformer 是一种基于 Transformer 架构的视觉模型，由刘少才等人于 2021 年提出。它主要针对 Transformer 在处理大规模图像时的计算成本和内存占用问题进行了优化。以下是对 Swin Transformer 的原理和网络结构的简要介绍：</p>
<p><strong>原理</strong>：</p>
<ol>
<li>
<p><strong>局部性感受野</strong>：Swin Transformer 引入了局部性感受野的概念，即在每个层级上，模型只关注图像的一个局部区域，而不是整个图像。这有助于降低计算量。</p>
</li>
<li>
<p><strong>分层结构</strong>：模型采用分层结构，每一层都会将图像的分辨率减半，同时增加感受野的大小。这样可以在不同的层级上捕捉不同尺度的特征。</p>
</li>
<li>
<p><strong>移位机制</strong>：Swin Transformer 引入了一种称为“移位窗口”的机制，它允许模型在不同的层次上动态地调整感受野的位置。这有助于模型捕捉全局上下文信息。</p>
</li>
<li>
<p><strong>有效扩展性</strong>：由于局部性感受野和移位机制，Swin Transformer 可以在不同的分辨率和图像大小上有效地扩展，而不需要显著增加计算成本。</p>
</li>
</ol>
<p><strong>网络结构</strong>：</p>
<ol>
<li>
<p><strong>Patch Partitioning</strong>：输入图像首先被划分成小块（patches），每个小块代表图像的一个局部区域。</p>
</li>
<li>
<p><strong>Layer Structure</strong>：每个 Swin Transformer 块由多个层组成，每层包含多头自注意力机制（Multi-Head Self-Attention, MHSA）和多层感知机（Multi-Layer Perceptron, MLP）。</p>
</li>
<li>
<p><strong>Shifted Window Attention</strong>：在自注意力机制中，模型使用一个可变大小的窗口来关注图像的局部区域。这个窗口在每一层中都会根据一定的模式进行移位，以覆盖整个图像。</p>
</li>
<li>
<p><strong>Hierarchical Structure</strong>：Swin Transformer 通过逐层降低图像分辨率的方式构建了一个层次化的特征表示。每经过一层，图像的分辨率就会减半，同时模型的感受野会增大。</p>
</li>
<li>
<p><strong>Efficient Computation</strong>：由于局部性感受野和移位机制，Swin Transformer 能够在保持 Transformer 架构优势的同时，显著降低计算和内存需求。</p>
</li>
</ol>
<p>Swin Transformer 由于其高效的计算特性和强大的性能，在计算机视觉领域，尤其是在图像识别和目标检测等任务中，得到了广泛的应用。如果你需要更深入的技术细节或数学公式，我可以提供进一步的解释。</p>
<h5 data-heading="Swin Transformer 提出的动机是什么? 解决了Transformer的什么缺点相对于CNN来说?" class="heading" id="Swin_Transformer_提出的动机是什么?_解决了Transformer的什么缺点相对于CNN来说?">Swin Transformer 提出的动机是什么? 解决了Transformer的什么缺点相对于CNN来说?</h5>
<p>Swin Transformer 提出的动机主要是为了解决标准 Transformer（如 Vision Transformer, ViT）在计算机视觉任务中遇到的一些限制，同时希望能够更好地利用 Transformer 在自然语言处理（NLP）中的成功经验。以下是 Swin Transformer 解决的几个关键问题以及相对于传统卷积神经网络（CNN）的优势：</p>
<ol>
<li><strong>计算复杂度</strong>：标准 Transformer（如 ViT）的自注意力机制具有二次方的计算复杂度（O(N^2)），其中 N 是输入序列的长度或图像中的像素数。这使得它在处理高分辨率图像时变得非常昂贵。Swin Transformer 通过引入局部感受野和移位窗口机制，将计算复杂度降低到线性（O(N)），使其更适合于大规模图像处理。</li>
<li><strong>内存占用</strong>：ViT 需要处理大量的 token，这在高分辨率图像上会导致巨大的内存占用。Swin Transformer 通过局部性感受野减少了同时处理的 token 数量，从而降低了内存占用。</li>
<li><strong>层次化特征</strong>：与 ViT 生成单一分辨率的特征图不同，Swin Transformer 构建了一个层次化的特征表示，这使得它能够更好地捕捉不同尺度的特征，类似于 CNN 中的层次化结构。</li>
<li><strong>适应性</strong>：Swin Transformer 通过其设计，能够适应不同分辨率的输入图像，而无需额外的上采样或下采样操作，这为多尺度任务提供了便利。</li>
<li><strong>有效扩展性</strong>：Swin Transformer 可以通过简单地调整模型的层数和每个层中的参数来扩展模型的大小，这使得它可以灵活地适应不同的计算预算和任务需求。</li>
<li><strong>跨模态能力</strong>：Transformer 架构在 NLP 领域的成功表明了其在处理序列数据方面的强大能力。Swin Transformer 旨在将这种能力扩展到视觉领域，为跨模态任务提供了可能性。</li>
<li><strong>统一建模</strong>：Swin Transformer 试图实现 NLP 和 CV 领域的统一建模，即用同一种架构处理文本和图像数据，这有助于模型在不同类型任务之间迁移和学习。</li>
</ol>
<h5 data-heading="Vision Transformer 相比 CNN 有什么缺陷和优点?" class="heading" id="Vision_Transformer_相比_CNN_有什么缺陷和优点?">Vision Transformer 相比 CNN 有什么缺陷和优点?</h5>
<p>Vision Transformer (ViT) 是一种直接将 Transformer 架构应用于图像识别任务的模型，它在自然语言处理 (NLP) 中取得成功后，被引入到计算机视觉 (CV) 领域。与卷积神经网络 (CNN) 相比，ViT 有其独特的优缺点：</p>
<p><strong>Vision Transformer (ViT) 的优点</strong>：</p>
<ol>
<li>
<p><strong>全局自注意力机制</strong>：ViT 能够捕捉图像中任意两个像素之间的依赖关系，这使得它在理解图像全局结构方面具有优势。</p>
</li>
<li>
<p><strong>并行化能力</strong>：Transformer 的自注意力机制可以很容易地并行化，这有助于在多核或多GPU环境中提高训练效率。</p>
</li>
<li>
<p><strong>长距离依赖</strong>：ViT 能够更好地处理长距离依赖，这对于理解图像中的复杂关系和模式很重要。</p>
</li>
<li>
<p><strong>泛化能力</strong>：ViT 在不同大小和分辨率的图像上表现出较好的泛化能力，因为它不依赖于固定的局部感受野。</p>
</li>
<li>
<p><strong>跨模态应用</strong>：由于 Transformer 在 NLP 中的成功，ViT 可以更容易地应用于跨模态任务，如图像和文本的联合建模。</p>
</li>
</ol>
<p><strong>Vision Transformer (ViT) 的缺点</strong>：</p>
<ol>
<li>
<p><strong>计算复杂度</strong>：ViT 的自注意力机制具有 O(N^2) 的时间复杂度，其中 N 是输入序列的长度，这使得它在处理高分辨率图像时计算成本很高。</p>
</li>
<li>
<p><strong>内存消耗</strong>：由于需要处理大量的 token，ViT 在训练和推理时可能会消耗大量内存。</p>
</li>
<li>
<p><strong>对局部特征的捕捉</strong>：ViT 可能不如 CNN 那样擅长捕捉局部特征，因为 CNN 通过卷积层自然地具有局部感受野。</p>
</li>
<li>
<p><strong>训练数据需求</strong>：ViT 通常需要更多的训练数据来达到与其等效的 CNN 模型相同的性能水平。</p>
</li>
<li>
<p><strong>超参数敏感性</strong>：ViT 对超参数的选择可能更敏感，特别是与位置相关的参数和注意力机制的配置。</p>
</li>
<li>
<p><strong>模型初始化和训练稳定性</strong>：ViT 的训练可能需要更仔细的超参数调整和模型初始化策略，以确保训练的稳定性和收敛性。</p>
</li>
<li>
<p><strong>对小目标的检测</strong>：ViT 在检测图像中的小目标方面可能不如专门为目标检测设计的 CNN 结构有效。</p>
</li>
</ol>
<p>Swin Transformer 正是为了解决 ViT 的一些缺点而设计的，它通过引入局部感受野和移位窗口机制来降低计算复杂度和内存消耗，同时保持了 Transformer 架构的优势。</p></div></div></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> iGPT </span></li>
<li data-line="1" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> ViT</span></li>
<li data-line="2" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> BEiT</span></li>
<li data-line="3" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> Mask Autoencoder </span></li>
<li data-line="4" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> Swin Transformer </span></li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="Natural Language Task" class="heading" id="Natural_Language_Task"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Natural Language Task</h3><div class="heading-children"><div><p>R-Transformer</p></div><div><ul>
<li data-line="0">针对高维嵌入导致位置编码失效的解决方案</li>
</ul></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin " style="--callout-color: 0, 200, 83;" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/500807675" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/500807675" target="_blank">一张图看懂BERT - 知乎</a></p>
<p><img src="https://pic2.zhimg.com/v2-24c08636bfa0c75bb9d9af831fa3650d_r.jpg" referrerpolicy="no-referrer"></p></div></div></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="GAN(对抗生成网络)" class="heading" id="GAN(对抗生成网络)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>GAN(对抗生成网络)</h2><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=WlUxIge5XjE" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=WlUxIge5XjE" target="_blank">2016年最火的模型：对抗生成网络GAN讲了什么？ - YouTube</a></p>
<p><img src="https://cdn.sa.net/2024/04/18/Lp31Wo7MJ6Z58KV.png" referrerpolicy="no-referrer"><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1406.2661" rel="noopener" class="external-link" href="https://arxiv.org/abs/1406.2661" target="_blank">[1406.2661] Generative Adversarial Networks</a><br>
<img src="https://cdn.sa.net/2024/04/16/spOkPS5qRoUnYa7.png" referrerpolicy="no-referrer"></p>
<ul>
<li><strong>原理</strong>: 基于博弈论** 生成模型G用于捕获数据分布， 判别器D判断样本是不是来自于G,G的训练过程是最大化D出错的概率 两者做一个Min-Max游戏。在训练或生成样本期间不需要任何马尔可夫链或展开的近似推理网络。</li>
<li><strong>优点</strong></li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>缺点</strong>
<ul>
<li>高分辩率的图片生成困难</li>
<li>难以训练</li>
<li>容易被恶意攻击</li>
<li>很难和其他 Control 方法结合</li>
</ul>
</li>
</ul>
<p>Generator 和 Discriminator. </p></div></div></div><div><p>GAN (生成对抗网络)、CGAN、DCGAN、WGAN (Wasserstein GAN)、StyleGAN、CycleGAN</p></div><div><p>RealESRGAN<br>
ESRGAN<br>
GFPGAN</p></div></div></div><div class="heading-wrapper"><h2 data-heading="Autoencoder(自编码器)" class="heading" id="Autoencoder(自编码器)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Autoencoder(自编码器)</h2><div class="heading-children"><div><p>auto xxx 就是 输入-输出 都是同一类型</p></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=6KH0moTLpF8" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=6KH0moTLpF8" target="_blank">无监督学习开山之作：自编码器AutoEncoder讲了什么？ - YouTube</a></p>
<p><img alt="ZywXCz1gJvmfp86.png" src="https://cdn.sa.net/2024/04/19/ZywXCz1gJvmfp86.png" referrerpolicy="no-referrer"></p>
<ul>
<li>
<p><strong>AutoEncoder 有 Encode和Decode两个部分</strong>. Encoder 负责将输入映射到 latent space, decoder 负责将 latent space 的输入尽可能映射回原始输入. 由于latent space 的维度要远远小于输入维度, 所以通常我们可以最终获得一个原始输入的压缩表示. (通过降维度和输入输出结偶进行输入输出的非线性变化, 完成压缩)</p>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p>历史</p>
<ul>
<li>Reducing the dimensionality of data with neural networks" (2006) by G. E.Hinton and R. R. Salakhutdinov. 这篇论文是深度自编码器的开创性工作, 提出了一个名为“受限玻滋曼机”模型, 用于进行数据降维任务, 这可以被认为是自编码器的前身. </li>
<li>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion(2008). 提出了一种堆叠去噪自编码器, 通过对自编码器训练是添加噪声使其编码器获得更加鲁棒的表示</li>
<li>"Contractive Auto-Encoders: Explicit Invariance During Feature Extraction"(2010) 这篇论文提出了收缩自编码器。收缩自编码器是一种自编码器的变体，它在损失函数中添加了一个项，使得编码对输入的小变化不敏感。这使得收缩自编码器能够学习到输入数据的稳健表示。</li>
<li>"Auto-Encoding Variational Bayes"：这篇论文提出了变分自编码器。变分自编码器是一种自编码器的变体，它使用了贝叶斯推理的思想，使得编码不仅仅是一个确定的值，而是一个概率分布。这使得变分自编码器能够生成类似于输入数据的新数据。</li>
</ul>
</li>
<li>
<p>U-Net 也算是一种Autoencoder的改进</p>
</li>
</ul></div></div></div><div><blockquote>
<p>生成模型<br>
<img alt="sXVMmcQZrtYpqn6.png" src="https://cdn.sa.net/2024/04/16/sXVMmcQZrtYpqn6.png" referrerpolicy="no-referrer"></p>
</blockquote></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p>Autoencoder + 概率模型</p>
<p>概率图 + 变分贝叶斯</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1sq4y1q77t/?spm_id_from=333.337.search-card.all.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1sq4y1q77t/?spm_id_from=333.337.search-card.all.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">MAE 论文逐段精读【论文精读】_哔哩哔哩_bilibili</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2111.06377" rel="noopener" class="external-link" href="https://arxiv.org/abs/2111.06377" target="_blank">Masked Autoencoders Are Scalable Vision Learners</a></p>
<p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/search?type=content&amp;q=Maked%20Autoencoder%20" rel="noopener" class="external-link" href="https://www.zhihu.com/search?type=content&amp;q=Maked%20Autoencoder%20" target="_blank">Maked Autoencoder - 搜索结果 - 知乎</a></p>
<p><img alt="OsZkClwKLYWTVJt.png" src="https://cdn.sa.net/2024/04/22/OsZkClwKLYWTVJt.png" referrerpolicy="no-referrer"></p>
<ul>
<li>Kaiming He 在2021年的一作, 在知乎上讨论很高</li>
<li>介绍了Masked Autoencoder 这篇文章, 客观上加速了Transformer中CV中的应用</li>
<li>实现的原理类似于(Noise2Noise, Denoising AE)和Bert. 随机遮蔽输入图像的一部分, 重构这些被遮蔽的像素信息. 使用一个非对称的编码-解码器, 编码器只看到哪些可见的快, 解码器只负责重构被遮蔽的块. 用于减少内存开销</li>
<li>发现遮蔽75%, 用剩下的25%去预测Mask的75%, 可以迫使大模型学习跟多特征</li>
<li>在ViT-huge上发现只使用ImageNet-1K进行self-supervised 可以媲美有标号的任务</li>
<li>发现Transformer 的确可以学习到很好hidden features</li>
<li>CNN也可以用Mask, 比如用一个特定的value设定Mask的值. 卷积很难区分正常pixel和mask pixel的边界</li>
<li>提到图片的像素是冗余的, 去除了邻居的像素(高比特率), 允许模型学习全局信息.</li>
<li>实验证明在Object Detection, Instance Segementation, Semantic Segmentation的效果都很好. 取得了BERT的在NLP任务类似的效果<br>
<img src="https://cdn.sa.net/2024/04/22/TJcpnLUi1P7hGNx.png" referrerpolicy="no-referrer"></li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="Diffusion(扩散模型)" class="heading" id="Diffusion(扩散模型)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Diffusion(扩散模型)</h2><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://course.fast.ai/" rel="noopener" class="external-link" href="https://course.fast.ai/" target="_blank">Practical Deep Learning for Coders - Practical Deep Learning</a></p></div><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p>这是原来的Diffusion model, 原来效果很差. 我们现在加一个CLIP控制模型的生成.然后将这个 加噪去噪的过程重复几次, 效果就发现很不错. 降噪的backbone 是 U-Net. 我们叫这个新的模型是Stable Diffusion.</p>
<p>后面有更多的更多控制生成的控制机制, 比如ControlNet.</p></div></div></div><div><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/634573765" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/634573765" target="_blank">stable diffusion原理解读通俗易懂，史诗级万字爆肝长文，喂到你嘴里 - 知乎</a><br>
<img src="https://pic1.zhimg.com/v2-34de1acf7e698b2ef5e37f9562ed15dc_r.jpg" referrerpolicy="no-referrer"></p></div><div><p><img src="https://pic1.zhimg.com/v2-9827eeb683a871ca5f08e64df6411294_r.jpg" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0">Stable Diffusion的原理可以抽象看作forward add noise 和 backward denoising 两部分</li>
<li data-line="1">VAE 首先将猫猫图片, 比如 512*512*3, 从像素空间转换到 latent space, 大小可能是4x64x64. 完成对输入图像的压缩. </li>
<li data-line="2"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>forward diffusion
<ul>
<li data-line="3"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>对 latent space中的猫猫照片(<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container></span>)一步一步的添加高斯噪声, 直到噪声布满整个猫猫(<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span>)
<ul>
<li data-line="4">由于添加的噪声是高斯噪声, 所以不用顺序的加噪. 可以直接累加.</li>
</ul>
</li>
</ul>
</li>
<li data-line="5"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>backward denoising (已知 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span>  -&gt; <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container></span>)
<ul>
<li data-line="6">一个使用ResNet作为主干的U-Net 进行 backward 降噪. </li>
<li data-line="7">输入: Text Embedding + 噪声图片</li>
<li data-line="8">输出: 一个去噪声潜特征空间表达</li>
<li data-line="9">因为满足高斯分布, 可以使用贝叶斯公式连立公式反转马尔可夫链.  只有噪声是未知的. </li>
<li data-line="10">网络输入为 当前时刻的分布<strong>Xt</strong>和时刻<strong>t</strong>，还有之前的<strong>文本向量</strong>. 输出预测噪声</li>
</ul>
</li>
</ul></div><div><p><img src="https://pic1.zhimg.com/v2-8f0baddac9b7893b4cc35cef7f606f58_r.jpg" referrerpolicy="no-referrer"></p></div><div><p>上面的Algorithm 1是训练过程，</p></div><div><p>其中第二步表示取数据，一般来说都是一类猫，狗什么的，或者一类风格的图片，不能乱七八糟什么图片都来，那模型学不了。</p></div><div><p>第三步是说每个图片随机赋予一个时刻的噪声（上面说过），</p></div><div><p>第四步，噪声符合高斯分布，</p></div><div><p>第五步，真实的噪声和预测的噪声算损失（DDPM输入没有文本向量，所有没有写，你就理解为多加了一个输入），更新参数。直到训练的输出的噪声和真实噪声相差很小，Unet模型训练完毕</p></div><div><p>下面我们来到Algorithm2采样(马尔可夫)过程</p></div><div><ol>
<li data-line="0">不就是说Xt符合高斯分布嘛</li>
<li data-line="1">执行T次，依次求Xt-1到X0，不是T个时刻嘛(两步之间可以预测)</li>
<li data-line="2">Xt-1不就是我们逆向扩散推出的公式，Xt-1=μ+σZ，均值和方差都是已知的，唯一的未知噪声Z被Unet模型预测出来，<strong>εθ</strong>这个是指已经训练好的UNet</li>
</ol></div><div><blockquote>
<p>Prompt -&gt; CLIP -&gt; Text Embedding -&gt; 借由 Cross-Attention 暴露给去噪U- Net</p>
</blockquote></div><div><pre><code>- 其他语义也可以借此串联的进行Control. e.g. 语义图、 图像、 Inpaint
</code><button class="copy-code-button">复制</button></pre></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=CSDgyma9RUo" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=CSDgyma9RUo" target="_blank">【博士Vlog】Diffusion生成式模型新思路，用时间序列来降噪 - YouTube</a></p>
<ul>
<li>介绍了使用Diffusion生成时间序列的论文</li>
<li>把UNet改成RNN</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=Uav0zrV6A9U" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=Uav0zrV6A9U" target="_blank">【博士详解】Diffusion和GAN是怎么回事？各自有什么优缺点？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2105.05233" rel="noopener" class="external-link" href="https://arxiv.org/abs/2105.05233" target="_blank">[2105.05233] Diffusion Models Beat GANs on Image Synthesis</a></p>
<ul>
<li>介绍了这篇论文, </li>
</ul>
<p>Diffusion 和 GAN 各自有什么优缺点?</p>
<p> Diffusion模型和GAN（生成对抗网络）都是深度学习中广泛使用的生成模型，它们各自有着不同的优缺点。</p>
<p>Diffusion模型：<br>
优点：</p>
<ol>
<li>理论依据强：Diffusion模型基于物理学中的扩散过程，有着坚实的理论依据。</li>
<li>可解释性强：相比于GAN，Diffusion模型的生成过程更容易解释和理解。</li>
<li>不依赖于对抗训练：Diffusion模型不需要像GAN那样进行对抗训练，训练过程更稳定。</li>
<li>Backward denoising 过程可以借由 cross-attention 机制, 增加各种形式的control</li>
</ol>
<p>缺点：</p>
<ol>
<li>生成速度慢：由于需要进行多步的扩散过程，Diffusion模型生成样本的速度相比于GAN要慢。</li>
<li>需要更多的计算资源：Diffusion模型通常需要更多的计算资源和存储空间。</li>
</ol>
<p>GAN：<br>
优点：</p>
<ol>
<li>生成质量高：GAN通常能够生成高质量、高分辨率的样本。</li>
<li>生成速度快：一旦训练完成，GAN生成样本的速度非常快。</li>
</ol>
<p>缺点：</p>
<ol>
<li>训练不稳定：GAN的训练过程常常不稳定，可能会遇到梯度消失、模式崩溃等问题。</li>
<li>对抗训练难以优化：GAN的对抗训练过程是一个最小化最大化问题，这使得优化过程非常困难。(熊猫_img + 线虫_img = 丹顶鹤_img ????) -&gt; 容易被攻击</li>
<li>可解释性差：GAN的生成过程是一个黑盒过程，可解释性较差。</li>
<li></li>
</ol></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="语言大模型(Large Language Model)" class="heading" id="语言大模型(Large_Language_Model)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>语言大模型(Large Language Model)</h2><div class="heading-children"><div><img alt="alt text" src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dc8387a-0a5a-4a87-89f6-f1688fd5ed99_1578x1436.png" referrerpolicy="no-referrer" style="width: 700px; max-width: 100%;">
[GitHub - Mooler0410/LLMsPracticalGuide: A curated list of practical guide resources of LLMs (LLMs Tree, Examples, Papers)](https://github.com/Mooler0410/LLMsPracticalGuide)</div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 2018 - Word2Vect 时代</p>
</span></li>
<li data-line="2" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 2019 - LSTM 时代</p>
</span></li>
<li data-line="4" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 2020 - Transformer 时代</p>
</span></li>
<li data-line="5" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 预训练方式(Mask方法，双向单向注意力)</p>
</span></li>
<li data-line="6">
<p>OpenAI </p>
</li>
</ul></div><div><ol>
<li data-line="0">Casual Mask Self-Supervised </li>
<li data-line="1">Supervised Fine-tuning</li>
<li data-line="2">Reinforcement </li>
</ol></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> Model Architecture </p>
</span></li>
<li data-line="6" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> Scaling Law 和 能力涌现</p>
</span></li>
</ul></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=57xSbOJoV_w" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=57xSbOJoV_w" target="_blank">LLM大语言模型的涌现能力是什么？大力出奇迹是否可行？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2206.07682" rel="noopener" class="external-link" href="https://arxiv.org/abs/2206.07682" target="_blank">[2206.07682] Emergent Abilities of Large Language Models - 2022</a></p>
<ul>
<li>Google发表的论文</li>
<li>发现GPT-3在模型参数量达到一定程度时, 模型性能出现了非线性爆发的现象</li>
<li>发现不仅在GPT-3 在其他LLM时也发生了 “涌现”</li>
</ul></div></div></div><div><p><strong>"Improving Language Understanding by Generative Pre-Training" by Alec Radford et al. (2018)</strong></p></div><div><p><strong>"Language Models are Unsupervised Multitask Learners" by Alec Radford et al. (2019)</strong></p></div><div><p><strong>"Language Models are Few-Shot Learners" by Tom B. Brown et al. (2020)</strong></p></div><div><p>BERT:<br>
Pre-training of Deep Bidirectional Transformers for Language Understanding" by Jacob Devlin et al. (2018)</p></div><div class="admonition-parent admonition-quote-parent"><div class="callout admonition admonition-quote admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="quote" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Quote</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2403.07183" rel="noopener" class="external-link" href="https://arxiv.org/abs/2403.07183" target="_blank">[2403.07183] Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews</a></p>
<p>Standford出品。发现在各大会议的同行评审中由解决15%的内容由LLM生成。</p>
<p>判断文本的来源的方法是创建一个最大似然MLE的统计学方法。（判断AI味有多浓）</p>
<p>捕捉到了同行评审同质化的倾向(用词倾向多样化降低)。解释了滥用ChatGPT的乱象。</p>
<p>在拒稿的论文中，发现不太回应作者反驳的评审中，使用LLM的比例较高。</p>
<p>截稿三天之前，AI味明显增加。</p>
<p>AI润色和直接写有区别。发现很多review是直接生成的。</p>
<p><img src="https://cdn.sa.net/2024/04/03/DVaiTnpkxsAt7BC.png" referrerpolicy="no-referrer"></p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="知识蒸馏(Knowledge Distilling)" class="heading" id="知识蒸馏(Knowledge_Distilling)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>知识蒸馏(Knowledge Distilling)</h2><div class="heading-children"><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=Kqr4jgkccD8" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=Kqr4jgkccD8" target="_blank">必读论文：知识蒸馏的奠基性工作，Label Smoothing讲了什么？ - YouTube</a></p>
<p>[[1503.02531] Distilling the Knowledge in a Neural Network ] (<a rel="noopener" class="external-link" href="https://arxiv.org/abs/1503.02531" target="_blank">https://arxiv.org/abs/1503.02531</a>)</p>
<p><strong>Geoffrey Hinton 2015年一作, 是知识蒸馏领域的奠基性论文.</strong> </p>
<p>这篇文章提出了一个 Label Smoothing 通过改变 损失函数中 class_index 的置信程度(e.g.比如原先是1现在改小一点为0.923), 来提高模型的泛化能力和过拟合风险. 这个改的方法由一个数学公式支持.</p>
<p>在知识蒸馏任务中，标签平滑可以有以下的贡献：</p>
<ol>
<li><strong>更好的泛化能力</strong>：由于标签平滑可以防止模型过拟合，因此使用标签平滑的学生模型通常可以在测试数据上获得更好的性能。</li>
<li><strong>更好的知识蒸馏效果</strong>：在知识蒸馏中，我们希望学生模型能够复制教师模型的输出概率分布。由于标签平滑可以使模型对每个样本的类别有一定的不确定性，因此使用标签平滑的学生模型可能可以更好地复制教师模型的输出概率分布。</li>
<li><strong>更稳定的训练过程</strong>：标签平滑可以使得模型的训练过程更稳定，因为它避免了模型对每个样本的类别过于确定，这可能导致模型在训练过程中产生大的梯度，从而导致训练过程不稳定。</li>
</ol>
<p>Inception v3 加入了这个模块. </p>
<p><img src="https://cdn.sa.net/2024/04/13/ucUvtyClaRb2QsB.png" referrerpolicy="no-referrer"></p>
<p>让轻量的学生网络学习教师模型的行为. 这样我们不仅可以获得一个轻量的网络, 还可以选择学习什么样的内容.</p>
<p>CLIP2 使用 ResNet 作为教师网络, 让 Transformer 学习 ResNet 的特征.</p>
<p>现在有很多人想 “偷” GPT4的模型行为. </p></div></div></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=DqE8tVjzqqY" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=DqE8tVjzqqY" target="_blank">模型压缩的开山之作：谷歌的《知识蒸馏》讲了什么？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1503.02531" rel="noopener" class="external-link" href="https://arxiv.org/abs/1503.02531" target="_blank">Distilling the Knowledge in a Neural Network - 2015</a></p>
<ul>
<li>三个大牛在2015年的论文, 讨论很general的如何进行知识蒸馏(模型压缩)的内容. </li>
<li>动机是把多个模型压缩到一个模型中, 进行知识迁移或者压缩</li>
<li>用到了 label smoothing, soft target. 就是不要让 student teacher 单纯傻记答案.  </li>
</ul>
<p><img alt="6HCpMuxFT5ZeB28.png" src="https://cdn.sa.net/2024/04/16/6HCpMuxFT5ZeB28.png" referrerpolicy="no-referrer"></p>
<p><strong>Distilling the Knowledge in a Neural Network 这篇论文? 主要探讨了什么问题? 有什么贡献?</strong></p>
<p> 这篇论文的主要研究方向是神经网络的知识蒸馏。知识蒸馏是一种将大型、复杂模型（教师模型）的知识转移到小型、简单模型（学生模型）的技术。这种方法可以减少模型的计算复杂度和存储需求，同时保持较高的性能。</p>
<p>论文的主要贡献包括：</p>
<ol>
<li>
<p>提出了一种新的训练方法，即通过训练一个简单的模型（学生模型）来模拟复杂模型（教师模型）的行为，从而达到压缩模型和提高效率的目的。</p>
</li>
<li>
<p>提出了一种新的软目标训练方法，即不仅使用原始硬目标（类别标签），还使用教师模型的软目标（类别概率分布）进行训练。这种方法可以使学生模型学习到教师模型的一些隐含知识，从而提高其泛化能力。</p>
</li>
<li>
<p>通过实验验证了知识蒸馏的有效性。结果表明，使用知识蒸馏训练的小模型可以达到与原始大模型相近的性能，且计算复杂度和存储需求大大降低。</p>
</li>
</ol></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=EoKBh658Dak" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=EoKBh658Dak" target="_blank">灵魂发问：知识蒸馏真的有用吗？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2106.05945" rel="noopener" class="external-link" href="https://arxiv.org/abs/2106.05945" target="_blank">[2106.05945] Does Knowledge Distillation Really Work?</a></p>
<p>Google 2021 年对知识蒸馏有效性进行质疑. 他从两个方面质疑:</p>
<ul>
<li>发现老师和学生预测的分布差别比较大, 意为着学生没有从老师很好的学习到特征</li>
<li>发现学生对老师的Agreement(和老师一致的程度)和测试的准确性没有很好的正相关性</li>
<li>作者任务优化算法无法很好的传递老师到学生的知识, 即使老师已经学习到了知识</li>
<li>作者实验发现复杂的数据和特点网络回使得学生更加难从老师学习到知识</li>
<li>作者提出使用类似Mixup和GAN生成数据等数据增强可以改善学生的Agreement和泛化能力</li>
<li>需要更多时间用于调参</li>
<li>知识蒸馏真的有效. 但有限. </li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=WMQLMo0H42Y" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=WMQLMo0H42Y" target="_blank">ViT太慢了，知识蒸馏后得到的DeiT又快又好！ - YouTube</a></p>
<ul>
<li>提出了一个DeiT</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="对比学习(Contrastive Learning)" class="heading" id="对比学习(Contrastive_Learning)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>对比学习(Contrastive Learning)</h2><div class="heading-children"><div><p><img src="https://miro.medium.com/v2/resize:fit:4416/1*bvMhd_xpVxfJYoKXYp5hug.png" referrerpolicy="no-referrer"></p></div><div><blockquote>
<p>上面是Y.LeCun 在 X 上 Post 的流传很广的一张图. 用蛋糕上的樱桃、奶油和蛋糕本身来类比强化学习, 有监督和自监督学习能产生信息量的大小. </p>
<p><strong>对比学习</strong>作为自监督学习中重要一部分, 着重于学习同类实例之间的共同特征，区分非同类实例之间的不同之处。对比学习通过区分不同类别的数据特征, 可以让模型在没有标签的情况下, 通过比较数据之间的差异来学习有用的特征. 这样的特性不但可以充分使用没有标签的数据, 而且也方便学习到的特征迁移到其他任务中(比如.CLIP). </p>
</blockquote></div><div><p><span style="background:#fff88f">对比学习有效的一点是其损失函数具备<strong>困难负样本自发现</strong>的特点, 让负样本已经远离的不要继续远离而是更加关注哪些还没有远离的负样本, 从而使表示空间更加均匀(uniformilty)</span></p></div><div><ul>
<li data-line="0"><code>alignment</code> : 正例之间表示保持较近距离。距离越小，alignment的程度越高</li>
<li data-line="1"><code>uniformity</code> :随机样例的表示应分散在超球面上。数据越均匀，保留的信息越多</li>
</ul></div><div><p><span style="background:#fff88f">一个典型的图片对比学习流程包含.</span></p></div><div><ol>
<li data-line="0">选择一个多类别的图像数据集</li>
<li data-line="1">生成正负样本对: 正样本对(挑选两个同一类的图片, 期望模型学习到它们是相似的); 负样本对(挑选两张不同类的图片, 期望模型学习到它们是不同的)</li>
<li data-line="2">使用神经网络提起图像的特征</li>
<li data-line="3">计算正样本和负样本特征向量的相似度/距离. </li>
</ol></div><div><blockquote>
<p>⬇️ 下面是对比学习中一个说明“代理任务“的图片示例. 我们希望 Anchor 和 正样本距离最近和负样本距离最远.<br>
<img src="https://cdn.sa.net/2024/05/05/LYGRgxmVT5aEhl4.png" referrerpolicy="no-referrer"></p>
</blockquote></div><div class="heading-wrapper"><h3 data-heading="CLIP(2021)" class="heading" id="CLIP(2021)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>CLIP(2021)</h3><div class="heading-children"><div><blockquote>
<p>OpenAI 的 CLIP 可能是最知名的对比学习应用. 在史无前例大图片分类数据集上进行了训练. 打通了文本和图片的边界, 为之后的多模态大模型打下了基础. </p>
</blockquote></div><div><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2103.00020" rel="noopener" class="external-link" href="https://arxiv.org/abs/2103.00020" target="_blank">[2103.00020] Learning Transferable Visual Models From Natural Language Supervision</a><br>
<a data-tooltip-position="top" aria-label="https://blog.csdn.net/weixin_44031582/article/details/120469669" rel="noopener" class="external-link" href="https://blog.csdn.net/weixin_44031582/article/details/120469669" target="_blank">CLIP论文笔记--《Learning Transferable Visual Models From Natural Language Supervision》_visual n-grams模型-CSDN博客</a></p></div><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=xkSVUjbDI6I" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=xkSVUjbDI6I" target="_blank">【博士Vlog】OpenAI最新模型CLIP，想法极其简单？但为什么咱们做不了？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://github.com/openai/CLIP" rel="noopener" class="external-link" href="https://github.com/openai/CLIP" target="_blank">GitHub - openai/CLIP: CLIP (Contrastive Language-Image Pretraining), Predict the most relevant text snippet given an image</a></p>
<p><img alt="v4yTcC7odluDjHG.png" src="https://cdn.sa.net/2024/04/10/v4yTcC7odluDjHG.png" referrerpolicy="no-referrer"></p>
<ul>
<li>对角线的值，关联度最高，应当为一，作为Goal, 训练两个encoder使其优化到Goal</li>
<li>训练的数据量极大，图片数据大概100T.(ImageNet是1400万)</li>
<li>对比了在ResNet-50和ViT上测试了效果，ViT效果好训练时间短</li>
</ul>
<ul>
<li>在有名词描述的数据集上效果好，动词描述的数据集效果差</li>
</ul>
<ul>
<li>输入 text 输出 77 token embedding(每个token有768维度)</li>
</ul></div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=SImpr4oEezg" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=SImpr4oEezg" target="_blank">深度学习艺术品鉴赏大师？OpenAI的CLIP崭露头角！ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2204.14244" rel="noopener" class="external-link" href="https://arxiv.org/abs/2204.14244" target="_blank">[2204.14244] CLIP-Art: Contrastive Pre-training for Fine-Grained Art Classification</a></p>
<p>介绍了CLIP-Art 这篇论文。</p>
<p>艺术品没有很好的标签，且艺术品的分类精细复杂，原有的CLIP不够用。</p>
<p>作者使用了一个带标签的艺术品数据集对CLIP模型进行微调。<br>
在IMG_ENCODER, TEXT_ENCODER， 通过使用最小化对比预训练过程中的InfoNCE, 进行了联合嵌入。</p>
<p><img alt="9CbtHiDMVGzxU2L.png" src="https://cdn.sa.net/2024/04/10/9CbtHiDMVGzxU2L.png" referrerpolicy="no-referrer"></p>
<p>做了一个消融实验发现比 CLIP 要好。且Label的确有帮助。</p></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p>Alpha-CLIP</p></div></div></div><div class="admonition-parent admonition-check-parent"><div class="callout admonition admonition-check admonition-plugin " style="--callout-color: 0, 200, 83;" data-callout="check" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="check-circle" class="svg-inline--fa fa-check-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"></path></svg></div><div class="callout-title-inner admonition-title-content">Check</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/688152203" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/688152203" target="_blank">CVPR 2024 | 医学异常检测新工作！采用VLM进行医学图像中的通用异常检测 - 知乎</a></p>
<p>介绍了</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="MoCo (2019)" class="heading" id="MoCo_(2019)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>MoCo (2019)</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://github.com/facebookresearch/moco" rel="noopener" class="external-link" href="https://github.com/facebookresearch/moco" target="_blank">GitHub - facebookresearch/moco: PyTorch implementation of MoCo: https://arxiv.org/abs/1911.05722</a></p></div><div><blockquote>
<p>Kaiming He 在2019年的作品. </p>
</blockquote></div><div><p>提出了一个 <code>InfoNCE Loss</code>的损失函数, 鼓励模型让正样本对距离越近, 负样本距离越小<br>
<img src="https://cdn.sa.net/2024/05/05/CbvJApymEkFQ51K.png" referrerpolicy="no-referrer"></p></div><div><p>Momentum Contrast for Unsupervised Visual Representation Learning 这篇论文的主要创新点在说明地方?</p></div><div><p> 这篇论文的主要创新点在于提出了一种新的无监督视觉表示学习方法，即动量对比（Momentum Contrast，MoCo）。</p></div><div><ol>
<li data-line="0">动量编码器：在大多数对比学习中，负样本库是固定的，而在MoCo中，负样本库是动态更新的，极大地丰富了负样本的多样性，提高了模型的泛化能力。</li>
<li data-line="1">MoCo提出了一个基于队列和动量更新的对比学习框架，该框架可以在大规模无标签数据上进行有效学习。</li>
</ol></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=pXvMXfPJZ2M" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=pXvMXfPJZ2M" target="_blank">MoCo 论文逐段精读【论文精读】 - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1911.05722" rel="noopener" class="external-link" href="https://arxiv.org/abs/1911.05722" target="_blank">[1911.05722] Momentum Contrast for Unsupervised Visual Representation Learning</a></p>
<ul>
<li>Kaiming He 在 2020年的一作, 这篇论文提出了一种新的无监督学习方法，名为MoCo（Momentum Contrast）</li>
<li>在对比学习中，模型需要学习区分不同的数据样本。具体来说，模型需要将相似（或相关）的数据样本映射到接近的表示，而将不相似（或无关）的数据样本映射到远离的表示。这种学习方式需要构造一种对比任务，即给模型提供一对数据样本，并让模型判断这两个样本是否相似或相关。</li>
<li>MoCo的一个关键创新是引入了动量编码器和记忆库。动量编码器是一个跟随主编码器更新的模型，但更新速度较慢（即有一定的“动量”）。记忆库则用于存储动量编码器的历史输出，这些输出被用作对比样本。通过这种方式，MoCo能够在大规模数据集上进行有效的对比学习。</li>
<li>MoCo在多个视觉任务上都取得了很好的效果，包括图像分类、物体检测等。这表明MoCo学习到的表示具有很好的泛化能力，可以用于多种不同的任务。</li>
</ul></div></div></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="强化学习(RF Learning)" class="heading" id="强化学习(RF_Learning)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>强化学习(RF Learning)</h2><div class="heading-children"><div><blockquote>
<p>有一个 Agent 在缓解中互动。我们给这个互动添加一点随机性，如果超出我们的预期就调整 Agent的参数。如果我们喜欢这个互动，那以后就多做一点。</p>
</blockquote></div><div><p>什么是强化学习？</p></div><div><p> 强化学习（Reinforcement Learning, RL）是机器学习的一个重要分支，它主要关注如何让智能体（agent）通过与环境的交互来学习最优策略。在强化学习中，智能体通过观察环境状态、采取行动并接收奖励或惩罚来学习。目标是最大化累积奖励，这通常涉及到平衡探索（尝试新行为以发现潜在的更好策略）和利用（使用已知的最优策略）。</p></div><div><p>强化学习的基本组成部分包括：</p></div><div><ol>
<li data-line="0"><strong>智能体（Agent）</strong>：学习的主体，它通过与环境交互来做出决策。</li>
<li data-line="1"><strong>环境（Environment）</strong>：智能体所处的世界，它会响应智能体的行动并提供反馈。</li>
<li data-line="2"><strong>状态（State）</strong>：环境的当前情况，智能体会根据状态做出决策。</li>
<li data-line="3"><strong>动作（Action）</strong>：智能体在某个状态下可以采取的行为。</li>
<li data-line="4"><strong>奖励（Reward）</strong>：环境对智能体行为的即时反馈，用于指导智能体的学习过程。</li>
<li data-line="5"><strong>策略（Policy）</strong>：智能体在不同状态下选择动作的规则或函数。</li>
<li data-line="6"><strong>价值函数（Value Function）</strong>：评估在某个状态下采取特定策略的长期期望回报。</li>
<li data-line="7"><strong>模型（Model）</strong>：对环境的内部表示，有些强化学习方法会尝试模拟环境的行为。</li>
</ol></div><div><p>强化学习的算法可以分为基于模型的方法和无模型方法两大类。基于模型的方法试图构建一个环境模型来进行规划，而无模型方法则直接从经验中学习策略和价值函数。常见的强化学习算法包括Q-learning、SARSA、Deep Q-Network (DQN)、Policy Gradient Methods等。这些算法在游戏、机器人控制、自动驾驶等领域有着广泛的应用。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="双流网络(Two-stream Network)" class="heading" id="双流网络(Two-stream_Network)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>双流网络(Two-stream Network)</h2><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/search?q=%E5%8F%8C%E6%B5%81%E7%BD%91%E7%BB%9C&amp;search_source=Suggestion&amp;utm_content=search_suggestion&amp;type=content" rel="noopener" class="external-link" href="https://www.zhihu.com/search?q=%E5%8F%8C%E6%B5%81%E7%BD%91%E7%BB%9C&amp;search_source=Suggestion&amp;utm_content=search_suggestion&amp;type=content" target="_blank">双流网络 - 搜索结果 - 知乎</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/516956247/answer/3310533088" rel="noopener" class="external-link" href="https://www.zhihu.com/question/516956247/answer/3310533088" target="_blank">医学图像的深度学习该怎么做? - 知乎</a></p></div><div><blockquote>
<p>Two-stream &gt; TSN &gt; I3D &gt; SlowFast &gt; Timesformer</p>
</blockquote></div><div class="admonition-parent admonition-caution-parent"><div class="callout admonition admonition-caution admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="caution" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Caution</div></div><div class="callout-content admonition-content heading-wrapper"><h2 data-heading="双流网络 (Two-Stream Network) - 视频理解领域开山之作" class="heading" id="双流网络_(Two-Stream_Network)_-_视频理解领域开山之作"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>双流网络 (Two-Stream Network) - 视频理解领域开山之作</h2>
<p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=vuqwKP2iDe0" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=vuqwKP2iDe0" target="_blank">双流网络论文逐段精读【论文精读】 - YouTube</a></p>
<p>双流网络是一种在视频动作识别领域中非常流行的深度学习架构。它通过同时分析视频的空间和时间信息来进行动作识别，从而提高了识别精度。</p>
<p>动机: 2014年的Deep Video 使用 CNN 意外的效果比传统手工特征提取要差, 想要知道为什么</p>
<p><strong>双流网络的核心思想是将视频信息分成两个独立的流进行处理：</strong></p>
<ul>
<li><strong>空间流 (Spatial Stream):</strong>&nbsp;该流处理视频帧，提取其中的静态外观信息，例如场景、物体和人物等。通常使用卷积神经网络 (CNN) 来进行特征提取。</li>
<li><strong>时间流 (Temporal Stream):</strong>&nbsp;该流处理视频帧之间的运动信息，例如光流 (Optical Flow)。光流描述了像素在相邻帧之间的运动方向和速度。通常使用 3D 卷积神经网络 (3D CNN) 或循环神经网络 (RNN) 来进行特征提取。</li>
</ul>
<p><strong>双流网络的优势：</strong></p>
<ul>
<li><strong>充分利用视频信息:</strong>&nbsp;通过同时分析空间和时间信息，双流网络可以更全面地理解视频内容，从而提高动作识别的准确性。比如, 2D视频有更多的连续, 动态信息. 还有音频信号. 视频更加符合人眼的习惯.</li>
<li><strong>鲁棒性强:</strong>&nbsp;即使视频中存在遮挡或光照变化等干扰因素，双流网络仍然能够有效地识别动作。</li>
</ul>
<p><strong>双流网络的应用:</strong></p>
<ul>
<li><strong>动作识别:</strong>&nbsp;这是双流网络最主要的应用领域，例如识别视频中的人物动作 (跑步、跳跃、游泳等)。</li>
<li><strong>行为分析:</strong>&nbsp;可以分析视频中人物的行为，例如打架、拥抱、握手等。</li>
<li><strong>视频描述:</strong>&nbsp;可以根据视频内容生成文本描述。</li>
</ul>
<p><strong>双流网络的局限性:</strong></p>
<ul>
<li><strong>计算量大:</strong>&nbsp;由于需要同时处理两个流的信息，双流网络的计算量较大，需要较高的计算资源。</li>
<li><strong>训练难度大:</strong>&nbsp;双流网络的训练过程比较复杂，需要大量的训练数据和调参技巧。</li>
</ul>
<p><strong>双流网络的发展趋势:</strong></p>
<ul>
<li><strong>更强大的特征提取网络:</strong>&nbsp;研究者们正在探索更强大的特征提取网络，例如基于 Transformer 的网络，以提高双流网络的性能。</li>
<li><strong>多模态融合:</strong>&nbsp;将双流网络与其他模态的信息 (例如音频) 进行融合，可以进一步提高动作识别的准确性。</li>
<li><strong>轻量化模型:</strong>&nbsp;研究者们正在研究如何降低双流网络的计算量，使其能够在移动设备上运行。</li>
</ul>
<p><strong>总而言之，双流网络是一种强大的视频动作识别模型，在视频理解领域具有广泛的应用前景。</strong></p>
<p>希望以上信息能帮助你了解双流网络。</p>
<p><strong>如果你想了解更多信息，可以参考以下资源:</strong></p>
<ul>
<li><strong>论文:</strong>&nbsp;Two-Stream Convolutional Networks for Action Recognition in Videos</li>
<li><strong>代码:</strong>&nbsp;<a rel="noopener" class="external-link" href="https://github.com/feichtenhofer/twostreamfusion" target="_blank">https://github.com/feichtenhofer/twostreamfusion</a></li>
</ul><div class="heading-children"></div></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=OOWZl4mx2fM" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=OOWZl4mx2fM" target="_blank">【博士Vlog】全网最全！十分钟看完所有双流神经网络，图片视频处理都在这里了！ - YouTube</a></p>
<p><img src="https://cdn.sa.net/2024/04/15/YWOvjX8w9GKulFb.png" referrerpolicy="no-referrer"><br>
<img src="https://cdn.sa.net/2024/04/15/BLhnCS2eKHw7lFv.png" referrerpolicy="no-referrer"></p>
<ul>
<li>提出了一个新的数据集</li>
<li>介绍了使用双流神经网路进行视频预测, 将2D ConvNet 膨胀到 双流 3D ConvNet(网络E)</li>
<li>“Two Stream Inflated 3D ConvNet(I3D)” (Inception, VGG16, ResNet)</li>
<li>现在已经成为Fondation model</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=vuqwKP2iDe0" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=vuqwKP2iDe0" target="_blank">双流网络论文逐段精读【论文精读】 - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1406.2199" rel="noopener" class="external-link" href="https://arxiv.org/abs/1406.2199" target="_blank">[1406.2199] Two-Stream Convolutional Networks for Action Recognition in Videos</a></p>
<p><img src="https://cdn.sa.net/2024/04/23/A9rn2DugdVT3vLb.png" referrerpolicy="no-referrer"></p>
<ul>
<li>作者是VGG提出者</li>
<li>介绍了双流网络的开山之作(上面), 用于视频动作识别任务. <span style="background:#fff88f">很有可能是多模态的先驱</span>. </li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>双流网络的提出动机
<ul>
<li>充分利用互联网上大量的视频内容</li>
<li>视频有很大信息(连续的, 音频, 更加符合人眼)</li>
<li>发现2014年Deep Video 在视频分类任务上效果很差</li>
<li>视频天生就提供了很好的数据增强</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>之前
<ul>
<li>抽取关键帧</li>
<li>或者将整个帧叠起来, 使用erarly fusion/latent fusion进行时空分析</li>
</ul>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>作者发现
<ul>
<li>之前模型效果差, 是因为CNN只对局部特征学习好</li>
<li>可以先提取光流信息, 学习动作分类和光流的映射关系</li>
</ul>
</li>
<li>模型两个CNN, 一个关注帧的空间信息(appearance information), 一个关注motion-information(光流). 两个网络都输出分类的概率, 然后简单加权平均就可以.</li>
<li>光流的使用可以让模型专注于动作本身, 而忽略图像的背景噪声等</li>
<li>提出multi-task learning, 在两个数据集上学习骨干网络</li>
<li>谈到 a stack of fream 和 individual frame 的效果差不多. aka. 没有学习到连续的context. 文献14. -&gt; temporal information 很重要, 学习到运动特征很重要</li>
<li>探讨了不同光流图叠加的方式</li>
<li>光流计算速度慢, 而且密集表示存储空间大</li>
</ul></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/692173683?utm_campaign=&amp;utm_medium=social&amp;utm_psn=1763139418939244544&amp;utm_source=io.raindrop.raindropio" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/692173683?utm_campaign=&amp;utm_medium=social&amp;utm_psn=1763139418939244544&amp;utm_source=io.raindrop.raindropio" target="_blank">结合创新！ResNet+Transformer，高性能低参数，准确率达99.12％ - 知乎</a></p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="多模态(multimodal)" class="heading" id="多模态(multimodal)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>多模态(multimodal)</h2><div class="heading-children"><div><blockquote>
<p>之前的single-task specific 的算法都强烈依赖任务数据不具有泛化能力。以 CLIP 和 SegmentAnyThing 为代表的模型具有了很大迁移潜力；Diffusion模型beatGAN也是依赖通用的文生图能力。未来的多模态允许比文字更多的输入，因为人类的文本很多场景不能或者需要很大成本才能达到我们期望的输出，或者模型无法单从或者很难通过文本理解这个世界。</p>
</blockquote></div><div><p><a data-tooltip-position="top" aria-label="https://mmmu-benchmark.github.io/#leaderboard" rel="noopener" class="external-link" href="https://mmmu-benchmark.github.io/#leaderboard" target="_blank">MMMU 多模态模型榜单</a></p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 基本架构</span></li>
<li data-line="1">Encoder : 将多种自然信号(waveform, frame, text)编码成离散的token或者连续的特征(没有cutoff但是更难训练)，映射到相同的空间</li>
<li data-line="2">LLM : 用于推理</li>
<li data-line="3">Decoder: LLM 只给出语义信息让下游的专有解码器实际执行数据的输出</li>
<li data-line="4" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 发展趋势</span></li>
<li data-line="5">2023年之前的多模态一般是在 SFT 后加入多模态功能。比如 freeze 图片 就 语言层，单独训练中间的Adapter层，然后打开语言层再做一个Fine-tune. 这种方法外挂的方法很难有助于 LLM 的推理能力也就是说 LLM 不理解图片之中的泛化，不理解原生图像之间细微的差别。</li>
<li data-line="6">一个更好的方法是在 Pre-train 阶段就统一训练多模态数据。但是最好不要一起训练，最好每个模态也是模块化的，方便后期调整.</li>
<li data-line="7" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 短期局限</span></li>
<li data-line="8">长文本对话窗口，长多模态对话窗口</li>
<li data-line="9">更加可控和连贯的多模态的输出</li>
<li data-line="10">如果找到一个通用的多模态 Pre-train 方法</li>
</ul></div></div></div><div class="heading-wrapper"><h2 data-heading="图神经网络(GNN)" class="heading" id="图神经网络(GNN)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>图神经网络(GNN)</h2><div class="heading-children"><div><p>图神经网络是什么? 详细解释它们的网路构造?</p></div><div><p> 图神经网络（Graph Neural Networks，GNN）是一种专门用于处理图结构数据的神经网络。与常见的卷积神经网络（Convolutional Neural Networks，CNN）或者循环神经网络（Recurrent Neural Networks，RNN）不同，GNN能够有效地处理图结构数据中的节点和边的关系。</p></div><div><p>GNN的网络构造主要包括以下几个步骤：</p></div><div><ol>
<li data-line="0">
<p>节点表示：每个节点在GNN中都会有一个向量表示。这个向量可以是节点的特征，也可以是节点的标签，或者其他的节点信息。</p>
</li>
<li data-line="2">
<p>邻居信息聚合：GNN会收集每个节点的邻居节点信息，并使用这些信息更新节点的表示。这个过程通常是通过一个聚合函数来实现的，比如求和、平均、最大���等。这个过程可以看作是在图上进行信息传递。</p>
</li>
<li data-line="4">
<p>节点更新：根据聚合得到的邻居信息，通过一个更新函数来更新每个节点的表示。这个更新函数通常是一个非线性函数，比如ReLU、tanh等。</p>
</li>
<li data-line="6">
<p>重复上述过程：在GNN中，上述的邻居信息聚合和节点更新过程会进行多次，每次都会使得节点的表示更加丰富和准确。</p>
</li>
<li data-line="8">
<p>最终输出：在进行了足够多次的邻居信息聚合和节点更新之后，每个节点的表示就可以作为最终的输出。这个输出可以用于节点分类、节点聚类、链接预测等任务。</p>
</li>
</ol></div><div><p>需要注意的是，上述的网络构造只是GNN最基础的形式，实际上还有很多种GNN的变种，比如图卷积网络（Graph Convolutional Networks，GCN）、图注意力网络（Graph Attention Networks，GAT）等，它们在节点表示、邻居信息聚合和节点更新等步骤上有各自的特点和优势。</p></div></div></div><div class="heading-wrapper"><h2 data-heading="模型可解释" class="heading" id="模型可解释"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>模型可解释</h2><div class="heading-children"><div class="admonition-parent admonition-failure-parent"><div class="callout admonition admonition-failure admonition-plugin " style="--callout-color: 255, 82, 82;" data-callout="failure" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times-circle" class="svg-inline--fa fa-times-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3 0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3 0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3 0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3 0 17L312 256l65.6 65.1z"></path></svg></div><div class="callout-title-inner admonition-title-content">Failure</div></div><div class="callout-content admonition-content"><p>模型解释性很重要，特别是医疗和金融领域。</p>
<p>主要的理由有：</p>
<ol>
<li><strong>可信任性</strong>：如果模型的预测能够被解释和理解，那么人们更可能信任模型，特别是在影响重大的决策场景，如医疗诊断和金融信贷中。</li>
<li><strong>模型调试</strong>：可解释性可以帮助我们理解模型在某些情况下为何预测错误，并可以指导我们如何改进模型。</li>
<li><strong>公平性和偏见检查</strong>：如果我们能理解模型的决策过程，那么我们可以更容易地检测到模型是否有不公平的偏见，例如预测是否受到某些不相关因素（如性别或种族）的影响。</li>
<li><strong>符合监管要求</strong>：在某些行业，如金融和医疗，预测模型可能需要满足监管机构的可解释性要求。</li>
<li><strong>特征工程和选择</strong>：解释性模型能够显示哪些特征对预测结果有重大影响，这有助于我们进行更有效的特征选择和工程</li>
</ol></div></div></div><div style="overflow-x: auto;"><table>
<thead>
<tr>
<th>方法</th>
<th>可解释性水平(0-100)</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td>线性回归/逻辑回归</td>
<td>90</td>
<td>输出可以清晰地用输入特征的线性组合表示，且模型权重可解释为各特征的影响大小和方向</td>
</tr>
<tr>
<td>决策树</td>
<td>95</td>
<td>每一个决策都基于清晰的规则，规则基于特征值，轻松理解每步决策</td>
</tr>
<tr>
<td>随机森林</td>
<td>80</td>
<td>尽管是多个决策树的集合，使得整体解释性相比单个树略弱，但每棵树都基于明确的规则，且可以通过特征重要性指标理解特征对模型的影响</td>
</tr>
<tr>
<td>支持向量机（线性核）</td>
<td>85</td>
<td>找到线性界限将特征空间分割为不同类别，权重向量可理解为特征影响，但分类决策通常只受支持向量影响</td>
</tr>
<tr>
<td>支持向量机（非线性核，如RBF）</td>
<td>40</td>
<td>通过核函数映射到高维空间，解释性变得复杂，难以明确理解特征如何影响分类界限</td>
</tr>
<tr>
<td>神经网络</td>
<td>20</td>
<td>隐藏层和参数众多，难以理解特征如何直接影响输出，通常被视为"黑箱"，尽管有一些可视化、解释工具尝试提高解释性</td>
</tr>
<tr>
<td>K-近邻</td>
<td>75</td>
<td>预测结果由最靠近的K个样本投票决定，直观易懂，但不提供特征权重或影响程度信息</td>
</tr>
<tr>
<td>LIME</td>
<td>80</td>
<td>LIME通过拟合一个局部线性模型来解释黑盒模型的预测。它可以对任何模型进行解释，并为每个预测指出了重要的局部特征，但结果可能依赖于随机样本和选择的超参数</td>
</tr>
<tr>
<td>SHAP</td>
<td>85</td>
<td>SHAP基于博弈论的概念，计算出每个特征对预测的平均贡献，甚至能够对复杂模型如XGBoost、神经网络等提供公平、准确的特征贡献解释。但它的计算复杂度较高，扩展到大型或深度模型需要特定的优化版本</td>
</tr>
<tr>
<td>GradCAM</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div><div class="admonition-parent admonition-attention-parent"><div class="callout admonition admonition-attention admonition-plugin " style="--callout-color: 255, 145, 0;" data-callout="attention" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="exclamation-triangle" class="svg-inline--fa fa-exclamation-triangle fa-w-18" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"></path></svg></div><div class="callout-title-inner admonition-title-content">Attention</div></div><div class="callout-content admonition-content"><p>有多种方法可以可视化神经网络的隐藏层。以下是一些常见的方法：</p>
<ol>
<li>
<p><strong>激活映射（Activation Maps）</strong>：对于卷积神经网络（CNN），我们可以直接可视化特定层的激活。这可以帮助我们理解网络在哪些区域和特征上被激活。</p>
</li>
<li>
<p><strong>过滤器可视化（Filter Visualization）</strong>：对于CNN的卷积层，我们可以可视化学习到的过滤器。对于第一层，这些过滤器可能会显示出一些简单的特征，如边缘和颜色斑块。对于更深的层，过滤器可能会表示更复杂的特征。</p>
</li>
<li>
<p><strong>类激活映射（Class Activation Mapping, CAM）</strong>：CAM是一种可视化技术，可以显示出网络在进行特定类别预测时关注的图像区域。</p>
</li>
<li>
<p><strong>特征反演（Feature Inversion）</strong>：这种方法试图找到一个输入，其通过网络传播后在特定层产生的激活最接近给定的激活。这可以用于理解特定层的激活表示什么。</p>
</li>
<li>
<p><strong>深度梦境（DeepDream）</strong>：DeepDream是一种使用神经网络自身来生成图像的技术。它通过选择一个层，然后尽可能地激活那一层来修改输入图像。</p>
</li>
<li>
<p><strong>神经网络去卷积（Deconvolutional Networks）</strong>：这种方法试图将特定层的激活映射回输入空间，以理解网络学习的特征。</p>
</li>
<li>
<p><strong>梯度上升（Gradient Ascent）</strong>：这种方法通过优化输入图像以最大化特定层的激活来理解该层的功能。</p>
</li>
<li>
<p><strong>t-SNE 和 PCA</strong>：这些降维技术可以用于可视化高维特征空间。</p>
</li>
</ol>
<p>每种方法都有其优点和缺点，适用于解决特定的问题。所以，选择哪种方法取决于你的具体需求和你想要解答的问题。</p>
<hr>
<p><strong>Grad-CAM</strong>，全称Gradient-weighted Class Activation Mapping，是一种可视化卷积神经网络决策的技术。它是类激活映射（Class Activation Mapping, CAM）的一种扩展，可以用于任何CNN架构，而不仅仅是那些有全局平均池化层的网络。</p>
<p>Grad-CAM的主要思想是使用目标类别相对于最后一个卷积层的梯度来生成一个粗糙的定位图（也称为热图）。这个热图突出显示了对分类决策最重要的图像区域。</p>
<p>下面是Grad-CAM的基本步骤：</p>
<ol>
<li>选择网络的某个卷积层。</li>
<li>对给定的输入和类别，运行前向传播。</li>
<li>使用类别的得分计算相对于选定卷积层的梯度。</li>
<li>对该层的每个特征映射的梯度进行全局平均池化，得到权重系数。</li>
<li>将这些权重应用于相应的特征映射，然后对结果进行求和，得到定位图。</li>
<li>对定位图应用ReLU激活函数以保留仅对目标类别有用的特征。</li>
<li>将定位图上采样（比如，使用双线性插值）到输入图像的大小，以产生热图。</li>
</ol>
<p>通过这种方式，Grad-CAM提供了一种可视化方法，可以清楚地看到模型在做出决策时关注的图像区域。这对于模型的解释性和透明度非常有帮助。</p>
<hr>
<p><strong>激活映射（Activation Maps）</strong>是一种可视化技术，用于显示卷积神经网络（CNN）中特定层的激活。这种方法通常用于理解网络在处理特定输入时，各层是如何响应的。以下是激活映射的主要原理、优势和劣势。</p>
<p><strong>原理</strong>：</p>
<p>在CNN中，每个卷积层都会生成一组特征映射（或激活映射），这些映射表示网络在特定层级捕获的特征。例如，第一层可能会捕获简单的边缘和纹理，而更深的层可能会捕获更复杂的特征，如对象的部分或整体。通过可视化这些激活映射，我们可以直观地理解网络在处理输入时的行为。</p>
<p><strong>优势</strong>：</p>
<ol>
<li>
<p><strong>直观理解</strong>：激活映射可以直观地揭示网络在处理特定输入时的内部行为，有助于理解网络如何识别和抽取特征。</p>
</li>
<li>
<p><strong>故障诊断</strong>：如果网络的性能不佳，检查激活映射可以帮助发现问题。例如，如果某些特征映射始终未激活，或者激活的模式看起来不正确，那可能就是网络配置或训练过程中存在问题。</p>
</li>
</ol>
<p><strong>劣势</strong>：</p>
<ol>
<li>
<p><strong>高维复杂性</strong>：对于具有大量特征映射的深度网络，激活映射可能会非常复杂，难以一次性全部理解。</p>
</li>
<li>
<p><strong>缺乏高级解释</strong>：虽然激活映射可以显示网络的内部行为，但它们并不能直接解释网络如何做出特定的预测。例如，它们不能告诉我们网络为什么将某个图像分类为猫而不是狗。</p>
</li>
<li>
<p><strong>难以解释抽象特征</strong>：在深层网络中，激活映射可能会表示非常抽象的概念，这些概念可能难以理解或解释。</p>
</li>
</ol>
<hr>
<p>t-SNE（t-Distributed Stochastic Neighbor Embedding）和PCA（Principal Component Analysis）是两种常用的降维技术，可以用于可视化神经网络的隐藏层。</p>
<ol>
<li>
<p><strong>PCA（主成分分析）</strong>：PCA是一种线性降维技术，它通过找到数据中的主要变化方向（即主成分）来减少数据的维数。在神经网络的隐藏层可视化中，PCA可以用来将高维的激活映射投影到2D或3D空间，以便我们可以在图形中看到它们。PCA的主要优点是计算效率高，但由于它是线性的，所以可能无法捕捉到数据中的非线性结构。</p>
</li>
<li>
<p><strong>t-SNE（t-分布随机邻域嵌入）</strong>：t-SNE是一种非线性降维技术，它通过尝试保持原始高维空间中的邻域关系来将高维数据映射到低维空间。在神经网络的隐藏层可视化中，t-SNE可以用来揭示高维激活映射中的结构和模式。t-SNE的优点是能够捕捉到数据中的非线性结构，但计算效率较低，且超参数调整可能会影响结果。</p>
</li>
</ol>
<p>这两种方法都可以用来理解和解释神经网络的行为。例如，你可以使用它们来查看网络在处理不同类型的输入时，隐藏层的激活是否有明显的区别。这可以帮助你理解网络是否成功地学习到了有用的特征，或者是否存在可能的问题（例如，所有的输入都映射到同一区域，可能表明网络没有成功地区分不同的输入）。</p></div></div></div><div class="admonition-parent admonition-abstract-parent"><div class="callout admonition admonition-abstract admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="abstract" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Abstract</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=5XiFThIeo2U" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=5XiFThIeo2U" target="_blank">【博士Vlog】模型解释哪家强？一篇文章节省你三个月时间！</a></p>
<p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2103.10689" rel="noopener" class="external-link" href="https://arxiv.org/abs/2103.10689" target="_blank">[2103.10689] Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond</a></p>
<p>介绍了一个模型可解释性的Survey。<br>
对比了 Human label, LIME, GradCAM, SmoothGrad 对图片label的标记水平。</p>
<p><img alt="sgeX82wc4EobN5F.png" src="https://cdn.sa.net/2024/04/08/sgeX82wc4EobN5F.png" referrerpolicy="no-referrer"></p>
<ol>
<li>
<p><strong>闭式(Closed-Form)</strong>：线性回归就是一个闭式的例子。线性回归模型的每个特征都有一个系<br>
数，这些系数直接表明了特征对于模型预测的贡献度。由于模型结构简单且系数直接对应于输入<br>
特征的重要性，所以可以直接将这些系数视为模型的解释。</p>
</li>
<li>
<p><strong>组合(Composition)</strong>:决策树模型是组合关系的一个例子。决策树在做出预测时，可以直接展示<br>
其决策路径，这个路径可以视为模型的解释。例如，在银行贷款批准的决策树中，每个决策节点<br>
(如信用评分、收入等)和它们的阈值就构成了预测的解释。</p>
</li>
<li>
<p><strong>依赖(Dependence)</strong>:深度学习模型中的特征重要性图（如Grad-CAM)就是依赖关系的一个例<br>
子。例如，在一个用于图像识别的卷积神经网络中，Grad-CAM使用模型的梯度信息来突出显示对<br>
于预测类别最重要的图像区域，这些区域的突出显示依赖于模型内部的特定层次。</p>
</li>
<li>
<p><strong>代理(Proxy)</strong>：LME(局部可解释模型不透明度解释)算法是代理关系的一个例子。LIME通过<br>
在原始复杂模型（如随机森林或深度神经网络）的预测周围采样，并用这些样本来训练一个简单<br>
的线性模型作为代理。这个简单的线性模型（如线性回归）然后用来解释原始模型在局部区域的<br>
行为。</p>
</li>
</ol></div></div></div><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=hdDE676jJU4" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=hdDE676jJU4" target="_blank">【博士Vlog】如何解释机器学习深度学习？LIME和SHAP方法介绍 - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1602.04938" rel="noopener" class="external-link" href="https://arxiv.org/abs/1602.04938" target="_blank">[1602.04938] "Why Should I Trust You?": Explaining the Predictions of Any Classifier</a></p>
<p>介绍了一篇论文关于<strong>LIME</strong>（Local Interpretable Model-Agnostic Explanations）模型解释技术的论文，用于任何分类器的预测。</p>
<p>在随机森林和神经网络证明解释性是可行的。</p>
<p><img alt="tU1hkHMvgICK3oZ.png" src="https://cdn.sa.net/2024/04/08/tU1hkHMvgICK3oZ.png" referrerpolicy="no-referrer"></p>
<p><img alt="joRClJi5gWaKL7p.png" src="https://cdn.sa.net/2024/04/08/joRClJi5gWaKL7p.png" referrerpolicy="no-referrer"></p>
<p>用一个可解释的模型(e.g.SVM,Random Forest) 来拟合模型局部在一个特定实例下的行为。</p>
<p>这个图是LIME一个简化的示例，用来解释LME(局部可解释模型-不透明度解释)的直观概念。在图<br>
中，有一个复杂的决策边界（由蓝色和粉色区域表示），它代表一个黑盒模型的决策函数：，LME<br>
并不了解这个函数。这个决策边界不是线性的，也就是说，它不能被一个简单的线性模型很好地近<br>
似。<br>
在这个决策空间中，有两类实例，用加号和圆点标记。红色粗十字代表需要解释的实例，也就是<br>
说，我们想要了解模型为何将这个特定的实例分类为当前的类别。<br>
LME通过在决策边界附近采样新的实例，并使用原模型来对这些新实例做出预测。这些预测然后<br>
根据它们与需要解释的实例的近似程度被加权（在图中以大小表示）。最后，LME会学习一个简<br>
单的模型（在图中用虚线表示），这个模型仅在局部是准确的，也就是说它在被解释实例的周围是<br>
可靠的，尽管在整个决策空间中不一定可靠。这样，即使原始的黑盒模型很复杂和不可解释，</p>
<p><strong>LME也能提供对<font color="#ffff00">单个</font>预测决策的直观理解。</strong></p>
<p><span style="background:#fff88f">LIME 库已停止维护</span></p></div></div></div><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=JT_9zozHCDM" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=JT_9zozHCDM" target="_blank">Lime和SHAP哪个更好？这个文章做了个对比！ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://www.researchgate.net/publication/378171944_E-XAI_Evaluating_Black-Box_Explainable_AI_Frameworks_for_Network_Intrusion_Detection" rel="noopener" class="external-link" href="https://www.researchgate.net/publication/378171944_E-XAI_Evaluating_Black-Box_Explainable_AI_Frameworks_for_Network_Intrusion_Detection" target="_blank">(PDF) E-XAI: Evaluating Black-Box Explainable AI Frameworks for Network Intrusion Detection</a></p>
<p>这篇论文介绍了网络安全领域AI模型的可解释性。</p>
<p>有一个E-XAI的一个黑盒用于检测黑客是否入侵，需要提供这个AI的可解释性。</p>
<p>作者提出了一个XAI的框架用于评估，使用了LIME和SHAP进行模型可视化和六个指标的评估(准确性稀疏性鲁棒性稳定性效率完整性)。</p>
<p><img src="https://cdn.sa.net/2024/04/08/EO4ughnR3jH7IMz.png" referrerpolicy="no-referrer"></p>
<p><img src="https://cdn.sa.net/2024/04/08/mVYvHPWbdZCFhz2.png" referrerpolicy="no-referrer"></p>
<p>对比了LIME和SHAP的可视化结果，发现LIME和SHAP可解释性差不多。文章说在他们的数据集，SHAP效果比较好。</p>
<p>这里有医疗AI模型的对比<br>
<a data-tooltip-position="top" aria-label="https://www.mdpi.com/2075-4418/12/2/237" rel="noopener" class="external-link" href="https://www.mdpi.com/2075-4418/12/2/237" target="_blank">Diagnostics | Free Full-Text | Applications of Explainable Artificial Intelligence in Diagnosis and Surgery</a></p>
<p><img alt="eUdzFsShQgtyrCK.png" src="https://cdn.sa.net/2024/04/08/eUdzFsShQgtyrCK.png" referrerpolicy="no-referrer"></p></div></div></div><div class="admonition-parent admonition-info-parent"><div class="callout admonition admonition-info admonition-plugin " style="--callout-color: 0, 184, 212;" data-callout="info" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="info-circle" class="svg-inline--fa fa-info-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"></path></svg></div><div class="callout-title-inner admonition-title-content">Info</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://shap.readthedocs.io/en/latest/" rel="noopener" class="external-link" href="https://shap.readthedocs.io/en/latest/" target="_blank">Welcome to the SHAP documentation — SHAP latest documentation</a></p>
<p><img alt="ByNfczr57LWdCb2.png" src="https://cdn.sa.net/2024/04/08/ByNfczr57LWdCb2.png" referrerpolicy="no-referrer"></p>
<p><strong>SHAP（SHapley Additive exPlanations）</strong>是一种用于解释机器学习模型的算法。它基于博弈论中的Shapley值，这是一种用于分配合作游戏收益的公平方式。在机器学习模型解释的上下文中，SHAP用于分配每个特征对预测结果的贡献。</p>
<p>SHAP值的计算过程涉及到所有可能的特征组合，这使得它在理论上是最公平的分配方式。这也意味着SHAP值满足了一些重要的属性，例如效率（所有特征的SHAP值之和等于预测结果）和对称性（如果两个特征对预测结果的贡献相同，那么他们的SHAP值应该相同）。</p>
<p>SHAP的主要优点是它提供了一种全局和局部的模型解释方式。全局解释是通过平均所有实例的SHAP值来得到的，它可以给出每个特征对模型预测结果的平均贡献。局部解释是通过计算单个实例的SHAP值来得到的，它可以给出在该实例中每个特征对预测结果的贡献。</p>
<p>然而，SHAP的计算过程可能会非常复杂和计算密集，尤其是在有大量特征的情况下。为了解决这个问题，有一些近似的方法被提出，例如KernelSHAP和TreeSHAP，它们在保持解释性的同时减少了计算复杂性。</p>
<p><strong>比如</strong>，一个简单的二元分类问题为例，假设我们有一个机器学习模型，它使用年龄，性别，和体重三个特征来预测一个人是否会患有某种疾病。给定一个具体的个体，例如一个40岁的男性，体重为75公斤，我们的模型预测他有70%的概率会患病。</p>
<p>我们可以使用SHAP来解释这个预测结果。首先，我们需要计算每个特征的SHAP值。这涉及到计算所有可能的特征组合，例如只有年龄，只有性别，只有体重，年龄和性别，年龄和体重，性别和体重，以及所有三个特征。对于每一种组合，我们都需要计算出该特征组合对预测结果的贡献，然后按照Shapley值的公式分配给每个特征。</p>
<p>假设我们得到的SHAP值是：年龄为0.3，性别为0.1，体重为0.3。这意味着年龄和体重对预测结果的贡献最大，每个都贡献了30%的可能性，性别贡献了10%的可能性。这些SHAP值的总和（0.3 + 0.1 + 0.3 = 0.7）等于模型的预测结果（70%的概率会患病），这满足了SHAP的效率属性。</p>
<p>这样，我们就可以更好地理解模型的预测结果了。例如，我们可以看到年龄和体重是最重要的因素，而性别的影响相对较小。这可以帮助我们理解模型的工作原理，以及如何改进模型的性能。</p></div></div></div><div><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/696407462" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/696407462" target="_blank">再见Matplotlib！强大的 Scikit-learn 可视化让模型说话 - 知乎</a></p></div></div></div><div class="heading-wrapper"><h2 data-heading="AI4Science" class="heading" id="AI4Science"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>AI4Science</h2><div class="heading-children"><div class="heading-wrapper"><h3 data-heading="AlphaFold" class="heading" id="AlphaFold"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>AlphaFold</h3><div class="heading-children"><div class="admonition-parent admonition-summary-parent"><div class="callout admonition admonition-summary admonition-plugin " style="--callout-color: 0, 176, 255;" data-callout="summary" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"></path></svg></div><div class="callout-title-inner admonition-title-content">Summary</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=LEmNu0UJaXc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=LEmNu0UJaXc" target="_blank">Nature上最重要的论文AlphaFold讲了什么？作为一个打工人，咱们需要会些什么才能进公司？ - YouTube</a></p>
<p><a data-tooltip-position="top" aria-label="https://pubmed.ncbi.nlm.nih.gov/34265844/" rel="noopener" class="external-link" href="https://pubmed.ncbi.nlm.nih.gov/34265844/" target="_blank">Highly accurate protein structure prediction with AlphaFold(2021) - PubMed</a></p>
<p><img src="https://cdn.sa.net/2024/04/11/rGUN9nmDtq1alKe.png" referrerpolicy="no-referrer"></p>
<p><img src="https://cdn.sa.net/2024/04/11/AiKFTORmXEzcL7n.png" referrerpolicy="no-referrer"></p>
<p><strong>AlphaFold v2</strong> 是一种由DeepMind开发的人工智能算法，它的主要目标是解决蛋白质折叠预测的问题。蛋白质折叠预测是一个非常重要的生物学问题，因为一个蛋白质的形状（即它如何折叠）决定了它在生物体内的功能。如果我们能够准确预测蛋白质的结构，那么我们就能更好地理解疾病的发生机制，甚至设计出新的药物。</p>
<p>AlphaFold的输入是蛋白质的氨基酸序列，这就像是蛋白质的"配方"。蛋白质是由20种不同的氨基酸以特定的顺序链接起来形成的。AlphaFold从这个序列中预测出蛋白质在三维空间中的结构。</p>
<p>AlphaFold的输出是每个氨基酸在三维空间中的位置，以及氨基酸之间的相对距离和角度。这就构成了蛋白质的三维结构模型。</p>
<p>AlphaFold2的模型主要由两个部分组成：卷积神经网络（CNN）和Transformer。首先，卷积神经网络用于处理输入的一维氨基酸序列和二维的配对势能图（包含了序列中两个氨基酸之间的相互作用信息），并将这些信息转换为一种更高级的内部表示。然后，这个内部表示被输入到一个Transformer模型中。</p>
<p>Transformer模型的关键特性是其自注意力机制，这使得模型能够考虑到输入序列中的所有位置之间的相互关系。在AlphaFold2中，这意味着模型可以考虑到蛋白质中的所有氨基酸对的相互作用，这对于预测蛋白质的三维结构是非常重要的。</p>
<p>AlphaFold的工作过程可以分为两个步骤。首先，它生成一个称为"距离图"的东西，这是一个二维图，这些图描述了蛋白质中的氨基酸对之间的相对位置和取向。然后，AlphaFold2使用一个叫做分子动力学的过程，从这些预测的距离和角度图生成蛋白质的三维结构。</p>
<p>AlphaFold的性能非常出色，它在2020年的蛋白质结构预测竞赛（CASP）中大放异彩，被誉为解决了蛋白质折叠问题。然而，虽然AlphaFold的预测非常准确，但它仍然有一些局限性，例如它对于某些类型的蛋白质结构（如膜蛋白）的预测能力有限。</p>
<p>用 V100 32G <em>128, (EPYC </em>2, 512G RAM) 8 * 16 Nodes,  训练了三个月.</p>
<p>额外(主要)介绍了: </p>
<ol>
<li>打工人需要对各种算法和技术原理需要有一定的理解和涉猎("面试造火箭"); 最好不要做模型人少大厂. </li>
<li>BPS和SPAM的HPC部署脚本</li>
<li>简单介绍了分布式式训练, 分布式方法, 节点离线处理方法.</li>
</ol></div></div></div><div><p><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1zC411474Z/?spm_id_from=333.1007.top_right_bar_window_history.content.click" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1zC411474Z/?spm_id_from=333.1007.top_right_bar_window_history.content.click" target="_blank">【吹爆！】强强联手！科大讯飞和中科院终于把【多模态大模型】给讲通透了！CLIP、blip、blip2三种模型原理一次性学透！全程干货分享无废话！_哔哩哔哩_bilibili</a></p></div><div><p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/661854155" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/661854155" target="_blank">视觉大模型 - 知乎</a></p></div></div></div><div class="heading-wrapper"><h3 data-heading="KAN ( MLP alternative)" class="heading" id="KAN_(_MLP_alternative)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>KAN ( MLP alternative)</h3><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2404.19756" rel="noopener" class="external-link" href="https://arxiv.org/abs/2404.19756" target="_blank">KAN:Kolmogorov-Arnold Networks</a><br>
<a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=OEvJE-O1R2k" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=OEvJE-O1R2k" target="_blank">超越谷歌DeepMind的最新大作：KAN全网最详细解读！ - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://kindxiaoming.github.io/pykan/index.html" rel="noopener" class="external-link" href="https://kindxiaoming.github.io/pykan/index.html" target="_blank">Welcome to Kolmogorov Arnold Network (KAN) documentation! — Kolmogorov Arnold Network documentation</a> ⬅️  基于Pytorch 的 KAN python库(official)<br>
<a data-tooltip-position="top" aria-label="https://github.com/Blealtan/efficient-kan" rel="noopener" class="external-link" href="https://github.com/Blealtan/efficient-kan" target="_blank">GitHub - Blealtan/efficient-kan: An efficient pure-PyTorch implementation of Kolmogorov-Arnold Network (KAN).</a> ⬅️ 基于 Pytorch 基本原地替代 <code>nn.Linear</code> 为 <code>KanLinear</code></p></div><div><p><img src="https://kindxiaoming.github.io/pykan/_images/kan_plot.png" referrerpolicy="no-referrer"><br>
<img src="https://pbs.twimg.com/media/GMhpsTCbUAAP0iC?format=jpg&amp;name=medium" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> MLP 不就是 带学习参数的线性变换+非线性的激活函数不断复合起来的么？那直接用函数复合不就好了么？MLP 是图灵完备的？计算机也是图灵完备的？为什么要在计算机里再跑一个 MLP？其实关键是我们在实际任务中，数学证明一个东西可以干什么还不够，我们还要能在合理的时间内获得其解析解。主要的缺点是很难解释学习到的参数，很难手动调节，容易过拟合</p>
</span></li>
<li data-line="1">
<p>KAN和Mamba一样是数学代替原始模型. 减少了参数量; 利于更好的拟合; 更好的解释性. </p>
</li>
<li data-line="2">
<p>科尔莫戈罗夫-阿诺德表示定理由科尔莫戈罗夫和阿诺德在1957年提出, 证明了希尔伯特的第13个问题的否定答案, 即即七次以上的方程并不需要特殊的函数来解决，而是可以通过使用二元一次函数的复合来解决. </p>
</li>
<li data-line="3">
<p><strong>Kolmogorov-Arnold  表示定理</strong>是一个类似泰勒展开, 傅立叶分解的<strong>函数逼近定理</strong>. 这个表达定理表明<span style="background:#fff88f">任何连续的多元实值函数都可以表示成有限个单元量函数的叠加和复合形式, 但是这个定理没有说明如何构建这个一元函数</span>. 换句话说, 无论一个连续的多元函数是如何的复杂, 只要这个函数是从多维空间变化到实数的, 你总可以找到一些一元函数复合来精准的逼近原始的函数. </p>
</li>
<li data-line="4" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span>  Kolmogorov-Arnold 表示定理 可以运用在神经网络上, 因为每个神经元都可以看作一个简单的函数. 神经元的互相连接可以看作简单函数复合成一个多元函数的状况. 函数有多少输入变量就是神经网络的输入维度.  </p>
</span></li>
<li data-line="5" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 可以理解为将 MLP 上激活函数从点换到边上，将固定的可微的的激活函数用 B 样条函数取代，函数形式固定，具体的函数值用训练学习。</p>
</span></li>
<li data-line="6" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">%</span> Kolmogorov-Arnold 表示定理和万能逼近定理(1989)都可以描述神经网络具有强大拟合能力. 后者者表明任何一个前馈网络如果具有线性输出层和任何一个具有“挤压性质” 的隐藏层(阶跃函数), 只要这个隐藏层具有足够宽, 就可以拟合从一个有限维度到另一个有限维度的Borel 可测函数. 前者提供了理论框架, 后者提供了一个具体的实现路径.<br>
<img src="https://zh.d2l.ai/_images/blocks.svg" referrerpolicy="no-referrer"></p>
</span></li>
<li data-line="9" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> Kolmogorov-Arnold 表示定理的公式形式如下(否定了希尔伯特第十三个问题-是否存在七次方程以上方程的通用解)：</p>
</span></li>
<li data-line="10">
<p>对于任何连续的函数 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo><mjx-script style="vertical-align: 0.363em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom></mjx-math></mjx-container></span>，存在整数 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> 和连续的函数 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom></mjx-math></mjx-container></span> 和 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A6"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom></mjx-math></mjx-container></span>，都有：</p>
</li>
</ul></div><div><p><span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2026"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munderover space="4"><mjx-over style="padding-bottom: 0.192em; padding-left: 0.333em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-over><mjx-box><mjx-munder><mjx-row><mjx-base><mjx-mo class="mjx-lop"><mjx-c class="mjx-c2211 TEX-S2"></mjx-c></mjx-mo></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.108em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom></mjx-under></mjx-row></mjx-munder></mjx-box></mjx-munderover><mjx-msub space="2"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A6"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-s4"><mjx-c class="mjx-c28 TEX-S4"></mjx-c></mjx-mo><mjx-munderover><mjx-over style="padding-bottom: 0.192em; padding-left: 0.51em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-over><mjx-box><mjx-munder><mjx-row><mjx-base><mjx-mo class="mjx-lop"><mjx-c class="mjx-c2211 TEX-S2"></mjx-c></mjx-mo></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.092em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-under></mjx-row></mjx-munder></mjx-box></mjx-munderover><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-s4"><mjx-c class="mjx-c29 TEX-S4"></mjx-c></mjx-mo></mjx-mrow></mjx-math></mjx-container></span></p></div><div><ul>
<li data-line="0">这个定理没有给出构建 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span> 和 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D713 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math></mjx-container></span> 函数的具体方法。在实践中，我们通常使用神经网络来逼近这些函数。每个神经元可以看作是一个简单的函数，通过组合大量的神经元，我们可以逼近任何复杂的函数。</li>
<li data-line="1">KAN 使用的 <strong>B样条函数(Basis Spline functions)</strong>.拥有平滑数值稳定高度可控简洁的的递归定义，故常用于计算机图形学，曲线拟合，数据逼近等场景。</li>
<li data-line="2" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper"><span class="lc-list-marker">%</span> 类似展开函数回顾</span></li>
<li data-line="3"><strong>傅立叶分解</strong> : 将任何周期函数表示为正弦和余弦函数的无穷级数. 这些正弦和余弦函数的频率是基础频率的整数倍，基础频率由原始函数的周期决定</li>
<li data-line="4"><strong>泰勒展开</strong> : 将任何可微函数展开为无穷级数，这个级数的每一项都是函数在某一点的导数和&nbsp;<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-script style="vertical-align: 0.363em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math></mjx-container></span>&nbsp;的乘积</li>
<li data-line="5" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> 特点
</span><ul>
<li data-line="6">拟合能力由数学证明支撑</li>
<li data-line="7">可解释性好. 甚至最后可以写出神经网络的公式. </li>
<li data-line="8">精度高。因为拟合函数本身</li>
<li data-line="9">参数效率高</li>
<li data-line="10">可以拟合周期函数</li>
<li data-line="11">要用的算力大（目前）- 关键不是网络大小而是forward的计算量</li>
</ul>
</li>
</ul></div><div><p>⬇️ 其实你大概率不需要用KAN. 它训练贼慢, 也很难拟合一般的图像或者文本数据.(维度太高了!)<br>
<img src="https://cdn.sa.net/2024/05/07/GpdFXES6ogfj7eW.png" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span>  KAN 主要是用于 AI For Science. 比如 MLP 无法拟合周期函数。</span></li>
<li data-line="1">质疑： PINN（Physics Informed Neural Network）在网络物理计算领域有广泛应用的作者，美国科学院院士George Carnevale。他拟合了几个常见的数学方程，非常笃定的锤 KAN 的效果。</li>
<li data-line="2"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Dr421j72d/?spm_id_from=333.1007.tianma.1-2-2.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Dr421j72d/?spm_id_from=333.1007.tianma.1-2-2.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">[中英字幕] KAN 论文一作 刘子鸣 亲自讲解 Kolmogorov-Arnold Networks_哔哩哔哩_bilibili</a></li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="邦联学习(Federated Learning)" class="heading" id="邦联学习(Federated_Learning)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>邦联学习(Federated Learning)</h2><div class="heading-children"><div><blockquote>
<p>邦联学习就是使多个设备（如手机或服务器）能够共享学习模型的更新，而不是共享原始数据. 用于<strong>保护数据的安全和隐私性并节省带宽</strong>. 在学习中每个计算单元都会有一个模型的副本, 在本地使用本地数据更新模型之后, 再将模型权重发回给中央服务器. 中央服务器再将所有模型聚合起来, 创建一个全局模型后发回给计算单元. 邦联学习在许多领域都有应用，包括医疗、金融和通信等。如，Google使用邦联学习来改进其手机键盘的预测能力，而不需要看到用户的实际输入。</p>
</blockquote></div><div><p><span style="background:#fff88f">联邦学习 = 分布式 + 加密算法</span></p></div><div><ul>
<li data-line="0">隐私数据保存在本地由加密算法保证其安全性</li>
<li data-line="1">梯度是不可能还原成数据的</li>
</ul></div><div><p><span style="background:#fff88f">联邦学习在实际部署中有什么挑战?</span></p></div><div><ol>
<li data-line="0"><strong>隐私保护</strong>：尽管联邦学习的目的是提高数据隐私保护，但它仍然可能面临信息泄露的风险。例如，即使不直接共享数据，通过共享模型更新（如梯度或权重更新）仍然可能暴露敏感信息。研究人员已经展示了如何从共享的模型更新中重构训练数据的攻击方法。</li>
<li data-line="1"><strong>通信开销</strong>: 联邦学习通常需要大量的网络通信，因为参与的设备或服务器需要频繁地发送模型更新到中心服务器进行聚合。这种频繁的通信可能导致显著的网络延迟和高带宽消耗。</li>
<li data-line="2"><strong>系统异质性</strong>：参与联邦学习的设备通常在硬件性能、网络连接速度和数据存储能力上存在差异。这种异质性可能导致训练过程中的不均衡，影响模型的效率和最终性能。</li>
<li data-line="3"><strong>非独立同分布(Non-ID)数据</strong>：在联邦学习场景中，各个节点持有的数据可能具有不同的分布（即非独立同分布）。这可能导致模型在某些节点上表现很好，在其他节点上表现不佳，从而降低了模型的整体性能和泛化能力。⬅️ Shuffle 数据可以缓解</li>
<li data-line="4"><strong>规模管理</strong>：随着参与计算的设备数量增加，协调和管理这些设备，确保稳定和有效的训练过程变得更加困难。</li>
<li data-line="5"><strong>模型毒化和数据毒化攻击</strong>：在开放的联邦学习环境中，恶意参与者可能通过向模型发送错误的更新来影响整体学习过程，即所谓的模型毒化攻击。此外，通过修改自己的本地数据来影响全局模型的学习过程也是可能的。</li>
<li data-line="6"><strong>法律和政策问题</strong>：联邦学习涉及多方数据和模型的共享，可能会触及数据保护法、跨境数据传输规定等复杂的法律和政策问题。</li>
</ol></div><div><p><code>FedAvg</code> <code>FedSgd</code> 是一个基于梯度下降的算法, 专门用于处理邦联学习中数据通信效率低和数据不平衡的问题. </p></div></div></div><div class="heading-wrapper"><h2 data-heading="并行化" class="heading" id="并行化"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>并行化</h2><div class="heading-children"><div><blockquote>
<p>邦联学习依赖并行化. 我们这里先学习一下深度学习并行化的基本概念. </p>
</blockquote></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> All-reduce 和 Map-reduce 是什么? </span></li>
<li data-line="1"><strong>All-reduce</strong> 和 <strong>MapReduce</strong> 是两种常见的并行和分布式计算模式，它们在数据并行、机器学习训练和其他大规模计算任务中都有广泛应用。</li>
</ul></div><div><ol>
<li data-line="0"><strong>All-reduce</strong>：All-reduce 是一种全局同步操作，被用于在所有参与的处理器（或节点）之间共享信息。在一个 All-reduce 操作中，每个处理器首先有一个初始值，然后这些值被某种方式（例如，求和、求最大值、求最小值等）结合在一起，最后每个处理器都会得到这个最终结果。在深度学习中，All-reduce <strong>常常用于数据并行训练中的梯度聚合步骤</strong>，所有处理器计算出的梯度被求和，然后这个和被分发给所有的处理器。</li>
<li data-line="1"><strong>MapReduce</strong>：MapReduce 是 Google 提出的一种用于<strong>大规模数据处理的编程模型</strong>。它包括两个主要步骤：Map（映射）和 Reduce（归约）。在 Map 步骤，输入数据被分割成多个子集，然后每个子集都独立地应用 Map 函数，生成一系列的键值对。在 Reduce 步骤，所有具有相同键的值被聚集在一起，然后应用 Reduce 函数，生成最终结果。MapReduce 能够处理大规模的数据集，它的主要优点是<strong>可以在成千上万的计算节点上并行执行</strong>。</li>
</ol></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> 什么是 Mirror Strategy?  </span></li>
<li data-line="1">Mirror Strategy 是 TensorFlow 的 <code>tf.distribute.Strategy</code> API 中的一种分布式训练策略，正式名称为 <code>tf.distribute.MirroredStrategy</code>。这种策略支持在一台机器上的多个 GPU 之间进行同步训练，是一种数据并行的实现。</li>
<li data-line="2">在 Mirror Strategy 中，模型的所有变量都会在每个设备（例如 GPU）上复制一份。也就是说，每个设备都有一份完整的模型。在训练过程中，每个设备会使用不同的数据批次对其模型副本进行更新，然后通过 All-reduce 操作同步更新后的变量，确保所有设备上的模型副本始终保持一致。这样，每个设备都可以并行地处理数据并更新模型，从而提高训练速度。</li>
<li data-line="3">Mirror Strategy 的主要优点是它可以提供良好的设备间通信性能，并且易于使用。但是，它<span style="background:#fff88f">只适用于一台机器上的多个 GPU</span>，不适用于跨多台机器的分布式训练。对于跨机器的分布式训练，TensorFlow 提供了其他的策略，如<code>tf.distribute.experimental.MultiWorkerMirroredStrategy</code>。</li>
<li data-line="4" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> 有什么大数据框架</span></li>
<li data-line="5" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>Hadoop</strong>  是一个由Apache基金会开发的开源框架，用于处理和存储大数据。它的主要特性是能够在大量的硬件中进行分布式处理，从而使得大规模数据的处理变得更加快速和可靠。Hadoop的主要组件包括Hadoop Distributed File System（HDFS）用于存储数据，和MapReduce用于处理数据。</span></li>
<li data-line="6" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>Spark</strong>：Spark是一个用于大规模数据处理的快速和通用的计算系统。它提供了一个高级的API，使得开发人员可以更容易地编写分布式代码。与Hadoop相比，Spark能够提供更快的计算速度和更强大的计算能力。</span></li>
<li data-line="7" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> Flink：Flink是一个用于批量和流数据处理的开源平台。它的主要特点是能够提供精确的计算结果，即使在面对大量数据时也能保持高性能。</span></li>
<li data-line="8" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> Storm：Storm是一个用于实时数据处理的开源框架。它能够处理大量的数据流，并且能够保证数据的处理过程是可靠的。</span></li>
<li data-line="9" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>Hive</strong>：Hive是一个基于Hadoop的数据仓库工具，能够将结构化的数据文件映射为一张数据库表，并提供SQL查询功能。</span></li>
<li data-line="10" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>HBase</strong>：HBase是一个基于Hadoop的分布式数据库，能够存储大量的非结构化和半结构化的数据。</span></li>
</ul></div><div><blockquote>
<p>并行训练的主要瓶颈是通信带宽. </p>
</blockquote></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>数据并行（Data Parallelism）</strong></span></li>
<li data-line="1">最常见, 基本思想是拆分数据集, 每个计算单元(e.g.一张GPU)独立的在一个模型副本上在小数据集上进行前向和反向传播最后权重聚合起来更新一个全局模型..所有计算单元共享网络结构和模型权重. <strong>适合处理大型数据集</strong>. 但受到到网络通信延迟、梯度聚合和参数更新的计算开销. 基本的流程如下:</li>
</ul></div><div><ol>
<li data-line="0"><strong>准备阶段</strong>：首先，将训练数据分成多个子集（通常称为批次）。然后，在每个处理器（或计算设备，如GPU）上复制模型的初始参数。</li>
<li data-line="1"><strong>并行训练</strong>：每个处理器使用其分配的数据子集独立地进行前向传播和反向传播，计算模型参数的梯度。</li>
<li data-line="2"><strong>梯度合并</strong>：所有处理器的梯度被收集后，各个GPU上计算得到的梯度需要被合并。这通常通过一种叫做 <code>All-reduce</code> 操作的通信过程完成。在 <code>All-reduce</code> 操作中，所有GPU的梯度会被累加（或取平均）.</li>
<li data-line="3"><strong>全局模型同步或异步更新</strong> : 将聚合权重同步或异步的广播到所有GPU。这确保了每个GPU在更新权重时使用的是全局累计的梯度。</li>
<li data-line="4"><strong>迭代</strong>：重复步骤2-4，直到模型收敛，或满足其他停止条件。</li>
</ol></div><div><ul>
<li data-line="0">
<p><strong>实现工具</strong>: 在PyTorch中，可以使用<code>torch.nn.DataParallel</code>和<code>torch.nn.parallel.DistributedDataParallel</code>类来实现数据并行。在TensorFlow中，可以使用<code>tf.distribute</code>策略来实现。</p>
</li>
<li data-line="3" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <strong>模型并行（Model Parallelism）</strong>：在模型并行中，模型被分割成几个部分，并在多个处理器上并行运行。这种方法特别适用于模型太大而无法在单个处理器上运行的情况。</p>
</span></li>
<li data-line="4">
<p>模型并行适用于模态太大而至于单卡无法装下的情况. 模型并行将模型分割成几个部分, 然后在不同的计算单元上并行的训练模型的部分. 模型并行也需要更新模型权重, 所以也收到通信带宽的限制 .模型并行的基本流程如下：</p>
</li>
</ul></div><div><ol>
<li data-line="0">将模型的不同部分分配到不同的设备上。例如，如果我们有一个深度神经网络，我们可以将网络的不同层分配到不同的GPU上。</li>
<li data-line="1">在每个设备上并行地计算模型的前向传播和反向传播。</li>
<li data-line="2">通过通信链接（例如，NVLink或PCIe）在设备之间交换必要的数据。例如，一层的输出可能需要作为下一层的输入。</li>
<li data-line="3">更新模型的参数。这可能需要在设备之间同步参数。</li>
</ol></div><div><ul>
<li data-line="0" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>管道并行（Pipeline Parallelism）</strong>：在管道并行中，模型的不同部分会在不同的时间处理不同的数据。这意味着一部分模型可以开始处理下一个数据点，而另一部分模型仍在处理上一个数据点。这种方法可以减少等待时间，提高效率。</span></li>
<li data-line="3" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> <strong>混合并行（Hybrid Parallelism）</strong>：混合并行结合了上述几种方法，以适应各种不同的需求和约束。例如，可以同时使用数据并行和模型并行，以处理大型数据集和大型模型。</span></li>
</ul></div></div></div><div class="heading-wrapper"><h2 data-heading="调优指南" class="heading" id="调优指南"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>调优指南</h2><div class="heading-children"><div class="admonition-parent admonition-hint-parent"><div class="callout admonition admonition-hint admonition-plugin " style="--callout-color: 0, 191, 165;" data-callout="hint" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="fire" class="svg-inline--fa fa-fire fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M216 23.86c0-23.8-30.65-32.77-44.15-13.04C48 191.85 224 200 224 288c0 35.63-29.11 64.46-64.85 63.99-35.17-.45-63.15-29.77-63.15-64.94v-85.51c0-21.7-26.47-32.23-41.43-16.5C27.8 213.16 0 261.33 0 320c0 105.87 86.13 192 192 192s192-86.13 192-192c0-170.29-168-193-168-296.14z"></path></svg></div><div class="callout-title-inner admonition-title-content">Hint</div></div><div class="callout-content admonition-content heading-wrapper"><p><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/41631631/answer/2812816993" rel="noopener" class="external-link" href="https://www.zhihu.com/question/41631631/answer/2812816993" target="_blank">你有哪些deep learning（rnn、cnn）调参的经验？ - 知乎</a></p>
<h3 data-heading="Code pipeline" class="heading" id="Code_pipeline">Code pipeline</h3>
<p>e.g. computer vision task</p>
<ol>
<li>data(dataset/dataloader)<br>
可以加一下lazy初始化<strong>slot</strong>的技巧减缓内存占用</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>model
<ol>
<li>模型代码</li>
<li>参数初始化</li>
</ol>
</li>
<li><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>optimizer 
<ol>
<li>adad/adamW</li>
<li>lr (3.5e-4)</li>
<li>lr schedule (warm-up + cos退火)</li>
<li>epochs </li>
</ol>
</li>
<li>loss func</li>
<li>train func</li>
<li>eval func</li>
</ol></div></div></div><div class="admonition-parent admonition-cite-parent"><div class="callout admonition admonition-cite admonition-plugin " style="--callout-color: 158, 158, 158;" data-callout="cite" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="quote-right" class="svg-inline--fa fa-quote-right fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Cite</div></div><div class="callout-content admonition-content"><p><a data-tooltip-position="top" aria-label="https://github.com/lartpang/PyTorchTricks" rel="noopener" class="external-link" href="https://github.com/lartpang/PyTorchTricks" target="_blank">GitHub - lartpang/PyTorchTricks: Some tricks of pytorch... :star:</a></p>
<p>一个主要讲解如何降低 Pytorch 工程中代码运行速度的repo.<br>
包含预处理加速, 训练策略加速, 优化器加速, 代码和网络构建时的 tricks</p></div></div></div><div><p>深度学习有什么调优的具体方针?</p></div><div><ol>
<li data-line="0">
<p>调整学习率：学习率是深度学习中的一个重要参数。它决定了模型在学习过程中对错误的反应程度。如果学习率太高，模型可能会在最优解附近震荡而无法收敛；如果学习率太低，模型收敛速度可能会非常慢。因此，合适的学习率是非常重要的。</p>
</li>
<li data-line="2">
<p>批量大小调整：批量大小决定了每次模型更新的样本数量。较大的批量可以提高计算效率，但可能导致模型过拟合；较小的批量可能使模型更容易收敛到全局最优，但计算效率较低。</p>
</li>
<li data-line="4">
<p>选择合适的优化器：有许多不同的优化器可供选择，如SGD、Adam、RMSProp等。不同的优化器有不同的优缺点，需要根据具体的任务和数据选择最合适的优化器。</p>
</li>
<li data-line="6">
<p>正则化：正则化是一种防止模型过拟合的技术，如L1、L2正则化、dropout等。通过在损失函数中添加一个正则化项，可以限制模型复杂度，防止过拟合。</p>
</li>
<li data-line="8">
<p>调整网络结构：网络结构（如层数、每层神经元数量等）对模型性能有很大影响。通常来说，更复杂的网络结构可以学习更复杂的特征，但也更容易过拟合。</p>
</li>
<li data-line="10">
<p>数据增强：通过对训练数据进行一些变换（如旋转、缩放、剪裁等），可以生成更多的训练样本，提高模型的泛化能力。</p>
</li>
<li data-line="12">
<p>早停：当验证集上的性能不再提高时，停止训练可以防止过拟合。</p>
</li>
<li data-line="14">
<p>参数初始化：参数的初始值对模型的学习过程和最终性能有很大影响。一般来说，合适的参数初始化可以加速模型的收敛，并提高最终性能。</p>
</li>
<li data-line="16">
<p>学习率衰减：随着训练的进行，逐步降低学习率可以帮助模型更好地收敛。</p>
</li>
</ol></div><div><ul>
<li data-line="0">项目一开始，尽量使用一个完善有效的模型，尽快先让代码跑起来</li>
<li data-line="1">Quasi-Random-Search 通过在搜索空间均匀的生成样本点来进行搜索，在优化探索阶段很好用</li>
<li data-line="2">初始超参配置: 1) 模型配置 2) 优化器 3) 训练步数</li>
<li data-line="3">先使用常见的优化器(e.g. Adam, NAdam, SGD with momentum)</li>
<li data-line="4">选用硬件可支持的最大Batch Size, 以减少训练时间， 测试更多新想法。e.g.(batch size = 2^n, 1,2....,n)</li>
<li data-line="5">大多超参最优值对batch size 敏感， 特别是优化器(学习率， 动量)和正则化超参</li>
<li data-line="6"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>提高模型性能遵循增量调优策略
<ul>
<li data-line="7"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<ol>
<li data-line="7">设定下一轮实验的目标(小步快跑)</li>
</ol>
</li>
<li data-line="8"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<ol start="2">
<li data-line="8">设计展开实验(识别目标, 冗余, 固定超参)</li>
</ol>
</li>
<li data-line="9"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<ol start="3">
<li data-line="9">从实验中获得经验</li>
</ol>
</li>
<li data-line="10"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<ol start="4">
<li data-line="10">考虑是否使用新配置</li>
</ol>
</li>
</ul>
</li>
<li data-line="11">如果最佳点在搜索空间的边界， 那么该模型很可能并不稳定</li>
<li data-line="12">训练函数的val error在某个时刻增加时， 就会发生over fitting. 缓解手段有e.g. dropout、标签平滑化、权重衰减</li>
<li data-line="13">isolation 图可以帮助检测更改是否有效</li>
<li data-line="14">贝叶斯优化工具可以在完成“好”的搜索空间探索后，继续精准优化超参</li>
<li data-line="15">在训练误差无限改善时，多试几个参数可能更有帮助</li>
<li data-line="16"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>对于多轮测试， 可以在第一轮进行短时间训练获取最佳模型和优化器参数。第二轮，在较佳超参数上
<ul>
<li data-line="17"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>在第一轮获得超参泛化到更长epochs可能是
<ul>
<li data-line="18">warmup 时长，模型参数初始值</li>
<li data-line="19">优化算法/优化器超参数</li>
<li data-line="20">数据增强方法</li>
<li data-line="21">正则化</li>
</ul>
</li>
<li data-line="22"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>不可能转移
<ul>
<li data-line="23">学习率衰减schedule</li>
</ul>
</li>
</ul>
</li>
<li data-line="24"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>pipeline性能受阻的常见原因
<ul>
<li data-line="25">数据未与训练进程存放在同一位置，导致I/O延迟</li>
<li data-line="26">开销很大的在线预处理</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h2 data-heading="硬件" class="heading" id="硬件"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>硬件</h2><div class="heading-children"><div><p><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1P44y1V7bu/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1P44y1V7bu/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">上帝视角看GPU（1）：图形流水线基础_哔哩哔哩_bilibili</a></p></div></div></div><div class="heading-wrapper"><h2 data-heading="阅读笔记" class="heading" id="阅读笔记"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>阅读笔记</h2><div class="heading-children"><div><ul>
<li data-line="0">
<p>什么是拓扑生成网络？</p>
</li>
<li data-line="3">
<p>深度学习中不可微性问题 <a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1vT42197fb/?spm_id_from=333.880.my_history.page.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1vT42197fb/?spm_id_from=333.880.my_history.page.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">如何解决深度学习中的不可微问题（一）</a></p>
</li>
<li data-line="4">
<p>众多周知 DL 依赖的梯度下降要求目标函数可微。对于单元函数来说就是在点 C存在极限且连续。对于多元函数来说就是所有方向的偏导数存在且连续。</p>
</li>
<li data-line="5">
<p>目标函数</p>
</li>
<li data-line="9" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">?</span> 为什么高维空间的两组随机向量几乎正交?</p>
</span></li>
<li data-line="10">
<p>很多数据都可以看作从某种分布中采样的高维向量。</p>
</li>
<li data-line="11">
<p><span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-munderover space="4"><mjx-over style="padding-bottom: 0.192em; padding-left: 0.51em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-over><mjx-box><mjx-munder><mjx-row><mjx-base><mjx-mo class="mjx-lop"><mjx-c class="mjx-c2211 TEX-S2"></mjx-c></mjx-mo></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.148em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-under></mjx-row></mjx-munder></mjx-box></mjx-munderover><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span></p>
</li>
<li data-line="12">
<p>假设 <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span> <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span> 都是从正太分布中独立同采样的。中心极限定理表明，当n 变大时，ab点积会趋向于正太分布的均值。同时，向量a和 b 的模长平方的期望值近似等于 n. 因为它们是由单位方差的独立分量构成的。就变成下面的样子</p>
</li>
<li data-line="13">
<p><span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2248"></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-munderover limits="false"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2211 TEX-S1"></mjx-c></mjx-mo><mjx-script style="vertical-align: -0.285em; margin-left: 0px;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-spacer style="margin-top: 0.284em;"></mjx-spacer><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-munderover><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container></span></p>
</li>
<li data-line="14">
<p>n 变大, 夹角肯定越趋向于 0</p>
</li>
<li data-line="15">
<p>简单来说，自由度变大导致任何特定的对齐方向都变得不太可能。</p>
</li>
</ul></div><div class="mod-footer"></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="💾-科技工程/4_深度学习.html#4_深度学习"><div class="tree-item-contents heading-link" heading-name="4_深度学习"><span class="tree-item-title">4_深度学习</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#深度学习名人堂"><div class="tree-item-contents heading-link" heading-name="深度学习名人堂"><span class="tree-item-title">深度学习名人堂</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#基本算法"><div class="tree-item-contents heading-link" heading-name="基本算法"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">基本算法</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#反向传播_&amp;_梯度优化"><div class="tree-item-contents heading-link" heading-name="反向传播 &amp; 梯度优化"><span class="tree-item-title">反向传播 &amp; 梯度优化</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#过拟合_&amp;_欠拟合"><div class="tree-item-contents heading-link" heading-name="过拟合 &amp; 欠拟合"><span class="tree-item-title">过拟合 &amp; 欠拟合</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#数值稳定性和参数初始化"><div class="tree-item-contents heading-link" heading-name="数值稳定性和参数初始化"><span class="tree-item-title">数值稳定性和参数初始化</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#优化器"><div class="tree-item-contents heading-link" heading-name="优化器"><span class="tree-item-title">优化器</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#线性层_+_激活函数"><div class="tree-item-contents heading-link" heading-name="线性层 + 激活函数"><span class="tree-item-title">线性层 + 激活函数</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#正则化"><div class="tree-item-contents heading-link" heading-name="正则化"><span class="tree-item-title">正则化</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#规范化(BN,_LN,_IN,_GN,_RMSNorm)"><div class="tree-item-contents heading-link" heading-name="规范化(BN, LN, IN, GN, RMSNorm)"><span class="tree-item-title">规范化(BN, LN, IN, GN, RMSNorm)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#数据增强"><div class="tree-item-contents heading-link" heading-name="数据增强"><span class="tree-item-title">数据增强</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#CNN(卷积神经网络)"><div class="tree-item-contents heading-link" heading-name="CNN(卷积神经网络)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">CNN(卷积神经网络)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#LeNet(鼻祖)_-_1998"><div class="tree-item-contents heading-link" heading-name="LeNet(鼻祖) - 1998"><span class="tree-item-title">LeNet(鼻祖) - 1998</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#LeNet_是什么?_对深度学习的发展有什么贡献?"><div class="tree-item-contents heading-link" heading-name="LeNet 是什么? 对深度学习的发展有什么贡献?"><span class="tree-item-title">LeNet 是什么? 对深度学习的发展有什么贡献?</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#AlexNet(重启深度学习)_-_2012"><div class="tree-item-contents heading-link" heading-name="AlexNet(重启深度学习) - 2012"><span class="tree-item-title">AlexNet(重启深度学习) - 2012</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#AlexNet_是什么?_对深度学习的发展有什么贡献?"><div class="tree-item-contents heading-link" heading-name="AlexNet 是什么? 对深度学习的发展有什么贡献?"><span class="tree-item-title">AlexNet 是什么? 对深度学习的发展有什么贡献?</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#ZfNet(第一次可视化隐藏层)_-_2013"><div class="tree-item-contents heading-link" heading-name="ZfNet(第一次可视化隐藏层) - 2013"><span class="tree-item-title">ZfNet(第一次可视化隐藏层) - 2013</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#分割网络(R-CNN,_SSD,_YOLO..)"><div class="tree-item-contents heading-link" heading-name="分割网络(R-CNN, SSD, YOLO..)"><span class="tree-item-title">分割网络(R-CNN, SSD, YOLO..)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#历史上的分割网路"><div class="tree-item-contents heading-link" heading-name="历史上的分割网路"><span class="tree-item-title">历史上的分割网路</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#R-CNN_是什么?_对深度学习的发展有什么贡献?"><div class="tree-item-contents heading-link" heading-name="R-CNN 是什么? 对深度学习的发展有什么贡献?"><span class="tree-item-title">R-CNN 是什么? 对深度学习的发展有什么贡献?</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#VGG(纯CNN最深网络)_-_2014"><div class="tree-item-contents heading-link" heading-name="VGG(纯CNN最深网络) - 2014"><span class="tree-item-title">VGG(纯CNN最深网络) - 2014</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#GoogLeNet_/_Inception(探索各种trick)_-_2014"><div class="tree-item-contents heading-link" heading-name="GoogLeNet / Inception(探索各种trick) - 2014"><span class="tree-item-title">GoogLeNet / Inception(探索各种trick) - 2014</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#FCN_-_2014_(骨干,_不平衡的U-Net)"><div class="tree-item-contents heading-link" heading-name="FCN - 2014 (骨干, 不平衡的U-Net)"><span class="tree-item-title">FCN - 2014 (骨干, 不平衡的U-Net)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#ResNet(残差网络)_-_2015"><div class="tree-item-contents heading-link" heading-name="ResNet(残差网络) - 2015"><span class="tree-item-title">ResNet(残差网络) - 2015</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#DenseNet(密集连接)_-_2016"><div class="tree-item-contents heading-link" heading-name="DenseNet(密集连接) - 2016"><span class="tree-item-title">DenseNet(密集连接) - 2016</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#MobileNet(网络做小)_-_2016"><div class="tree-item-contents heading-link" heading-name="MobileNet(网络做小) - 2016"><span class="tree-item-title">MobileNet(网络做小) - 2016</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#U-Net(骨干)"><div class="tree-item-contents heading-link" heading-name="U-Net(骨干)"><span class="tree-item-title">U-Net(骨干)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#摘要"><div class="tree-item-contents heading-link" heading-name="摘要"><span class="tree-item-title">摘要</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#引言"><div class="tree-item-contents heading-link" heading-name="引言"><span class="tree-item-title">引言</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#网络架构"><div class="tree-item-contents heading-link" heading-name="网络架构"><span class="tree-item-title">网络架构</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#训练"><div class="tree-item-contents heading-link" heading-name="训练"><span class="tree-item-title">训练</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#数据增强"><div class="tree-item-contents heading-link" heading-name="数据增强"><span class="tree-item-title">数据增强</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#实验"><div class="tree-item-contents heading-link" heading-name="实验"><span class="tree-item-title">实验</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#结论"><div class="tree-item-contents heading-link" heading-name="结论"><span class="tree-item-title">结论</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#后续改进"><div class="tree-item-contents heading-link" heading-name="后续改进"><span class="tree-item-title">后续改进</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#FPN(骨干)"><div class="tree-item-contents heading-link" heading-name="FPN(骨干)"><span class="tree-item-title">FPN(骨干)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#EfficientNet_(终结网络scale经验法则)_-_2019"><div class="tree-item-contents heading-link" heading-name="EfficientNet (终结网络scale经验法则) - 2019"><span class="tree-item-title">EfficientNet (终结网络scale经验法则) - 2019</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#BiT_(最大有监督CNN)"><div class="tree-item-contents heading-link" heading-name="BiT (最大有监督CNN)"><span class="tree-item-title">BiT (最大有监督CNN)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#VanillaNet_?"><div class="tree-item-contents heading-link" heading-name="VanillaNet ?"><span class="tree-item-title">VanillaNet ?</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#RNN(循环神经网络)"><div class="tree-item-contents heading-link" heading-name="RNN(循环神经网络)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">RNN(循环神经网络)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#LSTM(长短期记忆网络)_&amp;_GRU(门控循环单元)"><div class="tree-item-contents heading-link" heading-name="LSTM(长短期记忆网络) &amp; GRU(门控循环单元)"><span class="tree-item-title">LSTM(长短期记忆网络) &amp; GRU(门控循环单元)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#TransformerOut?_Linear_RNN_Can_help"><div class="tree-item-contents heading-link" heading-name="TransformerOut? Linear RNN Can help"><span class="tree-item-title">TransformerOut? Linear RNN Can help</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#Transformer_(2017)"><div class="tree-item-contents heading-link" heading-name="Transformer (2017)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">Transformer (2017)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#&quot;Attention_Is_All_You_Need&quot;"><div class="tree-item-contents heading-link" heading-name="&quot;Attention Is All You Need&quot;"><span class="tree-item-title">"Attention Is All You Need"</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#Vision_Task"><div class="tree-item-contents heading-link" heading-name="Vision Task"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">Vision Task</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="5"><a class="tree-link" href="💾-科技工程/4_深度学习.html#什么是Swin_Transformer._能不能详细讲解一下它的原理和网络结构."><div class="tree-item-contents heading-link" heading-name="什么是Swin Transformer. 能不能详细讲解一下它的原理和网络结构."><span class="tree-item-title">什么是Swin Transformer. 能不能详细讲解一下它的原理和网络结构.</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="5"><a class="tree-link" href="💾-科技工程/4_深度学习.html#Swin_Transformer_提出的动机是什么?_解决了Transformer的什么缺点相对于CNN来说?"><div class="tree-item-contents heading-link" heading-name="Swin Transformer 提出的动机是什么? 解决了Transformer的什么缺点相对于CNN来说?"><span class="tree-item-title">Swin Transformer 提出的动机是什么? 解决了Transformer的什么缺点相对于CNN来说?</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="5"><a class="tree-link" href="💾-科技工程/4_深度学习.html#Vision_Transformer_相比_CNN_有什么缺陷和优点?"><div class="tree-item-contents heading-link" heading-name="Vision Transformer 相比 CNN 有什么缺陷和优点?"><span class="tree-item-title">Vision Transformer 相比 CNN 有什么缺陷和优点?</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#Natural_Language_Task"><div class="tree-item-contents heading-link" heading-name="Natural Language Task"><span class="tree-item-title">Natural Language Task</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#GAN(对抗生成网络)"><div class="tree-item-contents heading-link" heading-name="GAN(对抗生成网络)"><span class="tree-item-title">GAN(对抗生成网络)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#Autoencoder(自编码器)"><div class="tree-item-contents heading-link" heading-name="Autoencoder(自编码器)"><span class="tree-item-title">Autoencoder(自编码器)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#Diffusion(扩散模型)"><div class="tree-item-contents heading-link" heading-name="Diffusion(扩散模型)"><span class="tree-item-title">Diffusion(扩散模型)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#语言大模型(Large_Language_Model)"><div class="tree-item-contents heading-link" heading-name="语言大模型(Large Language Model)"><span class="tree-item-title">语言大模型(Large Language Model)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#知识蒸馏(Knowledge_Distilling)"><div class="tree-item-contents heading-link" heading-name="知识蒸馏(Knowledge Distilling)"><span class="tree-item-title">知识蒸馏(Knowledge Distilling)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#对比学习(Contrastive_Learning)"><div class="tree-item-contents heading-link" heading-name="对比学习(Contrastive Learning)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">对比学习(Contrastive Learning)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#CLIP(2021)"><div class="tree-item-contents heading-link" heading-name="CLIP(2021)"><span class="tree-item-title">CLIP(2021)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#MoCo_(2019)"><div class="tree-item-contents heading-link" heading-name="MoCo (2019)"><span class="tree-item-title">MoCo (2019)</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#强化学习(RF_Learning)"><div class="tree-item-contents heading-link" heading-name="强化学习(RF Learning)"><span class="tree-item-title">强化学习(RF Learning)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#双流网络(Two-stream_Network)"><div class="tree-item-contents heading-link" heading-name="双流网络(Two-stream Network)"><span class="tree-item-title">双流网络(Two-stream Network)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#双流网络_(Two-Stream_Network)_-_视频理解领域开山之作"><div class="tree-item-contents heading-link" heading-name="双流网络 (Two-Stream Network) - 视频理解领域开山之作"><span class="tree-item-title">双流网络 (Two-Stream Network) - 视频理解领域开山之作</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#多模态(multimodal)"><div class="tree-item-contents heading-link" heading-name="多模态(multimodal)"><span class="tree-item-title">多模态(multimodal)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#图神经网络(GNN)"><div class="tree-item-contents heading-link" heading-name="图神经网络(GNN)"><span class="tree-item-title">图神经网络(GNN)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#模型可解释"><div class="tree-item-contents heading-link" heading-name="模型可解释"><span class="tree-item-title">模型可解释</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#AI4Science"><div class="tree-item-contents heading-link" heading-name="AI4Science"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">AI4Science</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#AlphaFold"><div class="tree-item-contents heading-link" heading-name="AlphaFold"><span class="tree-item-title">AlphaFold</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#KAN_(_MLP_alternative)"><div class="tree-item-contents heading-link" heading-name="KAN ( MLP alternative)"><span class="tree-item-title">KAN ( MLP alternative)</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#邦联学习(Federated_Learning)"><div class="tree-item-contents heading-link" heading-name="邦联学习(Federated Learning)"><span class="tree-item-title">邦联学习(Federated Learning)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#并行化"><div class="tree-item-contents heading-link" heading-name="并行化"><span class="tree-item-title">并行化</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#调优指南"><div class="tree-item-contents heading-link" heading-name="调优指南"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">调优指南</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/4_深度学习.html#Code_pipeline"><div class="tree-item-contents heading-link" heading-name="Code pipeline"><span class="tree-item-title">Code pipeline</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#硬件"><div class="tree-item-contents heading-link" heading-name="硬件"><span class="tree-item-title">硬件</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/4_深度学习.html#阅读笔记"><div class="tree-item-contents heading-link" heading-name="阅读笔记"><span class="tree-item-title">阅读笔记</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>