<!DOCTYPE html> <html><head>
		<title>6_AI 系统</title>
		<base href="../">
		<meta id="root-path" root-path="../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="🌱 Digital-Garden - 6_AI 系统">
		<meta property="og:title" content="6_AI 系统">
		<meta property="og:description" content="🌱 Digital-Garden - 6_AI 系统">
		<meta property="og:type" content="website">
		<meta property="og:url" content="💾-科技工程/6_ai-系统.html">
		<meta property="og:image" content="https://cdn.sa.net/2024/06/23/3lmI6kDQYZBWyPe.png">
		<meta property="og:site_name" content="🌱 Digital-Garden">
		<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.4.0/pixi.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="https://cdn.jsdelivr.net/npm/minisearch@6.3.0/dist/umd/index.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="preload" href="lib/styles/other-plugins.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/other-plugins.css"></noscript><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript></head><body class="publish css-settings-manager native-scrollbars theme-light show-inline-title show-ribbon"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles">mjx-c.mjx-c2190::before{padding:.511em 1em .011em 0;content:"←"}mjx-c.mjx-c1D7CF.TEX-B::before{padding:.655em .575em 0 0;content:"1"}mjx-mspace{display:inline-block;text-align:left}mjx-c.mjx-c1D6F4.TEX-I::before{padding:.683em .806em 0 0;content:"Σ"}mjx-c.mjx-c211D.TEX-A::before{padding:.683em .722em 0 0;content:"R"}mjx-c.mjx-c3A6::before{padding:.683em .722em 0 0;content:"Φ"}mjx-c.mjx-c28.TEX-S4::before{padding:1.75em .792em 1.249em 0;content:"("}mjx-c.mjx-c29.TEX-S4::before{padding:1.75em .792em 1.249em 0;content:")"}mjx-c.mjx-c1D713.TEX-I::before{padding:.694em .651em .205em 0;content:"ψ"}mjx-c.mjx-cD7::before{padding:.491em .778em 0 0;content:"×"}mjx-c.mjx-c221A.TEX-S1::before{padding:.85em 1.02em .35em 0;content:"√"}mjx-c.mjx-c41::before{padding:.716em .75em 0 0;content:"A"}mjx-c.mjx-c4D::before{padding:.683em .917em 0 0;content:"M"}mjx-c.mjx-c48::before{padding:.683em .75em 0 0;content:"H"}mjx-c.mjx-c43::before{padding:.705em .722em .021em 0;content:"C"}mjx-c.mjx-c63::before{padding:.448em .444em .011em 0;content:"c"}mjx-c.mjx-c45::before{padding:.68em .681em 0 0;content:"E"}mjx-c.mjx-c28.TEX-S3::before{padding:1.45em .736em .949em 0;content:"("}mjx-c.mjx-c29.TEX-S3::before{padding:1.45em .736em .949em 0;content:")"}mjx-c.mjx-c4C::before{padding:.683em .625em 0 0;content:"L"}mjx-c.mjx-c79::before{padding:.431em .528em .204em 0;content:"y"}mjx-c.mjx-c4E::before{padding:.683em .75em 0 0;content:"N"}mjx-c.mjx-c53::before{padding:.705em .556em .022em 0;content:"S"}mjx-c.mjx-c46::before{padding:.68em .653em 0 0;content:"F"}mjx-c.mjx-c5B.TEX-S3::before{padding:1.45em .528em .949em 0;content:"["}mjx-c.mjx-c5D.TEX-S3::before{padding:1.45em .528em .949em 0;content:"]"}mjx-c.mjx-c22A4::before{padding:.668em .778em 0 0;content:"⊤"}mjx-c.mjx-c2299::before{padding:.583em .778em .083em 0;content:"⊙"}mjx-c.mjx-c2297::before{padding:.583em .778em .083em 0;content:"⊗"}mjx-c.mjx-c77::before{padding:.431em .722em .011em 0;content:"w"}mjx-c.mjx-c6B::before{padding:.694em .528em 0 0;content:"k"}mjx-c.mjx-c1D410.TEX-B::before{padding:.696em .864em .193em 0;content:"Q"}mjx-c.mjx-c1D40A.TEX-B::before{padding:.686em .901em 0 0;content:"K"}mjx-c.mjx-c1D413.TEX-B::before{padding:.675em .8em 0 0;content:"T"}mjx-c.mjx-c1D415.TEX-B::before{padding:.686em .869em .007em 0;content:"V"}mjx-c.mjx-c1D405.TEX-B::before{padding:.68em .724em 0 0;content:"F"}mjx-c.mjx-c1D53C.TEX-A::before{padding:.683em .667em 0 0;content:"E"}mjx-c.mjx-c44.TEX-C::before{padding:.683em .771em 0 0;content:"D"}mjx-c.mjx-c1D6FD.TEX-I::before{padding:.705em .566em .194em 0;content:"β"}mjx-c.mjx-c1D6FE.TEX-I::before{padding:.441em .543em .216em 0;content:"γ"}mjx-c.mjx-c5B.TEX-S2::before{padding:1.15em .472em .649em 0;content:"["}mjx-c.mjx-c5D.TEX-S2::before{padding:1.15em .472em .649em 0;content:"]"}mjx-c.mjx-c2248::before{padding:.483em .778em 0 0;content:"≈"}mjx-munderover{display:inline-block;text-align:left}mjx-munderover:not([limits=false]){padding-top:.1em}mjx-munderover:not([limits=false])>*{display:block}mjx-munder{display:inline-block;text-align:left}mjx-over{text-align:left}mjx-munder:not([limits=false]){display:inline-table}mjx-munder>mjx-row{text-align:left}mjx-under{padding-bottom:.1em}mjx-mtable{display:inline-block;text-align:center;vertical-align:.25em;position:relative;box-sizing:border-box;border-spacing:0px;border-collapse:collapse}mjx-mstyle[size="s"] mjx-mtable{vertical-align:.354em}mjx-labels{position:absolute;left:0;top:0}mjx-table{display:inline-block;vertical-align:-.5ex;box-sizing:border-box}mjx-table>mjx-itable{vertical-align:middle;text-align:left;box-sizing:border-box}mjx-labels>mjx-itable{position:absolute;top:0}mjx-mtable[justify=left]{text-align:left}mjx-mtable[justify=right]{text-align:right}mjx-mtable[justify=left][side=left]{padding-right:0!important}mjx-mtable[justify=left][side=right]{padding-left:0!important}mjx-mtable[justify=right][side=left]{padding-right:0!important}mjx-mtable[justify=right][side=right]{padding-left:0!important}mjx-mtable[align]{vertical-align:baseline}mjx-mtable[align=top]>mjx-table{vertical-align:top}mjx-mtable[align=bottom]>mjx-table{vertical-align:bottom}mjx-mtable[side=right] mjx-labels{min-width:100%}mjx-mtr{display:table-row;text-align:left}mjx-mtr[rowalign=top]>mjx-mtd{vertical-align:top}mjx-mtr[rowalign=center]>mjx-mtd{vertical-align:middle}mjx-mtr[rowalign=bottom]>mjx-mtd{vertical-align:bottom}mjx-mtr[rowalign=baseline]>mjx-mtd{vertical-align:baseline}mjx-mtr[rowalign=axis]>mjx-mtd{vertical-align:.25em}mjx-mtd{display:table-cell;text-align:center;padding:.215em .4em}mjx-mtd:first-child{padding-left:0}mjx-mtd:last-child{padding-right:0}mjx-mtable>*>mjx-itable>:first-child>mjx-mtd{padding-top:0}mjx-mtable>*>mjx-itable>:last-child>mjx-mtd{padding-bottom:0}mjx-tstrut{display:inline-block;height:1em;vertical-align:-.25em}mjx-labels[align=left]>mjx-mtr>mjx-mtd{text-align:left}mjx-labels[align=right]>mjx-mtr>mjx-mtd{text-align:right}mjx-mtd[extra]{padding:0}mjx-mtd[rowalign=top]{vertical-align:top}mjx-mtd[rowalign=center]{vertical-align:middle}mjx-mtd[rowalign=bottom]{vertical-align:bottom}mjx-mtd[rowalign=baseline]{vertical-align:baseline}mjx-mtd[rowalign=axis]{vertical-align:.25em}mjx-stretchy-v.mjx-c221A mjx-beg mjx-c::before{content:"";padding:.605em 1.056em .014em 0}mjx-stretchy-v.mjx-c221A mjx-ext mjx-c::before{content:"";width:1.056em}mjx-stretchy-v.mjx-c221A mjx-end mjx-c::before{content:"⎷";padding:.935em 1.056em .885em 0}mjx-stretchy-v.mjx-c221A>mjx-end{margin-top:-1.82em}mjx-stretchy-v.mjx-c221A>mjx-ext{border-top-width:.589em;border-bottom-width:1.79em}mjx-stretchy-v.mjx-c5B mjx-beg mjx-c::before{content:"⎡";padding:1.154em .667em .645em 0}mjx-stretchy-v.mjx-c5B mjx-ext mjx-c::before{content:"⎢";width:.667em}mjx-stretchy-v.mjx-c5B mjx-end mjx-c::before{content:"⎣";padding:1.155em .667em .644em 0}mjx-stretchy-v.mjx-c5B>mjx-end{margin-top:-1.799em}mjx-stretchy-v.mjx-c5B>mjx-ext{border-top-width:1.769em;border-bottom-width:1.769em}mjx-stretchy-v.mjx-c5D mjx-beg mjx-c::before{content:"⎤";padding:1.154em .667em .645em 0}mjx-stretchy-v.mjx-c5D mjx-ext mjx-c::before{content:"⎥";width:.667em}mjx-stretchy-v.mjx-c5D mjx-end mjx-c::before{content:"⎦";padding:1.155em .667em .644em 0}mjx-stretchy-v.mjx-c5D>mjx-end{margin-top:-1.799em}mjx-stretchy-v.mjx-c5D>mjx-ext{border-top-width:1.769em;border-bottom-width:1.769em}mjx-stretchy-v.mjx-c7B mjx-beg mjx-c::before{content:"⎧";padding:.899em .889em .01em 0}mjx-stretchy-v.mjx-c7B mjx-ext mjx-c::before{content:"⎪";width:.889em}mjx-stretchy-v.mjx-c7B mjx-end mjx-c::before{content:"⎩";padding:.01em .889em .899em 0}mjx-stretchy-v.mjx-c7B mjx-mid mjx-c::before{content:"⎨";padding:1.16em .889em .66em 0}mjx-stretchy-v.mjx-c7B>mjx-mid{margin-top:-.91em;margin-bottom:-.91em}mjx-stretchy-v.mjx-c7B>mjx-end{margin-top:-.909em}mjx-stretchy-v.mjx-c7B>mjx-ext{height:50%;border-top-width:.879em;border-bottom-width:.879em}mjx-stretchy-v.mjx-c7D mjx-beg mjx-c::before{content:"⎫";padding:.899em .889em .01em 0}mjx-stretchy-v.mjx-c7D mjx-ext mjx-c::before{content:"⎪";width:.889em}mjx-stretchy-v.mjx-c7D mjx-end mjx-c::before{content:"⎭";padding:.01em .889em .899em 0}mjx-stretchy-v.mjx-c7D mjx-mid mjx-c::before{content:"⎬";padding:1.16em .889em .66em 0}mjx-stretchy-v.mjx-c7D>mjx-mid{margin-top:-.91em;margin-bottom:-.91em}mjx-stretchy-v.mjx-c7D>mjx-end{margin-top:-.909em}mjx-stretchy-v.mjx-c7D>mjx-ext{height:50%;border-top-width:.879em;border-bottom-width:.879em}mjx-c.mjx-c2211.TEX-S2::before{padding:.95em 1.444em .45em 0;content:"∑"}mjx-c.mjx-c1D706.TEX-I::before{padding:.694em .583em .012em 0;content:"λ"}mjx-c.mjx-c1D464.TEX-I::before{padding:.443em .716em .011em 0;content:"w"}mjx-c.mjx-c50::before{padding:.683em .681em 0 0;content:"P"}mjx-c.mjx-c1D446.TEX-I::before{padding:.705em .645em .022em 0;content:"S"}mjx-c.mjx-c223C::before{padding:.367em .778em 0 0;content:"∼"}mjx-c.mjx-c1D437.TEX-I::before{padding:.683em .828em 0 0;content:"D"}mjx-c.mjx-c54::before{padding:.677em .722em 0 0;content:"T"}mjx-c.mjx-c65::before{padding:.448em .444em .011em 0;content:"e"}mjx-c.mjx-c2264::before{padding:.636em .778em .138em 0;content:"≤"}mjx-c.mjx-c221A.TEX-S4::before{padding:1.75em 1.02em 1.25em 0;content:"√"}mjx-c.mjx-c46.TEX-C::before{padding:.683em .829em .032em 0;content:"F"}mjx-c.mjx-c2F::before{padding:.75em .5em .25em 0;content:"/"}mjx-c.mjx-c1D6FF.TEX-I::before{padding:.717em .444em .01em 0;content:"δ"}mjx-c.mjx-cA0::before{padding:0 .25em 0 0;content:" "}mjx-c.mjx-c66::before{padding:.705em .372em 0 0;content:"f"}mjx-c.mjx-c20::before{padding:0 .25em 0 0;content:" "}mjx-c.mjx-c2208::before{padding:.54em .667em .04em 0;content:"∈"}mjx-c.mjx-c3E::before{padding:.54em .778em .04em 0;content:">"}mjx-c.mjx-c40::before{padding:.705em .778em .011em 0;content:"@"}mjx-c.mjx-c1D440.TEX-I::before{padding:.683em 1.051em 0 0;content:"M"}mjx-c.mjx-c1D44E.TEX-I::before{padding:.441em .529em .01em 0;content:"a"}mjx-c.mjx-c210E.TEX-I::before{padding:.694em .576em .011em 0;content:"h"}mjx-c.mjx-c1D436.TEX-I::before{padding:.705em .76em .022em 0;content:"C"}mjx-c.mjx-c1D44F.TEX-I::before{padding:.694em .429em .011em 0;content:"b"}mjx-c.mjx-c1D466.TEX-I::before{padding:.442em .49em .205em 0;content:"y"}mjx-c.mjx-c1D463.TEX-I::before{padding:.443em .485em .011em 0;content:"v"}mjx-c.mjx-c1D458.TEX-I::before{padding:.694em .521em .011em 0;content:"k"}mjx-c.mjx-c1D45C.TEX-I::before{padding:.441em .485em .011em 0;content:"o"}mjx-c.mjx-c5C::before{padding:.75em .5em .25em 0;content:"\\"}mjx-c.mjx-c1D45F.TEX-I::before{padding:.442em .451em .011em 0;content:"r"}mjx-c.mjx-c2192::before{padding:.511em 1em .011em 0;content:"→"}mjx-c.mjx-c1D462.TEX-I::before{padding:.442em .572em .011em 0;content:"u"}mjx-c.mjx-c1D459.TEX-I::before{padding:.694em .298em .011em 0;content:"l"}mjx-c.mjx-c1D45A.TEX-I::before{padding:.442em .878em .011em 0;content:"m"}mjx-c.mjx-c1D45D.TEX-I::before{padding:.442em .503em .194em 0;content:"p"}mjx-c.mjx-c1D43B.TEX-I::before{padding:.683em .888em 0 0;content:"H"}mjx-c.mjx-c24::before{padding:.75em .5em .056em 0;content:"$"}mjx-c.mjx-c36::before{padding:.666em .5em .022em 0;content:"6"}mjx-c.mjx-c37::before{padding:.676em .5em .022em 0;content:"7"}mjx-c.mjx-c39::before{padding:.666em .5em .022em 0;content:"9"}mjx-c.mjx-c35::before{padding:.666em .5em .022em 0;content:"5"}mjx-c.mjx-c394::before{padding:.716em .833em 0 0;content:"Δ"}mjx-c.mjx-c1D454.TEX-I::before{padding:.442em .477em .205em 0;content:"g"}mjx-c.mjx-c2260::before{padding:.716em .778em .215em 0;content:"≠"}mjx-c.mjx-c1D45E.TEX-I::before{padding:.442em .46em .194em 0;content:"q"}mjx-c.mjx-c1D429.TEX-B::before{padding:.45em .639em .194em 0;content:"p"}mjx-c.mjx-c1D42A.TEX-B::before{padding:.45em .607em .194em 0;content:"q"}mjx-c.mjx-c2211.TEX-S1::before{padding:.75em 1.056em .25em 0;content:"∑"}mjx-c.mjx-c1D43D.TEX-I::before{padding:.683em .633em .022em 0;content:"J"}mjx-c.mjx-c1D434.TEX-I::before{padding:.716em .75em 0 0;content:"A"}mjx-c.mjx-c1D435.TEX-I::before{padding:.683em .759em 0 0;content:"B"}mjx-c.mjx-c2229::before{padding:.598em .667em .022em 0;content:"∩"}mjx-c.mjx-c222A::before{padding:.598em .667em .022em 0;content:"∪"}mjx-c.mjx-c1D44C.TEX-I::before{padding:.683em .763em 0 0;content:"Y"}mjx-c.mjx-c1D447.TEX-I::before{padding:.677em .704em 0 0;content:"T"}mjx-c.mjx-c1D44A.TEX-I::before{padding:.683em 1.048em .022em 0;content:"W"}mjx-c.mjx-c1D457.TEX-I::before{padding:.661em .412em .204em 0;content:"j"}mjx-c.mjx-c2225::before{padding:.75em .5em .25em 0;content:"∥"}mjx-c.mjx-c1D442.TEX-I::before{padding:.704em .763em .022em 0;content:"O"}mjx-c.mjx-c1D43E.TEX-I::before{padding:.683em .889em 0 0;content:"K"}mjx-c.mjx-c2032::before{padding:.56em .275em 0 0;content:"′"}mjx-c.mjx-c2026::before{padding:.12em 1.172em 0 0;content:"…"}mjx-c.mjx-c22C5::before{padding:.31em .278em 0 0;content:"⋅"}mjx-c.mjx-c3A::before{padding:.43em .278em 0 0;content:":"}mjx-c.mjx-c1D448.TEX-I::before{padding:.683em .767em .022em 0;content:"U"}mjx-c.mjx-c68::before{padding:.694em .556em 0 0;content:"h"}mjx-c.mjx-c70::before{padding:.442em .556em .194em 0;content:"p"}mjx-c.mjx-c59::before{padding:.683em .75em 0 0;content:"Y"}mjx-c.mjx-c3C::before{padding:.54em .778em .04em 0;content:"<"}mjx-c.mjx-c1D43A.TEX-I::before{padding:.705em .786em .022em 0;content:"G"}mjx-c.mjx-c2217::before{padding:.465em .5em 0 0;content:"∗"}mjx-c.mjx-c1D443.TEX-I::before{padding:.683em .751em 0 0;content:"P"}mjx-c.mjx-c1D467.TEX-I::before{padding:.442em .465em .011em 0;content:"z"}mjx-c.mjx-c1D449.TEX-I::before{padding:.683em .769em .022em 0;content:"V"}mjx-c.mjx-c1D43C.TEX-I::before{padding:.683em .504em 0 0;content:"I"}mjx-mtext{display:inline-block;text-align:left}mjx-msqrt{display:inline-block;text-align:left}mjx-root{display:inline-block;white-space:nowrap}mjx-surd{display:inline-block;vertical-align:top}mjx-sqrt{display:inline-block;padding-top:.07em}mjx-sqrt>mjx-box{border-top:.07em solid}mjx-sqrt.mjx-tall>mjx-box{padding-left:.3em;margin-left:-.3em}mjx-mroot{display:inline-block;text-align:left}mjx-c.mjx-c6E::before{padding:.442em .556em 0 0;content:"n"}mjx-c.mjx-c6F::before{padding:.448em .5em .01em 0;content:"o"}mjx-c.mjx-c72::before{padding:.442em .392em 0 0;content:"r"}mjx-c.mjx-c6D::before{padding:.442em .833em 0 0;content:"m"}mjx-c.mjx-c69::before{padding:.669em .278em 0 0;content:"i"}mjx-c.mjx-c61::before{padding:.448em .5em .011em 0;content:"a"}mjx-c.mjx-c78::before{padding:.431em .528em 0 0;content:"x"}mjx-c.mjx-c6C::before{padding:.694em .278em 0 0;content:"l"}mjx-c.mjx-c67::before{padding:.453em .5em .206em 0;content:"g"}mjx-c.mjx-c2061::before{padding:0;content:""}mjx-c.mjx-c73::before{padding:.448em .394em .011em 0;content:"s"}mjx-c.mjx-c74::before{padding:.615em .389em .01em 0;content:"t"}mjx-c.mjx-c64::before{padding:.694em .556em .011em 0;content:"d"}mjx-c.mjx-c62::before{padding:.694em .556em .011em 0;content:"b"}mjx-c.mjx-c75::before{padding:.442em .556em .011em 0;content:"u"}mjx-c.mjx-c1D444.TEX-I::before{padding:.704em .791em .194em 0;content:"Q"}mjx-c.mjx-c33::before{padding:.665em .5em .022em 0;content:"3"}mjx-c.mjx-c1D465.TEX-I::before{padding:.442em .572em .011em 0;content:"x"}mjx-c.mjx-c2C::before{padding:.121em .278em .194em 0;content:","}mjx-c.mjx-c2E::before{padding:.12em .278em 0 0;content:"."}mjx-c.mjx-c1D45B.TEX-I::before{padding:.442em .6em .011em 0;content:"n"}mjx-c.mjx-c7C::before{padding:.75em .278em .249em 0;content:"|"}mjx-c.mjx-c221A.TEX-S2::before{padding:1.15em 1.02em .65em 0;content:"√"}mjx-c.mjx-c1D441.TEX-I::before{padding:.683em .888em 0 0;content:"N"}mjx-c.mjx-c221A::before{padding:.8em .853em .2em 0;content:"√"}mjx-container[jax=CHTML]{line-height:0}mjx-container [space="1"]{margin-left:.111em}mjx-container [space="2"]{margin-left:.167em}mjx-container [space="3"]{margin-left:.222em}mjx-container [space="4"]{margin-left:.278em}mjx-container [space="5"]{margin-left:.333em}mjx-container [rspace="1"]{margin-right:.111em}mjx-container [rspace="2"]{margin-right:.167em}mjx-container [rspace="3"]{margin-right:.222em}mjx-container [rspace="4"]{margin-right:.278em}mjx-container [rspace="5"]{margin-right:.333em}mjx-container [size="s"]{font-size:70.7%}mjx-container [size=ss]{font-size:50%}mjx-container [size=Tn]{font-size:60%}mjx-container [size=sm]{font-size:85%}mjx-container [size=lg]{font-size:120%}mjx-container [size=Lg]{font-size:144%}mjx-container [size=LG]{font-size:173%}mjx-container [size=hg]{font-size:207%}mjx-container [size=HG]{font-size:249%}mjx-container [width=full]{width:100%}mjx-box{display:inline-block}mjx-block{display:block}mjx-itable{display:inline-table}mjx-row{display:table-row}mjx-row>*{display:table-cell}mjx-mtext{display:inline-block}mjx-mstyle{display:inline-block}mjx-merror{display:inline-block;color:red;background-color:#ff0}mjx-mphantom{visibility:hidden}mjx-assistive-mml{top:0;left:0;clip:rect(1px,1px,1px,1px);user-select:none;position:absolute!important;padding:1px 0 0!important;border:0!important;display:block!important;width:auto!important;overflow:hidden!important}mjx-assistive-mml[display=block]{width:100%!important}mjx-math{display:inline-block;text-align:left;line-height:0;text-indent:0;font-style:normal;font-weight:400;font-size:100%;letter-spacing:normal;border-collapse:collapse;overflow-wrap:normal;word-spacing:normal;white-space:nowrap;direction:ltr;padding:1px 0}mjx-container[jax=CHTML][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=CHTML][display=true][width=full]{display:flex}mjx-container[jax=CHTML][display=true] mjx-math{padding:0}mjx-container[jax=CHTML][justify=left]{text-align:left}mjx-container[jax=CHTML][justify=right]{text-align:right}mjx-mi{display:inline-block;text-align:left}mjx-c{display:inline-block}mjx-utext{display:inline-block;padding:.75em 0 .2em}mjx-mo{display:inline-block;text-align:left}mjx-stretchy-h{display:inline-table;width:100%}mjx-stretchy-h>*{display:table-cell;width:0}mjx-stretchy-h>*>mjx-c{display:inline-block;transform:scaleX(1)}mjx-stretchy-h>*>mjx-c::before{display:inline-block;width:initial}mjx-stretchy-h>mjx-ext{overflow:clip visible;width:100%}mjx-stretchy-h>mjx-ext>mjx-c::before{transform:scaleX(500)}mjx-stretchy-h>mjx-ext>mjx-c{width:0}mjx-stretchy-h>mjx-beg>mjx-c{margin-right:-.1em}mjx-stretchy-h>mjx-end>mjx-c{margin-left:-.1em}mjx-stretchy-v{display:inline-block}mjx-stretchy-v>*{display:block}mjx-stretchy-v>mjx-beg{height:0}mjx-stretchy-v>mjx-end>mjx-c{display:block}mjx-stretchy-v>*>mjx-c{transform:scaleY(1);transform-origin:left center;overflow:hidden}mjx-stretchy-v>mjx-ext{display:block;height:100%;box-sizing:border-box;border:0 solid transparent;overflow:visible clip}mjx-stretchy-v>mjx-ext>mjx-c::before{width:initial;box-sizing:border-box}mjx-stretchy-v>mjx-ext>mjx-c{transform:scaleY(500) translateY(.075em);overflow:visible}mjx-mark{display:inline-block;height:0}mjx-msub{display:inline-block;text-align:left}mjx-texatom{display:inline-block;text-align:left}mjx-mn{display:inline-block;text-align:left}mjx-msubsup{display:inline-block;text-align:left}mjx-script{display:inline-block;padding-right:.05em;padding-left:.033em}mjx-script>mjx-spacer{display:block}mjx-msup{display:inline-block;text-align:left}mjx-mfrac{display:inline-block;text-align:left}mjx-frac{display:inline-block;vertical-align:.17em;padding:0 .22em}mjx-frac[type="d"]{vertical-align:.04em}mjx-frac[delims]{padding:0 .1em}mjx-frac[atop]{padding:0 .12em}mjx-frac[atop][delims]{padding:0}mjx-dtable{display:inline-table;width:100%}mjx-dtable>*{font-size:2000%}mjx-dbox{display:block;font-size:5%}mjx-num{display:block;text-align:center}mjx-den{display:block;text-align:center}mjx-mfrac[bevelled]>mjx-num{display:inline-block}mjx-mfrac[bevelled]>mjx-den{display:inline-block}mjx-den[align=right],mjx-num[align=right]{text-align:right}mjx-den[align=left],mjx-num[align=left]{text-align:left}mjx-nstrut{display:inline-block;height:.054em;width:0;vertical-align:-.054em}mjx-nstrut[type="d"]{height:.217em;vertical-align:-.217em}mjx-dstrut{display:inline-block;height:.505em;width:0}mjx-dstrut[type="d"]{height:.726em}mjx-line{display:block;box-sizing:border-box;min-height:1px;height:.06em;border-top:.06em solid;margin:.06em -.1em;overflow:hidden}mjx-line[type="d"]{margin:.18em -.1em}mjx-mrow{display:inline-block;text-align:left}mjx-c::before{display:block;width:0}.MJX-TEX{font-family:MJXZERO,MJXTEX}.TEX-B{font-family:MJXZERO,MJXTEX-B}.TEX-I{font-family:MJXZERO,MJXTEX-I}.TEX-MI{font-family:MJXZERO,MJXTEX-MI}.TEX-BI{font-family:MJXZERO,MJXTEX-BI}.TEX-S1{font-family:MJXZERO,MJXTEX-S1}.TEX-S2{font-family:MJXZERO,MJXTEX-S2}.TEX-S3{font-family:MJXZERO,MJXTEX-S3}.TEX-S4{font-family:MJXZERO,MJXTEX-S4}.TEX-A{font-family:MJXZERO,MJXTEX-A}.TEX-C{font-family:MJXZERO,MJXTEX-C}.TEX-CB{font-family:MJXZERO,MJXTEX-CB}.TEX-FR{font-family:MJXZERO,MJXTEX-FR}.TEX-FRB{font-family:MJXZERO,MJXTEX-FRB}.TEX-SS{font-family:MJXZERO,MJXTEX-SS}.TEX-SSB{font-family:MJXZERO,MJXTEX-SSB}.TEX-SSI{font-family:MJXZERO,MJXTEX-SSI}.TEX-SC{font-family:MJXZERO,MJXTEX-SC}.TEX-T{font-family:MJXZERO,MJXTEX-T}.TEX-V{font-family:MJXZERO,MJXTEX-V}.TEX-VB{font-family:MJXZERO,MJXTEX-VB}mjx-stretchy-h mjx-c,mjx-stretchy-v mjx-c{font-family:MJXZERO,MJXTEX-S1,MJXTEX-S4,MJXTEX,MJXTEX-A!important}@font-face{font-family:MJXZERO;src:url("lib/fonts/mathjax_zero.woff") format("woff")}@font-face{font-family:MJXTEX;src:url("lib/fonts/mathjax_main-regular.woff") format("woff")}@font-face{font-family:MJXTEX-B;src:url("lib/fonts/mathjax_main-bold.woff") format("woff")}@font-face{font-family:MJXTEX-I;src:url("lib/fonts/mathjax_math-italic.woff") format("woff")}@font-face{font-family:MJXTEX-MI;src:url("lib/fonts/mathjax_main-italic.woff") format("woff")}@font-face{font-family:MJXTEX-BI;src:url("lib/fonts/mathjax_math-bolditalic.woff") format("woff")}@font-face{font-family:MJXTEX-S1;src:url("lib/fonts/mathjax_size1-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S2;src:url("lib/fonts/mathjax_size2-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S3;src:url("lib/fonts/mathjax_size3-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S4;src:url("lib/fonts/mathjax_size4-regular.woff") format("woff")}@font-face{font-family:MJXTEX-A;src:url("lib/fonts/mathjax_ams-regular.woff") format("woff")}@font-face{font-family:MJXTEX-C;src:url("lib/fonts/mathjax_calligraphic-regular.woff") format("woff")}@font-face{font-family:MJXTEX-CB;src:url("lib/fonts/mathjax_calligraphic-bold.woff") format("woff")}@font-face{font-family:MJXTEX-FR;src:url("lib/fonts/mathjax_fraktur-regular.woff") format("woff")}@font-face{font-family:MJXTEX-FRB;src:url("lib/fonts/mathjax_fraktur-bold.woff") format("woff")}@font-face{font-family:MJXTEX-SS;src:url("lib/fonts/mathjax_sansserif-regular.woff") format("woff")}@font-face{font-family:MJXTEX-SSB;src:url("lib/fonts/mathjax_sansserif-bold.woff") format("woff")}@font-face{font-family:MJXTEX-SSI;src:url("lib/fonts/mathjax_sansserif-italic.woff") format("woff")}@font-face{font-family:MJXTEX-SC;src:url("lib/fonts/mathjax_script-regular.woff") format("woff")}@font-face{font-family:MJXTEX-T;src:url("lib/fonts/mathjax_typewriter-regular.woff") format("woff")}@font-face{font-family:MJXTEX-V;src:url("lib/fonts/mathjax_vector-regular.woff") format("woff")}@font-face{font-family:MJXTEX-VB;src:url("lib/fonts/mathjax_vector-bold.woff") format("woff")}mjx-c.mjx-c1D445.TEX-I::before{padding:.683em .759em .021em 0;content:"R"}mjx-c.mjx-c28::before{padding:.75em .389em .25em 0;content:"("}mjx-c.mjx-c1D70F.TEX-I::before{padding:.431em .517em .013em 0;content:"τ"}mjx-c.mjx-c29::before{padding:.75em .389em .25em 0;content:")"}mjx-c.mjx-c3D::before{padding:.583em .778em .082em 0;content:"="}mjx-c.mjx-c1D438.TEX-I::before{padding:.68em .764em 0 0;content:"E"}mjx-c.mjx-c5B::before{padding:.75em .278em .25em 0;content:"["}mjx-c.mjx-c1D44B.TEX-I::before{padding:.683em .852em 0 0;content:"X"}mjx-c.mjx-c1D461.TEX-I::before{padding:.626em .361em .011em 0;content:"t"}mjx-c.mjx-c2212::before{padding:.583em .778em .082em 0;content:"−"}mjx-c.mjx-c1D707.TEX-I::before{padding:.442em .603em .216em 0;content:"μ"}mjx-c.mjx-c2B::before{padding:.583em .778em .082em 0;content:"+"}mjx-c.mjx-c5D::before{padding:.75em .278em .25em 0;content:"]"}mjx-c.mjx-c1D450.TEX-I::before{padding:.442em .433em .011em 0;content:"c"}mjx-c.mjx-c1D719.TEX-I::before{padding:.694em .596em .205em 0;content:"ϕ"}mjx-c.mjx-c31::before{padding:.666em .5em 0 0;content:"1"}mjx-c.mjx-c32::before{padding:.666em .5em 0 0;content:"2"}mjx-c.mjx-c1D716.TEX-I::before{padding:.431em .406em .011em 0;content:"ϵ"}mjx-c.mjx-c1D703.TEX-I::before{padding:.705em .469em .01em 0;content:"θ"}mjx-c.mjx-c1D439.TEX-I::before{padding:.68em .749em 0 0;content:"F"}mjx-c.mjx-c1D714.TEX-I::before{padding:.443em .622em .011em 0;content:"ω"}mjx-c.mjx-c222B.TEX-S2::before{padding:1.36em .944em .862em 0;content:"∫"}mjx-c.mjx-c221E::before{padding:.442em 1em .011em 0;content:"∞"}mjx-c.mjx-c1D453.TEX-I::before{padding:.705em .55em .205em 0;content:"f"}mjx-c.mjx-c1D452.TEX-I::before{padding:.442em .466em .011em 0;content:"e"}mjx-c.mjx-c1D456.TEX-I::before{padding:.661em .345em .011em 0;content:"i"}mjx-c.mjx-c1D451.TEX-I::before{padding:.694em .52em .01em 0;content:"d"}mjx-c.mjx-c1D70B.TEX-I::before{padding:.431em .57em .011em 0;content:"π"}mjx-c.mjx-c1D43F.TEX-I::before{padding:.683em .681em 0 0;content:"L"}mjx-c.mjx-c7B::before{padding:.75em .5em .25em 0;content:"{"}mjx-c.mjx-c7D::before{padding:.75em .5em .25em 0;content:"}"}mjx-c.mjx-c1D460.TEX-I::before{padding:.442em .469em .01em 0;content:"s"}mjx-c.mjx-c30::before{padding:.666em .5em .022em 0;content:"0"}mjx-c.mjx-c1D70E.TEX-I::before{padding:.431em .571em .011em 0;content:"σ"}mjx-c.mjx-c1D6FC.TEX-I::before{padding:.442em .64em .011em 0;content:"α"}</style><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="6_AI 系统"><p dir="auto">6_AI 系统</p></h1><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://space.bilibili.com/572575009?spm_id_from=333.788.0.0" rel="noopener" class="external-link" href="https://space.bilibili.com/572575009?spm_id_from=333.788.0.0" target="_blank">It_server技术分享的个人空间-It_server技术分享个人主页-哔哩哔哩视频</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://space.bilibili.com/517221395/video?tid=0&amp;pn=1&amp;keyword=&amp;order=pubdate" rel="noopener" class="external-link" href="https://space.bilibili.com/517221395/video?tid=0&amp;pn=1&amp;keyword=&amp;order=pubdate" target="_blank">ZOMI酱投稿视频-ZOMI酱视频分享-哔哩哔哩视频</a><br>
[GitHub1s - chenzomi12/AISystem: AISystem 主要是指AI系统，包括AI芯片、AI编译器、AI<br>
<a data-tooltip-position="top" aria-label="https://chenzomi12.github.io/" rel="noopener" class="external-link" href="https://chenzomi12.github.io/" target="_blank">课程内容大纲 — AI System</a></p></div><div><p dir="auto"><img alt="image.png" src="https://cdn.sa.net/2024/06/23/3lmI6kDQYZBWyPe.png" referrerpolicy="no-referrer"></p></div><div><img style="width:800" src="https://cdn.sa.net/2024/06/23/dIT6KeoUEwCbjaQ.png" referrerpolicy="no-referrer"></div><div class="heading-wrapper"><h2 data-heading="介绍(Introduction)" dir="auto" class="heading" id="介绍(Introduction)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>介绍(Introduction)</h2><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Db421H7rV/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Db421H7rV/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">独家整理：简单科普与”算力“相关的常见问题_哔哩哔哩_bilibili</a></p></div><div><ul>
<li data-line="0" dir="auto">问题1：现在很多<strong>智算中心</strong>的新闻报道，对外宣传规模很大，比如1000P,是什么精度算力？</li>
<li data-line="1" dir="auto">信通院发布了《中国算力发展智算白皮书》，作为权威机构的材料，对如何衡量“基础算力”、“智能算力”和“超算算力”给出了明确的解释。归纳就是：与<strong>智算中心或者AI相关（默认是FP16)</strong>、<strong>超算HPC(默认是FP64)</strong>、部分情况为了便于统计，会统一换算为FP32(目前见到的不多)，通常都会备注清楚。</li>
<li data-line="2" dir="auto">问题2：新手朋友看英伟达GPU卡，例如H100,同一个精度的(FP16)多个数值，以哪个为准？<span style="background:#fff88f">看实际 FP16 算力（没有任何优化在稠密矩阵上的 ）</span></li>
<li data-line="3" dir="auto">
<img style="width:500" src="https://cdn.sa.net/2024/06/22/dUeZmwYAOD8xpK1.png" referrerpolicy="no-referrer">
</li>
<li data-line="4" dir="auto">问题3：各类GPU相关的<strong>算力精度</strong>FP64、FP32、FP16、PF16等，怎么理解呢？</li>
<li data-line="5" dir="auto">
<img style="width:500" src="https://cdn.sa.net/2024/06/22/6EIOVT1csd5bjvW.png" referrerpolicy="no-referrer">
</li>
<li data-line="6" dir="auto">
<img style="width:500" src="https://cdn.sa.net/2024/06/22/Jk9p3oTQ2fKy8lz.png" referrerpolicy="no-referrer">
</li>
<li data-line="7" dir="auto">问题4：英伟达GPU卡会涉及“<strong>稀疏矩阵</strong>”的算力数字，是啥意思？</li>
</ul></div><div><p dir="auto">算力单位</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://docs.nvidia.com/cuda/floating-point/index.html" rel="noopener" class="external-link" href="https://docs.nvidia.com/cuda/floating-point/index.html" target="_blank">NVIDIA CUDA docs - Floating Point and IEEE 754</a></p></div><div dir="ltr" style="overflow-x: auto;"><table>
<thead>
<tr>
<th dir="ltr">算力精度</th>
<th dir="ltr">位数(符号 + 指数 +位数)</th>
<th dir="ltr">主要使用场景</th>
<th dir="auto"></th>
<th dir="auto"></th>
</tr>
</thead>
<tbody>
<tr>
<td dir="ltr">FP64</td>
<td dir="auto">1+11+52</td>
<td dir="ltr">科学计算模拟、高精度需求应用</td>
<td dir="auto"></td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">FP32</td>
<td dir="auto">1+8+23</td>
<td dir="ltr">机器学习和深度学习的标准精度，平衡了精度和计算速度</td>
<td dir="auto"></td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">FP16</td>
<td dir="auto">1+5+10</td>
<td dir="ltr">降低了精度，但大大提高了计算速度和内存效率，适用于训练和推理中对精度要求不高的模型</td>
<td dir="auto"></td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">PF16</td>
<td dir="auto">1+5+10</td>
<td dir="ltr">英伟达在其Tensor Cores中使用的一种浮点数格式，旨在加速深度学习中的矩阵乘法运算。与标准的FP16相同，但在硬件上进行了优化以提高计算效率</td>
<td dir="auto"></td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">BF16</td>
<td dir="auto">1+8+7</td>
<td dir="ltr">保留了FP32的动态范围但精度较低，适用于深度学习训练中的加速</td>
<td dir="auto"></td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">INT8</td>
<td dir="auto">1+8+10</td>
<td dir="ltr">高效推理</td>
<td dir="auto"></td>
<td dir="auto"></td>
</tr>
</tbody>
</table></div></div></div><div class="heading-wrapper"><h2 data-heading="分布式训练推理" dir="auto" class="heading" id="分布式训练推理"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>分布式训练推理</h2><div class="heading-children"><div class="admonition-parent admonition-info-parent"><div class="callout admonition admonition-info admonition-plugin " style="--callout-color: 0, 184, 212;" data-callout="info" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="info-circle" class="svg-inline--fa fa-info-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"></path></svg></div><div class="callout-title-inner admonition-title-content">Info</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/716156048?utm_psn=1811073266015678464" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/716156048?utm_psn=1811073266015678464" target="_blank"># larmbr宇 - LLM分布并行训练技术总结</a><br>
整理了100页ppt内容，主要覆盖当前主流的LLM大规模训练用到的并行技术，如<a data-tooltip-position="top" aria-label="https://zhida.zhihu.com/search?q=%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C" rel="noopener" class="external-link" href="https://zhida.zhihu.com/search?q=%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C" target="_blank">数据并行</a>，张量并行，流水线并行，序列并行，专家并行的内容。阅读门槛是对当前predict-next-token的Transformer模型有一定的基础认知（了解基础的模型组成结构即可）。我尝试在有限的篇幅内，以逻辑自包含的方式，把这些知识（争取）有机地整理。</p>
<p dir="auto">在内容选择上，以涵盖主流大模型及训练框架用到的技术为主，但每个方向仍有个别优秀的工作，囿于时间和精力，没能包含进来（希望在后续的版本中增加）。即便如此，也到了100页的长度。如果内容有错漏，感谢指出。</p></div></div></div><div><img style="width:500" src="https://cdn.sa.net/2024/06/23/VdeCb9MJD4RLamx.png" referrerpolicy="no-referrer">
<img style="width:700" src="https://cdn.sa.net/2024/08/02/haDnNfgvILqbH1m.png" referrerpolicy="no-referrer">
- @ Data Parallelism 数据并行
- 每个GPU存储完整的模型参数, 但是将输入数据分别交给不同的GPU 处理.
- 成倍提高吞吐量, 但是要求每个GPU要能放下完整的模型
- 训练室, 每个GPU计算一个batch的部分数据, 然后梯度汇总. 类似邦联学习.
- @ Pipeline Parallelism 流水并行
 <img style="width:500" src="https://cdn.sa.net/2024/08/02/KpON43BFigrLHn6.png" referrerpolicy="no-referrer">
 
- 将模型拆分成多个部分 分配 到不同的GPU上, 因为可以把显存压力平均分配到每个GPU上.
- 由于不同的节点的输入输出存在顺序关系. 一个时刻事实上只有一个GPU在计算, 其他GPU都在闲置. 
- 将显存压力平均分配, 但是吞吐量没有提高. 可以借鉴DP的方法, 减少GPU的闲置时间.
<img style="width:500" src="https://cdn.sa.net/2024/08/02/ZU8f5XcmxRSDVwj.png" referrerpolicy="no-referrer">
- @ Tensor Parallelism 张量并行
- 如果是流水“并行”水平的切分模型, 那么在层内“水平”拆分则被称为张量并行
- Transformer 的复杂度是 N^2 , 计算瓶颈很大程度上取决于 矩阵的激活 + 矩阵的计算. 所以我们可以在不同的 GPU 上计算独立的点积， 或者在不同的 GPU 上计算每个点积的部分并对结果求和. 无论使用哪种策略，我们都可以将权重矩阵切成**大小相等**的“分片”，将每个分片托管在不同的 GPU 上，并使用该分片计算整个矩阵产品的相关部分，然后再进行通信以合并&nbsp;结果。[MegatronLM: Training Billion+ Parameter Language Models Using GPU Model Parallelism - NVIDIA ADLR](https://nv-adlr.github.io/MegatronLM)
- @ Mixture-of-Experts (MoE)
- @ Gradient Checkpoint 
- @ Mixed Precision Training 
- @ Offloading </div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 部署和训练以Transformer类型结构的大模型</p>
</span></li>
<li data-line="1" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 提供数据并行、模型并行和流水线并行等分公并行模式；</p>
</span></li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 以集合通信和参数服务器方式进行资源整合；</p>
</span></li>
<li data-line="4" dir="auto">
<p>为大模型提供多维分布式并行能力，让大模型能够在A集群上快速训练推理</p>
</li>
<li data-line="5" dir="auto">
<p>提升模型和算力的利用率，提高集群的线性度</p>
</li>
<li data-line="7" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2402.15627" rel="noopener" class="external-link" href="https://arxiv.org/abs/2402.15627" target="_blank">[2402.15627] MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs</a></p>
</span></li>
<li data-line="8" dir="auto">
<p>字节和北大发表和论文主要介绍了一种名为"MegaScale"的系统，它专为大规模语言模型（LLM）的训练设计，能够将训练扩展至超过 10000个GPU。MegaScale通过设计、操作优化以及实现容错性和减轻拖后腿者（stragglers）的影响来提升训练效率。具体而言，系统在12000个GPU上训练 175B参数的LLM模型时，<strong>实现了55.2%的模型浮点运算利用率（MFU），比Megatron-LM提高了1.34倍</strong>。</p>
</li>
<li data-line="9" dir="auto">
<p>文章强调，在生产环境中维持高效率训练过程的重要性，尤其是在面对失败和拖后腿情况频发的情况下。为了实现这一目标，MegaScale采用了全面的监控和可视化策略，以及算法-系统协同设计的原则，以最大化系统性能，同时确保通信和计算之间有最大的重叠。此外，还介绍了数据预处理、加载、并行策略、检查点和恢复过程等关键组件的优化，以及用于实时异常检测和早期预警的性能分析工具。</p>
</li>
<li data-line="10" dir="auto">
<p>MegaScale基于Megatron-LM构建，但在所有设置下都能实现最高1.34倍的速度提升。随着GPU数量增加，MegaScale通过减少通信瓶颈进一步提升了效率。实验结果表明，即使使用较小的批次大小，MegaScale仍能与基线模型在训练超过100B令牌时达到相似的损失结果。此外，MegaScale还评估了LAMB优化器的效果，并在有效训练时间率上保持了90%以上的水平。</p>
</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=aey0qZzJg-Q&amp;t=38s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=aey0qZzJg-Q&amp;t=38s" target="_blank"># 【人工智能】如何搭建10万个H100的GPU集群 | 40亿美元成本| 电力150兆瓦 | 并行化挑战 | 网络拓扑结构 | 可靠性与恢复 | 成本优化</a><br>
<a data-tooltip-position="top" aria-label="https://www.semianalysis.com/p/100000-h100-clusters-power-network" rel="noopener" class="external-link" href="https://www.semianalysis.com/p/100000-h100-clusters-power-network" target="_blank">100k H100 Clusters: Power, Network Topology, Ethernet vs InfiniBand, Reliability, Failures, Checkpointing</a></p></div><div><ul>
<li data-line="0" dir="auto">10万卡集群功率大约150BW, 每年耗电 1.59Twh, 每度 0.79 刀，电费开销大约为 1.24 亿刀</li>
<li data-line="1" dir="auto">GPT-4 训练大约消耗 2.15e25 BF16 FLOPS,在两万个 A100 上训练了 90-100 天</li>
<li data-line="2" dir="auto">园区级别的数据中心，光纤的距离损耗</li>
</ul></div><div><img style="width:500" src="https://cdn.sa.net/2024/07/22/ZlapVNh9HU6f31M.png" referrerpolicy="no-referrer">
流水线并行 : RTX 4090 24G \*2 = RTX 4089 48G</div><div><img style="width:500" src="https://cdn.sa.net/2024/07/22/AiBbSZ4Gy6cr2aJ.png" referrerpolicy="no-referrer">
张量并行(数据): RTX 4090 24G\*2  = RTX 8170 48G </div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://colossalai.org/docs/concepts/paradigms_of_parallelism/" rel="noopener" class="external-link" href="https://colossalai.org/docs/concepts/paradigms_of_parallelism/" target="_blank">Paradigms of Parallelism | Colossal-AI</a><br>
分布式的的基本概念有：</p></div><div><ul>
<li data-line="0" dir="auto">
<p><code>Data Parallel</code>  数据并行是最常见的并行形式，因为它简单。在数据并行训练中，数据集被分成几个碎片，每个碎片分配给一个设备。这相当于沿着批次维度并行化训练过程。每个设备将持有模型副本的完整副本，并在分配的数据显示集碎片上进行训练。在反向传播后，模型的梯度将被全局归约，以便不同设备上的模型参数保持同步。<img style="width:500" src="https://cdn.sa.net/2024/07/30/zmKrHwG3haETYBQ.png" referrerpolicy="no-referrer"></p>
</li>
<li data-line="1" dir="auto">
<p><code>Model Parallel</code> 在数据并行训练中，一个显著的特点是每个 GPU 都持有整个模型权重的副本。这带来了冗余问题。另一种并行范式是模型并行，其中模型被拆分并分布在一组设备上。</p>
</li>
<li data-line="2" dir="auto">
<p>一般来说，有两种类型的并行性：<strong>张量并行性</strong>和<strong>流水线并行性</strong>。</p>
</li>
<li data-line="3" dir="auto">
<p>张量并行性是在操作内部（例如矩阵-矩阵乘法）并行化计算。<img style="width:500" src="https://cdn.sa.net/2024/07/30/nQblDusi9LZYpKH.png" referrerpolicy="no-referrer"></p>
</li>
<li data-line="5" dir="auto">
<p>流水线并行性是在层之间并行化计算. <img style="width:500" src="https://cdn.sa.net/2024/07/30/f3bXuhiqgNAjnVm.png" referrerpolicy="no-referrer"></p>
</li>
<li data-line="8" dir="auto">
<p>因此，从另一个角度来看，张量并行性可以视为层内并行性，而流水线并行性可以视为层间并行性。</p>
</li>
<li data-line="11" dir="auto">
<p><code>Optimizer-level Parallel</code></p>
</li>
<li data-line="12" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p>另一个范例在优化器层面上工作，目前这个范例中最著名的方法是 ZeRO，代表&nbsp;<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1910.02054" rel="noopener" class="external-link" href="https://arxiv.org/abs/1910.02054" target="_blank">零冗余优化器</a>。ZeRO 在三个层面上工作，以消除内存冗余（ZeRO 需要 fp16 训练）：</p>
<ul>
<li data-line="13" dir="auto">第 1 级：优化器状态在各个进程之间进行分区</li>
<li data-line="14" dir="auto">第 2 级：用于更新模型权重的减少的 32 位梯度也被划分，使得每个进程仅存储与其优化器状态分区对应的梯度。</li>
<li data-line="15" dir="auto">第 3 级：16 位模型参数在各个进程之间进行分区</li>
</ul>
</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://lilianweng.github.io/posts/2021-09-25-train-large/" rel="noopener" class="external-link" href="https://lilianweng.github.io/posts/2021-09-25-train-large/" target="_blank">How to Train Really Large Models on Many GPUs? | Lil'Log</a><br>
<a data-tooltip-position="top" aria-label="https://openai.com/index/techniques-for-training-large-neural-networks/" rel="noopener" class="external-link" href="https://openai.com/index/techniques-for-training-large-neural-networks/" target="_blank">Techniques for training large neural networks | OpenAI</a></p></div><div class="heading-wrapper"><h3 data-heading="DeepSpeed" dir="auto" class="heading" id="DeepSpeed"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>DeepSpeed</h3><div class="heading-children"><div class="admonition-parent admonition-note-parent"><div class="callout admonition admonition-note admonition-plugin " style="--callout-color: 68, 138, 255;" data-callout="note" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="pencil-alt" class="svg-inline--fa fa-pencil-alt fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"></path></svg></div><div class="callout-title-inner admonition-title-content">Note</div></div><div class="callout-content admonition-content heading-wrapper"><h4 data-heading="我在分布式系统中老是听到 All-Reduce 和 All-Gather. 它们是什么意思?" dir="auto" class="heading" id="我在分布式系统中老是听到_All-Reduce_和_All-Gather._它们是什么意思?"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>我在分布式系统中老是听到 All-Reduce 和 All-Gather. 它们是什么意思?</h4>
<p dir="auto">在分布式系统中，尤其是处理并行计算和数据处理时，<strong>All-Reduce</strong> 和 <strong>All-Gather</strong> 是两种常见的集合通信操作，通常在使用分布式并行处理框架（如 MPI，TensorFlow，PyTorch）时用于不同节点间的数据同步。让我们看看每个操作的含义：</p>
<h3 data-heading="All-Reduce" dir="auto" class="heading" id="All-Reduce">All-Reduce</h3>
<p dir="auto"><code>All-Reduce</code> 是一种将所有参与计算的节点上的数据进行某种“reduce”操作（比如求和、求最大值、求最小值等），然后将这个“reduce”结果分发给所有节点的过程。</p>
<p dir="auto">以求和为例，在分布式训练过程中，每个节点可能会产生自己的梯度值，我们需要将所有节点上的梯度相加（reduce操作）以得到总的梯度，然后这个总梯度会被分发给所有节点以便下一次迭代计算。</p>
<p dir="auto">这一操作对于并行训练大规模模型至关重要，因为它能确保每个节点上的模型参数同步更新。</p>
<h3 data-heading="All-Gather" dir="auto" class="heading" id="All-Gather">All-Gather</h3>
<p dir="auto"><code>All-Gather</code> 是另外一种集合通信操作，它简单地收集每个节点的数据，并将这些数据的集合发送给所有节点。</p>
<p dir="auto">如果有 N 个节点，每个节点都有一些独特的数据，<code>All-Gather</code> 会在每个节点上收集所有 N 个数据点，使每个节点最终都有完整的数据集。这通常用于需要在各个节点上获取完整数据集的情况，例如统一数据集后进行某种集体分析或决策。</p>
<h3 data-heading="区别" dir="auto" class="heading" id="区别">区别</h3>
<p dir="auto">二者的区别在于，<code>All-Reduce</code> 是有聚合过程的广播（每个节点最终得到的是一个聚合后的结果），而 <code>All-Gather</code> 是简单的广播（每个节点最终得到的是所有节点的数据的完整列表）。</p>
<p dir="auto">在分布式机器学习中，这些操作通常由后端框架（如 MPI, Horovod, NCCL 等）处理，对用户来说是透明的，但理解它们的功能对于设计并行算法和性能调优是非常重要的。</p><div class="heading-children"></div></div></div></div><div class="admonition-parent admonition-hint-parent"><div class="callout admonition admonition-hint admonition-plugin " style="--callout-color: 0, 191, 165;" data-callout="hint" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="fire" class="svg-inline--fa fa-fire fa-w-12" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M216 23.86c0-23.8-30.65-32.77-44.15-13.04C48 191.85 224 200 224 288c0 35.63-29.11 64.46-64.85 63.99-35.17-.45-63.15-29.77-63.15-64.94v-85.51c0-21.7-26.47-32.23-41.43-16.5C27.8 213.16 0 261.33 0 320c0 105.87 86.13 192 192 192s192-86.13 192-192c0-170.29-168-193-168-296.14z"></path></svg></div><div class="callout-title-inner admonition-title-content">Hint</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://github.com/microsoft/DeepSpeed" rel="noopener" class="external-link" href="https://github.com/microsoft/DeepSpeed" target="_blank">GitHub - microsoft/DeepSpeed: DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.</a></p></div></div></div><div><ul>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>微软开发，提高大模型训练效率和可扩展性：
<ol>
<li data-line="1" dir="auto">加速训练手段：数据并行（ZeRO系列)、模型并行(PP)、梯度累积、动态缩放、混合精度等。</li>
<li data-line="2" dir="auto">辅助工具：分布式训练管理、内存优化和模型压缩等，帮助开发者更好管理和优化大模型训练任务。</li>
<li data-line="3" dir="auto">快速迁移：通过Python Warp方式基于PyTorch来构建，直接调用即完成简单迁移。</li>
</ol>
</li>
<li data-line="4" dir="auto">其实主要是针对私有化训练部署的，真的千卡万卡场景 deepspeed 还是抽象程度太高了</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/688873027" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/688873027" target="_blank"># 从啥也不会到DeepSpeed————一篇大模型分布式训练的学习过程总结</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> 为什么需要分布式训练？
</span><ul>
<li data-line="1" dir="auto">小模型训的快，大模型训的起</li>
</ul>
</li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> 大模型的内存开销
</span><ul>
<li data-line="3" dir="auto">纯纯推理，模型权重参数 + Embedding。比如，7B 个参数 * FP16(2B) = 14G</li>
<li data-line="4" dir="auto">训练的化，需要模型权重 + Embedding + 优化器参数 + 训练的中间变量... 差不多需要推理的三倍显存开销。GPT-2 就需要 V100 的 32G 显存</li>
</ul>
</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=weLN-Bs5P2E" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=weLN-Bs5P2E" target="_blank">01 - ZOMI酱 - 大模型是怎么训起来的？分布式并行框架介绍 </a><a href="?query=tag:%E5%A4%A7%E6%A8%A1%E5%9E%8B" class="tag" target="_blank" rel="noopener">#大模型</a> <a href="?query=tag:%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C" class="tag" target="_blank" rel="noopener">#分布式并行</a> <a href="?query=tag:%E8%AE%AD%E7%BB%83" class="tag" target="_blank" rel="noopener">#训练</a> - YouTube<br>
<img style="width:500" src="https://cdn.sa.net/2024/07/31/8bqP47aktWhRNp3.png" referrerpolicy="no-referrer"><br>
分布式系统在大模型业务的</p></div><div><p dir="auto"><img src="https://cdn.sa.net/2024/07/31/FH7SripIc5xQuoO.png" referrerpolicy="no-referrer"><br>
DeepSpeed 在工程中简单, 提供数据并行的优雅解法,更多用在预训练微调, 在计算集群中使用略显幼稚.</p></div><div><img style="width:500" src="https://cdn.sa.net/2024/07/31/bZzhpwXak4yCfgn.png" referrerpolicy="no-referrer">
工程实验不优雅, 需要魔改(大厂)</div><div><img style="width:500" src="https://cdn.sa.net/2024/07/31/XUYViGCcReIEhTy.png" referrerpolicy="no-referrer"></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=J3Ag5W_tPeI" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=J3Ag5W_tPeI" target="_blank">02 - ZOMI 酱 - 分布式并行框架DeepSpeed介绍 </a><a href="?query=tag:%E5%A4%A7%E6%A8%A1%E5%9E%8B" class="tag" target="_blank" rel="noopener">#大模型</a> <a href="?query=tag:%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C" class="tag" target="_blank" rel="noopener">#分布式并行</a> <a href="?query=tag:%E8%AE%AD%E7%BB%83" class="tag" target="_blank" rel="noopener">#训练</a> - YouTube<br>
<img style="width:500" src="https://cdn.sa.net/2024/07/31/7obXxWlfuzkCEig.png" referrerpolicy="no-referrer"><br>
<span style="background:#fff88f">Microsoft DeepSpeed</span> ,提供的主要 Training, Inference, Compression, Science.<br>
Inference 可能有很多更好用的替代，但是 Training 基本独此一家。</p></div><div><img style="width:500" src="https://cdn.sa.net/2024/07/31/DoT4YXaQVwgEFN7.png" referrerpolicy="no-referrer">
在右侧的全栈，DeepSpeed 具体是使用 APIs 将 Pytorch/TF 调用起来。</div><div><img style="width:500" src="https://cdn.sa.net/2024/07/31/DNL67EG2tVyC95S.png" referrerpolicy="no-referrer">
ZeRO 并行算法的主要的是 ZeRO-DP 和 ZeRO-offload。ZeRO-Infinity 是 ZeRO-Offload 的增强版本，ZeRO-R 是一些额外特性。</div><div><img style="width:500" src="https://cdn.sa.net/2024/07/31/Dmi69o5KnRgeZPJ.png" referrerpolicy="no-referrer"></div><div><p dir="auto">从 ZeRO-1 到 ZeRO-3, 进行切分的对象越多，但同时也意味着卡/节点之间的通信量也会增加。</p></div><div><img style="width:500" src="https://cdn.sa.net/2024/07/31/QEBkGc2JHxYhZu6.png" referrerpolicy="no-referrer"></div><div><p dir="auto">先看一下，没有经过 DeepSpeed 优化的 baseline 是怎么算出来的。<br>
<img style="width:500" src="https://cdn.sa.net/2024/07/31/FcCs3AvfrPGRYDE.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto">Adam 比较耗显存，使用 SGD 或者不同的优化器可能有不同的占用。所以上上图里的 K 就是 优化器占用的计算。</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=r9DXamaZECc" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=r9DXamaZECc" target="_blank">03 - ZOMI 酱 - DeepSpeed优化器并行ZeRO1/2/3原理 </a><a href="?query=tag:%E5%A4%A7%E6%A8%A1%E5%9E%8B" class="tag" target="_blank" rel="noopener">#大模型</a> <a href="?query=tag:%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C" class="tag" target="_blank" rel="noopener">#分布式并行</a> <a href="?query=tag:%E8%AE%AD%E7%BB%83" class="tag" target="_blank" rel="noopener">#训练</a> - YouTube</p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> ZeRO-DP (Data Parallel) - 优化器并行三种方法</span></li>
</ul></div><div><img style="width:600" src="https://cdn.sa.net/2024/08/05/HQ7TyW8KkXvgjYz.png" referrerpolicy="no-referrer"></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span>  ZeRO-1 (Optimizer state partitioning)  - 只对 optimizer 状态进行切分，占原始内存的 1/4<br>
<img style="width:500" src="https://cdn.sa.net/2024/08/05/216rxP3MidOcbkj.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/05/q1fhQc7zKvEtGBd.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/05/9Bbk1z6rjEVPWvl.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/05/EGaVtr21AmJdoDB.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/05/JteLE5pB4fYIdAR.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/05/pPVqM6Egay9i4Ns.png" referrerpolicy="no-referrer"></p>
</span></li>
<li data-line="9" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> ZeRO-2 - 对 Optimizer 和 grad 进行拆分，占原始内存的 1/8<br>
<img style="width:500" src="https://cdn.sa.net/2024/08/05/AlUxuWTsR54tzeI.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/05/w2s9BHfxCe3p7Ia.png" referrerpolicy="no-referrer"><br>
这里和 Zero Stage 1 一样<br>
<img style="width:500" src="https://cdn.sa.net/2024/08/05/r5d9P3ZKoTACkge.png" referrerpolicy="no-referrer"></p>
</span></li>
</ul></div><div><img style="width:500" src="https://cdn.sa.net/2024/08/05/TEzeWOJ6MRvUQq3.png" referrerpolicy="no-referrer"></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span> Zero Stage 3 - 对optimizer、grad和模型参数进行切分，内存与数据并行线性关系，通信 DP I.5X<br>
<img style="width:500" src="https://cdn.sa.net/2024/08/05/gZNKWhVwd7osb82.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/05/Qt2vhpdmHISowY4.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/05/GSwqsrxPVYtQLKi.png" referrerpolicy="no-referrer"></span></li>
</ul></div><div><img style="width:500" src="https://cdn.sa.net/2024/08/05/8N45I7JoEhtRzGP.png" referrerpolicy="no-referrer"></div><div><img style="width:500" src="https://cdn.sa.net/2024/08/05/vaZwgXRCP983hmx.png" referrerpolicy="no-referrer"></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> ZeRO vs TP<br>
<img style="width:500" src="https://cdn.sa.net/2024/08/05/X4ZIHahC735ArJV.png" referrerpolicy="no-referrer"> </p>
</span></li>
<li data-line="3" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> ZeRO - Offline </p>
</span></li>
</ul></div><div><img style="width:500" src="https://cdn.sa.net/2024/08/05/pSjL5T9fbuoBVDU.png" referrerpolicy="no-referrer">
<img style="width:500" src="https://cdn.sa.net/2024/08/05/tb6JHL7pM8dxAaP.png" referrerpolicy="no-referrer"></div><div><img style="width:500" src="https://cdn.sa.net/2024/08/05/MCbDcfGqNmQ4xwK.png" referrerpolicy="no-referrer"></div><div><img style="width:500" src="https://cdn.sa.net/2024/08/05/kxi4URhMZvWgXEt.png" referrerpolicy="no-referrer">
```python
from transformers import trainer</div><div><p dir="auto">trainer = Trainer(<br>
model=model,<br>
args=args,<br>
train_dataset=train_dataset,<br>
data_collator=data_collator,<br>
optimizer=optimizer<br>
)</p></div><div><pre><code>
```bash
deepspeed --hostfile=hostfile --master_port 60000 --include="node1:0,1,2,3@node2:0,1,2,3" run.py\
-deepspeed ds_config.json
</code><button class="copy-code-button">复制</button></pre></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1ZZ421T7XJ/?spm_id_from=333.337.search-card.all.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1ZZ421T7XJ/?spm_id_from=333.337.search-card.all.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">【研1.5基本功 （真的很简单）DeepSpeed &amp; Accelerate】学点大模型基建准没错_哔哩哔哩_bilibili</a></p></div><div><p dir="auto">DeepSpeed 是由 Microsoft 开发的一个深度学习优化库，旨在提高大规模模型训练的效率。它提供了多种优化技术，包括混合精度训练、分布式训练、数据并行、模型并行和高效的梯度累积等。</p></div><div><p dir="auto">主要特点：</p></div><div><ol>
<li data-line="0" dir="auto">分布式训练: 支持数据并行、模型并行（包括管道并行和张量并行），使得训练超大规模模型成为可能。</li>
<li data-line="1" dir="auto">优化技术: 提供优化算法，如 ZeRO（Zero Redundancy Optimizer）以减少显存占用、加速训练速度。</li>
<li data-line="2" dir="auto">混合精度训练: 自动支持 FP16 和 BFLOAT16 精度训练，以减少计算开销和内存占用。</li>
<li data-line="3" dir="auto">高效的梯度累积: 提供高效的梯度累积方法来处理超大批量训练。</li>
<li data-line="4" dir="auto">弹性训练: 支持弹性训练，允许动态添加或移除计算资源。</li>
</ol></div><div><p dir="auto">功能非常的强大、丰富，但是配置起来比较复杂，需要一定的深度学习知识。</p></div><div><p dir="auto">Accelerate 是由 Hugging Face 开发的一个库，旨在简化分布式训练的设置。它提供了一种简洁的方式来配置和管理多 GPU 和 TPU 环境，支持数据并行和模型并行。</p></div><div><p dir="auto">主要特点</p></div><div><ol>
<li data-line="0" dir="auto">简化配置: 提供统一的接口来处理多 GPU 和 TPU 环境的配置，简化了分布式训练的复杂性。</li>
<li data-line="1" dir="auto">支持多种后端: 可以与不同的深度学习后端（如 PyTorch 和 TensorFlow）集成。</li>
<li data-line="2" dir="auto">集成 DeepSpeed: 可以与 DeepSpeed 集成，利用 DeepSpeed 的高级功能进行训练。</li>
<li data-line="3" dir="auto">简化分布式训练: 自动处理分布式训练的设置和同步问题，使用户能够专注于模型和数据</li>
</ol></div><div><p dir="auto">功能丰富，而且使用非常简单，但是配置不是非常精细。</p></div><div><p dir="auto"><strong>联系</strong></p></div><div><ol>
<li data-line="0" dir="auto">集成能力: Accelerate 可以与 DeepSpeed 集成，利用 DeepSpeed 的高级功能来优化训练。通过 Accelerate 的 DeepSpeedPlugin，可以在 Accelerate 的框架下使用 DeepSpeed 进行训练。</li>
<li data-line="1" dir="auto">共同目标: 两者都旨在提高大规模模型训练的效率和简化配置，但 DeepSpeed 提供了更多的优化功能，而 Accelerate 注重于简化配置和多后端支持。<br>
<img style="width:500" src="https://cdn.sa.net/2024/08/09/UCstpH1IhrAgjPT.png" referrerpolicy="no-referrer"></li>
</ol></div><div><ul>
<li data-line="0" dir="auto"><code>world</code> 指的是一个节点中的所有GPU </li>
<li data-line="1" dir="auto">一个节点中每个GPU拥有一个<code>gpu_id</code></li>
<li data-line="2" dir="auto">所有节点称为 <code>services</code></li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://dwexzknzsh8.feishu.cn/docx/VkYud3H0zoDTrrxNX5lce0S4nDh" rel="noopener" class="external-link" href="https://dwexzknzsh8.feishu.cn/docx/VkYud3H0zoDTrrxNX5lce0S4nDh" target="_blank">尽量“手撕”代码系列 - 飞书云文档</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Nj421U7jq/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Nj421U7jq/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">【研1基本功MultiGPU】多卡并行训练（以手写数字体识别为例）_哔哩哔哩_bilibili</a><br>
<a data-tooltip-position="top" aria-label="https://dwexzknzsh8.feishu.cn/docx/LvNEdzqbXohyWkxwwvLcrsUCnkf" rel="noopener" class="external-link" href="https://dwexzknzsh8.feishu.cn/docx/LvNEdzqbXohyWkxwwvLcrsUCnkf" target="_blank">MultiGPU - 飞书云文档</a></p></div></div></div><div class="heading-wrapper"><h3 data-heading="Megatron" dir="auto" class="heading" id="Megatron"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Megatron</h3><div class="heading-children"><div><p dir="auto">NVIDIA开发，提高大模型分布式并<br>
行练效率和线性度：</p></div><div><ol>
<li data-line="0" dir="auto">加速训练手段：系合数据并行（Data Parallelism),张量并行(Tensor Parallelism)和流水线并行(Pipeline Parallelism)来复现GPT-3。</li>
<li data-line="1" dir="auto">辅助工具：强大的数据处理&amp;Tokenizer,支持LLM&amp;VLM等基于Transformer结构。</li>
</ol></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/670958880" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/670958880" target="_blank"># 模型并行训练：为什么要用Megatron，DeepSpeed不够用吗？</a><br>
在BLOOM看来，tensor并行、Fused CUDA Kernels 和 DataLoader 是Megatron相对于 DeepSpeed 的三大特点</p></div><div><ul>
<li data-line="0" dir="auto">Tensor并行：一个神经网络层Tensor切成了多个小的Tenso，放在不同的 GPU 中</li>
<li data-line="1" dir="auto">Fused CUDA Kernels：nvidia对cuda运算的优化。本来需要进出显存三次，现在只需要一次</li>
<li data-line="2" dir="auto">DataLoader : 通过提前做tokenize、shuffle做成文件，训练时每个epoch根据索引来获取数据。</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/670958880" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/670958880" target="_blank"># 模型并行训练：为什么要用Megatron，DeepSpeed不够用吗？</a></p></div></div></div><div class="heading-wrapper"><h3 data-heading="ColossalAI" dir="auto" class="heading" id="ColossalAI"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>ColossalAI</h3><div class="heading-children"><div><p dir="auto">Colossal-AI通过多种优化策略提高<br>
训练效率和降低显存需求：<br>
1.加速训练手段：更加丰富张量并行策略(1<br>
D/2D/2.5D/3D-TP);<br>
2.丰富案例：提供20+大模型DEMO和配置文件，融入最新MoE技术和Sora：</p></div></div></div><div class="heading-wrapper"><h3 data-heading="BMTrain" dir="auto" class="heading" id="BMTrain"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>BMTrain</h3><div class="heading-children"><div><p dir="auto">BMTrain 用于训练数百亿规模参数（清华刘清远）<br>
大模型：<br>
1.模型支持：智源研究院Aquila系列的模型分布式并行框架；<br>
2.加速训练手段：支持DeepSpeed中并行策略深度优化；</p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="编译编程" dir="auto" class="heading" id="编译编程"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>编译编程</h2><div class="heading-children"></div></div><div class="heading-wrapper"><h2 data-heading="硬件(HardWare)" dir="auto" class="heading" id="硬件(HardWare)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>硬件(HardWare)</h2><div class="heading-children"><div><p dir="auto">训练是一整个完整的流水，相比推理需要保存额外的优化器和中间状态。所以需要更大的显存。推理卡只要能放下一整个显存可以进行各种优化。总之，推理可以用训练卡，但是训练不能用推理卡。</p></div><div><blockquote dir="auto">
<p><a data-tooltip-position="top" aria-label="https://lambdalabs.com/gpu-benchmarks" rel="noopener" class="external-link" href="https://lambdalabs.com/gpu-benchmarks" target="_blank">GPU Benchmarks for Deep Learning | Lambda</a> 提供关于深度学习 GPU 的 throughput, batch size, throught/watt, batch/watt, throughout/$, batch/$ 和在 SSD, ResNet50等网络运行的速度比较。</p>
</blockquote></div><div><blockquote dir="auto">
<p><a data-tooltip-position="top" aria-label="https://endoflife.date/nvidia-gpu" rel="noopener" class="external-link" href="https://endoflife.date/nvidia-gpu" target="_blank">NVIDIA GPUs | endoflife.date</a> 英伟达显卡历届架构的 Released、Discontinued、Active Support、Security Support</p>
</blockquote></div><div><blockquote dir="auto">
<p><a data-tooltip-position="top" aria-label="https://www.techpowerup.com/gpu-specs/" rel="noopener" class="external-link" href="https://www.techpowerup.com/gpu-specs/" target="_blank">GPU Database | TechPowerUp</a> 所有 GPU 的 Product Name、GPU Chip、Released、Bus、Memory、GPU clock、Memory clock、Shaders / TMUs / ROPs</p>
</blockquote></div><div><blockquote dir="auto">
<p><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=YiX9p8A7LqE&amp;t=158s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=YiX9p8A7LqE&amp;t=158s" target="_blank">AI/ML/DL GPU Buying Guide 2024: Get the Most AI Power for Your Budget - YouTube</a> 说 P40 和 P100 是美国ebay 上最具性价比的选择</p>
</blockquote></div><div dir="ltr" style="overflow-x: auto;"><table>
<thead>
<tr>
<th dir="ltr">显卡</th>
<th dir="ltr">显存</th>
<th dir="ltr">显存带宽（GB/S)</th>
<th dir="ltr">FP16 (TFLOPS)</th>
<th dir="ltr">功耗(W)</th>
<th dir="ltr">NVLink</th>
<th dir="ltr">发布日期</th>
<th dir="ltr">价格</th>
</tr>
</thead>
<tbody>
<tr>
<td dir="ltr">H100 SXM5</td>
<td dir="ltr">80GB HBM3</td>
<td dir="auto">3350</td>
<td dir="auto">989</td>
<td dir="auto">700</td>
<td dir="ltr">有</td>
<td dir="auto">2022-03</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">H100 PCIe</td>
<td dir="ltr">80GB HBM2e</td>
<td dir="auto">2039</td>
<td dir="auto">989</td>
<td dir="auto">350</td>
<td dir="ltr">有</td>
<td dir="auto">2022-03</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">A100</td>
<td dir="ltr">80GB HBM2e</td>
<td dir="auto">2039</td>
<td dir="auto">312</td>
<td dir="auto">400</td>
<td dir="ltr">有</td>
<td dir="auto">2020-05</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">A100</td>
<td dir="ltr">40GB HBM2</td>
<td dir="auto">1555</td>
<td dir="auto">312</td>
<td dir="auto">400</td>
<td dir="ltr">有</td>
<td dir="auto">2020-05</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">A800</td>
<td dir="ltr">80GB HBM2e</td>
<td dir="auto">2039</td>
<td dir="auto">312</td>
<td dir="auto">350</td>
<td dir="ltr">有</td>
<td dir="auto">2022-11</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">RTX 6000 Ada</td>
<td dir="ltr">48GB GDDR6</td>
<td dir="auto">960</td>
<td dir="auto">181.05</td>
<td dir="auto">300</td>
<td dir="ltr">无</td>
<td dir="auto">2022-11</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">L40</td>
<td dir="ltr">48GB GDDR6</td>
<td dir="auto">864</td>
<td dir="auto">181.05</td>
<td dir="auto">300</td>
<td dir="ltr">无</td>
<td dir="auto">2022-09</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">RTX 4090</td>
<td dir="ltr">24GB GDDR6X</td>
<td dir="auto">1008</td>
<td dir="auto">330.8</td>
<td dir="auto">450</td>
<td dir="ltr">无</td>
<td dir="auto">2022-10</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">RTX 4090D</td>
<td dir="ltr">24GB GDDR6X</td>
<td dir="auto">1008</td>
<td dir="auto">295.4</td>
<td dir="auto">425</td>
<td dir="ltr">无</td>
<td dir="auto">2023-12</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">A40</td>
<td dir="ltr">48GB GDDR6 ECC</td>
<td dir="auto">696</td>
<td dir="auto">149.7/299.4*</td>
<td dir="auto">300</td>
<td dir="ltr">有</td>
<td dir="auto">2021-03</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">A4000</td>
<td dir="ltr">16GB GDDR6 ECC</td>
<td dir="auto">448</td>
<td dir="auto">153.4*</td>
<td dir="auto">140</td>
<td dir="ltr">无</td>
<td dir="auto">2021-05</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">A10</td>
<td dir="ltr">24GB GDDR6 ECC</td>
<td dir="auto">600</td>
<td dir="auto">124.8*</td>
<td dir="auto">150</td>
<td dir="ltr">有</td>
<td dir="auto">2021-04</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">L20</td>
<td dir="ltr">48GB GDDR6 ECC</td>
<td dir="auto">864</td>
<td dir="auto">119.5</td>
<td dir="ltr">275W</td>
<td dir="ltr">否</td>
<td dir="auto">2023-11</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">V100 (各版本)</td>
<td dir="ltr">16/32GB HBM2(e) ECC</td>
<td dir="auto">900</td>
<td dir="auto">112</td>
<td dir="auto">250-300</td>
<td dir="ltr">有</td>
<td dir="auto">2017-2019</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">RTX 4060Ti</td>
<td dir="ltr">8/16GB GDDR6</td>
<td dir="auto">288</td>
<td dir="auto">88.2</td>
<td dir="auto">160</td>
<td dir="ltr">无</td>
<td dir="auto">2023-05</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">Tesla T4</td>
<td dir="ltr">16GB GDDR6</td>
<td dir="auto">320</td>
<td dir="auto">65</td>
<td dir="auto">70</td>
<td dir="ltr">否</td>
<td dir="auto">2018-9</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">RTX 8000</td>
<td dir="ltr">48GB GDDR6</td>
<td dir="auto">672</td>
<td dir="auto">29.9</td>
<td dir="auto">295</td>
<td dir="ltr">有</td>
<td dir="auto">2018-08</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">RTX 2080 Ti</td>
<td dir="ltr">11GB GDDR6</td>
<td dir="auto">616</td>
<td dir="auto">26.9</td>
<td dir="auto">250</td>
<td dir="ltr">无</td>
<td dir="auto">2018-09</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">GTX 1080 Ti</td>
<td dir="ltr">11GB GDDR5X</td>
<td dir="auto">484</td>
<td dir="auto">11.3</td>
<td dir="auto">250</td>
<td dir="ltr">无</td>
<td dir="auto">2017-03</td>
<td dir="auto"></td>
</tr>
</tbody>
</table></div><div><img style="width:800" src="https://cdn.sa.net/2024/07/03/v3kg2XsQKoyVwUn.png" referrerpolicy="no-referrer"></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.autodl.com/docs/gpu/" rel="noopener" class="external-link" href="https://www.autodl.com/docs/gpu/" target="_blank">AutoDL帮助文档 - GPU 选型</a><br>
<a data-tooltip-position="top" aria-label="https://www.autodl.com/docs/gpu_perf/" rel="noopener" class="external-link" href="https://www.autodl.com/docs/gpu_perf/" target="_blank">AutoDL帮助文档 - 性能测试</a></p></div><div class="heading-wrapper"><h3 data-heading="大模型硬件需求" dir="auto" class="heading" id="大模型硬件需求"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>大模型硬件需求</h3><div class="heading-children"><div><blockquote dir="auto">
<p>Meat 说 训练 llama3-7B 用了 H100 的 1.6M GPU Hours, 差不多是 H100 连续训练 182 年。</p>
</blockquote></div><div><ul>
<li data-line="0" dir="auto">
<p><a data-tooltip-position="top" aria-label="https://techdiylife.github.io/blog/blog.html?category1=c01&amp;blogid=0058" rel="noopener" class="external-link" href="https://techdiylife.github.io/blog/blog.html?category1=c01&amp;blogid=0058" target="_blank">LLM基础资料整理：推理所需显存与速度</a></p>
</li>
<li data-line="1" dir="auto">
<p><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/695163313" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/695163313" target="_blank"># 如何准确的估计llm推理和微调的内存消耗</a></p>
</li>
<li data-line="2" dir="auto">
<p><a data-tooltip-position="top" aria-label="https://01.me/2023/09/h100-vs-4090/" rel="noopener" class="external-link" href="https://01.me/2023/09/h100-vs-4090/" target="_blank">A100/H100 太贵，何不用 4090？ | Bojie Li</a></p>
</li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 模型推理(Inference)</p>
</span></li>
<li data-line="9" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> 模型训练(Training)</p>
</span></li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="CPU" dir="auto" class="heading" id="CPU"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>CPU</h3><div class="heading-children"></div></div><div class="heading-wrapper"><h3 data-heading="GPU" dir="auto" class="heading" id="GPU"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>GPU</h3><div class="heading-children"><div><blockquote dir="auto">
<p>GPU设计目标是最大化吞吐量 (Throughout)，比单任务执行快慢，更关心并行度 (parallelism)，即同时可以执行多少任务;CPU则更关心延迟 (latency) 和并发 (concurrency)。通常来说 CPU 比 GPU 单条复杂指令延迟快10倍以上, GPU 比 CPU 单位时间内执行指令数量10倍以上. </p>
</blockquote></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/683016265" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/683016265" target="_blank"># GPU基础知识</a></p></div><div><p dir="auto">随着AI大模型的发展，需要专业的高性能计算显卡来支持不断增长的计算规模，主要考虑以下几个方面：</p></div><div><ul>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>需要支持神经网络模型的计算逻辑</strong>
<ol>
<li data-line="1" dir="auto">权重共享</li>
<li data-line="2" dir="auto">激活、全连接等算子的支持</li>
</ol>
</li>
<li data-line="3" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>支持高维张量的存储与计算</strong>
<ol>
<li data-line="4" dir="auto">内存地址随机或自动索引</li>
<li data-line="5" dir="auto">大批量数据的高效加载</li>
</ol>
</li>
<li data-line="6" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>支持常用模型结构</strong>
<ol>
<li data-line="7" dir="auto">conv，transformers</li>
<li data-line="8" dir="auto">支持常见数据结构</li>
</ol>
</li>
<li data-line="9" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>提供不同的bit位数</strong>
<ol>
<li data-line="10" dir="auto">支持低bit量化</li>
<li data-line="11" dir="auto">在动态范围M-bits和指数E-bits</li>
</ol>
</li>
<li data-line="12" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>利用硬件提供稀疏计算</strong>
<ol>
<li data-line="13" dir="auto">硬件上减少 0 值的重复计算</li>
<li data-line="14" dir="auto">减少网络模型对内存的需求，稀疏化网络模型结构</li>
</ol>
</li>
<li data-line="15" dir="auto"><strong>支持高效的分布式计算</strong></li>
<li data-line="16" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>芯片间互连技术，提供 X00GB/s 带宽</strong>
<ol>
<li data-line="17" dir="auto">支持CPU+GPU 双架构，为大规模 AI 和HPC异构平台提供高速带宽</li>
<li data-line="18" dir="auto">专用大模型DSA IP模块，提供低比特快速计算</li>
</ol>
</li>
<li data-line="19" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>专用高速 Transformer 引擎</strong>
<ol>
<li data-line="20" dir="auto">大模型以Transformer为基础结构进行堆叠，高速的Transformer计算</li>
<li data-line="21" dir="auto">更低比特Transformer模块，并支持MoE构建万亿大模型</li>
</ol>
</li>
<li data-line="22" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>降低功耗</strong>
<ol>
<li data-line="23" dir="auto">减少 each MAC功耗</li>
<li data-line="24" dir="auto">避免无效 MACs 计算</li>
<li data-line="25" dir="auto">减少耗能的数据格式搬运 &gt;&gt; 数据重用</li>
</ol>
</li>
</ul></div><div><p dir="auto">GPU vs. CPU</p></div><div><ul>
<li data-line="0" dir="auto">
<p>GPU几乎主要由计算单元ALU组成，仅有少量的控制单元和存储单元。GPU采用了数量众多的计算单元和超长的流水线，但只有非常简单的控制逻辑并省去了Cache。</p>
</li>
<li data-line="1" dir="auto">
<p>CPU不仅被Cache占据了大量空间，而且还有有复杂的控制逻辑和诸多优化电路，相比之下计算能力只是CPU很小的一部。</p>
</li>
<li data-line="5" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> GPU 指标</p>
</span></li>
<li data-line="6" dir="auto">
<p>"<span style="background:#fff88f">FLOPS</span>" 是 "Floating Point Operations Per Second" 的缩写，意为每秒浮点运算次数。它是衡量计算机系统或处理器性能的一种常见指标，特别是在科学计算、工程领域和高性能计算中。例如，一个处理器的性能为 1 TFLOPS，意味着它可以每秒执行 1 万亿次浮点运算。</p>
</li>
<li data-line="7" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><span style="background:#fff88f">Multiply–Accumulate Operations</span>，乘加累积操作。1MACs包含一个乘法操作与一个加法操作（Ax+y）</p>
<ul>
<li data-line="8" dir="auto">优化：去掉没有意义的MACs**</li>
<li data-line="9" dir="auto">增加对稀疏数据的硬件结构 sparse data</li>
<li data-line="10" dir="auto">控制流控制和执行 control flow</li>
<li data-line="11" dir="auto">节省时钟周期 save cycles</li>
<li data-line="12" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>降低每次MACs的计算时间</strong>
<ul>
<li data-line="13" dir="auto">增加时钟频率 clock frequency</li>
<li data-line="14" dir="auto">增加PE单元计算能力，例如提高制程（7纳米-&gt;5纳米）</li>
<li data-line="15" dir="auto">减少指令开销 instruction overhead</li>
</ul>
</li>
<li data-line="16" dir="auto"><strong>增加MACs并行计算能力</strong>: 增加片上PE数量; 支持低bits</li>
<li data-line="17" dir="auto"><strong>增加PE利用率</strong>: 增加片上cache; 增加额外的内存带宽</li>
</ul>
</li>
<li data-line="18" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><span style="background:#fff88f">Accuracy</span>, 计算精度 (FP32/FP16 etc.), 模型结果精度 (ImageNet 78%)</p>
<ul>
<li data-line="19" dir="auto">优化：能够处理各类型的无规则数据 &gt;&gt; 异构平台; 能够应对复杂网络模型结构 &gt;&gt; 计算冗余性</li>
</ul>
</li>
<li data-line="20" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><span style="background:#fff88f">Throughput</span>, 高维张量处理 (high dimension tensor), 实时性能 (30 fps or 20 tokens)</p>
<ul>
<li data-line="21" dir="auto">优化：除了峰值算力，看计算单元的平均利用率 &gt;&gt; 负载均衡; SOTA网络模型的运行时间 &gt;&gt; MLPerf</li>
</ul>
</li>
<li data-line="22" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><span style="background:#fff88f">Latency</span></p>
<ul>
<li data-line="23" dir="auto">生成第一个 Token 的时间（Time To First Token (TTFT)</li>
<li data-line="24" dir="auto">生成每一个输出 Token 的时间（Time Per Output Token (TPOT)</li>
<li data-line="25" dir="auto">Latency = TTFT + TPOT *（要生成的 Token 数 - 1）</li>
<li data-line="26" dir="auto">Latency 可以转换为 Tokens Per Second (TPS)：TPS = (the number of tokens to be generated) / Latency。</li>
<li data-line="27" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>优化：
<ul>
<li data-line="28" dir="auto">通信时延对 MACs 的影响 &gt;&gt; 优化带宽</li>
<li data-line="29" dir="auto">Batch Size 大小与内存大小 &gt;&gt; 多级缓存设计</li>
</ul>
</li>
</ul>
</li>
<li data-line="30" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper">
<p><span class="lc-list-marker">@</span> 矩阵计算</p>
</span><ul>
<li data-line="31" dir="auto">乘加, 重排列降维, 分块-Tiling, GEMM(通用矩阵乘), FMA(融合矩阵乘加)</li>
</ul>
</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1rH4y1c7Zs/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1rH4y1c7Zs/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">AI 工程师都应该知道的GPU工作原理，TensorCore_哔哩哔哩_bilibili</a><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/01/4RlQIWqhFbUtw2u.png" referrerpolicy="no-referrer"></p></div><div><ul>
<li data-line="0" dir="auto">CPU 延迟低单次处理一个任务, 核心数少</li>
<li data-line="1" dir="auto">GPU 延迟高吞吐量高, 拥有贼多核心</li>
<li data-line="2" dir="auto">具体来说, GPU 使用 SIMT 的东西进行多线程执行.</li>
<li data-line="3" dir="auto">SIMT (single instruction multiple threads); 单一指令，多线程执行。比如矩阵乘法里结果里的每个元素可以分配一个线程。32个线程一组，叫做一个Warp。Warp是GPU里调度任务的最小单元。</li>
</ul></div><div><img style="width:800" src="https://cdn.sa.net/2024/08/01/y914QWkxTlP8I57.png" referrerpolicy="no-referrer">
这是A100的内部布局. 里面的每一个小块是一个 SM, A100 一共有 108个SM</div><div><img style="width:500" src="https://cdn.sa.net/2024/08/01/wdPjpcGOzg8ESCu.png" referrerpolicy="no-referrer">
每个SM拥有这些内容. 比较值得说的是 Volta 架构中引入的 TensorCore, 可以在一个时间周期内, 完成一个矩阵乘法/加法(两个FP16相乘, 或者两个 FP32 相加) 
<img style="width:500" src="https://cdn.sa.net/2024/08/01/CfD1aZU6JFor5mK.png" referrerpolicy="no-referrer">
<img style="width:500" src="https://cdn.sa.net/2024/08/01/79EM6aqxTnh3CDF.png" referrerpolicy="no-referrer"></div><div><img style="width:500" src="https://cdn.sa.net/2024/08/01/vJNT3UoBy5xCE7H.png" referrerpolicy="no-referrer">
但是虽然GPU是如此强大,吞吐量是如此之高, 带来的一个问题之一就是存储设备跟不上GPU的consume.
<img style="width:500" src="https://cdn.sa.net/2024/08/01/jMF3tmKnPuSbxoA.png" referrerpolicy="no-referrer">
这个问题可以用调整拆分大矩阵的多少进行调整</div><div><img style="width:500" src="https://cdn.sa.net/2024/08/01/HdSNxQInVwZv6p3.png" referrerpolicy="no-referrer">
<img style="width:500" src="https://cdn.sa.net/2024/08/01/hwE7WmVRdrkyujA.png" referrerpolicy="no-referrer">
片上显存(L1, L2) 和 片下(HBM) 和 接口之间的速度有数量级的差别.</div><div><img style="width:500" src="https://cdn.sa.net/2024/08/01/pksO1WHCIVoSTlX.png" referrerpolicy="no-referrer">
<img style="width:500" src="https://cdn.sa.net/2024/08/01/TCWzUAwecRnfkvb.png" referrerpolicy="no-referrer"></div><div><p dir="auto">这是稀疏矩阵运算的原理</p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper"><span class="lc-list-marker">!</span> NVIDIA 显卡分类</span></li>
<li data-line="1" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span>  GeForce RTX, 游戏卡， 针对图像计算特别优化，硬件上最高支持 FP32 的运算，有光追单元</span></li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span>  Quadro,  图形卡， 针对</span></li>
<li data-line="3" dir="auto" class="lc-list-callout" data-callout="@" style="--lc-callout-color: 0, 184, 212;"><span class="lc-li-wrapper"><span class="lc-list-marker">@</span>  DataCenter, </span></li>
</ul></div><div dir="ltr" style="overflow-x: auto;"><table>
<thead>
<tr>
<th dir="ltr">项目</th>
<th dir="ltr">游戏卡 GeForce</th>
<th dir="ltr">专业卡</th>
<th dir="auto"></th>
</tr>
</thead>
<tbody>
<tr>
<td dir="ltr">驱动</td>
<td dir="ltr">针对游戏娱乐优化</td>
<td dir="ltr">针对专业软件优化。 进过了稳定性测试</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">计算精度</td>
<td dir="ltr">最高FP32</td>
<td dir="ltr">最高FP64</td>
<td dir="ltr">支持双精度对科学计算训练更加优好</td>
</tr>
<tr>
<td dir="ltr">浮点性能</td>
<td dir="ltr">H100 PCIe, FP16, 1,513TFLOPS, 14,592个CUDA(完整)</td>
<td dir="ltr">4090, 82.58TFLOPS, 16384个 CUDA</td>
<td dir="ltr">更强强大的计算性能</td>
</tr>
<tr>
<td dir="ltr">显存带宽</td>
<td dir="ltr">A100, 80G, HBM3(2TB/s)</td>
<td dir="ltr">4090, 24G, GDDR6(1TB/s)</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">NVLink支持</td>
<td dir="auto">❌<br></td>
<td dir="auto">✅<br></td>
<td dir="ltr">NVLink : 600GB/S PCIe x5: 128GB/S</td>
</tr>
<tr>
<td dir="ltr">ECC 支持</td>
<td dir="auto">❌</td>
<td dir="auto">✅</td>
<td dir="ltr">确保计算的结果的可靠</td>
</tr>
<tr>
<td dir="ltr">接口支持</td>
<td dir="ltr"><code>PCI-E</code></td>
<td dir="ltr"><code>PCI-E</code>, <code>SXM</code></td>
<td dir="ltr">方便集群设计走线和散热</td>
</tr>
<tr>
<td dir="ltr">虚拟化</td>
<td dir="auto"></td>
<td dir="ltr">支持硬件直通</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="ltr">高级特性</td>
<td dir="ltr">光追DLSS</td>
<td dir="ltr">稀疏矩阵MIGFP64</td>
<td dir="auto"></td>
</tr>
<tr>
<td dir="auto"></td>
<td dir="auto"></td>
<td dir="auto"></td>
<td dir="auto"></td>
</tr>
</tbody>
</table></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/705372067" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/705372067" target="_blank"># A100 和 4090 傻傻分不清？看了就明白训练卡和推理卡的区别</a></p></div><div><ol>
<li data-line="0" dir="auto"><strong>硬件指标对比</strong>：文中详细列出了H100、A100和4090的Tensor FP16/FP32算力、内存容量、内存带宽、通信带宽和时延等关键性能指标，并提到了售价，强调了H100和A100在内存和通信方面的显著优势。</li>
<li data-line="1" dir="auto"><strong>训练与推理的区别</strong>：解释了训练过程中需要存储模型参数、梯度等，对内存要求更高，而推理过程则不需要存储大量的中间状态，因此对内存的需求相对较小。</li>
<li data-line="2" dir="auto"><strong>大模型训练卡的特点</strong>：强调了训练卡需要高浮点运算能力、大显存和高带宽，以支持复杂的模型训练，且价格昂贵。</li>
<li data-line="3" dir="auto"><strong>推理卡的特点</strong>：指出推理卡优化了计算单元，能够进行低精度运算，具有成本效益，并行处理能力强，适合生产环境的快速响应和高吞吐量应用。</li>
<li data-line="4" dir="auto"><strong>应用案例</strong>：举例说明了使用H100或4090进行70B模型推理的硬件需求，得出结论至少需要3张H100或8张4090才能满足需求</li>
<li data-line="5" dir="auto"><strong>总结与建议</strong>：归纳了训练卡与推理卡的适用场景，建议用户根据自身需求和经济条件选择合适的显卡类型。</li>
</ol></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/615946801/answer/3205148871" rel="noopener" class="external-link" href="https://www.zhihu.com/question/615946801/answer/3205148871" target="_blank"># 为什么4090速度比A100快很多呢？</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Mz421B7oi/?spm_id_from=333.1365.list.card_archive.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Mz421B7oi/?spm_id_from=333.1365.list.card_archive.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">NVIDIA数据中心和消费级GPU10点区别_哔哩哔哩_bilibili</a></p></div><div class="admonition-parent admonition-info-parent"><div class="callout admonition admonition-info admonition-plugin " style="--callout-color: 0, 184, 212;" data-callout="info" data-callout-fold="" data-callout-metadata=""><div class="callout-title admonition-title "><div class="callout-icon admonition-title-icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="info-circle" class="svg-inline--fa fa-info-circle fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"></path></svg></div><div class="callout-title-inner admonition-title-content">Info</div></div><div class="callout-content admonition-content"><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1CesJeUEox/?spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1CesJeUEox/?spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">黑神话悟空火了为何没人用H100打游戏？盘点H100和4090的十点区别_哔哩哔哩_bilibili</a></p>
<p dir="auto">价格差贵？没有视频输出？没有游戏驱动？对，这些是原因，但不是最本质的原因。</p>
<p dir="auto">产品定位 2C vs. 2C<br>
价格 1.2W-1.5W vs. 22W~<br>
形态：公版涡轮/风扇卡 vs. PCIE/SXM<br>
游戏特性: DLSS3 vs. 没有游戏相关特性<br>
GPU 和 GPGPU: 全功能GPU vs. 计算特化 GPU<br>
单卡算力: 330TFLOPS(FP16稀疏) vs. 1979TFLOPS(SXM)<br>
芯片结构: Ada Lovelace(16384CUDA, 128SM) vs. Hopper(16896CUDA, 132SM, 少了RT Core多了FP64处理单元)<br>
多卡互联：无 GPU Direct vs. 支持 GPU Direct 和 NVLink、RDMA、GDS<br>
仅售后严格程度：4090D，少了一些 CUDA,Tensor,RT Cores. 频率稍微低一点但是游戏性能差别 2%. H800 FP64 直接阉割没了，然后 NVLINk 砍一半<br>
销售模型: 服务器大厂严格禁止销售消费卡 vs. GPU代理商e.g.浪潮，H3C...</p></div></div></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Zu411g7Yx/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Zu411g7Yx/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">独家总结：NVLink常见问题十问十答_哔哩哔哩_bilibili</a><br>
<a data-tooltip-position="top" aria-label="https://www.nvidia.cn/design-visualization/nvlink-bridges/" rel="noopener" class="external-link" href="https://www.nvidia.cn/design-visualization/nvlink-bridges/" target="_blank">NVLink 高速 GPU 互连 | NVIDIA Quadro</a><br>
<a data-tooltip-position="top" aria-label="https://www.nvidia.com/zh-tw/data-center/nvlink/" rel="noopener" class="external-link" href="https://www.nvidia.com/zh-tw/data-center/nvlink/" target="_blank">NVLink 和 NVSwitch：最快速的高效能運算資料中心平台 | NVIDIA</a></p></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> <strong>NVLink是什么？</strong>  
</span><ul>
<li data-line="1" dir="auto">NVLink是加速系统(e.g.GPU服务器) 中GPU和CPU处理器的<strong>高速高宽带互连技术</strong>，目的是推动数据和计算加速得出可执行结果，提高集群的协同工作的效率。</li>
<li data-line="2" dir="auto">英伟达官方比喻：GPU和CPU处理器是"公路”沿途的资源，而快速互连通道是通往它们的“匝道”，NVLk是加速计算互连通消的黄金标准</li>
</ul>
</li>
<li data-line="3" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> <strong>为什么会出现NVLink?</strong>
</span><ul>
<li data-line="4" dir="auto">NVLink 最早 2016 年用在 IBM Power9 系统上。发明动机是在 集群中原有的 PCIE 接口的<strong>带宽</strong>和<strong>延迟</strong>，已经不能满足蓬勃发展的科学计算，深度学习和大数据场景。</li>
<li data-line="5" dir="auto">因此，NVIDIA开发了NVLink，以提供更高的带宽和更低的延迟，增强GPU之间的数据传输能力。</li>
</ul>
</li>
<li data-line="6" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> <strong>NVLink和之前的多卡交火是什么关系？</strong>
</span><ul>
<li data-line="7" dir="auto">AMD 的 CrossFire 和 NVIDIA SLI 的多卡交火，多卡间的通信还是依赖 PCIE 和 桥接器。两者虽然都在解决多GPU系统的互联问题，但NVLink在带宽、延迟和应用场景方面有显著的优势。</li>
</ul>
</li>
<li data-line="8" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> <strong>为什么现在多用在GPU 间互联？</strong>
</span><ul>
<li data-line="9" dir="auto">事实上 NVLink 只是一种传输协议。理论上也可以用在 CPU 间的互联，但这需要 CPU 和 主板厂商对英伟达生态的跟进。</li>
</ul>
</li>
<li data-line="10" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> <strong>对比PCIE快多少？</strong></span></li>
</ul></div><div dir="ltr" style="overflow-x: auto;"><table>
<thead>
<tr>
<th dir="ltr"><strong>参数</strong></th>
<th dir="ltr"><strong>PCIe 3.0</strong></th>
<th dir="ltr"><strong>PCIe 4.0</strong></th>
<th dir="ltr"><strong>PCIe 5.0</strong></th>
<th dir="ltr"><strong>NVLink 1.0</strong></th>
<th dir="ltr"><strong>NVLink 2.0</strong></th>
<th dir="ltr"><strong>NVLink 3.0</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td dir="ltr"><strong>带宽</strong>(双向)</td>
<td dir="ltr">16 GB/s (x16)</td>
<td dir="ltr">32 GB/s (x16)</td>
<td dir="ltr">64 GB/s (x16)</td>
<td dir="ltr">20 GB/s (每链路)</td>
<td dir="ltr">25 GB/s (每链路)</td>
<td dir="ltr">50 GB/s (每链路)</td>
</tr>
<tr>
<td dir="ltr"><strong>延迟</strong></td>
<td dir="ltr">150-500 ns</td>
<td dir="auto"></td>
<td dir="auto"></td>
<td dir="ltr">80-100 ns</td>
<td dir="auto"></td>
<td dir="ltr">7-10 ns低</td>
</tr>
<tr>
<td dir="ltr"><strong>拓扑结构</strong></td>
<td dir="ltr">树形或星形</td>
<td dir="ltr">树形或星形</td>
<td dir="ltr">树形或星形</td>
<td dir="ltr">全互联、环形等</td>
<td dir="ltr">全互联、环形等</td>
<td dir="ltr">全互联、环形等</td>
</tr>
<tr>
<td dir="ltr"><strong>应用场景</strong></td>
<td dir="ltr">广泛，连接各种设备</td>
<td dir="ltr">广泛，连接各种设备</td>
<td dir="ltr">广泛，连接各种设备</td>
<td dir="ltr">高性能计算，多GPU互联</td>
<td dir="ltr">高性能计算，多GPU互联</td>
<td dir="ltr">高性能计算，多GPU互联</td>
</tr>
</tbody>
</table></div><div><ul>
<li data-line="0" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> <strong>NVLink版本演进？</strong>
</span><ol>
<li data-line="1" dir="auto">
<img style="width:500" src="https://cdn.sa.net/2024/06/23/8tcNaH4JmKy6kAu.png" referrerpolicy="no-referrer">
</li>
</ol>
</li>
<li data-line="2" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> <strong>NVLink典型拓扑？</strong>
</span><ul>
<li data-line="3" dir="auto">NVlink的拓扑相对灵活，可以支持2GP、4GPU、8GPU、16GPU以上的互联，以2卡和8卡最为常见，</li>
</ul>
</li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> <strong>NVLink桥接器形态？</strong>
</span><ul>
<li data-line="5" dir="auto">用于PCIE接口的GPU卡两两互联，打通NVLink链路，提高卡卡之间的互联带宽，通常为Tesla和Quadro系列的卡<img style="width:500" src="https://www.pugetsystems.com/pic_disp.php?id=60051" referrerpolicy="no-referrer"></li>
</ul>
</li>
<li data-line="6" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> <strong>NVLink sw  芯片和设备？</strong>
</span><ul>
<li data-line="7" dir="auto">NVLink sw芯片是集成在HGX模组之上，是芯片的形态，一般是几个芯片共同工作实现8张GPU卡全互联。</li>
<li data-line="8" dir="auto">以DGXH100为例</li>
<li data-line="9" dir="auto">NV-Switchi设备用于多个设备的互联，也是基于NV Switchi芯片，只是形态上是一台设备。</li>
</ul>
</li>
<li data-line="10" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> <strong>NVLink和SXM区别？</strong>
</span><ul>
<li data-line="11" dir="auto">NVlink是链路技术是协议名称，SXM是接口名称，我们所说的NVlink整机通常指的是SXM接口的GPU服务器。</li>
<li data-line="12" dir="auto">PCIE GPU机型是可以通过NVLink桥机器实现两卡的互联，客户采购NVLink机型时往往需要多问一句是全互联还是桥机器互联。</li>
</ul>
</li>
<li data-line="13" dir="auto" class="lc-list-callout" data-callout="?" style="--lc-callout-color: 255, 145, 0;"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="lc-li-wrapper"><span class="lc-list-marker">?</span> <strong>可以替代或独立于PCIE吗？</strong>
</span><ul>
<li data-line="14" dir="auto">理论上可以，还有很多生态依赖 PCI-E</li>
</ul>
</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1oP411874R/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1oP411874R/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">知识科普：英伟达NVLink和PCIE版本的GPU服务器区别有哪些_哔哩哔哩_bilibili</a><br>
<img style="width:700" src="https://cdn.sa.net/2024/07/11/JAYzCsRoVKyEvS2.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1hf421D7vG/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1hf421D7vG/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">分析NVIDIA GPU从P100到V100到A100到H100再到B200的参数变化_哔哩哔哩_bilibili</a></p></div><div><ul>
<li data-line="0" dir="auto">
<img style="width:500" src="https://cdn.sa.net/2024/06/22/bv9zJZT5EyN7FfK.png" referrerpolicy="no-referrer">
</li>
<li data-line="1" dir="auto">H100 在最实用的稠密 FP16 算力上比 A100 快 3 倍</li>
<li data-line="2" dir="auto">这个美国对中国的算力禁售是在 2022.10 开始的. 所以可能国内 H100 的数量要远远少于 A100。并且 H100 的大规模铺货大约在 23 Q4</li>
<li data-line="3" dir="auto">H100NVL 支持多卡逻辑上变成一块</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Xr4y1v72H/?spm_id_from=333.788&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Xr4y1v72H/?spm_id_from=333.788&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">学习分享：浅谈GPU虚拟化不同的技术特点_哔哩哔哩_bilibili</a></p></div><div><ul>
<li data-line="0" dir="auto">GPU虚拟化 最终想做的是实现GPU 资源的池化，让每个虚拟机都能像桌面一样利用 GPU。<img style="width:500" src="https://cdn.sa.net/2024/06/23/GKfAQchBbI7Egul.png" referrerpolicy="no-referrer"></li>
<li data-line="1" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>直通 (Passthrough) ： 
<ul>
<li data-line="2" dir="auto"><strong>特点</strong>：每个虚拟机独占一个物理GPU。</li>
<li data-line="3" dir="auto"><strong>优点</strong>：性能接近原生，适用于需要高性能的应用。</li>
<li data-line="4" dir="auto"><strong>缺点</strong>：无法实现GPU资源的共享，一个GPU只能分配给一个虚拟机。</li>
</ul>
</li>
<li data-line="5" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>GPU 切分 (vGPU)
<ul>
<li data-line="6" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>特点</strong>：将一个物理GPU切分为多个虚拟GPU，每个虚拟GPU分配给不同的虚拟机。
<ul>
<li data-line="7" dir="auto"><strong>优点</strong>：提高GPU利用率，多个虚拟机可以共享同一个物理GPU。</li>
<li data-line="8" dir="auto"><strong>缺点</strong>：性能较直通稍差，配置复杂。</li>
</ul>
</li>
</ul>
</li>
<li data-line="9" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>API 拦截 (API Remoting)
<ul>
<li data-line="10" dir="auto"><strong>特点</strong>：虚拟机中的应用程序调用GPU API，这些API被拦截并转发到宿主机的物理GPU执行。</li>
<li data-line="11" dir="auto"><strong>优点</strong>：不需要修改硬件，灵活性高。</li>
<li data-line="12" dir="auto"><strong>缺点</strong>：性能损耗较大，适用于对性能要求不高的应用</li>
</ul>
</li>
<li data-line="13" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>共享库 (Library-based)
<ul>
<li data-line="14" dir="auto"><strong>特点</strong>：虚拟机共享宿主机的GPU库和驱动程序。</li>
<li data-line="15" dir="auto"><strong>优点</strong>：实现简单，适用于轻量级应用。</li>
<li data-line="16" dir="auto"><strong>缺点</strong>：性能不如直通和vGPU，适用场景有限</li>
</ul>
</li>
<li data-line="17" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>硬件支持的虚拟化 (SR-IOV)
<ul>
<li data-line="18" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>特点</strong>：利用硬件支持的SR-IOV技术将一个物理GPU虚拟化为多个虚拟函数(VF)，每个虚拟函数分配给不同的虚拟机。
<ul>
<li data-line="19" dir="auto"><strong>优点</strong>：性能较高，支持资源隔离和QoS。</li>
<li data-line="20" dir="auto"><strong>缺点</strong>：需要硬件支持，配置复杂。</li>
</ul>
</li>
</ul>
</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1HLpSe4EGp/?spm_id_from=333.1007.tianma.1-1-1.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1HLpSe4EGp/?spm_id_from=333.1007.tianma.1-1-1.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">最新整理：聊聊英伟达Nvlink技术相关的各类知识点（24年8月）_哔哩哔哩_bilibili</a><br>
<img style="width:700" src="https://cdn.sa.net/2024/08/20/zTPLrDlVdC1mqW8.png" referrerpolicy="no-referrer"><br>
<img style="width:700" src="https://cdn.sa.net/2024/08/20/NouFM94WsiXnAde.png" referrerpolicy="no-referrer"><br>
<img style="width:700" src="https://cdn.sa.net/2024/08/20/Bt3Ye1lHa9i4fIJ.png" referrerpolicy="no-referrer"><br>
<img style="width:700" src="https://cdn.sa.net/2024/08/20/srhBNeQgctkWOV2.png" referrerpolicy="no-referrer"></p></div></div></div><div class="heading-wrapper"><h3 data-heading="NPU" dir="auto" class="heading" id="NPU"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>NPU</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1jx421C7mG/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1jx421C7mG/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">地表最强推理芯片Groq原理剖析&amp;洞察 </a><a href="?query=tag:%E5%A4%A7%E6%A8%A1%E5%9E%8B" class="tag" target="_blank" rel="noopener">#大模型</a> <a href="?query=tag:Groq" class="tag" target="_blank" rel="noopener">#Groq</a> <a href="?query=tag:%E6%8E%A8%E7%90%86" class="tag" target="_blank" rel="noopener">#推理</a>_哔哩哔哩_bilibili<br>
<a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=V3pv0le0cy8&amp;t=175s" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=V3pv0le0cy8&amp;t=175s" target="_blank">【回答】Etched Sohu和Groq LPU的区别 | Transformer ASIC | TSP LPU | 主打推理场景 - YouTube</a><br>
<a data-tooltip-position="top" aria-label="https://ieeexplore.ieee.org/document/9138986" rel="noopener" class="external-link" href="https://ieeexplore.ieee.org/document/9138986" target="_blank"># Think Fast: A Tensor Streaming Processor (TSP) for Accelerating Deep Learning Workloads</a></p></div><div><ul>
<li data-line="0" dir="auto">Groq 发明了 Tensor Streaming Processor </li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="国产化" dir="auto" class="heading" id="国产化"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>国产化</h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Ew4m1i7nC/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Ew4m1i7nC/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">YEF2024，清华大学陈文光：国产智能算力训练大模型的经验与教训_哔哩哔哩_bilibili</a></p></div><div><ul>
<li data-line="0" dir="auto">
<p>清华大学陈文光教授以《国产智能算力训练大模型的经验与教训》为题的报告中表示，过去四五年中，自身所在团队已经分别在新神威计算机和鹏城云脑2两个国产平台上训练了大模型，并总结了经验。</p>
</li>
<li data-line="1" dir="auto">
<p>训练了174T参数 (稀疏)的MoE 模型； 和百川合作训练的 33B 的 128K 长窗口模型</p>
</li>
<li data-line="2" dir="auto">
<p>新神威计算机 片上异构众核 </p>
</li>
<li data-line="3" dir="auto">
<p>鹏城云脑2 是一个由 4096个昇腾910(NPU) + 2048个鲲鹏 3920(CPU) 组成的集群</p>
</li>
<li data-line="4" dir="auto" class="lc-list-callout" data-callout="!" style="--lc-callout-color: 255, 23, 68;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">!</span> Huawei Ascend 910A vs. NVDIA A100 80G, 在实际训练中的经验教训</p>
</span></li>
<li data-line="5" dir="auto">
<img style="width:500" src="https://cdn.sa.net/2024/06/22/KapzdQc9xb1XHIe.png" referrerpolicy="no-referrer"> 
</li>
<li data-line="6" dir="auto">
<p>910A 的 FP16 精度比 A100 的 BF16 小很多，对训练造成了很大的影响；单卡的显存和带宽要小很多；卡间的带宽两者几乎相同；HFU 是集群的利用效率 910A 低一点。</p>
</li>
<li data-line="7" dir="auto">
<p>多产平台生态明显没有 CUDA 丰富，需要考虑Pytorch 和 CUDA 之下的 AI 系统。比如算子实现，并行策略选取，容错。</p>
</li>
<li data-line="8" dir="auto">
<p>陈文光说语言模型训练所需要的算子比 CV 少，他们团队开发了 swTensor 算子库针对新神威进行优化；华为系统没有开放底层，依赖华为团队开发的的算子。</p>
</li>
<li data-line="9" dir="auto">
<p>千卡的并行策略，现在有一些通用的并行策略比如数据并行，张量并行，流水并行。或者针对特定模型，e.g.MoE的专家并行。但是整个业界没有自动化的对显存，网络拓扑，可扩展性的并行策略。陈文光团队针对 在 3456 的 910A 上训练任务，提出了自适应重计算算法，让模型训练效率优化 10%。从 23% 的 HFU 提升到了 34%。</p>
</li>
<li data-line="10" dir="auto">
<p>新神威 10万个节点平均无故障率大概是几个小时的量级。云脑 II 4000 个节点，大概在 100 小时这个量级。使用的容错机制是 checkpoint(短期)和 借鉴 spark 的容错机制，实现在数据并行节点间的自动容错(长期)</p>
</li>
<li data-line="11" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">%</span> 910A 鹏城云脑-华为生态 - 经验与教训</p>
</span></li>
<li data-line="12" dir="auto">
<img style="width:500" src="https://cdn.sa.net/2024/06/22/TDoFjYMxHJt6QXw.png" referrerpolicy="no-referrer">
</li>
<li data-line="13" dir="auto">
<p>经验与教训: 经过优化后升腾910A在3000卡下的单卡训练性能达到0.5- 0.6A100</p>
</li>
<li data-line="14" dir="auto">
<p>液冷系统，稳定性超过nVidial风冷系统</p>
</li>
<li data-line="15" dir="auto">
<p>软件生态问题，在大模型训练方面没有那么严重，可以花个月移植优化，3-5个月训练</p>
</li>
<li data-line="16" dir="auto">
<p>BF16精度非常重要，在干亿模型和长上下文窗口时，</p>
</li>
<li data-line="17" dir="auto">
<p>FP16精度不足(910B已经修正)</p>
</li>
<li data-line="18" dir="auto" class="lc-list-callout" data-callout="%" style="--lc-callout-color: 158, 158, 158;"><span class="lc-li-wrapper">
<p><span class="lc-list-marker">%</span> 新神威超算 - 经验与教训</p>
</span></li>
<li data-line="19" dir="auto">
<p>超算可以用来训川练超大模型，Bagualu系统2021年训川练了174T参数的MOE大模型，参数量仍然保持世界纪录</p>
</li>
<li data-line="20" dir="auto">
<p>系统的半精度算力较低，仅为双精度算力的4倍，如能做到16倍，就是一台非常有竞争力的超级计算机</p>
</li>
<li data-line="21" dir="auto">
<p>顶层网络裁剪1：8，使得最大模型下的训练性能较差</p>
</li>
<li data-line="23" dir="auto">
<p>总结： 发挥我国在超算领域的技术优势，实现超智融合，是提升超算投资效率、解决我国智算瓶颈的一条重要路线</p>
</li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1or421p7MT/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1or421p7MT/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">国产GPU：国产主流AI芯片产品参数速查工具3.3（有下架风险）_哔哩哔哩_bilibili</a><br>
<a data-tooltip-position="top" aria-label="https://docs.google.com/spreadsheets/d/1izZJvG1Snzs0qSBJ2WXz4AM-OpXcM-jM/edit?usp=drive_web&amp;ouid=105587029428541518102&amp;rtpof=true" rel="noopener" class="external-link" href="https://docs.google.com/spreadsheets/d/1izZJvG1Snzs0qSBJ2WXz4AM-OpXcM-jM/edit?usp=drive_web&amp;ouid=105587029428541518102&amp;rtpof=true" target="_blank">003独家整理：国产主流AI芯片产品参数速查工具3.3</a><br>
<img style="width:800" src="https://cdn.sa.net/2024/06/25/6KTtl1SRUPIOwne.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1yb421J7Jk/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1yb421J7Jk/?spm_id_from=333.999.0.0&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">再不了解昇腾 AI服务器就要被公关掉了，随时删库跑路！ </a><a href="?query=tag:%E5%A4%A7%E6%A8%A1%E5%9E%8B" class="tag" target="_blank" rel="noopener">#大模型</a> <a href="?query=tag:%E6%98%87%E8%85%BE" class="tag" target="_blank" rel="noopener">#昇腾</a> <a href="?query=tag:AI%E8%8A%AF%E7%89%87" class="tag" target="_blank" rel="noopener">#AI芯片</a>_哔哩哔哩_bilibili<br>
<img style="width:500" src="https://cdn.sa.net/2024/06/23/wmW6FykIZlEaUNt.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/06/23/HxC8Pvr3Fm4ULka.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1Ls421M7t1/?spm_id_from=333.788&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1Ls421M7t1/?spm_id_from=333.788&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">你居然？敢说昇腾310/910 SOC处理器架构！#昇腾 </a><a href="?query=tag:AI%E8%8A%AF%E7%89%87" class="tag" target="_blank" rel="noopener">#AI芯片</a>_哔哩哔哩_bilibili</p></div><div><ul>
<li data-line="0" dir="auto">昇腾是一个SoC架构，基于 DaVinci AI 架构，特点是高算力和高效<br>
<img style="width:500" src="https://cdn.sa.net/2024/06/23/tAjkB5xm7QFuPMS.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/06/23/M6DVbBQriSa79XT.png" referrerpolicy="no-referrer"></li>
</ul></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://chenzomi12.github.io/02Hardware06Domestic/AscendArch.html" rel="noopener" class="external-link" href="https://chenzomi12.github.io/02Hardware06Domestic/AscendArch.html" target="_blank">昇腾 AI 处理器 — AI System</a><br>
<img style="width:500" src="https://cdn.sa.net/2024/06/23/9vpoJ2LFGlgyPWr.png" referrerpolicy="no-referrer"></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1fNYDe7EBv/?spm_id_from=333.1365.list.card_archive.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1fNYDe7EBv/?spm_id_from=333.1365.list.card_archive.click&amp;vd_source=427a8f6991c46f06262700ed0e9203dc" target="_blank">独家整理：一起了解10家主流国产AI芯片的产品参数情况_哔哩哔哩_bilibili</a><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/14/WBYb6zcZ2Ff4LeT.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/14/cqvTn4zVKYf5rQb.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/14/Bfk4867Znb5dDKG.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/14/l5pMCbsHgDAazrZ.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/14/Oszet58HmZX6MFl.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/14/JlMYbqS12vQ64Xi.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/14/EKA7qD2FSXP3aO6.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/14/sxu9YZtrUJbwfEk.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/14/nvyqfbRVIcaxNep.png" referrerpolicy="no-referrer"><br>
<img style="width:500" src="https://cdn.sa.net/2024/08/14/U6WB7f1FMy3tXJP.png" referrerpolicy="no-referrer"></p></div><div><hr></div><div><p dir="auto">服务型制造研究院(杭州) 有限公司</p></div><div><ul>
<li data-line="0" dir="auto">～ 工信部直属国家级新型研发机构(国企)， 主要任务把 AI 赋能传统制造业，实现新质生产力。</li>
<li data-line="1" dir="auto">我们现在是给企业做客， 每年可触达 2000 + 企业。对接过。全国各地（调训）</li>
<li data-line="2" dir="auto">做一个平台 实现新 AI 产品在企业端的落地。对接需求。</li>
<li data-line="3" dir="auto">我们是国企 + 工信部直属。我们的职责之一就是促进企业需求，加快资源配置效率。我们和国家级超算有合作。</li>
<li data-line="4" dir="auto">现在想做一个人工智能服务平台，会有一些2B </li>
<li data-line="5" dir="auto">企业决策环节</li>
</ul></div><div class="mod-footer"></div></div></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#6_AI 系统"><div class="tree-item-contents heading-link" heading-name="6_AI 系统"><span class="tree-item-title">6_AI 系统</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#介绍(Introduction)"><div class="tree-item-contents heading-link" heading-name="介绍(Introduction)"><span class="tree-item-title">介绍(Introduction)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#分布式训练推理"><div class="tree-item-contents heading-link" heading-name="分布式训练推理"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">分布式训练推理</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item mod-collapsible" data-depth="3"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#DeepSpeed"><div class="tree-item-contents heading-link" heading-name="DeepSpeed"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">DeepSpeed</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="4"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#我在分布式系统中老是听到_All-Reduce_和_All-Gather._它们是什么意思?"><div class="tree-item-contents heading-link" heading-name="我在分布式系统中老是听到 All-Reduce 和 All-Gather. 它们是什么意思?"><span class="tree-item-title">我在分布式系统中老是听到 All-Reduce 和 All-Gather. 它们是什么意思?</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#All-Reduce"><div class="tree-item-contents heading-link" heading-name="All-Reduce"><span class="tree-item-title">All-Reduce</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#All-Gather"><div class="tree-item-contents heading-link" heading-name="All-Gather"><span class="tree-item-title">All-Gather</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#区别"><div class="tree-item-contents heading-link" heading-name="区别"><span class="tree-item-title">区别</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#Megatron"><div class="tree-item-contents heading-link" heading-name="Megatron"><span class="tree-item-title">Megatron</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#ColossalAI"><div class="tree-item-contents heading-link" heading-name="ColossalAI"><span class="tree-item-title">ColossalAI</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#BMTrain"><div class="tree-item-contents heading-link" heading-name="BMTrain"><span class="tree-item-title">BMTrain</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#编译编程"><div class="tree-item-contents heading-link" heading-name="编译编程"><span class="tree-item-title">编译编程</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#硬件(HardWare)"><div class="tree-item-contents heading-link" heading-name="硬件(HardWare)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">硬件(HardWare)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#大模型硬件需求"><div class="tree-item-contents heading-link" heading-name="大模型硬件需求"><span class="tree-item-title">大模型硬件需求</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#CPU"><div class="tree-item-contents heading-link" heading-name="CPU"><span class="tree-item-title">CPU</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#GPU"><div class="tree-item-contents heading-link" heading-name="GPU"><span class="tree-item-title">GPU</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#NPU"><div class="tree-item-contents heading-link" heading-name="NPU"><span class="tree-item-title">NPU</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="💾-科技工程/6_ai-系统.html#国产化"><div class="tree-item-contents heading-link" heading-name="国产化"><span class="tree-item-title">国产化</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>